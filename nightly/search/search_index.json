{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is Harvest?","text":"<p>Harvest is the open-metrics endpoint for ONTAP, StorageGRID, E-Series and Cisco Nexus Switches</p> <p>NetApp Harvest brings observability to ONTAP, StorageGRID, E-Series and Cisco Nexus Switches by collecting performance, capacity and hardware metrics, transforming them, and routing them to your choice of a time-series database.</p> <p>The included Grafana dashboards deliver the datacenter insights you need, while new metrics can be collected with a few edits of the included template files.</p> <p>Harvest is open-source, released under an Apache2 license, and offers great flexibility in how you collect, augment, and export your datacenter metrics. </p> <p>Out-of-the-box Harvest provides a set of pollers, collectors, templates, exporters, an optional auto-discover daemon, and a set of ONTAP, StorageGRID, E-Series and Cisco Nexus Switch dashboards for Prometheus and Grafana. Harvest collects the metrics and makes them available to a separately installed instance of Prometheus/InfluxDB and Grafana.</p> <ul> <li> Concepts</li> <li> Quickstart Guide</li> </ul> <p>If you'd like to familiarize yourself with Harvest's core concepts, we recommend reading concepts.</p> <p>If you feel comfortable with the concepts, we recommend our quickstart guide, which takes you through a practical example.</p> <p>Note</p> <p>Hop onto our Discord  or GitHub discussions and say hi. \ud83d\udc4b\ud83c\udffd</p>"},{"location":"MigratePrometheusDocker/","title":"Migrate Prometheus Docker Volume","text":"<p>If you want to keep your historical Prometheus data, and you generated your <code>harvest-compose.yml</code> file via <code>bin/harvest generate</code> before Harvest <code>22.11</code>, please follow the steps below to migrate your historical Prometheus data.</p> <p>This is not required if you generated your <code>harvest-compose.yml</code> file via <code>bin/harvest generate</code> at Harvest release <code>22.11</code> or after.</p> <p>Outline of steps: 1. Stop Prometheus container so data acquiesces 2. Find historical Prometheus volume and create new Prometheus data volume 3. Create a new Prometheus volume that Harvest 22.11 and after will use 4. Copy the historical Prometheus data from the old volume to the new one 5. Optionally remove the historical Prometheus volume</p>"},{"location":"MigratePrometheusDocker/#stop-prometheus-container","title":"Stop Prometheus container","text":"<p>It's safe to run the <code>stop</code> and <code>rm</code> commands below regardless if Prometheus is running or not since removing the container does not touch the historical data stored in the volume.</p> <p>Stop all containers named Prometheus and remove them.</p> <pre><code>docker stop (docker ps -fname=prometheus -q) &amp;&amp; docker rm (docker ps -a -fname=prometheus -q)\n</code></pre> <p>Docker may complain if the container is not running, like so. You can ignore this.</p> Ignorable output when container is not running (click me) <pre><code>\"docker stop\" requires at least 1 argument.\nSee 'docker stop --help'.\n\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\n\nStop one or more running containers\n</code></pre>"},{"location":"MigratePrometheusDocker/#find-the-name-of-the-prometheus-volume-that-has-the-historical-data","title":"Find the name of the Prometheus volume that has the historical data","text":"<pre><code>docker volume ls -f name=prometheus -q\n</code></pre> <p>Output should look like this: <pre><code>harvest-22080-1_linux_amd64_prometheus_data  # historical Prometheus data here\nharvest_prometheus_data                      # it is fine if this line is missing\n</code></pre></p> <p>We want to copy the historical data from <code>harvest-22080-1_linux_amd64_prometheus_data</code> to <code>harvest_prometheus_data</code></p> <p>If <code>harvest_prometheus_data</code> already exists, you need to decide if you want to move that volume's data to a different volume or remove it. If you want to remove the volume, run <code>docker volume rm harvest_prometheus_data</code>. If you want to move the data, adjust the command below to first copy <code>harvest_prometheus_data</code> to a different volume and then remove it.</p>"},{"location":"MigratePrometheusDocker/#create-new-prometheus-volume","title":"Create new Prometheus volume","text":"<p>We're going to create a new mount named, <code>harvest_prometheus_data</code> by executing:</p> <pre><code>docker volume create --name harvest_prometheus_data\n</code></pre>"},{"location":"MigratePrometheusDocker/#copy-the-historical-prometheus-data","title":"Copy the historical Prometheus data","text":"<p>We will copy the historical Prometheus data from the old volume to the new one by mounting both volumes and copying data between them. NOTE: Prometheus only supports copying a single volume. It will not work if you attempt to copy multiple volumes into the same destination volume.</p> <pre><code># replace  `HISTORICAL_VOLUME` with the name of the Prometheus volume that contains you historical data found in step 2.\ndocker run --rm -it -v $HISTORICAL_VOLUME:/from -v harvest_prometheus_data:/to alpine ash -c \"cd /from ; cp -av . /to\"\n</code></pre> <p>Output will look something like this:</p> <pre><code>'./wal' -&gt; '/to/./wal'\n'./wal/00000000' -&gt; '/to/./wal/00000000'\n'./chunks_head' -&gt; '/to/./chunks_head'\n...\n</code></pre>"},{"location":"MigratePrometheusDocker/#optionally-remove-historical-prometheus-data","title":"Optionally remove historical Prometheus data","text":"<p>Before removing the historical data, start your compose stack and make sure everything works.</p> <p>Once you're satisfied that you can destroy the old data, remove it like so.</p> <pre><code># replace `HISTORICAL_VOLUME` with the name of the Prometheus volume that contains your historical data found in step 2.\ndocker volume rm $HISTORICAL_VOLUME\n</code></pre>"},{"location":"MigratePrometheusDocker/#reference","title":"Reference","text":"<ul> <li>Rename Docker Volume</li> </ul>"},{"location":"asar2/","title":"ASA r2","text":"<p>Since Harvest 24.11.1, NetApp ASA r2 systems monitoring is supported. We recommend using the latest Harvest version to ensure compatibility with ASA r2 systems.</p>"},{"location":"asar2/#prepare-asa-r2-cluster","title":"Prepare ASA r2 cluster","text":"<p>You need to prepare your ASA r2 cluster for monitoring by following the steps outlined in monitoring ONTAP systems, and then perform the following steps.</p>"},{"location":"asar2/#rest-least-privilege-role","title":"REST least-privilege role","text":"<p>Verify there are no errors when you copy/paste these. Warnings are fine.</p> <pre><code>security login rest-role create -role harvest-rest-role -access readonly -api /api/storage/storage-units\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/availability-zones\n</code></pre>"},{"location":"asar2/#supported-harvest-metrics","title":"Supported Harvest Metrics","text":"<p>Most capacity metrics collected via the REST collector are available in ASA r2 monitoring. However, only limited performance metrics are supported in ASA r2 systems. That is because ASA r2 clusters do not support the <code>RestPerf</code> collector. Instead, Harvest uses the KeyPerf collector to gather latency, IOPS, and throughput performance metrics for a limited set of objects.</p> <p>Harvest automatically detects ASA r2 systems and replaces any <code>ZapiPerf</code> or <code>RestPerf</code> collectors with the <code>KeyPerf</code> collector. </p> <p>We also recommend enabling the StatPerf collector for ASA r2 systems to collect performance metrics that are not available via the <code>KeyPerf</code> collector. You need to make sure the <code>StatPerf</code> collector is listed first in your list of collectors. e.g. </p> <pre><code>collectors:\n    - StatPerf\n    - Rest\n    - RestPerf\n</code></pre> <p>Performance metrics with the API name <code>KeyPerf</code> or <code>StatPerf</code> in the ONTAP metrics documentation are supported in ASA r2 systems. As a result, some panels in the dashboards may be missing information. </p>"},{"location":"cisco-switch-metrics/","title":"Cisco Switch Metrics","text":"<p>This document describes which Cisco switch metrics are collected and what those metrics are named in Harvest, including:</p> <ul> <li>Details about which Harvest metrics each dashboard uses. These can be generated on demand by running <code>bin/harvest grafana metrics</code>. See #1577 for details.</li> </ul> <pre><code>Creation Date : 2026-Feb-20\nNX-OS Version: 9.3.12\n</code></pre> Navigate to Grafana dashboards <p>Add your Grafana instance to the following form and save it. When you click on dashboard links on this page, a link to your dashboard will be opened. NAbox hosts Grafana on a subdomain like so: https://localhost/grafana/</p> <p> Grafana Host Save </p>"},{"location":"cisco-switch-metrics/#understanding-the-structure","title":"Understanding the structure","text":"<p>Below is an annotated example of how to interpret the structure of each of the metrics.</p> <p>cisco_switch_uptime Name of the metric exported by Harvest</p> <p>Displays uptime duration of the Cisco switch. Description of the Cisco switch metric</p> <ul> <li>API Harvest uses the NXAPI protocol to collect metrics</li> <li>Endpoint name of the CLI used to collect this metric</li> <li>Metric name of the Cisco switch metric</li> <li>Template path of the template that collects the metric</li> </ul> API Endpoint Metric Template NXAPI <code>show version</code> kern_uptm_days, kern_uptm_hrs, kern_uptm_mins, kern_uptm_secs conf/ciscorest/nxos/9.3.12/version.yaml Example to invoke CLI show commands via curl <p>In this example, we would demonstrate invoking the <code>show version</code> CLI command via curl.</p> <p>To do this, send a POST request to your switch\u2019s IP address with the desired command as input. Replace RO_USER, PASSWORD, and CISCO_SWITCH_IP with your actual read-only username, password, and the switch\u2019s IP address.</p> <pre><code>curl -sk -u RO_USER:PASSWORD POST 'https://CISCO_SWITCH_IP/ins_api' -d\n'{\n\"ins_api\": {\n\"version\": \"1.0\",\n\"type\": \"cli_show\",\n\"chunk\": \"0\",\n\"sid\": \"1\",\n\"input\": \"show version\",\n\"output_format\": \"json\"\n}\n}'\n</code></pre> <p>After invoking the above Curl command, You would get this response <pre><code>{\n        \"ins_api\":      {\n                \"type\": \"cli_show\",\n                \"version\":      \"1.0\",\n                \"sid\":  \"eoc\",\n                \"outputs\":      {\n                        \"output\":       {\n                                \"input\": \"show version\",\n                                \"msg\":  \"Success\",\n                                \"code\": \"200\",\n                                \"body\": {\n                                        \"header_str\":   \"Cisco Nexus Operating System (NX-OS) Software\\nTAC support: http://www.cisco.com/tac\\nCopyright (C) 2002-2023, Cisco and/or its affiliates.\\nAll rights reserved.\\nThe copyrights to certain works contained in this software are\\nowned by other third parties and used and distributed under their own\\nlicenses, such as open source.  This software is provided \\\"as is,\\\" and unless\\notherwise stated, there is no warranty, express or implied, including but not\\nlimited to warranties of merchantability and fitness for a particular purpose.\\nCertain components of this software are licensed under\\nthe GNU General Public License (GPL) version 2.0 or \\nGNU General Public License (GPL) version 3.0  or the GNU\\nLesser General Public License (LGPL) Version 2.1 or \\nLesser General Public License (LGPL) Version 2.0. \\nA copy of each such license is available at\\nhttp://www.opensource.org/licenses/gpl-2.0.php and\\nhttp://opensource.org/licenses/gpl-3.0.html and\\nhttp://www.opensource.org/licenses/lgpl-2.1.php and\\nhttp://www.gnu.org/licenses/old-licenses/library.txt.\\n\",\n                                        \"bios_ver_str\": \"04.25\",\n                                        \"kickstart_ver_str\":    \"9.3(12)\",\n                                        \"nxos_ver_str\": \"9.3(12)\",\n                                        \"bios_cmpl_time\":       \"05/22/2019\",\n                                        \"kick_file_name\":       \"bootflash:///nxos.9.3.12.bin\",\n                                        \"nxos_file_name\":       \"bootflash:///nxos.9.3.12.bin\",\n                                        \"kick_cmpl_time\":       \"6/20/2023 12:00:00\",\n                                        \"nxos_cmpl_time\":       \"6/20/2023 12:00:00\",\n                                        \"kick_tmstmp\":  \"06/23/2023 17:33:36\",\n                                        \"nxos_tmstmp\":  \"06/23/2023 17:33:36\",\n                                        \"chassis_id\":   \"Nexus 3132QV Chassis\",\n                                        \"cpu_name\":     \"Intel(R) Core(TM) i3- CPU @ 2.50GHz\",\n                                        \"memory\":       16399572,\n                                        \"mem_type\":     \"kB\",\n                                        \"proc_board_id\":        \"FOC24213H5C\",\n                                        \"host_name\":    \"Switch-A1\",\n                                        \"bootflash_size\":       15137792,\n                                        \"slot0_size\":   0,\n                                        \"kern_uptm_days\":       256,\n                                        \"kern_uptm_hrs\":        19,\n                                        \"kern_uptm_mins\":       3,\n                                        \"kern_uptm_secs\":       50,\n                                        \"rr_usecs\":     24056,\n                                        \"rr_ctime\":     \"Wed Nov  6 14:02:05 2024\",\n                                        \"rr_reason\":    \"Reset Requested by CLI command reload\",\n                                        \"rr_sys_ver\":   \"9.3(12)\",\n                                        \"rr_service\":   \"\",\n                                        \"plugins\":      \"Core Plugin, Ethernet Plugin\",\n                                        \"manufacturer\": \"Cisco Systems, Inc.\",\n                                        \"TABLE_package_list\":   {\n                                                \"ROW_package_list\":     {\n                                                        \"package_id\":   \"\"\n                                                }\n                                        }\n                                }\n                        }\n                }\n        }\n}\n</code></pre></p>"},{"location":"cisco-switch-metrics/#metrics","title":"Metrics","text":""},{"location":"cisco-switch-metrics/#cisco_cdp_neighbor_labels","title":"cisco_cdp_neighbor_labels","text":"<p>Displays cisco discovery protocol information about neighbors in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show cdp neighbors detail</code> <code>device_id, platform_id, port_id, ttl, version, local_intf_mac, remote_intf_mac, capability</code> conf/ciscorest/nxos/9.3.12/cdp.yaml <p>The <code>cisco_cdp_neighbor_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Neighbors table Cisco Discovery Protocol"},{"location":"cisco-switch-metrics/#cisco_environment_fan_speed","title":"cisco_environment_fan_speed","text":"<p>Displays fan speed.</p> API Endpoint Metric Template NXAPI <code>show environment fan detail</code> <code>speed</code> conf/ciscorest/nxos/9.3.12/environment.yaml <p>The <code>cisco_environment_fan_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Temperature and Fan table Fan Details"},{"location":"cisco-switch-metrics/#cisco_environment_fan_up","title":"cisco_environment_fan_up","text":"<p>Displays Present/Absent Status of the fan in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show environment</code> <code>fanstatus</code> conf/ciscorest/nxos/9.3.12/environment.yaml <p>The <code>cisco_environment_fan_up</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Temperature and Fan table Fan Details"},{"location":"cisco-switch-metrics/#cisco_environment_fan_zone_speed","title":"cisco_environment_fan_zone_speed","text":"<p>Displays the zone fan speed of the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show environment fan detail</code> <code>zonespeed</code> conf/ciscorest/nxos/9.3.12/environment.yaml <p>The <code>cisco_environment_fan_zone_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Highlights table Switch Details"},{"location":"cisco-switch-metrics/#cisco_environment_power_capacity","title":"cisco_environment_power_capacity","text":"<p>Displays total capacity of the power supply in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show environment</code> <code>tot_capa</code> conf/ciscorest/nxos/9.3.12/environment.yaml <p>The <code>cisco_environment_power_capacity</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Power table PSU Details"},{"location":"cisco-switch-metrics/#cisco_environment_power_in","title":"cisco_environment_power_in","text":"<p>Displays actual input power in watts of power supply in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show environment</code> <code>actual_input OR watts</code> conf/ciscorest/nxos/9.3.12/environment.yaml <p>The <code>cisco_environment_power_in</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Power stat Total Power Cisco: Switch Power stat PSU Efficiency Cisco: Switch Power timeseries Top $TopResources Power Consumption"},{"location":"cisco-switch-metrics/#cisco_environment_power_mode","title":"cisco_environment_power_mode","text":"<p>Displays redundant or operational Mode of power supply in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show environment</code> <code>ps_redun_mode, ps_oper_mode</code> conf/ciscorest/nxos/9.3.12/environment.yaml <p>The <code>cisco_environment_power_mode</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Highlights table Switch Details"},{"location":"cisco-switch-metrics/#cisco_environment_power_out","title":"cisco_environment_power_out","text":"<p>Displays actual output power in watts of power supply in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show environment</code> <code>actual_out</code> conf/ciscorest/nxos/9.3.12/environment.yaml <p>The <code>cisco_environment_power_out</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Power stat PSU Efficiency Cisco: Switch Power timeseries Top $TopResources Power Consumption Cisco: Switch Power table PSU Details"},{"location":"cisco-switch-metrics/#cisco_environment_power_up","title":"cisco_environment_power_up","text":"<p>Displays power supply status in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show environment</code> <code>ps_status</code> conf/ciscorest/nxos/9.3.12/environment.yaml <p>The <code>cisco_environment_power_up</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Power table PSU Details"},{"location":"cisco-switch-metrics/#cisco_environment_sensor_temp","title":"cisco_environment_sensor_temp","text":"<p>Displays current temperature of sensor in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show environment</code> <code>curtemp</code> conf/ciscorest/nxos/9.3.12/environment.yaml <p>The <code>cisco_environment_sensor_temp</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Temperature and Fan timeseries Top $TopResources Sensor Temperatures"},{"location":"cisco-switch-metrics/#cisco_interface_admin_up","title":"cisco_interface_admin_up","text":"<p>Displays admin state of the interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface</code> <code>admin_state</code> conf/ciscorest/nxos/9.3.12/interface.yaml <p>The <code>cisco_interface_admin_up</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Interfaces stat Down (Last 24h) Cisco: Switch Interfaces table Down (Last 24h) Cisco: Switch Interfaces timeseries Down (Last 24h)"},{"location":"cisco-switch-metrics/#cisco_interface_crc_errors","title":"cisco_interface_crc_errors","text":"<p>Displays CRC of interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface</code> <code>eth_crc</code> conf/ciscorest/nxos/9.3.12/interface.yaml <p>The <code>cisco_interface_crc_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Traffic timeseries Top $TopResources Interface CRC error"},{"location":"cisco-switch-metrics/#cisco_interface_eth_out_discards","title":"cisco_interface_eth_out_discards","text":"<p>Number of Ethernet output discarded packets in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface</code> <code>eth_outdiscard</code> conf/ciscorest/nxos/9.3.12/interface.yaml <p>The <code>cisco_interface_eth_out_discards</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Traffic timeseries Top $TopResources Ethernet Out Discards"},{"location":"cisco-switch-metrics/#cisco_interface_receive_bytes","title":"cisco_interface_receive_bytes","text":"<p>Displays bytes input of the interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface</code> <code>eth_inbytes</code> conf/ciscorest/nxos/9.3.12/interface.yaml <p>The <code>cisco_interface_receive_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Traffic table Traffic on Switch Cisco: Switch Traffic timeseries Top $TopResources Interface Receive Throughput"},{"location":"cisco-switch-metrics/#cisco_interface_receive_drops","title":"cisco_interface_receive_drops","text":"<p>Displays input if-down drops of interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface</code> <code>eth_in_ifdown_drops</code> conf/ciscorest/nxos/9.3.12/interface.yaml <p>The <code>cisco_interface_receive_drops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Traffic timeseries Top $TopResources Interface Receive Drops"},{"location":"cisco-switch-metrics/#cisco_interface_receive_errors","title":"cisco_interface_receive_errors","text":"<p>Displays input errors of interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface</code> <code>eth_inerr</code> conf/ciscorest/nxos/9.3.12/interface.yaml <p>The <code>cisco_interface_receive_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Traffic timeseries Top $TopResources Interface Errors"},{"location":"cisco-switch-metrics/#cisco_interface_transmit_bytes","title":"cisco_interface_transmit_bytes","text":"<p>Displays bytes output of interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface</code> <code>eth_outbytes</code> conf/ciscorest/nxos/9.3.12/interface.yaml <p>The <code>cisco_interface_transmit_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Traffic table Traffic on Switch Cisco: Switch Traffic timeseries Top $TopResources Interface Send Throughput"},{"location":"cisco-switch-metrics/#cisco_interface_transmit_drops","title":"cisco_interface_transmit_drops","text":"<p>Displays output drops of interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface</code> <code>eth_out_drops</code> conf/ciscorest/nxos/9.3.12/interface.yaml <p>The <code>cisco_interface_transmit_drops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Traffic timeseries Top $TopResources Interface Receive Drops"},{"location":"cisco-switch-metrics/#cisco_interface_transmit_errors","title":"cisco_interface_transmit_errors","text":"<p>Displays output errors of interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface</code> <code>eth_outerr</code> conf/ciscorest/nxos/9.3.12/interface.yaml <p>The <code>cisco_interface_transmit_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Traffic timeseries Top $TopResources Interface Errors"},{"location":"cisco-switch-metrics/#cisco_lldp_neighbor_labels","title":"cisco_lldp_neighbor_labels","text":"<p>Displays link layer discovery protocol information about neighbours in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show lldp neighbors detail</code> <code>sys_name, sys_desc, chassis_id, l_port_id, ttl, port_id, enabled_capability</code> conf/ciscorest/nxos/9.3.12/lldp.yaml <p>The <code>cisco_lldp_neighbor_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Neighbors table Link Layer Discovery Protocol"},{"location":"cisco-switch-metrics/#cisco_optic_rx","title":"cisco_optic_rx","text":"<p>Displays rx power of the interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface transceiver details</code> <code>rx_pwr</code> conf/ciscorest/nxos/9.3.12/optic.yaml <p>The <code>cisco_optic_rx</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Transceiver timeseries Top $TopResources Transceiver RX Power"},{"location":"cisco-switch-metrics/#cisco_optic_tx","title":"cisco_optic_tx","text":"<p>Displays tx power of the interface in the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show interface transceiver details</code> <code>tx_pwr</code> conf/ciscorest/nxos/9.3.12/optic.yaml <p>The <code>cisco_optic_tx</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Transceiver timeseries Top $TopResources Transceiver TX Power"},{"location":"cisco-switch-metrics/#cisco_switch_labels","title":"cisco_switch_labels","text":"<p>Displays configuration detail of the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show version</code> <code>bios_ver_str, chassis_id, host_name, nxos_ver_str,</code> conf/ciscorest/nxos/9.3.12/version.yaml NXAPI <code>show banner motd</code> <code>banner_msg.b_msg</code> conf/ciscorest/nxos/9.3.12/version.yaml"},{"location":"cisco-switch-metrics/#cisco_switch_uptime","title":"cisco_switch_uptime","text":"<p>Displays uptime duration of the Cisco switch.</p> API Endpoint Metric Template NXAPI <code>show version</code> <code>kern_uptm_days, kern_uptm_hrs, kern_uptm_mins, kern_uptm_secs</code> conf/ciscorest/nxos/9.3.12/version.yaml <p>The <code>cisco_switch_uptime</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Cisco: Switch Highlights table Switch Details"},{"location":"concepts/","title":"Concepts","text":"<p>In order to understand how Harvest works, it's important to understand the following concepts:</p> <ul> <li> Poller</li> <li> Collectors</li> <li> Templates</li> <li> Exporters</li> <li> Dashboards</li> <li> Port Map</li> </ul> <p>In addition to the above concepts, Harvest uses the following software that you will want to be familiar with:</p> <ul> <li> Prometheus</li> <li> InfluxDB</li> <li> Dashboards</li> <li> Prometheus Auto-discover</li> <li> Docker</li> <li> NABox</li> </ul>"},{"location":"concepts/#poller","title":"Poller","text":"<p>The poller is the resident daemon process that coordinates the collectors and exporters. There will be one poller per monitored cluster.</p>"},{"location":"concepts/#collectors","title":"Collectors","text":"<p>Collectors implement the necessary protocol required to speak to the cluster. Harvest ships with ZAPI, REST, EMS, and StorageGRID collectors. Collectors use a set of per-object template files to determine which metrics to collect.</p> <p>More information:</p> <ul> <li>Configuring Collectors</li> </ul>"},{"location":"concepts/#templates","title":"Templates","text":"<p>Templates define which metrics should be collected for an object (e.g. volume, lun, SVM, etc.). Harvest ships with a set of templates for each collector. The templates are written in YAML and are straightforward to read and modify. The templates are located in the <code>conf</code> directory.</p> <p>There are two kinds of templates:</p>"},{"location":"concepts/#collector-templates","title":"Collector Templates","text":"<p>Collector templates (e.g. <code>conf/rest/default.yaml</code>) define which set of objects Harvest should collect from the system being monitored when that collector runs. For example, the <code>conf/rest/default.yaml</code> collector template defines which objects should be collected by the REST collector, while <code>conf/storagegrid/default.yaml</code> lists which objects should be collected by the StorageGRID collector.</p>"},{"location":"concepts/#object-templates","title":"Object Templates","text":"<p>Object templates (e.g. <code>conf/rest/9.12.0/disk.yaml</code>) define which metrics should be collected and exported for an object. For example, the <code>disk.yaml</code> object template defines which disk metrics should be collected (e.g. <code>disk_bytes_per_sector</code>, <code>disk_stats_average_latency</code>, <code>disk_uptime</code>, etc.) </p> <p>More information:</p> <ul> <li>Templates</li> <li>Templates and Metrics</li> </ul>"},{"location":"concepts/#exporters","title":"Exporters","text":"<p>Exporters are responsible for encoding the collected metrics and making them available to time-series databases. Harvest ships with Prometheus and InfluxDB exporters. Harvest does not include Prometheus and InfluxDB, only the exporters for them. Prometheus and InfluxDB must be installed separately via Docker, NAbox, or other means.</p>"},{"location":"concepts/#prometheus","title":"Prometheus","text":"<p>Prometheus is an open-source time-series database. It is a popular choice for storing and querying metrics. </p> <p>Don't call us, we'll call you</p> <p>None of the pollers know anything about Prometheus. That's because Prometheus pulls metrics from the poller's Prometheus exporter. The exporter creates an HTTP(s) endpoint that Prometheus scrapes on its own schedule. </p> <p>More information:</p> <ul> <li>Prometheus Exporter</li> </ul>"},{"location":"concepts/#influxdb","title":"InfluxDB","text":"<p>InfluxDB is an open-source time-series database. Harvest ships with some sample Grafana dashboards that are designed to work with InfluxDB. Unlike the Prometheus exporter, Harvest's InfluxDB exporter pushes metrics from the poller to InfluxDB via InfluxDB's line protocol. The exporter is compatible with InfluxDB v2.0. </p> <p>Note</p> <p>Harvest includes a subset of dashboards for InfluxDB. There is a richer set of dashboards available for Prometheus.</p> <p>More information:</p> <ul> <li>InfluxDB Exporter</li> </ul>"},{"location":"concepts/#victoriametrics","title":"VictoriaMetrics","text":"<p>VictoriaMetrics is an open-source time-series database. Existing Harvest dashboards supported for Prometheus will also work with VictoriaMetrics. Unlike the Prometheus exporter, Harvest's VictoriaMetrics exporter pushes metrics from the poller to VictoriaMetrics via Prometheus exposition format. The exporter is compatible with VictoriaMetrics v1.129.1.</p> <p>More information:</p> <ul> <li>VictoriaMetrics Exporter</li> </ul>"},{"location":"concepts/#dashboards","title":"Dashboards","text":"<p>Harvest ships with a set of Grafana dashboards that are primarily designed to work with Prometheus. The dashboards are located in the <code>grafana/dashboards</code> directory. Harvest does not include Grafana, only the dashboards for it. Grafana must be installed separately via Docker, NAbox, or other means.</p> <p>Harvest includes CLI tools to import and export dashboards to Grafana. The CLI tools are available by running <code>bin/harvest grafana --help</code></p> <p>More information:</p> <ul> <li>Import or Export Dashboards</li> <li>How to Create A New Dashboard</li> </ul>"},{"location":"concepts/#prometheus-auto-discovery","title":"Prometheus Auto-Discovery","text":"<p>Because of Prometheus's pull model, you need to configure Prometheus to tell it where to pull metrics from. There are two ways to tell Prometheus how to scrape Harvest: 1) listing each poller's address and port individually in Prometheus's config file or 2) using HTTP service discovery. </p> <p>Harvest's admin node implements Prometheus's HTTP service discovery API. Each poller registers its address and port with the admin node and Prometheus consults with the admin node for the list of targets it should scrape.</p> <p>More information:</p> <ul> <li>Configure Prometheus to scrape Harvest pollers</li> <li>Prometheus Admin Node</li> <li>Prometheus HTTP Service Discovery</li> </ul>"},{"location":"concepts/#docker","title":"Docker","text":"<p>Harvest runs natively in containers. The Harvest container includes the <code>harvest</code> and <code>poller</code> binaries as well as all templates and dashboards. If you want to standup Harvest, Prometheus, and Grafana all together, you can use the Docker Compose workflow. The Docker Compose workflow is a good way to quickly get started with Harvest.</p> <p>More information:</p> <ul> <li>Running Harvest in Docker</li> <li>Running Harvest, Prometheus, and Grafana in Docker</li> </ul>"},{"location":"concepts/#nabox","title":"NABox","text":"<p>NABox is a separate virtual appliance (.ova) that acts as a front-end to Harvest and includes Promethus and Grafana setup to use with Harvest. NABox is a great option for customers that prefer a virtual appliance over containers.</p> <p>More information:</p> <ul> <li>NABox</li> </ul>"},{"location":"concepts/#port-map","title":"Port Map","text":"<p>The default ports for ONTAP, Grafana, and Prometheus are shown below, along with three pollers. Poller1 is using the PrometheusExporter with a statically defined port in <code>harvest.yml</code>. Poller2 and Poller3 are using Harvest's admin node, port range, and Prometheus HTTP service discovery. </p> <pre><code>graph LR\n  Poller1 --&gt;|:443|ONTAP1;\n  Prometheus --&gt;|:promPort1|Poller1;\n  Prometheus --&gt;|:promPort2|Poller2;\n  Prometheus --&gt;|:promPort3|Poller3;\n  Prometheus --&gt;|:8887|AdminNode;\n\n  Poller2 --&gt;|:443|ONTAP2;\n  AdminNode &lt;--&gt;|:8887|Poller3;\n  Poller3 --&gt;|:443|ONTAP3;\n  AdminNode &lt;--&gt;|:8887|Poller2;\n\n  Grafana --&gt;|:9090|Prometheus;\n  Browser --&gt;|:3000|Grafana;</code></pre> <ul> <li>Grafana's default port is <code>3000</code> and is used to access the Grafana user-interface via a web browser</li> <li>Prometheus's default port is <code>9090</code> and Grafana talks to the Prometheus datasource on that port</li> <li>Prometheus scrapes each poller-exposed Prometheus port (<code>promPort1</code>, <code>promPort2</code>, <code>promPort3</code>)</li> <li>Poller2 and Poller3 are configured to use a PrometheusExporter with port range. Each pollers picks a free port within the port_range and sends that port to the AdminNode.</li> <li> <p>The Prometheus config file, <code>prometheus.yml</code> is updated with two scrape targets:</p> <ol> <li>the static <code>address:port</code> for Poller1</li> <li>the <code>address:port</code> for the AdminNode</li> </ol> </li> <li> <p>Poller1 creates an HTTP endpoint on the static port defined in the <code>harvest.yml</code> file</p> </li> <li>All pollers use ZAPI or REST to communicate with ONTAP on port <code>443</code></li> </ul>"},{"location":"concepts/#reference","title":"Reference","text":"<ul> <li>Architecture.md</li> </ul>"},{"location":"configure-cisco-rest/","title":"CiscoRest","text":""},{"location":"configure-cisco-rest/#ciscorest-collector","title":"CiscoRest Collector","text":"<p>The CiscoRest collector uses NX-API REST calls to collect data from Cisco switches.</p>"},{"location":"configure-cisco-rest/#target-system","title":"Target System","text":"<p>Harvest supports all Cisco switches listed in NetApp's Hardware Universe.</p>"},{"location":"configure-cisco-rest/#requirements","title":"Requirements","text":"<p>The NX-API feature must be enabled on the switch. No SDK or other requirements. It is recommended to create a read-only user for Harvest on the Cisco switch (see prepare monitored clusters for details)</p>"},{"location":"configure-cisco-rest/#metrics","title":"Metrics","text":"<p>The collector collects a dynamic set of metrics via Cisco's NX-API. The switch returns JSON documents, and unlike other Harvest collectors, the CiscoRest collector does not provide template customization.</p>"},{"location":"configure-cisco-rest/#parameters","title":"Parameters","text":"<p>The parameters of the collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>CiscoRest configuration file (default: <code>conf/ciscorest/default.yaml</code>)</li> <li>Each object has its own configuration file (located in <code>conf/ciscorest/nxos/$version/</code>)</li> </ul> <p>Except for <code>addr</code> and <code>datacenter</code>, all other parameters of the CiscoRest collector can be defined in any of these three files. Parameters defined in a lower-level file override those in higher-level files. This allows you to configure each object individually or use the same parameters for all objects.</p> <p>The full set of parameters are described below.</p>"},{"location":"configure-cisco-rest/#harvest-configuration-file","title":"Harvest configuration file","text":"<p>Parameters in the poller section should define the following required parameters.</p> parameter type description default Poller name (header) string, required Poller name, user-defined value <code>addr</code> string, required IPv4, IPv6 or FQDN of the target system <code>datacenter</code> string, required Datacenter name, user-defined value <code>username</code>, <code>password</code> string, required Cisco switch username and password with at least <code>network-operator</code> permissions <code>collectors</code> list, required Name of collector to run for this poller, use <code>CiscoRest</code> for this collector"},{"location":"configure-cisco-rest/#ciscorest-configuration-file","title":"CiscoRest configuration file","text":"<p>This configuration file contains a list of objects that should be collected and the filenames of their templates (explained in the next section).</p> <p>Additionally, this file contains the parameters that are applied as defaults to all objects. As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well.</p> parameter type description default <code>client_timeout</code> duration (Go-syntax) how long to wait for server responses 30s <code>schedule</code> list, required how frequently to retrieve metrics from StorageGRID - <code>data</code> duration (Go-syntax) how frequently this collector/object should retrieve metrics from StorageGRID 5 minutes <code>only_cluster_instance</code> bool, optional don't require instance key. assume the only instance is the cluster itself <p>The template should define objects in the <code>objects</code> section. Example:</p> <pre><code>objects:\n  Optic: optic.yaml\n</code></pre> <p>For each object, we define the filename of the object configuration file. The object configuration files are located in subdirectories matching the CiscoRest version that was used to create these files. It is possible to have multiple version-subdirectories for multiple CiscoRest versions. At runtime, the collector will select the object configuration file that closest matches the version of the target CiscoRest system.</p>"},{"location":"configure-cisco-rest/#object-configuration-file","title":"Object configuration file","text":"<p>The Object configuration file (\"subtemplate\") should contain the following parameters:</p> parameter type description default <code>name</code> string, required display name of the collector that will collect this object <code>query</code> string, required Cisco switch CLI command used to issue a REST request <code>object</code> string, required short name of the object <code>plugins</code> list plugins and their parameters to run on the collected data"},{"location":"configure-ems/","title":"EMS","text":""},{"location":"configure-ems/#ems-collector","title":"EMS collector","text":"<p>The <code>EMS collector</code> collects ONTAP event management system (EMS) events via the ONTAP REST API.</p> <p>The EMS alert runbook includes descriptions and remediation steps for the EMS events that Harvest collects.</p> <p>This collector uses a YAML template file to define which events to collect, export, and what labels to attach to each metric. This means you can collect new EMS events or attach new labels by editing the default template file or by extending existing templates. Events that occurred when the EMS collector was not running will not be captured.</p> <p>The default template file contains 98 EMS events.</p>"},{"location":"configure-ems/#supported-ontap-systems","title":"Supported ONTAP Systems","text":"<p>Any cDOT ONTAP system using 9.6 or higher.</p>"},{"location":"configure-ems/#requirements","title":"Requirements","text":"<p>It is recommended to create a read-only user on the ONTAP system. See prepare an ONTAP cDOT cluster for details.</p>"},{"location":"configure-ems/#metrics","title":"Metrics","text":"<p>This collector collects EMS events from ONTAP and for each received EMS event, creates new metrics prefixed with <code>ems_events</code>.</p> <p>Harvest supports two types of ONTAP EMS events:</p> <ul> <li>Normal EMS events</li> </ul> <p>Single shot events. When ONTAP detects a problem, an event is raised. When the issue is addressed, ONTAP does not raise another event reflecting that the problem was resolved.</p> <ul> <li>Bookend EMS events</li> </ul> <p>ONTAP creates bookend events in matching pairs. ONTAP creates an event when an issue is detected and another paired event when the event is resolved. Typically, these events share a common set of properties.</p>"},{"location":"configure-ems/#collector-configuration","title":"Collector Configuration","text":"<p>The parameters of the collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>EMS collector configuration file (default: <code>conf/ems/default.yaml</code>)</li> <li>EMS template file (located in <code>conf/ems/9.6.0/ems.yaml</code>)</li> </ul> <p>Except for <code>addr</code>, <code>datacenter</code>, and <code>auth_style</code>, all other parameters of the EMS collector can be defined in either of these three files. Parameters defined in the lower-level files, override parameters in the higher-level file. This allows you to configure each EMS event individually, or use the same parameters for all events.</p>"},{"location":"configure-ems/#ems-collector-configuration-file","title":"EMS Collector Configuration File","text":"<p>This configuration file contains the parameters that are used to configure the EMS collector. These parameters can be defined in your <code>harvest.yml</code> or <code>conf/ems/default.yaml</code> file.</p> parameter type description default <code>client_timeout</code> Go duration how long to wait for server responses 1m <code>schedule</code> list, required the polling frequency of the collector/object. Should include exactly the following two elements in the order specified: - <code>instance</code> Go duration polling frequency for updating the instance cache (example value: <code>24h</code> = <code>1440m</code>) - <code>data</code> Go duration polling frequency for updating the data cache (example value: <code>3m</code>)Note Harvest allows defining poll intervals on sub-second level (e.g. <code>1ms</code>), however keep in mind the following:<ul><li>API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than <code>client_timeout</code>.</li><li>Small poll intervals will create significant workload on the ONTAP system.</li></ul> <p>The EMS configuration file should contain the following section mapping the <code>Ems</code> object to the corresponding template file.</p> <pre><code>objects:\n  Ems: ems.yaml\n</code></pre> <p>Even though the EMS mapping shown above references a single file named <code>ems.yaml</code>, there may be multiple versions of that file across subdirectories named after ONTAP releases. See cDOT for examples.</p> <p>At runtime, the EMS collector will select the appropriate object configuration file that most closely matches the targeted ONTAP system.</p>"},{"location":"configure-ems/#ems-template-file","title":"EMS Template File","text":"<p>The EMS template file should contain the following parameters:</p> parameter type description default <code>name</code> string display name of the collector. this matches the named defined in your <code>conf/ems/default.yaml</code> file EMS <code>object</code> string short name of the object, used to prefix metrics ems <code>query</code> string REST API endpoint used to query EMS events <code>api/support/ems/events</code> <code>exports</code> list list of default labels attached to each exported metric <code>events</code> list list of EMS events to collect. See Event Parameters"},{"location":"configure-ems/#event-parameters","title":"Event Parameters","text":"<p>This section defines the list of EMS events you want to collect, which properties to export, what labels to attach, and how to handle bookend pairs. The EMS event template parameters are explained below along with an example for reference.</p> <ul> <li><code>name</code> is the ONTAP EMS event name. (collect ONTAP EMS events with the name of <code>LUN.offline</code>)</li> <li><code>matches</code> list of name-value pairs used to further filter ONTAP events.   Some EMS events include arguments and these name-value pairs provide a way to filter on those arguments.   (Only collect ONTAP EMS events where <code>volume_name</code> has the value <code>abc_vol</code>)</li> <li><code>exports</code> list of EMS event parameters to export. These exported parameters are attached as labels to each matching   EMS event.<ul> <li>labels that are prefixed with <code>^^</code> use that parameter to   define instance uniqueness.</li> </ul> </li> <li><code>resolve_when_ems</code> (applicable to bookend events only). Lists the resolving event that pairs with the issuing event<ul> <li><code>name</code> is the ONTAP EMS event name of the resolving EMS event (<code>LUN.online</code>).   When the resolving event is received, the issuing EMS event will be resolved. In this example, Harvest will raise   an event when it finds the ONTAP EMS event named <code>LUN.offline</code> and that event will be resolved when the EMS event   named <code>LUN.online</code> is received.</li> <li><code>resolve_after</code> (optional, Go duration, default = 28 days) resolve the issuing EMS after the specified duration   has elapsed (<code>672h</code> = <code>28d</code>).   If the bookend pair is not received within the <code>resolve_after</code> duration, the Issuing EMS event expires. When that   happens, Harvest will mark the event as auto resolved by adding the <code>autoresolved=true</code> label to the issuing EMS event.</li> <li><code>resolve_key</code> (optional) bookend key used to match bookend EMS events. Defaults to prefixed (<code>^^</code>) labels   in <code>exports</code> section. <code>resolve_key</code> allows you to override what is defined in the <code>exports</code> section.</li> </ul> </li> </ul> <p>Labels are only exported if they are included in the <code>exports</code> section.</p> <p>Example template definition for the <code>LUN.offline</code> EMS event:</p> <pre><code>  - name: LUN.offline\n    matches:\n      - name: volume_name\n        value: abc_vol\n    exports:\n      - ^^parameters.object_uuid            =&gt; object_uuid\n      - parameters.object_type              =&gt; object_type\n      - parameters.lun_path                 =&gt; lun_path\n      - parameters.volume_name              =&gt; volume\n      - parameters.volume_dsid              =&gt; volume_ds_id\n    resolve_when_ems:\n      - name: LUN.online\n        resolve_after: 672h\n        resolve_key:\n          - ^^parameters.object_uuid        =&gt; object_uuid\n</code></pre>"},{"location":"configure-ems/#how-do-i-find-the-full-list-of-supported-ems-events","title":"How do I find the full list of supported EMS events?","text":"<p>ONTAP documents the list of EMS events created in the ONTAP EMS Event Catalog.</p> <p>You can also query a live system and ask the cluster for its event catalog like so:</p> <pre><code>curl --insecure --user \"user:password\" 'https://10.61.124.110/api/support/ems/messages?fields=*'\n</code></pre> <p>Example Output</p> <pre><code>{\n  \"records\": [\n    {\n      \"name\": \"AccessCache.NearLimits\",\n      \"severity\": \"alert\",\n      \"description\": \"This message occurs when the access cache module is near its limits for entries or export rules. Reaching these limits can prevent new clients from being able to mount and perform I/O on the storage system, and can also cause clients to be granted or denied access based on stale cached information.\",\n      \"corrective_action\": \"Ensure that the number of clients accessing the storage system continues to be below the limits for access cache entries and export rules across those entries. If the set of clients accessing the storage system is constantly changing, consider using the \\\"vserver export-policy access-cache config modify\\\" command to reduce the harvest timeout parameter so that cache entries for clients that are no longer accessing the storage system can be evicted sooner.\",\n      \"snmp_trap_type\": \"severity_based\",\n      \"deprecated\": false\n    },\n...\n    {\n      \"name\": \"ztl.smap.online.status\",\n      \"severity\": \"notice\",\n      \"description\": \"This message occurs when the specified partition on a Software Defined Flash drive could not be onlined due to internal S/W or device error.\",\n      \"corrective_action\": \"NONE\",\n      \"snmp_trap_type\": \"severity_based\",\n      \"deprecated\": false\n    }\n  ],\n  \"num_records\": 7273\n}\n</code></pre>"},{"location":"configure-ems/#ems-prometheus-alerts","title":"Ems Prometheus Alerts","text":"<p>Refer Prometheus-Alerts</p>"},{"location":"configure-eseries/","title":"E-Series","text":"<p>Beta Feature</p> <p>The Eseries and EseriesPerf collectors are new in Harvest and should be considered beta.  Feedback and bug reports are welcome on GitHub Discussions.</p> <p>The Eseries collectors use the REST protocol to collect data from NetApp E-Series storage systems.</p> <p>The EseriesPerf collector is an extension of this collector for performance metrics, therefore they share many parameters and configuration settings.</p>"},{"location":"configure-eseries/#requirements","title":"Requirements","text":"<ul> <li>E-Series storage array with REST API support</li> <li>A user account with Monitor role permissions on the E-Series array (see prepare-eseries.md)</li> </ul> <p>No SDK or other requirements.</p>"},{"location":"configure-eseries/#metrics","title":"Metrics","text":"<p>The Eseries collector collects a dynamic set of metrics from E-Series storage arrays. The E-Series REST API returns JSON documents and Harvest extracts values from the JSON using template definitions with dot notation paths.</p> <p>The collector automatically discovers the storage array and extracts metrics for volumes, controllers, hardware components, and other objects.</p>"},{"location":"configure-eseries/#parameters","title":"Parameters","text":"<p>The parameters of the collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>Eseries configuration file (default: <code>conf/eseries/default.yaml</code>)</li> <li>Each object has its own configuration file (located in <code>conf/eseries/$version/</code>)</li> </ul> <p>Except for <code>addr</code> and <code>datacenter</code>, all other parameters of the Eseries collector can be defined in either of these three files. Parameters defined in the lower-level file override parameters in the higher-level ones. This allows you to configure each object individually, or use the same parameters for all objects.</p> <p>The full set of parameters are described below.</p>"},{"location":"configure-eseries/#harvest-configuration-example","title":"Harvest Configuration Example","text":"<p>In your <code>harvest.yml</code>, configure a poller pointing to your E-Series storage array:</p> <pre><code>Pollers:\n  eseries-array:\n    datacenter: DC-01\n    addr: 10.0.1.100                    # E-Series array management address\n    username: monitor                   # Array user with Monitor role\n    password: enterpass                 # Or use credential_script\n    collectors:\n      - Eseries\n      - EseriesPerf\n    exporters:\n      - prometheus\n</code></pre>"},{"location":"configure-eseries/#collector-configuration-file","title":"Collector Configuration File","text":"<p>This configuration file contains a list of objects that should be collected and the filenames of their templates. Additionally, this file contains the parameters that are applied as defaults to all objects.</p> parameter type description default <code>client_timeout</code> duration (Go-syntax) how long to wait for server responses 30s <code>jitter</code> duration (Go-syntax), optional Each Harvest collector runs independently, which means that at startup, each collector may send its REST queries at nearly the same time. To spread out the collector startup times over a broader period, you can use <code>jitter</code> to randomly distribute collector startup across a specified duration. For example, a <code>jitter</code> of <code>1m</code> starts each collector after a random delay between 0 and 60 seconds. For more details, refer to this discussion. <code>schedule</code> list, required how frequently to retrieve metrics from the E-Series array - <code>counter</code> duration (Go-syntax) poll frequency of updating the counter metadata cache 24 hours - <code>data</code> duration (Go-syntax) how frequently this collector/object should retrieve metrics 3 minutes <p>The default configuration file (<code>conf/eseries/default.yaml</code>) defines the objects to collect:</p> <pre><code>collector: Eseries\n\nschedule:\n  - counter: 24h\n  - data: 3m\n\nobjects:\n  Volume:            volume.yaml\n  Array:             array.yaml\n  Host:              host.yaml\n  Controller:        controller.yaml\n</code></pre> <p>For each object, we define the filename of the object configuration file. The object configuration files are located in subdirectories matching the SANtricity OS version (e.g., <code>11.80.0</code>). At runtime, the collector will select the object configuration file that closest matches the version of the target E-Series system.</p>"},{"location":"configure-eseries/#object-configuration-file","title":"Object Configuration File","text":"<p>The Object configuration file (\"subtemplate\") should contain the following parameters:</p> parameter type description default <code>name</code> string, required display name of the collector that will collect this object <code>object</code> string, required short name of the object, used to prefix metrics (e.g., <code>eseries_volume</code>) <code>query</code> string, required REST API endpoint to query, relative to <code>/devmgr/v2/</code> (can include <code>{array_id}</code> placeholder) <code>counters</code> list, required list of counters to collect (see Counters section below) <code>plugins</code> list, optional list of plugins to run on the collected data (see Plugins section below) <code>export_options</code> section, required defines how to export instance labels and keys"},{"location":"configure-eseries/#template-example","title":"Template Example","text":"<p>Here's an example of the Volume object template (<code>conf/eseries/11.80.0/volume.yaml</code>):</p> <pre><code>name: Volume\nquery: storage-systems/{array_id}/volumes\nobject: eseries_volume\n\ncounters:\n  - ^^name                                =&gt; volume\n  - ^listOfMappings                       =&gt; list_of_mappings\n  - ^metadata                             =&gt; metadata\n  - ^offline                              =&gt; offline\n  - ^raidLevel                            =&gt; raid_level\n  - ^status                               =&gt; status\n  - ^volumeGroupRef                       =&gt; volume_group_ref\n  - ^wwn                                  =&gt; wwid\n  - blkSize                               =&gt; block_size\n  - capacity                              =&gt; reported_capacity\n  - totalSizeInBytes                      =&gt; allocated_capacity\n\nplugins:\n  - Volume\n  - VolumeMapping\n\nexport_options:\n  instance_keys:\n    - volume\n  instance_labels:\n    - hosts\n    - luns\n    - mapping_types\n    - offline\n    - pool\n    - raid_level\n    - status\n    - volume\n    - workload\n    - wwid\n</code></pre>"},{"location":"configure-eseries/#query-path-and-array-id-injection","title":"Query Path and Array ID Injection","text":"<p>The <code>query</code> parameter can include <code>{array_id}</code> as a placeholder. The collector automatically discovers the storage array ID and injects it into the query URL at runtime.</p> <p>For example: - Template: <code>storage-systems/{array_id}/volumes</code> - Runtime: <code>storage-systems/1/volumes</code></p>"},{"location":"configure-eseries/#counters","title":"Counters","text":"<p>Counters define which metrics and labels to collect from the REST API response. Each counter line follows this format:</p> <pre><code>[prefix]json_field_name =&gt; display_name\n</code></pre> <p>Counter Prefixes:</p> <ul> <li><code>^^</code> - Instance key: Uniquely identifies each instance (required, must have at least one)</li> <li><code>^</code> - Instance label: String metadata exported to <code>&lt;object&gt;_labels</code> metric</li> <li>No prefix - Numeric metric: Exported as its own time-series metric</li> </ul> <p>Arrow Syntax (<code>=&gt;</code>):</p> <p>The arrow renames the JSON field to a shorter display name used in metrics:</p> <pre><code>counters:\n  - ^^name                    =&gt; volume           # Instance key\n  - ^raidLevel                =&gt; raid_level       # Label\n  - totalSizeInBytes          =&gt; allocated_capacity  # Metric\n</code></pre> <p>This produces metrics like: - <code>eseries_volume_allocated_capacity{volume=\"MyVolume\", ...}</code> - <code>eseries_volume_labels{volume=\"MyVolume\", raid_level=\"raid6\", ...}</code></p>"},{"location":"configure-eseries/#export-options","title":"Export Options","text":"<p>The <code>export_options</code> section controls what gets exported:</p> <pre><code>export_options:\n  instance_keys:      # Primary identifier(s)\n    - volume          \n  instance_labels:    # Labels to include in &lt;object&gt;_labels metric\n    - hosts\n    - luns\n    - mapping_types\n    - offline\n    - pool\n    - raid_level\n    - status\n    - volume\n    - workload\n    - wwid    \n</code></pre> <ul> <li>instance_keys: Unique identifier labels (from <code>^^</code> counters)</li> <li>instance_labels: All labels to export in the <code>&lt;object&gt;_labels</code> metric</li> </ul>"},{"location":"configure-eseries/#plugins","title":"Plugins","text":"<p>Eseries collectors support plugins that enrich or transform collected data:</p> <p>Plugin Configuration:</p> <pre><code>plugins:\n  - Volume\n  - VolumeMapping\n</code></pre> <p>Plugins run after data collection but before export, allowing them to add computed metrics or enrich labels.</p>"},{"location":"configure-eseries/#eseriesperf-collector","title":"EseriesPerf Collector","text":"<p>The EseriesPerf collector extends Eseries to collect performance metrics from E-Series arrays using the <code>/live-statistics</code> endpoint. It implements delta calculations similar to RestPerf and ZapiPerf collectors.</p>"},{"location":"configure-eseries/#static-counter-definitions","title":"Static Counter Definitions","text":"<p>Unlike Eseries, EseriesPerf uses a static counter definitions file (<code>conf/eseriesperf/static_counter_definitions.yaml</code>) that defines how to process each counter:</p> <pre><code>objects:\n  eseries_volume:\n    counter_definitions:\n      - name: readOps\n        type: rate                    # Calculate delta per second\n      - name: readTimeTotal\n        type: average                 # Divide by base counter\n        base_counter: readOps         # readTimeTotal / readOps = avg latency\n      - name: readBytes\n        type: rate\n      - name: writeOps\n        type: rate\n</code></pre>"},{"location":"configure-eseries/#performance-template-example","title":"Performance Template Example","text":"<p>Here's an example EseriesPerf template (<code>conf/eseriesperf/11.80.0/volume.yaml</code>):</p> <pre><code>name: Volume\nquery: storage-systems/{array_id}/live-statistics\nobject: eseries_volume\ntype: volume\n\ncounters:\n  - ^^volumeName                  =&gt; volume\n  - lastResetTimeInMS             =&gt; last_reset_time\n  - observedTimeInMS              =&gt; observed_time\n  - readBytes                     =&gt; read_data\n  - readHitOps                    =&gt; read_hit_ops\n  - readOps                       =&gt; read_ops\n  - readTimeTotal                 =&gt; read_latency\n  - writeBytes                    =&gt; write_data\n  - writeHitOps                   =&gt; write_hit_ops\n  - writeOps                      =&gt; write_ops\n  - writeTimeTotal                =&gt; write_latency\n\nplugins:\n  - CacheHitRatio\n\nexport_options:\n  instance_keys:\n    - volume\n</code></pre> <p>The static counter definitions file determines how each counter is processed:</p> <pre><code># conf/eseriesperf/static_counter_definitions.yaml\nobjects:\n  eseries_volume:\n    counter_definitions:\n      - name: readOps\n        type: rate                    # Becomes read_ops (ops/sec)\n      - name: readTimeTotal\n        type: average                 # Becomes read_latency (ms/op)\n        base_counter: readOps\n      - name: readBytes\n        type: rate                    # Becomes read_data (bytes/sec)\n</code></pre> <p>See prepare-eseries.md for system setup and troubleshooting guide for general issues.</p>"},{"location":"configure-grafana/","title":"Configure Grafana","text":""},{"location":"configure-grafana/#grafana","title":"Grafana","text":"<p>Grafana hosts the Harvest dashboards and needs to be setup before importing your dashboards. </p>"},{"location":"configure-harvest-advanced/","title":"Configure Harvest (advanced)","text":"<p>This chapter describes additional advanced configuration possibilities of NetApp Harvest. For a typical installation,  this level of detail is likely not needed.</p>"},{"location":"configure-harvest-advanced/#variable-expansion","title":"Variable Expansion","text":"<p>The <code>harvest.yml</code> configuration file supports variable expansion. This allows you to use environment variables in the configuration file. Harvest will expand strings with the format <code>$__env{VAR}</code> or <code>${VAR}</code>, replacing the variable <code>VAR</code> with the value of the environment variable. If the environment variable is not set, the variable will be replaced with an empty string.</p> <p>Here's an example snippet from <code>harvest.yml</code>:</p> <pre><code>Pollers:\n  netapp_frankfurt:\n    addr: 10.0.1.2\n    username: $__env{NETAPP_FRANKFURT_RO_USER}\n  netapp_london:\n    addr: uk-cluster\n    username: ${NETAPP_LONDON_RO_USER}\n  netapp_rtp:\n    addr: 10.0.1.4\n    username: $__env{NETAPP_RTP_RO_USER}\n</code></pre> <p>If you set the environment variable <code>NETAPP_FRANKFURT_RO_USER</code> to <code>harvest1</code> and <code>NETAPP_LONDON_RO_USER</code> to <code>harvest2</code>, the configuration will be expanded to:</p> <pre><code>Pollers:\n  netapp_frankfurt:\n    addr: 10.0.1.2\n    username: harvest1\n  netapp_london:\n    addr: uk-cluster\n    username: harvest2\n  netapp_rtp:\n    addr: 10.0.1.4\n    username: \n</code></pre>"},{"location":"configure-harvest-basic/","title":"Configure Harvest (basic)","text":"<p>The main configuration file, <code>harvest.yml</code>, consists of the following sections, described below:</p>"},{"location":"configure-harvest-basic/#pollers","title":"Pollers","text":"<p>All pollers are defined in <code>harvest.yml</code>, the main configuration file of Harvest, under the section <code>Pollers</code>.</p> parameter type description default Poller name (header) required Poller name, user-defined value <code>datacenter</code> required Datacenter name, user-defined value <code>addr</code> required by some collectors IPv4, IPv6 or FQDN of the target system <code>collectors</code> required List of collectors to run for this poller. Possible values are <code>Zapi</code>, <code>ZapiPerf</code>, <code>Rest</code>, <code>RestPerf</code>, <code>KeyPerf</code>, <code>StatPerf</code>, <code>Ems</code>, <code>StorageGrid</code>, <code>CiscoRest</code>, <code>Eseries</code>, <code>EseriesPerf</code>. <code>exporters</code> required List of exporter names from the <code>Exporters</code> section. Note: this should be the name of the exporter (e.g. <code>prometheus1</code>), not the value of the <code>exporter</code> key (e.g. <code>Prometheus</code>) <code>auth_style</code> required by Zapi* collectors Either <code>basic_auth</code> or <code>certificate_auth</code> See authentication for details <code>basic_auth</code> <code>username</code>, <code>password</code> required if <code>auth_style</code> is <code>basic_auth</code> <code>ssl_cert</code>, <code>ssl_key</code> optional if <code>auth_style</code> is <code>certificate_auth</code> Paths to SSL (client) certificate and key used to authenticate with the target system.If not provided, the poller will look for <code>&lt;hostname&gt;.key</code> and <code>&lt;hostname&gt;.pem</code> in <code>$HARVEST_HOME/cert/</code>.To create certificates for ONTAP systems, see using certificate authentication <code>ca_cert</code> optional if <code>auth_style</code> is <code>certificate_auth</code> Path to file that contains PEM encoded certificates. Harvest will append these certificates to the system-wide set of root certificate authorities (CA).If not provided, the OS's root CAs will be used.To create certificates for ONTAP systems, see using certificate authentication <code>use_insecure_tls</code> optional, bool If true, disable TLS verification when connecting to ONTAP cluster false <code>credentials_file</code> optional, string Path to a yaml file that contains cluster credentials. The file should have the same shape as <code>harvest.yml</code>. See here for examples. Path can be relative to <code>harvest.yml</code> or absolute. <code>credentials_script</code> optional, section Section that defines how Harvest should fetch credentials via external script. See here for details. <code>tls_min_version</code> optional, string Minimum TLS version to use when connecting to ONTAP cluster: One of tls10, tls11, tls12 or tls13 Platform decides <code>labels</code> optional, list of key-value pairs Each of the key-value pairs will be added to a poller's metrics. Details below <code>log_max_bytes</code> Maximum size of the log file before it will be rotated <code>10 MB</code> <code>log_max_files</code> Number of rotated log files to keep <code>5</code> <code>log</code> optional, list of collector names Matching collectors log their ZAPI request/response <code>prefer_zapi</code> optional, bool Use the ZAPI API if the cluster supports it, otherwise allow Harvest to choose REST or ZAPI, whichever is appropriate to the ONTAP version. See rest-strategy for details. <code>conf_path</code> optional, <code>:</code> separated list of directories The search path Harvest uses to load its templates. Harvest walks each directory in order, stopping at the first one that contains the desired template. conf <code>recorder</code> optional, section Section that determines if Harvest should record or replay HTTP requests. See here for details. <code>pool</code> optional, section Section that determines if Harvest should limit the number of concurrent collectors. See here for details."},{"location":"configure-harvest-basic/#defaults","title":"Defaults","text":"<p>This section is optional. If there are parameters identical for all your pollers (e.g., datacenter, authentication method, login preferences), they can be grouped under this section. The poller section will be checked first, and if the values aren't found there, the defaults will be consulted.</p>"},{"location":"configure-harvest-basic/#exporters","title":"Exporters","text":"<p>All exporters need two types of parameters:</p> <ul> <li><code>exporter parameters</code> - defined in <code>harvest.yml</code> under <code>Exporters</code> section</li> <li><code>export_options</code> - these options are defined in the <code>Matrix</code> data structure emitted from collectors and   plugins</li> </ul> <p>The following two parameters are required for all exporters:</p> parameter type description default Exporter name (header) required Name of the exporter instance, this is a user-defined value <code>exporter</code> required Name of the exporter class (e.g. Prometheus, InfluxDB, Http) - these can be found under the <code>cmd/exporters/</code> directory <p>Note: when we talk about the Prometheus Exporter or InfluxDB Exporter, we mean the Harvest modules that send the data to a database, NOT the names used to refer to the actual databases.</p>"},{"location":"configure-harvest-basic/#prometheus-exporter","title":"Prometheus Exporter","text":""},{"location":"configure-harvest-basic/#influxdb-exporter","title":"InfluxDB Exporter","text":""},{"location":"configure-harvest-basic/#victoriametrics-exporter","title":"VictoriaMetrics Exporter","text":""},{"location":"configure-harvest-basic/#tools","title":"Tools","text":"<p>This section is optional. You can uncomment the <code>grafana_api_token</code> key and add your Grafana API token so <code>harvest</code> does not prompt you for the key when importing dashboards.</p> <pre><code>Tools:\n  #grafana_api_token: 'aaa-bbb-ccc-ddd'\n</code></pre>"},{"location":"configure-harvest-basic/#poller_files","title":"Poller_files","text":"<p>Harvest supports loading pollers from multiple files specified in the <code>Poller_files</code> section of your <code>harvest.yml</code> file. For example, the following snippet tells harvest to load pollers from all the <code>*.yml</code> files under the <code>configs</code> directory,  and from the <code>path/to/single.yml</code> file.</p> <p>Paths may be relative or absolute.</p> <pre><code>Poller_files:\n    - configs/*.yml\n    - path/to/single.yml\n\nPollers:\n    u2:\n        datacenter: dc-1\n</code></pre> <p>Each referenced file can contain one or more unique pollers. Ensure that you include the top-level <code>Pollers</code> section in these files. All other top-level sections will be ignored. For example:</p> <pre><code># contents of configs/00-rtp.yml\nPollers:\n  ntap3:\n    datacenter: rtp\n\n  ntap4:\n    datacenter: rtp\n---\n# contents of configs/01-rtp.yml\nPollers:\n  ntap5:\n    datacenter: blr\n---\n# contents of path/to/single.yml\nPollers:\n  ntap1:\n    datacenter: dc-1\n\n  ntap2:\n    datacenter: dc-1\n</code></pre> <p>At runtime, all files will be read and combined into a single configuration. The example above would result in the following set of pollers in this order. <pre><code>- u2\n- ntap3\n- ntap4\n- ntap5\n- ntap1\n- ntap2\n</code></pre></p> <p>When using glob patterns, the list of matching paths will be sorted before they are read. Errors will be logged for all duplicate pollers and Harvest will refuse to start.</p>"},{"location":"configure-harvest-basic/#configuring-collectors","title":"Configuring collectors","text":"<p>Collectors are configured by their own configuration files (templates), which are stored in subdirectories in conf/. Most collectors run concurrently and collect a subset of related metrics. For example, node related metrics are grouped together and run independently of the disk-related metrics. Below is a snippet from <code>conf/zapi/default.yaml</code></p> <p>In this example, the <code>default.yaml</code> template contains a list of objects (e.g., Node) that reference sub-templates (e.g., node.yaml). This decomposition groups related metrics together and at runtime, a <code>Zapi</code> collector per object will be created and each of these collectors will run concurrently.</p> <p>Using the snippet below, we expect there to be four <code>Zapi</code> collectors running, each with a different subtemplate and object.</p> <pre><code>collector:          Zapi\nobjects:\n  Node:             node.yaml\n  Aggregate:        aggr.yaml\n  Volume:           volume.yaml\n  SnapMirror:       snapmirror.yaml\n</code></pre> <p>At start-up, Harvest looks for two files (<code>default.yaml</code> and <code>custom.yaml</code>) in the <code>conf</code> directory of the collector (e.g. <code>conf/zapi/default.yaml</code>). The <code>default.yaml</code> is installed by default, while the <code>custom.yaml</code> is an optional file you can create to add new templates.</p> <p>When present, the <code>custom.yaml</code> file will be merged with the <code>default.yaml</code> file. This behavior can be overridden in your <code>harvest.yml</code>, see here for an example.</p> <p>For a list of collector-specific parameters, refer to their individual documentation.</p>"},{"location":"configure-harvest-basic/#zapi-and-zapiperf","title":"Zapi and ZapiPerf","text":""},{"location":"configure-harvest-basic/#rest-and-restperf","title":"Rest and RestPerf","text":""},{"location":"configure-harvest-basic/#ems","title":"EMS","text":""},{"location":"configure-harvest-basic/#storagegrid","title":"StorageGRID","text":""},{"location":"configure-harvest-basic/#unix","title":"Unix","text":""},{"location":"configure-harvest-basic/#labels","title":"Labels","text":"<p>Labels offer a way to add additional key-value pairs to a poller's metrics. These allow you to tag a cluster's metrics in a cross-cutting fashion. Here's an example:</p> <pre><code>  cluster-03:\n    datacenter: DC-01\n    addr: 10.0.1.1\n    labels:\n      - org: meg       # add an org label with the value \"meg\"\n      - ns:  rtp       # add a namespace label with the value \"rtp\"\n</code></pre> <p>These settings add two key-value pairs to each metric collected from <code>cluster-03</code> like this:</p> <pre><code>node_vol_cifs_write_data{org=\"meg\",ns=\"rtp\",datacenter=\"DC-01\",cluster=\"cluster-03\",node=\"umeng-aff300-05\"} 10\n</code></pre> <p>Keep in mind that each unique combination of key-value pairs increases the amount of stored data. Use them sparingly. See PrometheusNaming for details.</p>"},{"location":"configure-harvest-basic/#http-recorder","title":"HTTP Recorder","text":"<p>When troubleshooting, it can be useful to record HTTP requests and responses to disk for later replay.</p> <p>Harvest removes <code>Authorization</code> and <code>Host</code> headers from recorded requests and responses to prevent sensitive information from being stored on disk.</p> <p>The <code>recorder</code> section in the <code>harvest.yml</code> file allows you to configure the HTTP recorder.</p> parameter type description default <code>path</code> string required Path to a directory. Recorded requests and responses will be stored here. Replaying will read the requests and responses from this directory. <code>mode</code> string required <code>record</code> or <code>replay</code> <code>keep_last</code> optional, int When mode is <code>record</code>, the number of records to keep before overwriting 60"},{"location":"configure-harvest-basic/#pool","title":"Pool","text":"<p>By default, Harvest does not limit the number of concurrent collectors. To limit the number of concurrent collectors, use the <code>pool</code> section in the <code>harvest.yaml</code> file.</p> parameter type description <code>limit</code> int required The maximum number of allowed concurrent collectors <p>Here is an example:</p> <pre><code> cluster-03:\n    datacenter: DC-01\n    addr: 10.0.1.1\n    pool:\n      limit: 10 # no more than 10 concurrent collectors will run at a time\n</code></pre>"},{"location":"configure-harvest-basic/#authentication","title":"Authentication","text":"<p>When authenticating with ONTAP and StorageGRID clusters, Harvest supports both client certificates and basic authentication.</p> <p>These methods of authentication are defined in the <code>Pollers</code> or <code>Defaults</code> section of your <code>harvest.yml</code> using one or more of the following parameters.</p> parameter description default Link <code>auth_sytle</code> One of <code>basic_auth</code> or <code>certificate_auth</code> Optional when using <code>credentials_file</code> or <code>credentials_script</code> <code>basic_auth</code> link <code>username</code> Username used for authenticating to the remote system link <code>password</code> Password used for authenticating to the remote system link <code>credentials_file</code> Relative or absolute path to a yaml file that contains cluster credentials link <code>credentials_script</code> External script Harvest executes to retrieve credentials link"},{"location":"configure-harvest-basic/#precedence","title":"Precedence","text":"<p>When multiple authentication parameters are defined at the same time, Harvest tries each method listed below, in the following order, to resolve authentication requests.  The first method that returns a non-empty password stops the search. </p> <p>When these parameters exist in both the <code>Pollers</code> and <code>Defaults</code> section, the <code>Pollers</code> section will be consulted before the <code>Defaults</code>.</p> section parameter <code>Pollers</code> auth_style: <code>certificate_auth</code> <code>Pollers</code> auth_style: <code>basic_auth</code> with username and password <code>Pollers</code> <code>credentials_script</code> <code>Pollers</code> <code>credentials_file</code> <code>Defaults</code> auth_style: <code>certificate_auth</code> <code>Defaults</code> auth_style: <code>basic_auth</code> with username and password <code>Defaults</code> <code>credentials_script</code> <code>Defaults</code> <code>credentials_file</code>"},{"location":"configure-harvest-basic/#credentials-file","title":"Credentials File","text":"<p>If you would rather not list cluster credentials in your <code>harvest.yml</code>, you can use the <code>credentials_file</code> section in your <code>harvest.yml</code> to point to a file that contains the credentials. At runtime, the <code>credentials_file</code> will be read and the included credentials will be used to authenticate with the matching cluster(s).</p> <p>This is handy when integrating with 3rd party credential stores. See #884 for examples.</p> <p>The format of the <code>credentials_file</code> is similar to <code>harvest.yml</code> and can contain multiple cluster credentials.</p> <p>Example:</p> <p>Snippet from <code>harvest.yml</code>:</p> <pre><code>Pollers:\n  cluster1:\n    addr: 10.193.48.11\n    credentials_file: secrets/cluster1.yml\n    exporters:\n      - prom1 \n</code></pre> <p>File <code>secrets/cluster1.yml</code>:</p> <pre><code>Pollers:\n  cluster1:\n    username: harvest\n    password: foo\n</code></pre>"},{"location":"configure-harvest-basic/#credentials-script","title":"Credentials Script","text":"<p>The <code>credentials_script</code> feature allows you to fetch authentication information via an external script. This can be configured in the <code>Pollers</code> section of your <code>harvest.yml</code> file, as shown in the example below.</p> <p>At runtime, Harvest will invoke the script specified in the <code>credentials_script</code> <code>path</code> section. Harvest will call the script with one or two arguments depending on how your poller is configured in the <code>harvest.yml</code> file. The script will be called like this: <code>./script $addr</code> or <code>./script $addr $username</code>.</p> <ul> <li>The first argument <code>$addr</code> is the address of the cluster taken from the <code>addr</code> field under the <code>Pollers</code> section of your <code>harvest.yml</code> file.</li> <li>The second argument <code>$username</code> is the username for the cluster taken from the <code>username</code> field under the <code>Pollers</code> section of your <code>harvest.yml</code> file. If your <code>harvest.yml</code> does not include a username, nothing will be passed.</li> </ul> <p>The script should communicate the credentials to Harvest by writing the response to its standard output (stdout). Harvest supports two output formats from the script: YAML and plain text.</p> <p>When running Harvest inside a container, tools like <code>jq</code> and <code>curl</code> are not available. In such cases, you can use a Go binary as a credential script to fetch authentication information. For details on using a Go binary as a credential script for Harvest container deployment, please refer to the GitHub discussion.</p>"},{"location":"configure-harvest-basic/#yaml-format","title":"YAML format","text":"<p>If the script outputs a YAML object with <code>username</code> and <code>password</code> keys, Harvest will use both the <code>username</code> and <code>password</code> from the output. For example, if the script writes the following, Harvest will use <code>myuser</code> and <code>mypassword</code> for the poller's credentials.    <pre><code>username: myuser\npassword: mypassword\n</code></pre>    If only the <code>password</code> is provided, Harvest will use the <code>username</code> from the <code>harvest.yml</code> file, if available. If your username or password contains spaces, <code>#</code>, or other characters with special meaning in YAML, make sure you quote the value like so:    <code>password: \"my password with spaces\"</code></p> <p>If the script outputs a YAML object containing an <code>authToken</code>, Harvest will use this <code>authToken</code> when communicating with ONTAP or StorageGRID clusters. Harvest will include the <code>authToken</code> in the HTTP request's authorization header using the Bearer authentication scheme.    <pre><code>authToken: eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJEcEVkRmgyODlaTXpYR25OekFvaWhTZ0FaUnBtVlVZSDJ3R3dXb0VIWVE0In0.eyJleHAiOjE3MjE4Mj\n</code></pre>   When using <code>authToken</code>, the <code>username</code> and <code>password</code> fields are ignored if they are present in the script's output.</p>"},{"location":"configure-harvest-basic/#plain-text-format","title":"Plain text format","text":"<p>If the script outputs plain text, Harvest will use the output as the password. The <code>username</code> will be taken from the <code>harvest.yml</code> file, if available.  For example, if the script writes the following to its stdout, Harvest will use the username defined in that poller's section of the <code>harvest.yml</code> and <code>mypassword</code> for the poller's credentials.    <pre><code>mypassword\n</code></pre></p> <p>If the script doesn't finish within the specified <code>timeout</code>, Harvest will terminate the script and any spawned processes.</p> <p>Credential scripts are defined under the <code>credentials_script</code> section within <code>Pollers</code> in your <code>harvest.yml</code>. Below are the options for the <code>credentials_script</code> section:</p> parameter type description default path string Absolute path to the script that takes two arguments: <code>addr</code> and <code>username</code>, in that order. schedule go duration or <code>always</code> Schedule for calling the authentication script. If set to <code>always</code>, the script is called every time a password is requested; otherwise, the previously cached value is used. 24h timeout go duration Maximum time Harvest will wait for the script to finish before terminating it and its descendants. 10s"},{"location":"configure-harvest-basic/#example","title":"Example","text":"<p>Here is an example of how to configure the <code>credentials_script</code> in the <code>harvest.yml</code> file:</p> <pre><code>Pollers:\n  ontap1:\n    datacenter: rtp\n    addr: 10.1.1.1\n    username: admin # Optional: if not provided, the script must return the username\n    collectors:\n      - Rest\n      - RestPerf\n    credentials_script:\n      path: ./get_credentials\n      schedule: 3h\n      timeout: 10s\n</code></pre> <p>In this example, the <code>get_credentials</code> script should be located in the same directory as the <code>harvest.yml</code> file and should be executable. It should output the credentials in either YAML or plain text format. Here are three example scripts:</p> <p><code>get_credentials</code> that outputs username and password in YAML format: <pre><code>#!/bin/bash\ncat &lt;&lt; EOF\nusername: myuser\npassword: mypassword\nEOF\n</code></pre></p> <p><code>get_credentials</code> that outputs authToken in YAML format: <pre><code>#!/bin/bash\n# script requests an access token from the authorization server\n# authorization returns an access token to the script\n# script writes the YAML formatted authToken like so:\ncat &lt;&lt; EOF\nauthToken: $authToken\nEOF\n</code></pre></p> <p>Below are a couple of OAuth2 credential script examples for authenticating with ONTAP or StorageGRID OAuth2-enabled clusters.</p> These are examples that you will need to adapt to your environment. <p>Example OAuth2 script authenticating with the Keycloak auth provider via <code>curl</code>. Uses jq to extract the token. This script outputs the authToken in YAML format.</p> <pre><code>#!/bin/bash\n\nresponse=$(curl --silent \"http://{KEYCLOAK_IP:PORT}/realms/{REALM_NAME}/protocol/openid-connect/token\" \\\n  --header \"Content-Type: application/x-www-form-urlencoded\" \\\n  --data-urlencode \"grant_type=password\" \\\n  --data-urlencode \"username={USERNAME}\" \\\n  --data-urlencode \"password={PASSWORD}\" \\\n  --data-urlencode \"client_id={CLIENT_ID}\" \\\n  --data-urlencode \"client_secret={CLIENT_SECRET}\")\n\naccess_token=$(echo \"$response\" | jq -r '.access_token')\n\ncat &lt;&lt; EOF\nauthToken: $access_token\nEOF\n</code></pre> <p>Example OAuth2 script authenticating with the Auth0 auth provider via <code>curl</code>. Uses jq to extract the token. This script outputs the authToken in YAML format.</p> <pre><code>#!/bin/bash\nresponse=$(curl --silent https://{AUTH0_TENANT_URL}/oauth/token \\\n  --header 'content-type: application/json' \\\n  --data '{\"client_id\":\"{CLIENT_ID}\",\"client_secret\":\"{CLIENT_SECRET}\",\"audience\":\"{ONTAP_CLUSTER_IP}\",\"grant_type\":\"client_credentials\"')\n\naccess_token=$(echo \"$response\" | jq -r '.access_token')\n\ncat &lt;&lt; EOF\nauthToken: $access_token\nEOF\n</code></pre> <p><code>get_credentials</code> that outputs only the password in plain text format: <pre><code>#!/bin/bash\necho \"mypassword\"\n</code></pre></p>"},{"location":"configure-harvest-basic/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Make sure your script is executable</li> <li>If running the poller from a container, ensure that you have mounted the script so that it is available inside the container and that you have updated the path in the <code>harvest.yml</code> file to reflect the path inside the container.</li> <li>If running the poller from a container, ensure that your shebang references an interpreter that exists inside the container. Harvest containers are built from Distroless images, so you may need to use <code>#!/busybox/sh</code>.</li> <li>Ensure the user/group that executes your poller also has read and execute permissions on the script.    One way to test this is to <code>su</code> to the user/group that runs Harvest   and ensure that the <code>su</code>-ed user/group can execute the script too.</li> <li>When your script outputs YAML, make sure it is valid YAML. You can use YAML Lint to check your output.</li> <li>When your script outputs YAML and you want to include debug logging, make sure to redirect the debug output to <code>stderr</code> instead of <code>stdout</code>, or write the debug output as YAML comments prefixed with <code>#.</code></li> </ul>"},{"location":"configure-keyperf/","title":"KeyPerf","text":""},{"location":"configure-keyperf/#keyperf-collector","title":"KeyPerf Collector","text":"<p>The KeyPerf collector is designed to gather performance counters from ONTAP objects that include a statistics field in their REST responses. This collector is an alternative to ZapiPerf and RestPerf collectors when these collectors cannot be used due to unavailable relevant APIs.</p>"},{"location":"configure-keyperf/#target-system","title":"Target System","text":"<p>The KeyPerf collector targets ONTAP systems that support the statistics field in their REST responses.</p>"},{"location":"configure-keyperf/#requirements","title":"Requirements","text":"<p>No additional SDK or software requirements are needed. It is recommended to create a read-only user for Harvest on the ONTAP system. For more details, refer to the prepare monitored clusters documentation.</p>"},{"location":"configure-keyperf/#metrics","title":"Metrics","text":"<p>The KeyPerf collector gathers a dynamic set of performance metrics, including IOPS, latency, and throughput. These metrics are extracted from the statistics fields in the ONTAP REST responses.</p>"},{"location":"configure-keyperf/#parameters","title":"Parameters","text":"<p>The parameters for the KeyPerf collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>KeyPerf configuration file (default: <code>conf/keyperf/default.yaml</code>)</li> <li>Each object has its own configuration file (located in <code>conf/keyperf/$version/</code>)</li> </ul> <p>Except for <code>addr</code> and <code>datacenter</code>, all other parameters of the KeyPerf collector can be defined in any of these three files. Parameters defined in the lower-level file override parameters in the higher-level ones, allowing for individual object configuration or shared parameters across all objects.</p>"},{"location":"configure-keyperf/#collector-configuration-file","title":"Collector Configuration File","text":"<p>This configuration file contains a list of objects that should be collected and the filenames of their templates (explained in the next section).</p> <p>Additionally, this file contains the parameters that are applied as defaults to all objects. As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well.</p> parameter type description default <code>use_insecure_tls</code> bool, optional skip verifying TLS certificate of the target system false <code>client_timeout</code> duration (Go-syntax) how long to wait for server responses 30s <code>latency_io_reqd</code> int, optional threshold of IOPs for calculating latency metrics (latencies based on very few IOPs are unreliable) 0 <code>jitter</code> duration (Go-syntax), optional Each Harvest collector runs independently, which means that at startup, each collector may send its REST queries at nearly the same time. To spread out the collector startup times over a broader period, you can use <code>jitter</code> to randomly distribute collector startup across a specified duration. For example, a <code>jitter</code> of <code>1m</code> starts each collector after a random delay between 0 and 60 seconds. For more details, refer to this discussion. <code>schedule</code> list, required the poll frequencies of the collector/object, should include exactly these three elements in the exact same other: - <code>counter</code> duration (Go-syntax) poll frequency of updating the counter metadata cache 24 hours - <code>data</code> duration (Go-syntax) poll frequency of updating the data cache Note Harvest allows defining poll intervals on sub-second level (e.g. <code>1ms</code>), however keep in mind the following:<ul><li>API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than <code>client_timeout</code>.</li><li>Small poll intervals will create significant workload on the ONTAP system, as many counters are aggregated on-demand.</li><li>Some metric values become less significant if they are calculated for very short intervals (e.g. latencies)</li></ul> 1  minute <p>The template should define objects in the <code>objects</code> section. Example:</p> <pre><code>objects:\n  Aggregate: aggr.yaml\n</code></pre> <p>For each object, we define the filename of the object configuration file. The object configuration files are located in subdirectories matching the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches the version of the target ONTAP system.</p>"},{"location":"configure-keyperf/#object-configuration-file","title":"Object Configuration File","text":"<p>The object configuration file should contain the following parameters:</p> Parameter Type Description Default <code>name</code> string, required Display name of the collector that will collect this object <code>query</code> string, required REST endpoint used to issue a REST request <code>object</code> string, required Short name of the object <code>counters</code> string List of counters to collect (see notes below) <code>plugins</code> list Plugins and their parameters to run on the collected data <code>export_options</code> list Parameters to pass to exporters (see notes below)"},{"location":"configure-keyperf/#template-example","title":"Template Example","text":"<pre><code>name:                     Volume\nquery:                    api/storage/volumes\nobject:                   volume\n\ncounters:\n  - ^^name                                =&gt; volume\n  - ^^svm.name                            =&gt; svm\n  - ^statistics.status                    =&gt; status\n  - ^style                                =&gt; style\n  - statistics.iops_raw.other             =&gt; other_ops\n  - statistics.iops_raw.read              =&gt; read_ops\n  - statistics.iops_raw.total             =&gt; total_ops\n  - statistics.iops_raw.write             =&gt; write_ops\n  - statistics.latency_raw.other          =&gt; other_latency\n  - statistics.latency_raw.read           =&gt; read_latency\n  - statistics.latency_raw.total          =&gt; avg_latency\n  - statistics.latency_raw.write          =&gt; write_latency\n  - statistics.throughput_raw.other       =&gt; other_data\n  - statistics.throughput_raw.read        =&gt; read_data\n  - statistics.throughput_raw.total       =&gt; total_data\n  - statistics.throughput_raw.write       =&gt; write_data\n  - statistics.timestamp(timestamp)       =&gt; timestamp\n  - hidden_fields:\n    - statistics\n  - filter:\n    - statistics.timestamp=!\"-\"\n\nendpoints:\n  - query: api/private/cli/volume\n    counters:\n      - ^^volume                          =&gt; volume\n      - ^^vserver                         =&gt; svm\n      - ^aggr_list                        =&gt; aggr\n      - ^nodes                            =&gt; node\n\nplugins:\n  - Aggregator:\n      # Plugin will create summary/average for each object\n      # Any names after the object names will be treated as label names that will be added to instances\n      - node\n      - svm&lt;&gt;svm_vol\n\nexport_options:\n  instance_keys:\n    - aggr\n    - node\n    - style\n    - svm\n    - volume\n</code></pre>"},{"location":"configure-keyperf/#counters","title":"Counters","text":"<p>The <code>counters</code> section defines the list of counters to be collected. These counters can be labels or numeric metrics. The display name of a counter can be changed using <code>=&gt;</code>.</p>"},{"location":"configure-keyperf/#hidden-fields","title":"Hidden Fields","text":"<p>Some fields are not returned by ONTAP unless explicitly requested. The <code>hidden_fields</code> parameter specifies additional fields to include in the REST response.</p>"},{"location":"configure-keyperf/#filter","title":"Filter","text":"<p>The <code>filter</code> parameter constrains the data returned by the endpoint, allowing for more targeted data retrieval. The filtering uses ONTAP's REST record filtering.</p>"},{"location":"configure-keyperf/#export-options","title":"Export Options","text":"<p>The <code>export_options</code> section defines how to handle the collected data. It includes parameters such as <code>instance_keys</code> and <code>instance_labels</code> to specify which labels to export with the metrics and instance labels.</p>"},{"location":"configure-keyperf/#endpoints","title":"Endpoints","text":"<p>Refer to the endpoints section for more details.</p>"},{"location":"configure-keyperf/#partial-aggregation","title":"Partial Aggregation","text":"<p>For more details about partial aggregation behavior and configuration, see Partial Aggregation.</p>"},{"location":"configure-rest/","title":"REST","text":""},{"location":"configure-rest/#rest-collector","title":"Rest Collector","text":"<p>The Rest collectors uses the REST protocol to collect data from ONTAP systems.</p> <p>The RestPerf collector is an extension of this collector, therefore they share many parameters and configuration settings.</p>"},{"location":"configure-rest/#target-system","title":"Target System","text":"<p>Target system can be cDot ONTAP system. 9.12.1 and after are supported, however the default configuration files may not completely match with all versions. See REST Strategy for more details.</p>"},{"location":"configure-rest/#requirements","title":"Requirements","text":"<p>No SDK or other requirements. It is recommended to create a read-only user for Harvest on the ONTAP system (see prepare monitored clusters for details)</p>"},{"location":"configure-rest/#metrics","title":"Metrics","text":"<p>The collector collects a dynamic set of metrics. ONTAP returns JSON documents and Harvest allows you to define templates to extract values from the JSON document via a dot notation path. You can view ONTAP's full set of REST APIs by visiting <code>https://docs.netapp.com/us-en/ontap-automation/reference/api_reference.html#access-a-copy-of-the-ontap-rest-api-reference-documentation</code></p> <p>As an example, the <code>/api/storage/aggregates</code> endpoint, lists all data aggregates in the cluster. Below is an example response from this endpoint:</p> <pre><code>{\n  \"records\": [\n    {\n      \"uuid\": \"3e59547d-298a-4967-bd0f-8ae96cead08c\",\n      \"name\": \"umeng_aff300_aggr2\",\n      \"space\": {\n        \"block_storage\": {\n          \"size\": 8117898706944,\n          \"available\": 4889853616128\n        }\n      },\n      \"state\": \"online\",\n      \"volume_count\": 36\n    }\n  ]\n}\n</code></pre> <p>The Rest collector will take this document, extract the <code>records</code> section and convert the metrics above into: <code>name</code>, <code>space.block_storage.size</code>, <code>space.block_storage.available</code>, <code>state</code> and <code>volume_count</code>. Metric names will be taken, as is, unless you specify a short display name. See counters for more details.</p>"},{"location":"configure-rest/#parameters","title":"Parameters","text":"<p>The parameters of the collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>Rest configuration file (default: <code>conf/rest/default.yaml</code>)</li> <li>Each object has its own configuration file (located in <code>conf/rest/$version/</code>)</li> </ul> <p>Except for <code>addr</code> and <code>datacenter</code>, all other parameters of the Rest collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level ones. This allows you to configure each object individually, or use the same parameters for all objects.</p> <p>The full set of parameters are described below.</p>"},{"location":"configure-rest/#collector-configuration-file","title":"Collector configuration file","text":"<p>This configuration file contains a list of objects that should be collected and the filenames of their templates ( explained in the next section).</p> <p>Additionally, this file contains the parameters that are applied as defaults to all objects. As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well.</p> parameter type description default <code>client_timeout</code> duration (Go-syntax) how long to wait for server responses 30s <code>jitter</code> duration (Go-syntax), optional Each Harvest collector runs independently, which means that at startup, each collector may send its REST queries at nearly the same time. To spread out the collector startup times over a broader period, you can use <code>jitter</code> to randomly distribute collector startup across a specified duration. For example, a <code>jitter</code> of <code>1m</code> starts each collector after a random delay between 0 and 60 seconds. For more details, refer to this discussion. <code>schedule</code> list, required how frequently to retrieve metrics from ONTAP - <code>counter</code> duration (Go-syntax) poll frequency of updating the counter metadata cache 24 hours - <code>data</code> duration (Go-syntax) how frequently this collector/object should retrieve metrics from ONTAP 3 minutes <p>The template should define objects in the <code>objects</code> section. Example:</p> <pre><code>objects:\n  Aggregate: aggr.yaml\n</code></pre> <p>For each object, we define the filename of the object configuration file. The object configuration files are located in subdirectories matching the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches the version of the target ONTAP system.</p>"},{"location":"configure-rest/#object-configuration-file","title":"Object configuration file","text":"<p>The Object configuration file (\"subtemplate\") should contain the following parameters:</p> parameter type description default <code>name</code> string, required display name of the collector that will collect this object <code>query</code> string, required REST endpoint used to issue a REST request <code>object</code> string, required short name of the object <code>counters</code> string list of counters to collect (see notes below) <code>plugins</code> list plugins and their parameters to run on the collected data <code>export_options</code> list parameters to pass to exporters (see notes below)"},{"location":"configure-rest/#template-example","title":"Template Example:","text":"<pre><code>name:                     Volume\nquery:                    api/storage/volumes\nobject:                   volume\n\ncounters:\n  - ^^name                                        =&gt; volume\n  - ^^svm.name                                    =&gt; svm\n  - ^aggregates.#.name                            =&gt; aggr\n  - ^anti_ransomware.state                        =&gt; antiRansomwareState\n  - ^state                                        =&gt; state\n  - ^style                                        =&gt; style\n  - space.available                               =&gt; size_available\n  - space.overwrite_reserve                       =&gt; overwrite_reserve_total\n  - space.overwrite_reserve_used                  =&gt; overwrite_reserve_used\n  - space.percent_used                            =&gt; size_used_percent\n  - space.physical_used                           =&gt; space_physical_used\n  - space.physical_used_percent                   =&gt; space_physical_used_percent\n  - space.size                                    =&gt; size\n  - space.used                                    =&gt; size_used\n  - hidden_fields:\n      - anti_ransomware.state\n      - space\n  - filter:\n      - name=*harvest*\n\nplugins:\n  - LabelAgent:\n      exclude_equals:\n        - style `flexgroup_constituent`\n\nexport_options:\n  instance_keys:\n    - aggr\n    - style\n    - svm\n    - volume\n  instance_labels:\n    - antiRansomwareState\n    - state\n</code></pre>"},{"location":"configure-rest/#counters","title":"Counters","text":"<p>This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from ONTAP and updated periodically.</p> <p>The display name of a counter can be changed with <code>=&gt;</code> (e.g., <code>space.block_storage.size =&gt; space_total</code>).</p> <p>Counters that are stored as labels will only be exported if they are included in the <code>export_options</code> section.</p> <p>The <code>counters</code> section allows you to specify <code>hidden_fields</code> and <code>filter</code> parameters. Please find the detailed explanation below.</p>"},{"location":"configure-rest/#hidden_fields","title":"Hidden_fields","text":"<p>There are some fields that ONTAP will not return unless you explicitly ask for them, even when using the URL parameter <code>fields=**</code>. <code>hidden_fields</code> is how you tell ONTAP which additional fields it should include in the REST response.</p>"},{"location":"configure-rest/#filter","title":"Filter","text":"<p>The <code>filter</code> is used to constrain the data returned by the endpoint, allowing for more targeted data retrieval. The filtering uses ONTAP's REST record filtering. The example above asks ONTAP to only return records where a volume's name matches <code>*harvest*</code>.</p> <p>If you're familiar with ONTAP's REST record filtering, the example above would become <code>name=*harvest*</code> and appended to the final URL like so:</p> <pre><code>https://CLUSTER_IP/api/storage/volumes?fields=*,anti_ransomware.state,space&amp;name=*harvest*\n</code></pre> <p>Refer to the ONTAP API specification, sections: <code>query parameters</code> and <code>record filtering</code>, for more details.</p>"},{"location":"configure-rest/#export_options","title":"Export_options","text":"<p>Parameters in this section tell the exporters how to handle the collected data.</p> <p>There are two different kinds of time-series that Harvest publishes: metrics and instance labels.</p> <ul> <li>Metrics are numeric data with associated labels (key-value pairs). E.g. <code>volume_read_ops_total{cluster=\"cluster1\", node=\"node1\", volume=\"vol1\"} 123</code>. The <code>volume_read_ops_total</code> metric is exporting three labels: <code>cluster</code>, <code>node</code>, and <code>volume</code> and the metric value is <code>123</code>.</li> <li>Instance labels are named after their associated config object (e.g., <code>volume_labels</code>, <code>qtree_labels</code>, etc.). There will be one instance label for each object instance, and each instance label will contain a set of associated labels (key-value pairs) that are defined in the templates <code>instance_labels</code> parameter. E.g. <code>volume_labels{cluster=\"cluster1\", node=\"node1\", volume=\"vol1\", svm=\"svm1\"} 1</code>. The <code>volume_labels</code> instance label is exporting four labels: <code>cluster</code>, <code>node</code>, <code>volume</code>, and <code>svm</code>. Instance labels always export a metric value of <code>1</code>.</li> </ul> <p>The <code>export_options</code> section allows you to define how to export these time-series.</p> <ul> <li><code>instances_keys</code> (list): display names of labels to export to both metric and instance labels.   For example, if you list the <code>svm</code> counter under <code>instances_keys</code>,   that key-value will be included in all time-series metrics and all instance-labels.</li> <li><code>instance_labels</code> (list): display names of labels to export with the corresponding instance label config object. For example, if you want the <code>volume</code> counter to be exported with the <code>volume_labels</code> instance label, you would list <code>volume</code> in the <code>instance_labels</code> section.</li> <li><code>include_all_labels</code> (bool): exports all labels for all time-series metrics. If there are no metrics defined in the template, this option will do nothing. This option also overrides the previous two parameters. See also collect_only_labels.</li> </ul>"},{"location":"configure-rest/#endpoints","title":"Endpoints","text":"<p>In Harvest REST templates, <code>endpoints</code> are additional queries that enhance the data collected from the main query. The main query, identified by the <code>query</code> parameter, is the primary REST API for data collection. For example, the main query for a <code>disk</code> object is <code>api/storage/disks</code>. This main query collects disk objects from the ONTAP API and converts them into a matrix.</p> <p>Typically <code>endpoints</code> are used to query the private CLI to add metrics that are not available via ONTAP's public REST API. Within the <code>endpoints</code> section of a Harvest REST template, you can define multiple endpoint entries. Each entry supports its own <code>query</code> and associated <code>counters</code>, allowing you to collect additional metrics or labels from various API. These additional metrics or labels are combined with the main matrix via a key. The key is denoted by the <code>^^</code> notation in the counters of both the main query and the <code>endpoints</code>.</p> <p>If the <code>instance_add</code> flag is set to <code>true</code> within an endpoint, new records will be created rather than modifying existing ones. This allows for the collection of additional instances without altering the existing matrix.</p> <p>In the example below, the <code>endpoints</code> section makes an additional query to <code>api/private/cli/disk</code>, which collects metrics such as <code>stats_io_kbps</code>, <code>stats_sectors_read</code>, and <code>stats_sectors_written</code>. The <code>uuid</code> is the key that links the data from the <code>api/storage/disks</code> and <code>api/private/cli/disk</code> API. The <code>type</code> label from the <code>api/private/cli/disk</code> endpoint is included as outlined in the <code>export_options</code>.</p> <pre><code>name:             Disk\nquery:            api/storage/disks\nobject:           disk\n\ncounters:\n  - ^^uid                       =&gt; uuid\n  - ^bay                        =&gt; shelf_bay\n  - ^container_type\n  - ^home_node.name             =&gt; owner_node\n  - ^model\n  - ^name                       =&gt; disk\n  - ^node.name                  =&gt; node\n  - ^node.uuid\n  - ^outage.reason              =&gt; outage\n  - ^serial_number\n  - ^shelf.uid                  =&gt; shelf\n  - ^state\n  - bytes_per_sector            =&gt; bytes_per_sector\n  - sector_count                =&gt; sectors\n  - stats.average_latency       =&gt; stats_average_latency\n  - stats.power_on_hours        =&gt; power_on_hours\n  - usable_size\n\nendpoints:\n  - query: api/private/cli/disk\n    counters:\n      - ^^uid                   =&gt; uuid\n      - ^type\n      - disk_io_kbps_total      =&gt; stats_io_kbps\n      - sectors_read            =&gt; stats_sectors_read\n      - sectors_written         =&gt; stats_sectors_written\n\nplugins:\n  - Disk\n  - LabelAgent:\n      value_to_num:\n        - new_status outage - - `0` #ok_value is empty value, '-' would be converted to blank while processing.\n      join:\n        - index `_` node,disk\n  - MetricAgent:\n      compute_metric:\n        - uptime MULTIPLY stats.power_on_hours 60 60 #convert to second for zapi parity\n\nexport_options:\n  instance_keys:\n    - disk\n    - index\n    - node\n  instance_labels:\n    - container_type\n    - failed\n    - model\n    - outage\n    - owner_node\n    - serial_number\n    - shared\n    - shelf\n    - shelf_bay\n    - type\n</code></pre> <p>Example with <code>instance_add</code></p> <p>In the example below, the use of <code>instance_add</code> is necessary to collect both flexvols and their flexgroup constituents, which cannot be retrieved in a single ONTAP API call. Therefore, two separate API calls are required. Initially, volume data excluding flexgroups is gathered from <code>api/storage/volumes</code> and added to the <code>matrix</code>. The endpoint with <code>instance_add: true</code> enables the collection and addition of flexgroup constituent volumes to the matrix. Subsequently, the endpoint query <code>api/private/cli/volume</code> is used to add <code>aggr</code> and <code>node</code> labels to the data collected from both the main query and the first endpoint query, modifying the matrix with additional details.</p> <p>This example is from <code>conf/keyperf/9.15.0/volume.yaml</code>.</p> <pre><code>name:                     Volume\nquery:                    api/storage/volumes\nobject:                   volume\n\ncounters:\n  - ^^name                                    =&gt; volume\n  - ^^svm.name                                =&gt; svm\n  - ^statistics.status                        =&gt; status\n  - ^style                                    =&gt; style\n  - statistics.iops_raw.other                 =&gt; other_ops\n  - statistics.iops_raw.read                  =&gt; read_ops\n  - statistics.iops_raw.total                 =&gt; total_ops\n  - statistics.iops_raw.write                 =&gt; write_ops\n  - statistics.latency_raw.other              =&gt; other_latency\n  - statistics.latency_raw.read               =&gt; read_latency\n  - statistics.latency_raw.total              =&gt; avg_latency\n  - statistics.latency_raw.write              =&gt; write_latency\n  - statistics.throughput_raw.other           =&gt; other_data\n  - statistics.throughput_raw.read            =&gt; read_data\n  - statistics.throughput_raw.total           =&gt; total_data\n  - statistics.throughput_raw.write           =&gt; write_data\n  - statistics.timestamp(timestamp)           =&gt; timestamp\n  - hidden_fields:\n      - statistics\n  - filter:\n      - statistics.timestamp=!\"-\"\n      - style=!flexgroup     # collected via endpoints\n\nendpoints:\n  - query: api/storage/volumes\n    instance_add: true\n    counters:\n      - ^^name                                =&gt; volume\n      - ^^svm.name                            =&gt; svm\n      - ^statistics.status                    =&gt; status\n      - ^style                                =&gt; style\n      - statistics.iops_raw.other             =&gt; other_ops\n      - statistics.iops_raw.read              =&gt; read_ops\n      - statistics.iops_raw.total             =&gt; total_ops\n      - statistics.iops_raw.write             =&gt; write_ops\n      - statistics.latency_raw.other          =&gt; other_latency\n      - statistics.latency_raw.read           =&gt; read_latency\n      - statistics.latency_raw.total          =&gt; avg_latency\n      - statistics.latency_raw.write          =&gt; write_latency\n      - statistics.throughput_raw.other       =&gt; other_data\n      - statistics.throughput_raw.read        =&gt; read_data\n      - statistics.throughput_raw.total       =&gt; total_data\n      - statistics.throughput_raw.write       =&gt; write_data\n      - statistics.timestamp(timestamp)       =&gt; timestamp\n      - hidden_fields:\n          - statistics\n      - filter:\n          - statistics.timestamp=!\"-\"\n          - is_constituent=true\n\n  - query: api/private/cli/volume\n    counters:\n      - ^^volume                              =&gt; volume\n      - ^^vserver                             =&gt; svm\n      - ^aggr_list                            =&gt; aggr\n      - ^nodes                                =&gt; node\n      - filter:\n          - is_constituent=*\n</code></pre>"},{"location":"configure-rest/#restperf-collector","title":"RestPerf Collector","text":"<p>RestPerf collects performance metrics from ONTAP systems using the REST protocol. The collector is designed to be easily extendable to collect new objects or to collect additional counters from already configured objects.</p> <p>This collector is an extension of the Rest collector. The major difference between them is that RestPerf collects only the performance (<code>perf</code>) APIs. Additionally, RestPerf always calculates final values from the deltas of two subsequent polls.</p>"},{"location":"configure-rest/#metrics_1","title":"Metrics","text":"<p>RestPerf metrics are calculated the same as ZapiPerf metrics. More details about how performance metrics are calculated can be found here.</p>"},{"location":"configure-rest/#parameters_1","title":"Parameters","text":"<p>The parameters of the collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>RestPerf configuration file (default: <code>conf/restperf/default.yaml</code>)</li> <li>Each object has its own configuration file (located in <code>conf/restperf/$version/</code>)</li> </ul> <p>Except for <code>addr</code>, <code>datacenter</code> and <code>auth_style</code>, all other parameters of the RestPerf collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level file. This allows the user to configure each objects individually, or use the same parameters for all objects.</p> <p>The full set of parameters are described below.</p>"},{"location":"configure-rest/#restperf-configuration-file","title":"RestPerf configuration file","text":"<p>This configuration file (the \"template\") contains a list of objects that should be collected and the filenames of their configuration (explained in the next section).</p> <p>Additionally, this file contains the parameters that are applied as defaults to all objects. (As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well).</p> parameter type description default <code>use_insecure_tls</code> bool, optional skip verifying TLS certificate of the target system false <code>client_timeout</code> duration (Go-syntax) how long to wait for server responses 30s <code>latency_io_reqd</code> int, optional threshold of IOPs for calculating latency metrics (latencies based on very few IOPs are unreliable) 0 <code>jitter</code> duration (Go-syntax), optional Each Harvest collector runs independently, which means that at startup, each collector may send its REST queries at nearly the same time. To spread out the collector startup times over a broader period, you can use <code>jitter</code> to randomly distribute collector startup across a specified duration. For example, a <code>jitter</code> of <code>1m</code> starts each collector after a random delay between 0 and 60 seconds. For more details, refer to this discussion. <code>schedule</code> list, required the poll frequencies of the collector/object, should include exactly these three elements in the exact same other: - <code>counter</code> duration (Go-syntax) poll frequency of updating the counter metadata cache 24 hours - <code>data</code> duration (Go-syntax) poll frequency of updating the data cache Note Harvest allows defining poll intervals on sub-second level (e.g. <code>1ms</code>), however keep in mind the following:<ul><li>API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than <code>client_timeout</code>.</li><li>Small poll intervals will create significant workload on the ONTAP system, as many counters are aggregated on-demand.</li><li>Some metric values become less significant if they are calculated for very short intervals (e.g. latencies)</li></ul> 1  minute <p>The template should define objects in the <code>objects</code> section. Example:</p> <pre><code>objects:\n  SystemNode: system_node.yaml\n  HostAdapter: hostadapter.yaml\n</code></pre> <p>Note that for each object we only define the filename of the object configuration file. The object configuration files are located in subdirectories matching to the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches to the version of the target ONTAP system. (A mismatch is tolerated since RestPerf will fetch and validate counter metadata from the system.)</p>"},{"location":"configure-rest/#object-configuration-file_1","title":"Object configuration file","text":"<p>Refer Object configuration file</p>"},{"location":"configure-rest/#counters_1","title":"Counters","text":"<p>See Counters</p> <p>Some counters require a \"base-counter\" for post-processing. If the base-counter is missing, RestPerf will still run, but the missing data won't be exported.</p>"},{"location":"configure-rest/#export_options_1","title":"Export_options","text":"<p>See Export Options</p>"},{"location":"configure-rest/#filter_1","title":"Filter","text":"<p>This guide provides instructions on how to use the <code>filter</code> feature in RestPerf. Filtering is useful when you need to query a subset of instances. For example, suppose you have a small number of high-value volumes from which you want Harvest to collect performance metrics every five seconds. Collecting data from all volumes at this frequency would be too resource-intensive. Therefore, filtering allows you to create/modify a template that includes only the high-value volumes.</p>"},{"location":"configure-rest/#objects-excluding-workload","title":"Objects (Excluding Workload)","text":"<p>In RestPerf templates, you can set up filters under <code>counters</code>. Wildcards like <code>*</code> are useful if you don't want to specify all instances.</p> <p>For instance, to filter <code>volume</code> performance instances by volume name, use the following configuration in RestPerf <code>volume.yaml</code> under <code>counters</code>. This example will return the volumes named <code>NS_svm_nvme</code> or the volumes with <code>Test</code> in their name.</p> <pre><code>counters:\n...\n  - filter:\n    - query=NS_svm_nvme|*Test*\n    - query_fields=properties.value\n</code></pre>"},{"location":"configure-rest/#workload-templates","title":"Workload Templates","text":"<p>Performance workload templates require a different syntax because instances are retrieved from the <code>api/storage/qos/workloads</code> Rest call.</p> <p>The <code>api/storage/qos/workloads</code> Rest supports filtering on the following fields:</p> <ul> <li>name</li> <li>workload_class</li> <li>wid</li> <li>policy.name</li> <li>svm.name</li> <li>volume</li> <li>lun</li> <li>file</li> <li>qtree</li> </ul> <p>You can include these fields under the <code>filter</code> parameter. For example, to filter Workload performance instances by <code>name</code> where the name contains <code>NS</code> or <code>Test</code> and <code>svm</code> is <code>vs1</code>, use the following configuration in RestPerf <code>workload.yaml</code> under <code>counters</code>:</p> <pre><code>counters:\n...\n  - filter:\n    - name=*NS*|*Test*\n    - svm.name=vs1\n</code></pre>"},{"location":"configure-rest/#ontap-private-cli","title":"ONTAP Private CLI","text":"<p>The ONTAP private CLI allows for more granular control and access to non-public counters. It can be used to fill gaps in the REST API, especially in cases where certain data is not yet available through the REST API. Harvest's REST collector can make full use of ONTAP's private CLI. This means when ONTAP's public REST API is missing counters, Harvest can still collect them as long as those counters are available via ONTAP's CLI.</p> <p>For more information on using the ONTAP private CLI with the REST API, you can refer to the following resources:</p> <ul> <li>NetApp Documentation: Accessing ONTAP CLI through REST APIs</li> <li>NetApp Blog: Private CLI Passthrough with ONTAP REST API</li> </ul>"},{"location":"configure-rest/#creating-templates-that-use-ontaps-private-cli","title":"Creating Templates That Use ONTAP's Private CLI","text":"<p>Let's take an example of how we can make Harvest use the <code>system fru-check show</code> CLI command.</p> <pre><code>system fru-check show\n</code></pre> <p>REST APIs endpoint:</p> <pre><code>/api/private/cli/system/fru-check?fields=node,fru_name,fru_status\n</code></pre> <p>Converting the CLI command <code>system fru-check show</code> for use with a private CLI REST API can be achieved by adhering to the path rules outlined in the ONTAP documentation. Generally, this involves substituting all spaces within the CLI command with a forward slash (/), and converting the ONTAP CLI verb into the corresponding REST verb.</p> <p>The <code>show</code> command gets converted to the HTTP method GET call. From the CLI, look at the required field names and pass them as a comma-separated value in <code>fields=</code> in the API endpoint.</p> <p>Note: If the field name contains a hyphen (<code>-</code>), it should be converted to an underscore (<code>_</code>) in the REST API field. For example, <code>fru-name</code> becomes <code>fru_name</code>. ONTAP is flexible with the input format and can freely convert between hyphen (<code>-</code>) and underscore (<code>_</code>) forms. However, when it comes to output, ONTAP returns field names with underscores. For compatibility and consistency, it is mandatory to use underscores in field names when working with Harvest REST templates for ONTAP private CLI.</p>"},{"location":"configure-rest/#advanced-and-diagnostic-mode-commands","title":"Advanced and Diagnostic Mode Commands","text":"<p>The CLI pass through allows you to execute advanced and diagnostic mode CLI commands by including the <code>privilege_level</code> field in your request under the <code>filter</code> setting like so: <pre><code>counters:\n  - filter:\n      - privilege_level=diagnostic\n</code></pre></p>"},{"location":"configure-rest/#creating-a-harvest-template-for-private-cli","title":"Creating a Harvest Template for Private CLI","text":"<p>Here's a Harvest template that uses ONTAP's private CLI to collect field-replaceable units (FRU) counters by using ONTAP's CLI command <code>system fru-check show</code></p> <pre><code>name:                         FruCheck\nquery:                        api/private/cli/system/fru-check\nobject:                       fru_check\n\ncounters:\n  - ^^node\n  - ^^serial_number              =&gt; serial_number\n  - ^fru_name                    =&gt; name\n  - ^fru_status                  =&gt; status\n\nexport_options:\n  instance_keys:\n    - node\n    - serial_number\n  instance_labels:\n    - name\n    - status\n</code></pre> <p>In this template, the <code>query</code> field specifies the private CLI command to be used (<code>system fru-check show</code>). The <code>counters</code> field maps the output of the private CLI command to the fields of the <code>fru_check</code> object. To identify the ONTAP counter names (the left side of the '=&gt;' symbol in the template, such as <code>fru_name</code>), you can establish an SSH connection to your ONTAP cluster. Once connected, leverage ONTAP's command completion functionality to reveal the counter names. For instance, you can type <code>system fru-check show -fields</code>, then press the '?' key. This will display a list of ONTAP field names, as demonstrated below.</p> <pre><code>cluster-01::&gt; system fru-check show -fields ?\n  node                        Node\n  serial-number               FRU Serial Number\n  fru-name                    FRU Name\n  fru-type                    FRU Type\n  fru-status                  Status\n  display-name                Display Name\n  location                    Location\n  additional-info             Additional Info\n  reason                      Details\n</code></pre> <p>The <code>export_options</code> field specifies how the data should be exported. The <code>instance_keys</code> field lists the fields that will be added as labels to all exported instances of the <code>fru_check</code> object. The <code>instance_labels</code> field lists the fields that should be included as labels in the exported data.</p> <p>The output of this template would look like:</p> <pre><code>fru_check_labels{cluster=\"umeng-aff300-01-02\",datacenter=\"u2\",name=\"DIMM-1\",node=\"umeng-aff300-02\",serial_number=\"s2\",status=\"pass\"} 1.0\nfru_check_labels{cluster=\"umeng-aff300-01-02\",datacenter=\"u2\",name=\"PCIe Devices\",node=\"umeng-aff300-02\",serial_number=\"s1\",status=\"pass\"} 1.0\n</code></pre>"},{"location":"configure-rest/#partial-aggregation","title":"Partial Aggregation","text":"<p>For more details about partial aggregation behavior and configuration, see Partial Aggregation.</p>"},{"location":"configure-statperf/","title":"StatPerf Collector","text":"<p>StatPerf collects performance metrics from ONTAP by invoking the ONTAP CLI statistics command via the private Rest CLI. The full ONTAP CLI command used is:</p> <pre><code>statistics show -raw -object $object\n</code></pre> <p>This collector is designed for performance metrics collection in environments where the ZapiPerf/RestPerf/KeyPerf collectors can not be used.</p> <p>Note: The StatPerf collector requires additional ONTAP permissions. Please refer to the StatPerf Collector Permissions section for details. If you are using multi-admin verification in your cluster, you need to allow diagnostic mode queries in the MAV rules for the StatPerf collector to run. For more details, refer to the Multi-Admin Verification documentation.</p>"},{"location":"configure-statperf/#metrics","title":"Metrics","text":"<p>StatPerf metrics are calculated similar to those in ZapiPerf. For details on how performance metrics are processed and calculated, please refer to the ZapiPerf Metrics Documentation.</p>"},{"location":"configure-statperf/#parameters","title":"Parameters","text":"<p>The parameters for the StatPerf collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>StatPerf configuration file (default: <code>conf/statperf/default.yaml</code>)</li> <li>Object configuration file (located in <code>conf/statperf/$version/</code>)</li> </ul> <p>Except for <code>addr</code>, <code>datacenter</code> and <code>auth_style</code>, all other parameters of the StatPerf collector can be defined in either the Harvest configuration, the main StatPerf configuration file, or the object configuration files. Lower-level file definitions override higher-level ones. This enables configuring objects on an individual basis or applying common settings across all objects.</p>"},{"location":"configure-statperf/#statperf-configuration-file","title":"StatPerf Configuration File","text":"<p>The StatPerf configuration file (also known as the \"template\") includes a list of objects to collect and their corresponding configuration file names. Additionally, it defines parameters applied as defaults to all objects. As with RestPerf, parameters defined in lower-level files (object or Harvest configuration files) will override the ones provided here.</p> Parameter Type Description Default <code>use_insecure_tls</code> bool, optional Skip verifying the TLS certificate of the target system. false <code>client_timeout</code> duration (Go-syntax) Maximum time to wait for server responses. 30s <code>latency_io_reqd</code> int, optional Threshold of IOPs for calculating latency metrics; latencies based on very few IOPs are unreliable. 0 <code>jitter</code> duration (Go-syntax), optional Randomly delay collector startup by up to the specified duration to prevent simultaneous REST queries during startup. For example, a jitter value of <code>1m</code> will delay startup by a random duration between 0 seconds and 60 seconds. For more details, see this discussion. <code>schedule</code> list, required Specifies the polling frequencies, which must include exactly these three elements in the exact specified order: - <code>counter</code> duration (Go-syntax) Poll frequency for updating the counter metadata cache. 24 hours - <code>instance</code> duration (Go-syntax) Poll frequency for updating the instance cache. 10 minutes - <code>data</code> duration (Go-syntax) Poll frequency for updating the data cache. Note that while Harvest allows sub-second poll intervals (e.g. <code>1ms</code>), factors such as API response times and system load should be considered. In short intervals, performance counters may not be aggregated accurately, potentially leading to a failed state in the collector if the poll interval is less than <code>client_timeout</code>. Additionally, very short intervals may cause heavier loads on the ONTAP system and lead to less meaningful metric values (e.g. for latencies). 1 minute <p>The template should list objects in the <code>objects</code> section. For example:</p> <pre><code>objects:\n   Flexcache:   flexcache.yaml\n</code></pre> <p>Each object is defined by the filename of its configuration file. The file is located in the subdirectory corresponding to the ONTAP version (e.g., <code>conf/statperf/$version/</code>). At runtime, StatPerf selects the object configuration file that best matches the ONTAP system version. In case of mismatches, StatPerf will still fetch and validate counter metadata from the system.</p>"},{"location":"configure-statperf/#object-configuration-file","title":"Object Configuration File","text":"<p>The object configuration file allows users to specify object-level parameters for StatPerf. It follows the same concept as other collectors and includes details such as instances, counters, and export options.</p> <p>For further details, please refer to the guiding documentation below:</p> <ul> <li>Object Configuration File</li> <li>Counters</li> <li>Export Options</li> </ul>"},{"location":"configure-statperf/#template-example","title":"Template Example","text":"<pre><code>name:                     FlexCache\nquery:                    flexcache_per_volume\nobject:                   flexcache\n\ncounters:\n  - ^^instance_name                                     =&gt; volume\n  - ^^instance_uuid                                     =&gt; svm\n  - blocks_requested_from_client\n  - blocks_retrieved_from_origin\n  - evict_rw_cache_skipped_reason_disconnected\n  - evict_skipped_reason_config_noent\n  - evict_skipped_reason_disconnected\n  - evict_skipped_reason_offline\n  - invalidate_skipped_reason_config_noent\n  - invalidate_skipped_reason_disconnected\n  - invalidate_skipped_reason_offline\n  - nix_retry_skipped_reason_initiator_retrieve\n  - nix_skipped_reason_config_noent\n  - nix_skipped_reason_disconnected\n  - nix_skipped_reason_in_progress\n  - nix_skipped_reason_offline\n  - reconciled_data_entries\n  - reconciled_lock_entries\n\nplugins:\n  - FlexCache\n  - MetricAgent:\n      compute_metric:\n        - miss_percent PERCENT blocks_retrieved_from_origin blocks_requested_from_client\n\nexport_options:\n  instance_keys:\n    - svm\n    - volume\n</code></pre>"},{"location":"configure-statperf/#filter","title":"Filter","text":"<p>This guide provides instructions on how to use the <code>filter</code> feature in StatPerf. Filtering is useful when you need to query a subset of instances. For example, suppose you have a small number of high-value volumes from which you want Harvest to collect performance metrics every five seconds. Collecting data from all volumes at this frequency would be too resource-intensive. Therefore, filtering allows you to create/modify a template that includes only the high-value volumes.</p> <p>In StatPerf templates, you can set up filters under the <code>counters</code> section using the <code>filter</code> key as shown below.</p> <p>To filter <code>volume</code> performance instances by instance name, where the name is either <code>NS_svm_nvme</code> or contains <code>Test</code>, and the <code>vserver_name</code> is <code>osc</code>, use the following configuration in the <code>volume.yaml</code> file under the <code>counters</code> section of StatPerf:</p> <pre><code>counters:\n  ...\n  - filter:\n     - instance_name=NS_svm_nvme|instance_name=*Test*\n     - vserver_name=osc\n</code></pre>"},{"location":"configure-statperf/#partial-aggregation","title":"Partial Aggregation","text":"<p>For more details about partial aggregation behavior and configuration, see Partial Aggregation.</p>"},{"location":"configure-storagegrid/","title":"StorageGRID","text":""},{"location":"configure-storagegrid/#storagegrid-collector","title":"StorageGRID Collector","text":"<p>The StorageGRID collector uses REST calls to collect data from StorageGRID systems.</p>"},{"location":"configure-storagegrid/#target-system","title":"Target System","text":"<p>All StorageGRID versions are supported, however the default configuration files may not completely match with older systems.</p>"},{"location":"configure-storagegrid/#requirements","title":"Requirements","text":"<p>No SDK or other requirements. It is recommended to create a read-only user for Harvest on the StorageGRID system (see prepare monitored clusters for details)</p>"},{"location":"configure-storagegrid/#metrics","title":"Metrics","text":"<p>The collector collects a dynamic set of metrics via StorageGRID's REST API. StorageGRID returns JSON documents and Harvest allows you to define templates to extract values from the JSON document via a dot notation path. You can view StorageGRID's full set of REST APIs by visiting <code>https://$STORAGE_GRID_HOSTNAME/grid/apidocs.html</code></p> <p>As an example, the <code>/grid/accounts-cache</code> endpoint, lists the tenant accounts in the cache and includes additional information, such as objectCount and dataBytes. Below is an example response from this endpoint:</p> <pre><code>{\n  \"data\": [\n    {\n      \"id\": \"95245224059574669217\",\n      \"name\": \"foople\",\n      \"policy\": {\n        \"quotaObjectBytes\": 50000000000\n      },\n      \"objectCount\": 6,\n      \"dataBytes\": 10473454261\n    }\n  ]\n}\n</code></pre> <p>The StorageGRID collector will take this document, extract the <code>data</code> section and convert the metrics above into: <code>name</code>, <code>policy.quotaObjectBytes</code>, <code>objectCount</code>, and <code>dataBytes</code>. Metric names will be taken, as is, unless you specify a short display name. See counters for more details.</p>"},{"location":"configure-storagegrid/#parameters","title":"Parameters","text":"<p>The parameters of the collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>StorageGRID configuration file (default: <code>conf/storagegrid/default.yaml</code>)</li> <li>Each object has its own configuration file (located in <code>conf/storagegrid/$version/</code>)</li> </ul> <p>Except for <code>addr</code> and <code>datacenter</code>, all other parameters of the StorageGRID collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level ones. This allows you to configure each object individually, or use the same parameters for all objects.</p> <p>The full set of parameters are described below.</p>"},{"location":"configure-storagegrid/#harvest-configuration-file","title":"Harvest configuration file","text":"<p>Parameters in the poller section should define the following required parameters.</p> parameter type description default Poller name (header) string, required Poller name, user-defined value <code>addr</code> string, required IPv4, IPv6, or FQDN of the target system. To specify a custom port, use the format <code>&lt;host&gt;:&lt;port&gt;</code>. Example: <code>storagegrid.example.com:8080</code> <code>datacenter</code> string, required Datacenter name, user-defined value <code>username</code>, <code>password</code> string, required StorageGRID username and password with at least <code>Tenant accounts</code> permissions <code>collectors</code> list, required Name of collector to run for this poller, use <code>StorageGrid</code> for this collector"},{"location":"configure-storagegrid/#storagegrid-configuration-file","title":"StorageGRID configuration file","text":"<p>This configuration file contains a list of objects that should be collected and the filenames of their templates ( explained in the next section).</p> <p>Additionally, this file contains the parameters that are applied as defaults to all objects. As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well.</p> parameter type description default <code>client_timeout</code> duration (Go-syntax) how long to wait for server responses 30s <code>schedule</code> list, required how frequently to retrieve metrics from StorageGRID - <code>data</code> duration (Go-syntax) how frequently this collector/object should retrieve metrics from StorageGRID 5 minutes <code>only_cluster_instance</code> bool, optional don't require instance key. assume the only instance is the cluster itself <p>The template should define objects in the <code>objects</code> section. Example:</p> <pre><code>objects:\n  Tenant: tenant.yaml\n</code></pre> <p>For each object, we define the filename of the object configuration file. The object configuration files are located in subdirectories matching the StorageGRID version that was used to create these files. It is possible to have multiple version-subdirectories for multiple StorageGRID versions. At runtime, the collector will select the object configuration file that closest matches the version of the target StorageGRID system.</p>"},{"location":"configure-storagegrid/#object-configuration-file","title":"Object configuration file","text":"<p>The Object configuration file (\"subtemplate\") should contain the following parameters:</p> parameter type description default <code>name</code> string, required display name of the collector that will collect this object <code>query</code> string, required REST endpoint used to issue a REST request <code>object</code> string, required short name of the object <code>api</code> string StorageGRID REST endpoint version to use, overrides default management API version 3 <code>counters</code> list list of counters to collect (see notes below) <code>plugins</code> list plugins and their parameters to run on the collected data <code>export_options</code> list parameters to pass to exporters (see notes below)"},{"location":"configure-storagegrid/#counters","title":"Counters","text":"<p>This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from StorageGRID and updated periodically.</p> <p>The display name of a counter can be changed with <code>=&gt;</code> (e.g., <code>policy.quotaObjectBytes =&gt; logical_quota</code>).</p> <p>Counters that are stored as labels will only be exported if they are included in the <code>export_options</code> section.</p>"},{"location":"configure-storagegrid/#export_options","title":"Export_options","text":"<p>Parameters in this section tell the exporters how to handle the collected data.</p> <p>There are two different kinds of time-series that Harvest publishes: metrics and instance labels.</p> <ul> <li>Metrics are numeric data with associated labels (key-value pairs). E.g. <code>volume_read_ops_total{cluster=\"cluster1\", node=\"node1\", volume=\"vol1\"} 123</code>. The <code>volume_read_ops_total</code> metric is exporting three labels: <code>cluster</code>, <code>node</code>, and <code>volume</code> and the metric value is <code>123</code>.</li> <li>Instance labels are named after their associated config object (e.g., <code>volume_labels</code>, <code>qtree_labels</code>, etc.). There will be one instance label for each object instance, and each instance label will contain a set of associated labels (key-value pairs) that are defined in the templates <code>instance_labels</code> parameter. E.g. <code>volume_labels{cluster=\"cluster1\", node=\"node1\", volume=\"vol1\", svm=\"svm1\"} 1</code>. The <code>volume_labels</code> instance label is exporting four labels: <code>cluster</code>, <code>node</code>, <code>volume</code>, and <code>svm</code>. Instance labels always export a metric value of <code>1</code>.</li> </ul> <p>The <code>export_options</code> section allows you to define how to export these time-series.</p> <ul> <li><code>instances_keys</code> (list): display names of labels to export to both metric and instance labels.   For example, if you list the <code>svm</code> counter under <code>instances_keys</code>,   that key-value will be included in all time-series metrics and all instance-labels.</li> <li><code>instance_labels</code> (list): display names of labels to export with the corresponding instance label config object. For example, if you want the <code>volume</code> counter to be exported with the <code>volume_labels</code> instance label, you would list <code>volume</code> in the <code>instance_labels</code> section.</li> <li><code>include_all_labels</code> (bool): exports all labels for all time-series metrics. If there are no metrics defined in the template, this option will do nothing. This option also overrides the previous two parameters. See also collect_only_labels.</li> </ul>"},{"location":"configure-templates/","title":"Templates","text":""},{"location":"configure-templates/#templates-customization","title":"Templates customization","text":"<p>This document covers how to use Collector and Object templates to extend Harvest. Customization of templates should use the <code>custom.yaml</code> approach to avoid overwriting your changes when Harvest is upgraded, or you rerun the <code>generate</code> command.</p> <ol> <li>How to add a new object template via custom.yaml</li> <li>How to extend an existing object template via custom.yaml</li> </ol> <p>There are a couple of ways to learn about ZAPIs and their attributes:</p> <ul> <li>ONTAP's documentation</li> <li>Using Harvest's <code>zapi</code> tool to explore available APIs and metrics on your cluster. Examples:</li> </ul> <pre><code>$ harvest zapi --poller &lt;poller&gt; show apis\n  # will print list of apis that are available\n  # usually apis with the \"get-iter\" suffix can provide useful metrics\n$ harvest zapi --poller &lt;poller&gt; show attrs --api volume-get-iter\n  # will print the attribute tree of the API\n$ harvest zapi --poller &lt;poller&gt; show data --api volume-get-iter\n  # will print raw data of the API attribute tree\n</code></pre> <p>(Replace <code>&lt;poller&gt;</code> with the name of a poller that can connect to an ONTAP system.)</p>"},{"location":"configure-templates/#conf-path","title":"Conf Path","text":"<p>The conf path is the colon-separated list of directories that Harvest searches to load templates.  Harvest walks each directory in order, stopping at the first one that contains the desired template. The default value of <code>confpath</code> is <code>conf</code>, meaning that only the <code>conf</code> directory is searched for templates.</p> <p>There are two ways to change the conf path. </p> <ul> <li> <p>You can specify the <code>-confpath</code> command line argument to <code>bin/harvest</code> or <code>bin/poller</code>, e.g. <code>-confpath customconf:conf</code>. Harvest will search the <code>customconf</code> directory followed by the <code>conf</code> directory.</p> </li> <li> <p>You can specify the <code>conf_path</code> parameter in the <code>Pollers</code> section of your <code>harvest.yml</code> file, e.g.</p> </li> </ul> <pre><code>Pollers:\n  netapp-cluster1: \n    datacenter: dc-1\n    addr: 10.193.48.163\n    conf_path: customconf:/etc/harvest/conf:conf\n</code></pre> <p>This <code>conf_path</code> example will search for templates in this order, stopping at the first one that contains the template.</p> <ol> <li>local directory <code>customconf</code></li> <li>absolute directory <code>/etc/harvest/conf</code></li> <li>local directory <code>conf</code></li> </ol> <p>Use the conf path to isolate your edits and extensions to Harvest's builtin templates. This ensures that your customizations won't be affected when you upgrade Harvest.</p> <p>When using a custom confpath, make sure your custom directories have the same structure as the default <code>conf</code> directory.  In the example below, four template modifications have been setup in the <code>/etc/harvest/customconf</code> directory.</p> <p>The poller's <code>conf_path</code> parameter is set to <code>/etc/harvest/customconf:conf</code> to use these modified templates.  Harvest will use the custom templates when they match and the default templates otherwise.</p> <p>See issue #2330 for more examples.</p> <pre><code># tree /etc/harvest/customconf\n\n/etc/harvest/customconf\n\u251c\u2500\u2500 rest\n\u2502   \u251c\u2500\u2500 9.12.0\n\u2502   \u2502 \u251c\u2500\u2500 aggr.yaml\n\u2502   \u2502 \u2514\u2500\u2500 volume.yaml\n\u251c\u2500\u2500 restperf\n\u2502   \u251c\u2500\u2500 9.13.0\n\u2502   \u2502 \u2514\u2500\u2500 qtree.yaml\n\u251c\u2500\u2500 zapi\n\u2514\u2500\u2500 zapiperf\n    \u251c\u2500\u2500 cdot\n    \u2502 \u2514\u2500\u2500 9.8.0\n    \u2502     \u2514\u2500\u2500 qtree.yaml\n</code></pre>"},{"location":"configure-templates/#collector-templates","title":"Collector templates","text":"<p>Collector templates define which set of objects Harvest should collect from the system being monitored. In your <code>harvest.yml</code> configuration file, when you say that you want to use a <code>Zapi</code> collector, that collector will read the matching <code>conf/zapi/default.yaml</code> - same with <code>ZapiPerf</code>, it will read the <code>conf/zapiperf/default.yaml</code> file. Below is a snippet from <code>conf/zapi/default.yaml</code>. Each object is mapped to a corresponding object template file. For example, the <code>Node</code> object searches for the most appropriate version of the <code>node.yaml</code> file in the <code>conf/zapi/cdot/**</code> directory.</p> <pre><code>collector:          Zapi\nobjects:\n  Node:             node.yaml\n  Aggregate:        aggr.yaml\n  Volume:           volume.yaml\n  Disk:             disk.yaml\n</code></pre> <p>Each collector will also check if a matching file named, <code>custom.yaml</code> exists, and if it does, it will read that file and merge it with <code>default.yaml</code>. The <code>custom.yaml</code> file should be located beside the matching <code>default.yaml</code> file. ( eg. <code>conf/zapi/custom.yaml</code> is beside <code>conf/zapi/default.yaml</code>).</p> <p>Let's take a look at some examples.</p> <ol> <li>Define a poller that uses the default Zapi collector. Using the default template is the easiest and most used option.</li> </ol> <pre><code>Pollers:\n  jamaica:\n    datacenter: munich\n    addr: 10.10.10.10\n    collectors:\n      - Zapi # will use conf/zapi/default.yaml and optionally merge with conf/zapi/custom.yaml\n</code></pre> <ol> <li>Define a poller that uses the Zapi collector, but with a custom template file:</li> </ol> <pre><code>Pollers:\n  jamaica:\n    datacenter: munich\n    addr: 10.10.10.10\n    collectors:\n      - ZapiPerf:\n          - limited.yaml # will use conf/zapiperf/limited.yaml\n        # more templates can be added, they will be merged\n</code></pre>"},{"location":"configure-templates/#object-templates","title":"Object Templates","text":"<p>Object templates (example: <code>conf/zapi/cdot/9.8.0/lun.yaml</code>) describe what to collect and export. These templates are used by collectors to gather metrics and send them to your time-series db.</p> <p>Object templates are made up of the following parts:</p> <ol> <li>the name of the object (or resource) to collect</li> <li>the ZAPI or REST query used to collect the object</li> <li>a list of object counters to collect and how to export them</li> </ol> <p>Instead of editing one of the existing templates, it's better to extend one of them. That way, your custom template will not be overwritten when upgrading Harvest. For example, if you want to extend <code>conf/zapi/cdot/9.8.0/aggr.yaml</code>, first create a copy (e.g., <code>conf/zapi/cdot/9.8.0/custom_aggr.yaml</code>), and then tell Harvest to use your custom template by adding these lines to <code>conf/zapi/custom.yaml</code>:</p> <pre><code>objects:\n  Aggregate: custom_aggr.yaml\n</code></pre> <p>After restarting your pollers, <code>aggr.yaml</code> and <code>custom_aggr.yaml</code> will be merged.</p>"},{"location":"configure-templates/#create-a-new-object-template","title":"Create a new object template","text":"<p>In this example, imagine that Harvest doesn't already collect environment sensor data, and you wanted to collect it. Sensor does come from the <code>environment-sensors-get-iter</code> ZAPI. Here are the steps to add a new object template.</p> <p>Create the file <code>conf/zapi/cdot/9.8.0/sensor.yaml</code> (optionally replace <code>9.8.0</code> with the earliest version of ONTAP that supports sensor data. Refer to Harvest Versioned Templates for more information. Add the following content to your new <code>sensor.yaml</code> file.</p> <pre><code>name: Sensor                      # this name must match the key in your custom.yaml file\nquery: environment-sensors-get-iter\nobject: sensor\n\nmetric_type: int64\n\ncounters:\n  environment-sensors-info:\n    - critical-high-threshold    =&gt; critical_high\n    - critical-low-threshold     =&gt; critical_low\n    - ^discrete-sensor-state     =&gt; discrete_state\n    - ^discrete-sensor-value     =&gt; discrete_value\n    - ^^node-name                =&gt; node\n    - ^^sensor-name              =&gt; sensor\n    - ^sensor-type               =&gt; type\n    - ^threshold-sensor-state    =&gt; threshold_state\n    - threshold-sensor-value     =&gt; threshold_value\n    - ^value-units               =&gt; unit\n    - ^warning-high-threshold    =&gt; warning_high\n    - ^warning-low-threshold     =&gt; warning_low\n\nexport_options:\n  include_all_labels: true\n</code></pre>"},{"location":"configure-templates/#enable-the-new-object-template","title":"Enable the new object template","text":"<p>To enable the new sensor object template, create the <code>conf/zapi/custom.yaml</code> file with the lines shown below.</p> <pre><code>objects:\n  Sensor: sensor.yaml                 # this key must match the name in your sensor.yaml file\n</code></pre> <p>The <code>Sensor</code> key used in the <code>custom.yaml</code> must match the name defined in the <code>sensor.yaml</code> file. That mapping is what connects this object with its template. In the future, if you add more object templates, you can add those in your existing <code>custom.yaml</code> file.</p>"},{"location":"configure-templates/#test-your-object-template-changes","title":"Test your object template changes","text":"<p>Test your new <code>Sensor</code> template with a single poller like this:</p> <pre><code>./bin/harvest start &lt;poller&gt; --foreground --verbose --collectors Zapi --objects Sensor\n</code></pre> <p>Replace <code>&lt;poller&gt;</code> with the name of one of your ONTAP pollers.</p> <p>Once you have confirmed that the new template works, restart any already running pollers that you want to use the new template(s).</p>"},{"location":"configure-templates/#check-the-metrics","title":"Check the metrics","text":"<p>If you are using the Prometheus exporter, you can scrape the poller's HTTP endpoint with curl or a web browser. E.g., my poller exports its data on port 15001. Adjust as needed for your exporter.</p> <pre><code>curl -s 'http://localhost:15001/metrics' | grep ^sensor_  # sensor_ name matches the object: value in your sensor.yaml file.\n\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_high=\"3664\",node=\"shopfloor-02\",sensor=\"P3.3V STBY\",type=\"voltage\",warning_low=\"3040\",critical_low=\"2960\",threshold_state=\"normal\",unit=\"mV\",warning_high=\"3568\"} 3280\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"P1.2V STBY\",type=\"voltage\",threshold_state=\"normal\",warning_high=\"1299\",warning_low=\"1105\",critical_low=\"1086\",node=\"shopfloor-02\",critical_high=\"1319\",unit=\"mV\"} 1193\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",unit=\"mV\",critical_high=\"15810\",critical_low=\"0\",node=\"shopfloor-02\",sensor=\"P12V STBY\",type=\"voltage\",threshold_state=\"normal\"} 11842\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"P12V STBY Curr\",type=\"current\",threshold_state=\"normal\",unit=\"mA\",critical_high=\"3182\",critical_low=\"0\",node=\"shopfloor-02\"} 748\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_low=\"1470\",node=\"shopfloor-02\",sensor=\"Sysfan2 F2 Speed\",type=\"fan\",threshold_state=\"normal\",unit=\"RPM\",warning_low=\"1560\"} 2820\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"PSU2 Fan1 Speed\",type=\"fan\",threshold_state=\"normal\",unit=\"RPM\",warning_low=\"4600\",critical_low=\"4500\",node=\"shopfloor-01\"} 6900\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"PSU1 InPwr Monitor\",type=\"unknown\",threshold_state=\"normal\",unit=\"mW\",node=\"shopfloor-01\"} 132000\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_high=\"58\",type=\"thermal\",unit=\"C\",warning_high=\"53\",critical_low=\"0\",node=\"shopfloor-01\",sensor=\"Bat Temp\",threshold_state=\"normal\",warning_low=\"5\"} 24\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_high=\"9000\",node=\"shopfloor-01\",sensor=\"Bat Charge Volt\",type=\"voltage\",threshold_state=\"normal\",unit=\"mV\",warning_high=\"8900\"} 8200\nsensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",node=\"shopfloor-02\",sensor=\"PSU1 InPwr Monitor\",type=\"unknown\",threshold_state=\"normal\",unit=\"mW\"} 132000\n</code></pre>"},{"location":"configure-templates/#extend-an-existing-object-template","title":"Extend an existing object template","text":""},{"location":"configure-templates/#how-to-extend-a-restrestperfstoragegridems-collectors-existing-object-template","title":"How to extend a Rest/RestPerf/StorageGRID/Ems collector's existing object template","text":"<p>Instead of editing one of the existing templates, it's better to copy one and edit the copy. That way, your custom template will not be overwritten when upgrading Harvest. For example, if you want to change <code>conf/rest/9.12.0/aggr.yaml</code>, first create a copy (e.g., <code>conf/rest/9.12.0/custom_aggr.yaml</code>), then add these lines to <code>conf/rest/custom.yaml</code>:</p> <pre><code>objects:\n  Aggregate: custom_aggr.yaml\n</code></pre> <p>After restarting pollers, <code>aggr.yaml</code> will be ignored and the new, <code>custom_aggr.yaml</code> subtemplate will be used instead.</p>"},{"location":"configure-templates/#how-to-extend-a-zapizapiperf-collectors-existing-object-template","title":"How to extend a Zapi/ZapiPerf collector's existing object template","text":"<p>In this example, we want to extend one of the existing object templates that Harvest ships with, e.g. <code>conf/zapi/cdot/9.8.0/lun.yaml</code> and collect additional information as outlined below.</p> <p>Let's say you want to extend <code>lun.yaml</code> to:</p> <ol> <li>Increase <code>client_timeout</code> (You want to increase the default timeout of the lun ZAPI because it    keeps timing out)</li> <li>Add additional counters, e.g. <code>multiprotocol-type</code>, <code>application</code></li> <li>Add a new counter to the already collected lun metrics using the <code>value_to_num</code> plugin</li> <li>Add a new <code>application</code> instance_keys and labels to the collected metrics</li> </ol> <p>Let's assume the existing template is located at <code>conf/zapi/cdot/9.8.0/lun.yaml</code> and contains the following.</p> <pre><code>name: Lun\nquery: lun-get-iter\nobject: lun\n\ncounters:\n  lun-info:\n    - ^node\n    - ^path\n    - ^qtree\n    - size\n    - size-used\n    - ^state\n    - ^^uuid\n    - ^volume\n    - ^vserver =&gt; svm\n\nplugins:\n  - LabelAgent:\n    # metric label zapi_value rest_value `default_value`\n    value_to_num:\n      - new_status state online online `0`\n    split:\n      - path `/` ,,,lun\n\nexport_options:\n  instance_keys:\n    - node\n    - qtree\n    - lun\n    - volume\n    - svm\n  instance_labels:\n    - state\n</code></pre> <p>To extend the out-of-the-box <code>lun.yaml</code> template, create a <code>conf/zapi/custom.yaml</code> file if it doesn't already exist and add the lines shown below:</p> <pre><code>objects:\n  Lun: custom_lun.yaml\n</code></pre> <p>Create a new object template <code>conf/zapi/cdot/9.8.0/custom_lun.yaml</code> with the lines shown below.</p> <pre><code>client_timeout: 5m\ncounters:\n  lun-info:\n    - ^multiprotocol-type\n    - ^application\n\nplugins:\n  - LabelAgent:\n    value_to_num:\n      - custom_status state online online `0`\n\nexport_options:\n  instance_keys:\n    - application\n</code></pre> <p>When you restart your pollers, Harvest will take the out-of-the-box template (<code>lun.yaml</code>) and your new one (<code>custom_lun.yaml</code>) and merge them into the following:</p> <pre><code>name: Lun\nquery: lun-get-iter\nobject: lun\ncounters:\n  lun-info:\n    - ^node\n    - ^path\n    - ^qtree\n    - size\n    - size-used\n    - ^state\n    - ^^uuid\n    - ^volume\n    - ^vserver =&gt; svm\n    - ^multiprotocol-type\n    - ^application\nplugins:\n  LabelAgent:\n    value_to_num:\n      - new_status state online online `0`\n      - custom_status state online online `0`\n    split:\n      - path `/` ,,,lun\nexport_options:\n  instance_keys:\n    - node\n    - qtree\n    - lun\n    - volume\n    - svm\n    - application\nclient_timeout: 5m\n</code></pre> <p>To help understand the merging process and the resulting combined template, you can view the result with:</p> <pre><code>bin/harvest doctor merge --template conf/zapi/cdot/9.8.0/lun.yaml --with conf/zapi/cdot/9.8.0/custom_lun.yaml\n</code></pre>"},{"location":"configure-templates/#replace-an-existing-object-template-for-zapizapiperf-collector","title":"Replace an existing object template for Zapi/ZapiPerf Collector","text":"<p>You can only extend existing templates for Zapi/ZapiPerf Collector as explained above. If you need to replace one of the existing object templates, let us know on Discord or GitHub.</p>"},{"location":"configure-templates/#harvest-versioned-templates","title":"Harvest Versioned Templates","text":"<p>Harvest ships with a set of versioned templates tailored for specific versions of ONTAP. At runtime, Harvest uses a BestFit heuristic to pick the most appropriate template. The BestFit heuristic compares the list of Harvest templates with the ONTAP version and selects the best match. There are versioned templates for both the ZAPI and REST collectors. Below is an example of how the BestFit algorithm works - assume Harvest has these templated versions:</p> <ul> <li>9.6.0</li> <li>9.6.1</li> <li>9.8.0</li> <li>9.9.0</li> <li>9.10.1</li> </ul> <p>if you are monitoring a cluster at these versions, Harvest will select the indicated template:</p> <ul> <li>ONTAP version 9.4.1, Harvest will select the templates for 9.6.0</li> <li>ONTAP version 9.6.0, Harvest will select the templates for 9.6.0</li> <li>ONTAP version 9.7.X, Harvest will select the templates for 9.6.1</li> <li>ONTAP version 9.12, Harvest will select the templates for 9.10.1</li> </ul>"},{"location":"configure-templates/#counters","title":"counters","text":"<p>This section contains the complete or partial attribute tree of the queried API. Since the collector does not get counter metadata from the ONTAP system, two additional symbols are used for non-numeric attributes:</p> <ul> <li><code>^</code> used as a prefix indicates that the attribute should be stored as a label</li> <li><code>^^</code> indicates that the attribute is a label and an instance key (i.e., a label that uniquely identifies an instance,   such as <code>name</code>, <code>uuid</code>). If a single label does not uniquely identify an instance, then multiple instance keys should   be indicated.</li> </ul> <p>Additionally, the symbol <code>=&gt;</code> can be used to set a custom display name for both instance labels and numeric counters. Example:</p> <pre><code>name: Spare\nquery: aggr-spare-get-iter\nobject: spare\ncollect_only_labels: true\ncounters:\n  aggr-spare-disk-info:\n    - ^^disk                                # creates label aggr-disk\n    - ^disk-type                            # creates label aggr-disk-type\n    - ^is-disk-zeroed   =&gt; is_disk_zeroed   # creates label is_disk_zeroed\n    - ^^original-owner  =&gt; original_owner   # creates label original_owner\nexport_options:\n  instance_keys:\n    - disk\n    - original_owner\n  instance_labels:\n    - disk_type\n    - is_disk_zeroed\n</code></pre> <p>Harvest does its best to determine a unique display name for each template's label and metric. Instead of relying on this heuristic, it is better to be explicit in your templates and define a display name using the caret (<code>^</code>) mapping. For example, instead of this:</p> <pre><code>aggr-spare-disk-info:\n    - ^^disk\n    - ^disk-type\n</code></pre> <p>do this:</p> <pre><code>aggr-spare-disk-info:\n    - ^^disk      =&gt; disk\n    - ^disk-type  =&gt; disk_type\n</code></pre> <p>See also #585</p>"},{"location":"configure-unix/","title":"Unix","text":"<p>This collector polls resource usage by Harvest pollers on the local system. Collector might be extended in the future to monitor any local or remote process.</p>"},{"location":"configure-unix/#target-system","title":"Target System","text":"<p>The machine where Harvest is running (\"localhost\").</p>"},{"location":"configure-unix/#requirements","title":"Requirements","text":"<p>Collector requires any OS where the proc-filesystem is available. If you are a developer, you are welcome to add support for other platforms. Currently, supported platforms includes most Unix/Unix-like systems:</p> <ul> <li>Android / Termux</li> <li>DragonFly BSD</li> <li>FreeBSD</li> <li>IBM AIX</li> <li>Linux</li> <li>NetBSD</li> <li>Plan9</li> <li>Solaris</li> </ul> <p>(On FreeBSD and NetBSD the proc-filesystem needs to be manually mounted).</p>"},{"location":"configure-unix/#parameters","title":"Parameters","text":"parameter type description default <code>mount_point</code> string, optional path to the <code>proc</code> filesystem `/proc"},{"location":"configure-unix/#metrics","title":"Metrics","text":"<p>The Collector follows the Linux proc(5) manual to parse a static set of metrics. Unless otherwise stated, the metric has a scalar value:</p> metric type unit description <code>start_time</code> counter, <code>float64</code> seconds process uptime <code>cpu_percent</code> gauge, <code>float64</code> percent CPU used since last poll <code>memory_percent</code> gauge, <code>float64</code> percent Memory used (RSS) since last poll <code>cpu</code> histogram, <code>float64</code> seconds CPU used since last poll (<code>system</code>, <code>user</code>, <code>iowait</code>) <code>memory</code> histogram, <code>uint64</code> kB Memory used since last poll (<code>rss</code>, <code>vms</code>, <code>swap</code>, etc) <code>io</code> histogram, <code>uint64</code> bytecount IOs performed by process:<code>rchar</code>, <code>wchar</code>, <code>read_bytes</code>, <code>write_bytes</code> - read/write IOs<code>syscr</code>, <code>syscw</code> - syscalls for IO operations <code>net</code> histogram, <code>uint64</code> count/byte Different IO operations over network devices <code>ctx</code> histogram, <code>uint64</code> count Number of context switched (<code>voluntary</code>, <code>involuntary</code>) <code>threads</code> counter, <code>uint64</code> count Number of threads <code>fds</code> counter, <code>uint64</code> count Number of file descriptors <p>Additionally, the collector provides the following instance labels:</p> label description poller name of the poller pid PID of the poller"},{"location":"configure-unix/#issues","title":"Issues","text":"<ul> <li>Collector will fail on WSL because some non-critical files, in the proc-filesystem, are not present.</li> </ul>"},{"location":"configure-zapi/","title":"ZAPI","text":"<p>What about REST?</p> <p>ZAPI will reach end of availability in ONTAP  9.13.1 released Q2 2023. Don't worry, Harvest has you covered. Switch to Harvest's REST collectors and collect identical metrics. See REST Strategy for more details.</p>"},{"location":"configure-zapi/#zapi-collector","title":"Zapi Collector","text":"<p>The Zapi collectors use the ZAPI protocol to collect data from ONTAP systems. The collector submits data as received from the target system, and does not perform any calculations or post-processing. Since the attributes of most APIs have an irregular tree structure, sometimes a plugin will be required to collect all metrics from an API.</p> <p>The ZapiPerf collector is an extension of this collector, therefore, they share many parameters and configuration settings.</p>"},{"location":"configure-zapi/#target-system","title":"Target System","text":"<p>Target system can be any cDot or 7Mode ONTAP system. Any version is supported, however the default configuration files may not completely match with older systems.</p>"},{"location":"configure-zapi/#requirements","title":"Requirements","text":"<p>No SDK or other requirements. It is recommended to create a read-only user for Harvest on the ONTAP system (see  prepare monitored clusters for details)</p>"},{"location":"configure-zapi/#metrics","title":"Metrics","text":"<p>The collector collects a dynamic set of metrics. Since most ZAPIs have a tree structure, the collector converts that structure into a flat metric representation. No post-processing or calculation is performed on the collected data itself.</p> <p>As an example, the <code>aggr-get-iter</code> ZAPI provides the following partial attribute tree:</p> <pre><code>aggr-attributes:\n  - aggr-raid-attributes:\n      - disk-count\n  - aggr-snapshot-attributes:\n      - files-total\n</code></pre> <p>The Zapi collector will convert this tree into two \"flat\" metrics: <code>aggr_raid_disk_count</code> and <code>aggr_snapshot_files_total</code>. (The algorithm to generate a name for the metrics will attempt to keep it as simple as possible, but sometimes it's useful to manually set a short display name. See counters for more details.</p>"},{"location":"configure-zapi/#parameters","title":"Parameters","text":"<p>The parameters of the collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>ZAPI configuration file (default: <code>conf/zapi/default.yaml</code>)</li> <li>Each object has its own configuration file (located in <code>conf/zapi/$version/</code>)</li> </ul> <p>Except for <code>addr</code> and <code>datacenter</code>, all other parameters of the ZAPI collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level ones. This allows you to configure each object individually, or use the same parameters for all objects.</p> <p>The full set of parameters are described below.</p>"},{"location":"configure-zapi/#collector-configuration-file","title":"Collector configuration file","text":"<p>The parameters are similar to those of the ZapiPerf collector. Parameters different from ZapiPerf:</p> parameter type description default <code>jitter</code> duration (Go-syntax), optional Each Harvest collector runs independently, which means that at startup, each collector may send its ZAPI queries at nearly the same time. To spread out the collector startup times over a broader period, you can use <code>jitter</code> to randomly distribute collector startup across a specified duration. For example, a <code>jitter</code> of <code>1m</code> starts each collector after a random delay between 0 and 60 seconds. For more details, refer to this discussion. <code>schedule</code> required same as for ZapiPerf, but only two elements: <code>instance</code> and <code>data</code> (collector does not run a <code>counter</code> poll) <code>no_max_records</code> bool, optional don't add <code>max-records</code> to the ZAPI request <code>collect_only_labels</code> bool, optional don't look for numeric metrics, only submit labels  (suppresses the <code>ErrNoMetrics</code> error) <code>only_cluster_instance</code> bool, optional don't look for instance keys and assume only instance is the cluster itself"},{"location":"configure-zapi/#object-configuration-file","title":"Object configuration file","text":"<p>The Zapi collector does not have the parameters <code>instance_key</code> and <code>override</code> parameters. The optional parameter <code>metric_type</code> allows you to override the default metric type (<code>uint64</code>). The value of this parameter should be one of the metric types supported by the matrix data-structure.</p> <p>The Object configuration file (\"subtemplate\") should contain the following parameters:</p> parameter type description default <code>name</code> string, required display name of the collector that will collect this object <code>query</code> string, required REST endpoint used to issue a REST request <code>object</code> string, required short name of the object <code>counters</code> string list of counters to collect (see notes below) <code>plugins</code> list plugins and their parameters to run on the collected data <code>export_options</code> list parameters to pass to exporters (see notes below)"},{"location":"configure-zapi/#counters","title":"Counters","text":"<p>This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from ONTAP and updated periodically.</p> <p>Some counters require a \"base-counter\" for post-processing. If the base-counter is missing, ZapiPerf will still run, but the missing data won't be exported.</p> <p>The display name of a counter can be changed with <code>=&gt;</code> (e.g., <code>nfsv3_ops =&gt; ops</code>). There's one conversion Harvest does for you by default, the <code>instance_name</code> counter will be renamed to the value of <code>object</code>.</p> <p>Counters that are stored as labels will only be exported if they are included in the <code>export_options</code> section.</p>"},{"location":"configure-zapi/#export_options","title":"Export_options","text":"<p>Parameters in this section tell the exporters how to handle the collected data.</p> <p>There are two different kinds of time-series that Harvest publishes: metrics and instance labels.</p> <ul> <li>Metrics are numeric data with associated labels (key-value pairs). E.g. <code>volume_read_ops_total{cluster=\"cluster1\", node=\"node1\", volume=\"vol1\"} 123</code>. The <code>volume_read_ops_total</code> metric is exporting three labels: <code>cluster</code>, <code>node</code>, and <code>volume</code> and the metric value is <code>123</code>.</li> <li>Instance labels are named after their associated config object (e.g., <code>volume_labels</code>, <code>qtree_labels</code>, etc.). There will be one instance label for each object instance, and each instance label will contain a set of associated labels (key-value pairs) that are defined in the templates <code>instance_labels</code> parameter. E.g. <code>volume_labels{cluster=\"cluster1\", node=\"node1\", volume=\"vol1\", svm=\"svm1\"} 1</code>. The <code>volume_labels</code> instance label is exporting four labels: <code>cluster</code>, <code>node</code>, <code>volume</code>, and <code>svm</code>. Instance labels always export a metric value of <code>1</code>.</li> </ul> <p>The <code>export_options</code> section allows you to define how to export these time-series.</p> <ul> <li><code>instances_keys</code> (list): display names of labels to export to both metric and instance labels.   For example, if you list the <code>svm</code> counter under <code>instances_keys</code>,   that key-value will be included in all time-series metrics and all instance-labels.</li> <li><code>instance_labels</code> (list): display names of labels to export with the corresponding instance label config object. For example, if you want the <code>volume</code> counter to be exported with the <code>volume_labels</code> instance label, you would list <code>volume</code> in the <code>instance_labels</code> section.</li> <li><code>include_all_labels</code> (bool): exports all labels for all time-series metrics. If there are no metrics defined in the template, this option will do nothing. This option also overrides the previous two parameters. See also collect_only_labels.</li> </ul>"},{"location":"configure-zapi/#zapiperf-collector","title":"ZapiPerf Collector","text":""},{"location":"configure-zapi/#zapiperf","title":"ZapiPerf","text":"<p>ZapiPerf collects performance metrics from ONTAP systems using the ZAPI protocol. The collector is designed to be easily extendable to collect new objects or to collect additional counters from already configured objects.</p> <p>This collector is an extension of the Zapi collector. The major difference between them is that ZapiPerf collects only the performance (<code>perf</code>) APIs. Additionally, ZapiPerf always calculates final values from the deltas of two subsequent polls.</p>"},{"location":"configure-zapi/#metrics_1","title":"Metrics","text":"<p>The collector collects a dynamic set of metrics. The metric values are calculated from two consecutive polls (therefore, no metrics are emitted after the first poll). The calculation algorithm depends on the <code>property</code> and <code>base-counter</code> attributes of each metric, the following properties are supported:</p> property formula description raw x = x<sub>i</sub> no post-processing, value x is submitted as it is delta x = x<sub>i</sub> - x<sub>i-1</sub> delta of two poll values, x<sub>i<sub> and x<sub>i-1<sub> rate x = (x<sub>i</sub> - x<sub>i-1</sub>) / (t<sub>i</sub> - t<sub>i-1</sub>) delta divided by the interval of the two polls in seconds average x = (x<sub>i</sub> - x<sub>i-1</sub>) / (y<sub>i</sub> - y<sub>i-1</sub>) delta divided by the delta of the base counter y percent x = 100 * (x<sub>i</sub> - x<sub>i-1</sub>) / (y<sub>i</sub> - y<sub>i-1</sub>) average multiplied by 100"},{"location":"configure-zapi/#parameters_1","title":"Parameters","text":"<p>The parameters of the collector are distributed across three files:</p> <ul> <li>Harvest configuration file (default: <code>harvest.yml</code>)</li> <li>ZapiPerf configuration file (default: <code>conf/zapiperf/default.yaml</code>)</li> <li>Each object has its own configuration file (located in <code>conf/zapiperf/cdot/</code> and <code>conf/zapiperf/7mode/</code> for cDot and   7Mode systems respectively)</li> </ul> <p>Except for <code>addr</code>, <code>datacenter</code> and <code>auth_style</code>, all other parameters of the ZapiPerf collector can be defined in either of these three files.  Parameters defined in the lower-level file, override parameters in the higher-level file.  This allows the user to configure each object individually,  or use the same parameters for all objects.</p> <p>The full set of parameters are described below.</p>"},{"location":"configure-zapi/#zapiperf-configuration-file","title":"ZapiPerf configuration file","text":"<p>This configuration file (the \"template\") contains a list of objects that should be collected and the filenames of their configuration (explained in the next section).</p> <p>Additionally, this file contains the parameters that are applied as defaults to all objects. (As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well).</p> parameter type description default <code>use_insecure_tls</code> bool, optional skip verifying TLS certificate of the target system <code>false</code> <code>client_timeout</code> duration (Go-syntax) how long to wait for server responses 30s <code>batch_size</code> int, optional max instances per API request <code>500</code> <code>latency_io_reqd</code> int, optional threshold of IOPs for calculating latency metrics (latencies based on very few IOPs are unreliable) 0 <code>jitter</code> duration (Go-syntax), optional Each Harvest collector runs independently, which means that at startup, each collector may send its ZAPI queries at nearly the same time. To spread out the collector startup times over a broader period, you can use <code>jitter</code> to randomly distribute collector startup across a specified duration. For example, a <code>jitter</code> of <code>1m</code> starts each collector after a random delay between 0 and 60 seconds. For more details, refer to this discussion. <code>schedule</code> list, required the poll frequencies of the collector/object, should include exactly these three elements in the exact same other: - <code>counter</code> duration (Go-syntax) poll frequency of updating the counter metadata cache (example value: <code>20m</code>) - <code>instance</code> duration (Go-syntax) poll frequency of updating the instance cache (example value: <code>10m</code>) - <code>data</code> duration (Go-syntax) poll frequency of updating the data cache (example value: <code>1m</code>)Note Harvest allows defining poll intervals on sub-second level (e.g. <code>1ms</code>), however keep in mind the following:<ul><li>API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than <code>client_timeout</code>.</li><li>Small poll intervals will create significant workload on the ONTAP system, as many counters are aggregated on-demand.</li><li>Some metric values become less significant if they are calculated for very short intervals (e.g. latencies)</li></ul> <p>The template should define objects in the <code>objects</code> section. Example:</p> <pre><code>objects:\n  SystemNode: system_node.yaml\n  HostAdapter: hostadapter.yaml\n</code></pre> <p>Note that for each object we only define the filename of the object configuration file. The object configuration files are located in subdirectories matching to the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches to the version of the target ONTAP system. (A mismatch is tolerated since ZapiPerf will fetch and validate counter metadata from the system.)</p>"},{"location":"configure-zapi/#object-configuration-file_1","title":"Object configuration file","text":"<p>The Object configuration file (\"subtemplate\") should contain the following parameters:</p> parameter type description default <code>name</code> string display name of the collector that will collect this object <code>object</code> string short name of the object <code>query</code> string raw object name used to issue a ZAPI request <code>counters</code> list list of counters to collect (see notes below) <code>instance_key</code> string label to use as instance key (either <code>name</code> or <code>uuid</code>) <code>override</code> list of key-value pairs override counter properties that we get from ONTAP (allows circumventing ZAPI bugs) <code>plugins</code> list plugins and their parameters to run on the collected data <code>export_options</code> list parameters to pass to exporters (see notes below)"},{"location":"configure-zapi/#counters_1","title":"<code>counters</code>","text":"<p>This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from ONTAP and updated periodically.</p> <p>Some counters require a \"base-counter\" for post-processing. If the base-counter is missing, ZapiPerf will still run, but the missing data won't be exported.</p> <p>The display name of a counter can be changed with <code>=&gt;</code> (e.g., <code>nfsv3_ops =&gt; ops</code>). There's one conversion Harvest does for you by default, the <code>instance_name</code> counter will be renamed to the value of <code>object</code>.</p> <p>Counters that are stored as labels will only be exported if they are included in the <code>export_options</code> section.</p>"},{"location":"configure-zapi/#export_options_1","title":"<code>export_options</code>","text":"<p>Parameters in this section tell the exporters how to handle the collected data.</p> <p>There are two different kinds of time-series that Harvest publishes: metrics and instance labels.</p> <ul> <li>Metrics are numeric data with associated labels (key-value pairs). E.g. <code>volume_read_ops_total{cluster=\"cluster1\", node=\"node1\", volume=\"vol1\"} 123</code>. The <code>volume_read_ops_total</code> metric is exporting three labels: <code>cluster</code>, <code>node</code>, and <code>volume</code> and the metric value is <code>123</code>.</li> <li>Instance labels are named after their associated config object (e.g., <code>volume_labels</code>, <code>nic_labels</code>, etc.).   There will be one instance label for each object instance,    and each instance label will contain a set of associated labels   (key-value pairs) that are defined in the templates <code>instance_labels</code> parameter.   E.g. <code>volume_labels{cluster=\"cluster1\", node=\"node1\", volume=\"vol1\", svm=\"svm1\"} 1</code>.    The <code>volume_labels</code> instance label is exporting four labels:   <code>cluster</code>, <code>node</code>, <code>volume</code>, and <code>svm</code>.   Instance labels always export a metric value of <code>1</code>.</li> </ul> Instance labels are rarely used with ZapiPerf templates <p>They can be useful for exporting labels that are not associated with a metric value.</p> <p>The <code>export_options</code> section allows you to define how to export these time-series.</p> <ul> <li><code>instances_keys</code> (list): display names of labels to export to both metric and instance labels.   For example, if you list the <code>svm</code> counter under <code>instances_keys</code>,   that key-value will be included in all time-series metrics and all instance-labels.</li> <li><code>instance_labels</code> (list): display names of labels to export with the corresponding instance label config object. For example, if you want the <code>volume</code> counter to be exported with the <code>volume_labels</code> instance label, you would list <code>volume</code> in the <code>instance_labels</code> section.</li> </ul>"},{"location":"configure-zapi/#filter","title":"Filter","text":"<p>This guide provides instructions on how to use the <code>filter</code> feature in ZapiPerf. Filtering is useful when you need to query a subset of instances. For example, suppose you have a small number of high-value volumes from which you want Harvest to collect performance metrics every five seconds. Collecting data from all volumes at this frequency would be too resource-intensive. Therefore, filtering allows you to create/modify a template that includes only the high-value volumes.</p>"},{"location":"configure-zapi/#objects-excluding-workload","title":"Objects (Excluding Workload)","text":"<p>In ZapiPerf templates, you can set up filters under <code>counters</code>. Wildcards like * are useful if you don't want to specify all instances. Please note, ONTAP Zapi filtering does not support regular expressions, only wildcard matching with <code>*</code>.</p> <p>For instance, to filter <code>volume</code> performance instances by instance name where the name is <code>NS_svm_nvme</code> or contains <code>Test</code>, use the following configuration in ZapiPerf <code>volume.yaml</code> under <code>counters</code>:</p> <pre><code>counters:\n  ...\n  - filter:\n     - instance_name=NS_svm_nvme|instance_name=*Test*\n</code></pre> <p>You can define multiple values within the filter array. These will be interpreted as <code>AND</code> conditions by ONTAP. Alternatively, you can specify a complete expression within a single array element, as described in the ONTAP filtering section below.</p> ONTAP Filtering Details <p>For a better understanding of ONTAP's filtering mechanism, it allows the use of <code>filter-data</code> for the <code>perf-object-instance-list-info-iter</code> Zapi.</p> <p>The <code>filter-data</code> is a string that signifies filter data, adhering to the format: <code>counter_name=counter_value</code>. You can define multiple pairs, separated by either a comma (\",\") or a pipe (\"|\").</p> <p>Here's the interpretation:</p> <ul> <li>A comma (\",\") signifies an AND operation.</li> <li>A pipe (\"|\") signifies an OR operation.</li> <li>The precedence order is AND first, followed by OR.</li> </ul> <p>For instance, the filter string <code>instance_name=volA,vserver_name=vs1|vserver_name=vs2</code> translates to <code>(instance_name=volA &amp;&amp; vserver_name=vs1) || (vserver_name=vs2)</code>.</p> <p>This filter will return instances on Vserver <code>vs1</code> named <code>volA</code>, and all instances on Vserver <code>vs2</code>.</p>"},{"location":"configure-zapi/#workload-templates","title":"Workload Templates","text":"<p>Performance workload templates require a different syntax because instances are retrieved from the <code>qos-workload-get-iter</code> ZAPI instead of <code>perf-object-instance-list-info-iter</code>.</p> <p>The <code>qos-workload-get-iter</code> ZAPI supports filtering on the following fields:</p> <ul> <li>workload-uuid</li> <li>workload-name</li> <li>workload-class</li> <li>wid</li> <li>category</li> <li>policy-group</li> <li>vserver</li> <li>volume</li> <li>lun</li> <li>file</li> <li>qtree</li> <li>read-ahead</li> <li>max-throughput</li> <li>min-throughput</li> <li>is-adaptive</li> <li>is-constituent</li> </ul> <p>You can include these fields under the <code>filter</code> parameter. For example, to filter Workload performance instances by <code>workload-name</code> where the name contains <code>NS</code> or <code>Test</code> and <code>vserver</code> is <code>vs1</code>, use the following configuration in ZapiPerf <code>workload.yaml</code> under <code>counters</code>:</p> <pre><code>counters:\n  ...\n  - filter:\n      - workload-name: \"*NS*|*Test*\"\n      - vserver: vs1\n</code></pre>"},{"location":"configure-zapi/#partial-aggregation","title":"Partial Aggregation","text":"<p>For more details about partial aggregation behavior and configuration, see Partial Aggregation.</p>"},{"location":"create-storagegrid-dashboard/","title":"Create StorageGRID Dashboard","text":"<p>This guide walks you through the process of creating custom Grafana dashboards in Harvest that are similar to StorageGRID's built-in dashboards.</p>"},{"location":"create-storagegrid-dashboard/#step-1-identify-storagegrid-dashboard-metrics","title":"Step 1: Identify StorageGrid Dashboard Metrics","text":"<p>First examine the metrics used in StorageGrid's built-in dashboards to understand what needs to be collected by Harvest.</p>"},{"location":"create-storagegrid-dashboard/#access-storagegrids-native-dashboards","title":"Access StorageGrid's Native Dashboards","text":"<ol> <li>Log into your StorageGrid Grid Management Interface</li> <li>Navigate to SUPPORT &gt; Tools &gt; Metrics</li> <li>Access the built-in Grafana instance</li> <li>Review the available dashboards</li> </ol>"},{"location":"create-storagegrid-dashboard/#inspect-dashboard-queries","title":"Inspect Dashboard Queries","text":"<p>For each dashboard panel you want to check the Grafana panel to identify which metrics are being used. Note the metric names for your template configuration.</p>"},{"location":"create-storagegrid-dashboard/#step-2-configure-harvest-to-collect-storagegrid-metrics","title":"Step 2: Configure Harvest to Collect StorageGrid Metrics","text":"<p>Now configure Harvest to collect the metrics identified in Step 1.</p>"},{"location":"create-storagegrid-dashboard/#update-the-metrics-template","title":"Update the Metrics Template","text":"<p>Edit the StorageGrid metrics template to include the required metrics:</p> <pre><code># Edit the template file\nvim conf/storagegrid/11.6.0/storagegrid_metrics.yaml\n</code></pre> <p>Add the identified metrics to the <code>counters</code> section of the template file.</p>"},{"location":"create-storagegrid-dashboard/#verify-metric-availability","title":"Verify Metric Availability","text":"<p>Before adding metrics to your template verify they're available from your StorageGrid Prometheus endpoint (usually found at <code>https://sg_ip/metrics/graph</code>).</p>"},{"location":"create-storagegrid-dashboard/#step-3-restart-harvest-and-verify-collection","title":"Step 3: Restart Harvest and Verify Collection","text":"<p>After updating the template, restart your Harvest poller to collect the new metrics. Verify that the metrics are available in your time-series database (Prometheus, InfluxDB, etc.)</p>"},{"location":"create-storagegrid-dashboard/#step-4-create-similar-dashboards-in-harvest-grafana","title":"Step 4: Create Similar Dashboards in Harvest Grafana","text":"<p>Now create dashboards in your Harvest Grafana instance that mirror the functionality of StorageGRID's built-in dashboards, using the metrics being collected by Harvest.</p> <p>For detailed guidance on creating dashboards with Harvest metrics, refer to the Creating a Custom Grafana Dashboard guide.</p> <p>You can also use the existing StorageGrid dashboards as references for panel designs and query patterns.</p> <p>Note that StorageGrid and Harvest may use different label names for the same concepts so update your dashboard queries to use Harvest's label conventions as needed.</p>"},{"location":"create-storagegrid-dashboard/#troubleshooting","title":"Troubleshooting","text":""},{"location":"create-storagegrid-dashboard/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ul> <li> <p>Metrics not appearing in Prometheus: Check that metrics are available in StorageGrid's native Prometheus endpoint. Verify template syntax and restart poller. Check Harvest logs for collection errors. </p> </li> <li> <p>Dashboard panels showing \"No data\": Verify metric names match exactly between template and dashboard. Check label selectors and variable names. Ensure time range covers period when metrics were collected.</p> </li> </ul>"},{"location":"create-storagegrid-dashboard/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configure StorageGrid Collector</li> <li>StorageGrid Metrics Reference </li> <li>Creating Custom Dashboards</li> </ul>"},{"location":"dashboards/","title":"Dashboards","text":"<p>Harvest can be used to import dashboards to Grafana.</p> <p>The <code>bin/harvest grafana</code> utility requires the address (hostname or IP), port of the Grafana server, and a Grafana API token. The port can be omitted if Grafana is configured to redirect the URL. Use the <code>-d</code> flag to point to the directory that contains the dashboards.</p>"},{"location":"dashboards/#grafana-api-token","title":"Grafana API token","text":"<p>The utility tool asks for an API token which can be generated from the Grafana web-gui.</p> <p></p> <p>Click on <code>Configuration</code> in the left menu bar (1), click on <code>API Keys</code> (2) and click on the <code>New API Key</code> button. Choose a Key name (3), choose <code>Editor</code> for role (4) and click on add (5). Copy the generated key and paste it in your terminal or add the token to the <code>Tools</code> section of your configuration file. (see below)</p> <p>For example, let's say your Grafana server is on <code>http://my.grafana.server:3000</code> and you want to import the Prometheus-based dashboards from the <code>grafana</code> directory. You would run this:</p> <pre><code>bin/harvest grafana import --addr my.grafana.server:3000\n</code></pre> <p>Similarly, to export:</p> <pre><code>bin/harvest grafana export --addr my.grafana.server:3000 --directory /path/to/export/directory --serverfolder grafanaFolderName\n</code></pre> <p>By default, the dashboards are connected to a datasource named <code>prometheus</code> (case-sensitive). This is a datasource of the Prometheus type, defined in Grafana. However, despite the type, the datasource can have any name. If you have a Prometheus type datasource with a name different from <code>prometheus</code>, you can specify this name using the <code>--datasource</code> flag during import/export like this:</p> <pre><code>bin/harvest grafana import --addr my.grafana.server:3000 --datasource custom_datasource_name\n</code></pre>"},{"location":"dashboards/#cli","title":"CLI","text":"<p>The <code>bin/harvest grafana</code> tool includes CLI help when passing the <code>--help</code> command line argument flag like so:</p> <pre><code>bin/harvest grafana import --help\n</code></pre> <p>The <code>labels</code> argument requires more explanation.</p>"},{"location":"dashboards/#labels","title":"Labels","text":"<p>The grafana import <code>--labels</code> argument goes hand-in-hand with a poller's <code>Labels</code> section described here. <code>Labels</code> are used to add additional key-value pairs to a poller's metrics.</p> <p>When you run <code>bin/harvest grafana import</code>, you may optionally pass a set of labels like so:</p> <p><code>bin/harvest grafana import --labels org --labels dept</code></p> <p>This will cause Harvest to do the following for each dashboard: 1. Parse each dashboard and add a new variable for each label passed on the command line 2. Modify each dashboard variable to use the new label variable(s) in a chained query.</p> <p>Here's an example:</p> <pre><code>bin/harvest grafana import --labels \"org,dept\"\n</code></pre> <p>This will add the <code>Org</code> and <code>Dept</code> variables, as shown below, and modify the existing variables as shown.</p> <p>Results in</p> <p></p>"},{"location":"dashboards/#creating-a-custom-grafana-dashboard-with-harvest-metrics-stored-in-prometheus","title":"Creating a Custom Grafana Dashboard with Harvest Metrics Stored in Prometheus","text":"<p>This guide assumes that you have already installed and configured Harvest, Prometheus, and Grafana. Instead of creating a new Grafana dashboard from scratch, you might find it more efficient to clone and modify an existing one. Alternatively, you can copy/paste an existing dashboard's panel from an existing dashboard into your new one.</p> <p>Harvest collects a wide range of metrics from ONTAP, StorageGRID, E-Series and Cisco Nexus Switches. These metrics can be used to create dashboards in Grafana.</p>"},{"location":"dashboards/#step-1-confirm-that-prometheus-is-receiving-metrics-from-harvest","title":"Step 1: Confirm that Prometheus is Receiving Metrics from Harvest","text":"<p>Before creating a dashboard, make sure the relevant metric is present via a PromQL query in the Prometheus UI. If the metric is not present, navigate to <code>Status -&gt; Targets</code> in the Prometheus UI to check the state and any potential errors of the scrape target.</p>"},{"location":"dashboards/#step-2-add-prometheus-as-a-data-source-in-grafana","title":"Step 2: Add Prometheus as a Data Source in Grafana","text":"<p>If you haven't already, add Prometheus as a data source in Grafana:</p> <ol> <li>In the Grafana UI, go to <code>Configuration &gt; Data Sources</code>.</li> <li>Click <code>Add data source</code>.</li> <li>Select <code>Prometheus</code>.</li> <li>Enter the URL of your Prometheus server, and click <code>Save &amp; Test</code>.</li> </ol>"},{"location":"dashboards/#step-3-create-a-new-dashboard","title":"Step 3: Create a New Dashboard","text":"<p>Now you're ready to create a new dashboard:</p> <ol> <li>In the Grafana UI, click the <code>+</code> icon on the left menu and select <code>Dashboard</code>.</li> <li>Click <code>Add new panel</code>.</li> </ol>"},{"location":"dashboards/#step-4-add-queries-to-visualize-harvest-metrics","title":"Step 4: Add Queries to Visualize Harvest Metrics","text":"<p>In the new panel, you can add queries to visualize the Harvest metrics:</p> <ol> <li>In the query editor, select <code>Prometheus</code> as the data source.</li> <li>Write your query to visualize the Harvest counters. Prometheus uses a language called PromQL for querying data. The exact query will depend on the specific Harvest counters you want to visualize. You can refer to the Harvest metrics documentation for details on the available metrics.</li> <li>Adjust the visualization settings as needed, and click <code>Apply</code> to add the panel to the dashboard.</li> </ol>"},{"location":"dashboards/#step-5-save-the-dashboard","title":"Step 5: Save the Dashboard","text":"<p>Once you're satisfied with the panels and layout of your dashboard, don't forget to save it. You can then share it with others, or keep it for your own use.</p> <p>Remember, the specifics of these steps can vary depending on your exact setup and requirements. This guide provides a general approach, but you may need to adjust it for your situation.</p>"},{"location":"eseries-metrics/","title":"E-Series Metrics","text":"<p>This document describes which E-Series metrics are collected and what those metrics are named in Harvest, including:</p> <ul> <li>Details about which Harvest metrics each dashboard uses. These can be generated on demand by running <code>bin/harvest grafana metrics</code>. See #1577 for details.</li> </ul> <pre><code>Creation Date : 2026-Feb-20\nE-Series Version: 11.80.0\n</code></pre> Navigate to Grafana dashboards <p>Add your Grafana instance to the following form and save it. When you click on dashboard links on this page, a link to your dashboard will be opened. NAbox hosts Grafana on a subdomain like so: https://localhost/grafana/</p> <p> Grafana Host Save </p>"},{"location":"eseries-metrics/#understanding-the-structure","title":"Understanding the structure","text":"<p>Below is an annotated example of how to interpret the structure of each of the metrics.</p> <p>eseries_volume_read_ops Name of the metric exported by Harvest</p> <p>Volume read I/O operations per second. Description of the E-Series metric</p> <ul> <li>API will be REST since E-Series uses the REST API</li> <li>Endpoint name of the REST API endpoint used to collect this metric</li> <li>Metric name of the E-Series counter</li> <li>Template path of the template that collects the metric</li> </ul> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> readOps conf/eseriesperf/11.80.0/volume.yaml"},{"location":"eseries-metrics/#metrics","title":"Metrics","text":""},{"location":"eseries-metrics/#eseries_array_cache_hit_ops","title":"eseries_array_cache_hit_ops","text":"<p>Total number of IO operations that hit cache on the array</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>cacheHitsIopsTotal</code> conf/eseriesperf/11.80.0/array.yaml <p>The <code>eseries_array_cache_hit_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Highlights timeseries Top $TopResources Arrays by Cache Hit IOPS"},{"location":"eseries-metrics/#eseries_array_drive_count","title":"eseries_array_drive_count","text":"<p>Total number of drives in the storage array</p> API Endpoint Metric Template REST <code>storage-systems</code> <code>driveCount</code> conf/eseries/11.80.0/array.yaml"},{"location":"eseries-metrics/#eseries_array_free_pool_space","title":"eseries_array_free_pool_space","text":"<p>Free space available in storage pools in bytes</p> API Endpoint Metric Template REST <code>storage-systems</code> <code>freePoolSpace</code> conf/eseries/11.80.0/array.yaml <p>The <code>eseries_array_free_pool_space</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Capacity timeseries Top $TopResources Systems by Storage Capacity Used % E-Series: Array Capacity timeseries Top $TopResources Systems by Free Space"},{"location":"eseries-metrics/#eseries_array_host_spares_used","title":"eseries_array_host_spares_used","text":"<p>Number of hot spare drives currently in use</p> API Endpoint Metric Template REST <code>storage-systems</code> <code>hostSparesUsed</code> conf/eseries/11.80.0/array.yaml"},{"location":"eseries-metrics/#eseries_array_labels","title":"eseries_array_labels","text":"<p>This metric provides information about E-Series storage arrays.</p> API Endpoint Metric Template REST <code>storage-systems</code> <code>Harvest generated</code> conf/eseries/11.80.0/array.yaml <p>The <code>eseries_array_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Capacity table Array Configuration"},{"location":"eseries-metrics/#eseries_array_read_data","title":"eseries_array_read_data","text":"<p>Array-wide read data throughput in bytes per second</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>readBytesTotal</code> conf/eseriesperf/11.80.0/array.yaml <p>The <code>eseries_array_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Highlights timeseries Top $TopResources Arrays by Read Throughput"},{"location":"eseries-metrics/#eseries_array_read_ops","title":"eseries_array_read_ops","text":"<p>Array-wide read I/O operations per second</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>readIopsTotal</code> conf/eseriesperf/11.80.0/array.yaml <p>The <code>eseries_array_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Highlights timeseries Top $TopResources Arrays by Read IOPS"},{"location":"eseries-metrics/#eseries_array_tray_count","title":"eseries_array_tray_count","text":"<p>Number of drive trays in the storage array</p> API Endpoint Metric Template REST <code>storage-systems</code> <code>trayCount</code> conf/eseries/11.80.0/array.yaml"},{"location":"eseries-metrics/#eseries_array_unconfigured_space","title":"eseries_array_unconfigured_space","text":"<p>Unconfigured space available in the storage array in bytes</p> API Endpoint Metric Template REST <code>storage-systems</code> <code>unconfiguredSpace</code> conf/eseries/11.80.0/array.yaml <p>The <code>eseries_array_unconfigured_space</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Capacity timeseries Top $TopResources Systems by Unconfigured Space"},{"location":"eseries-metrics/#eseries_array_used_pool_space","title":"eseries_array_used_pool_space","text":"<p>Used space in storage pools in bytes</p> API Endpoint Metric Template REST <code>storage-systems</code> <code>usedPoolSpace</code> conf/eseries/11.80.0/array.yaml <p>The <code>eseries_array_used_pool_space</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Capacity timeseries Top $TopResources Systems by Storage Capacity Used % E-Series: Array Capacity timeseries Top $TopResources Systems by Used Space"},{"location":"eseries-metrics/#eseries_array_write_data","title":"eseries_array_write_data","text":"<p>Array-wide write data throughput in bytes per second</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>writeBytesTotal</code> conf/eseriesperf/11.80.0/array.yaml <p>The <code>eseries_array_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Highlights timeseries Top $TopResources Arrays by Write Throughput"},{"location":"eseries-metrics/#eseries_array_write_ops","title":"eseries_array_write_ops","text":"<p>Array-wide write I/O operations per second</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>writeIopsTotal</code> conf/eseriesperf/11.80.0/array.yaml <p>The <code>eseries_array_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Highlights timeseries Top $TopResources Arrays by Write IOPS"},{"location":"eseries-metrics/#eseries_battery_labels","title":"eseries_battery_labels","text":"<p>This metric provides information about batteries.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml <p>The <code>eseries_battery_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Battery table Battery"},{"location":"eseries-metrics/#eseries_cache_backup_device_capacity","title":"eseries_cache_backup_device_capacity","text":"<p>Capacity of the cache backup device in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>cacheBackupDevices.capacityInMegabytes</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_cache_backup_device_capacity</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Cache table Cache Backup Devices"},{"location":"eseries-metrics/#eseries_cache_backup_device_labels","title":"eseries_cache_backup_device_labels","text":"<p>This metric provides information about cache backup devices.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_cache_backup_device_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Cache table Cache Backup Devices"},{"location":"eseries-metrics/#eseries_cache_memory_dimm_capacity","title":"eseries_cache_memory_dimm_capacity","text":"<p>Capacity of the cache memory DIMM in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>cacheMemoryDimms.capacityInMegabytes</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_cache_memory_dimm_capacity</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Cache table Cache Memory DIMMs"},{"location":"eseries-metrics/#eseries_cache_memory_dimm_labels","title":"eseries_cache_memory_dimm_labels","text":"<p>This metric provides information about cache memory DIMMs.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_cache_memory_dimm_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Cache table Cache Memory DIMMs"},{"location":"eseries-metrics/#eseries_controller_cache_hit_ops","title":"eseries_controller_cache_hit_ops","text":"<p>Total number of IO operations that hit cache on the controller</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>cacheHitsIopsTotal</code> conf/eseriesperf/11.80.0/controller.yaml <p>The <code>eseries_controller_cache_hit_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Highlights timeseries Top $TopResources Controllers by Cache Hit Ops"},{"location":"eseries-metrics/#eseries_controller_code_version_labels","title":"eseries_controller_code_version_labels","text":"<p>This metric provides information about controller code versions.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_controller_code_version_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Highlights table Code Versions"},{"location":"eseries-metrics/#eseries_controller_cpu_utilization","title":"eseries_controller_cpu_utilization","text":"<p>Controller CPU utilization percentage</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>cpuUtilizationStats.0.sumCpuUtilization</code> conf/eseriesperf/11.80.0/controller.yaml <p>The <code>eseries_controller_cpu_utilization</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Highlights timeseries Top $TopResources Controllers by CPU Utilization"},{"location":"eseries-metrics/#eseries_controller_drive_interface_labels","title":"eseries_controller_drive_interface_labels","text":"<p>This metric provides information about controller drive-side interfaces.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/interfaces?channelType=driveside</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_controller_drive_interface_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Drive Interfaces table Drive Interfaces"},{"location":"eseries-metrics/#eseries_controller_host_interface_labels","title":"eseries_controller_host_interface_labels","text":"<p>This metric provides information about controller host-side interfaces.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/interfaces?channelType=hostside</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_controller_host_interface_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Host Interfaces table Host Interfaces"},{"location":"eseries-metrics/#eseries_controller_labels","title":"eseries_controller_labels","text":"<p>This metric provides information about controllers.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml <p>The <code>eseries_controller_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Controller Details table Controller Configuration E-Series: Hardware Highlights table Controller Configuration"},{"location":"eseries-metrics/#eseries_controller_net_interface_labels","title":"eseries_controller_net_interface_labels","text":"<p>This metric provides information about controller network interfaces.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_controller_net_interface_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Management table Management Ports E-Series: Hardware Management table DNS &amp; NTP Configuration"},{"location":"eseries-metrics/#eseries_controller_processor_memory","title":"eseries_controller_processor_memory","text":"<p>Controller processor memory size in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>controllers.processorMemorySizeMiB</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_controller_processor_memory</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Controller Details table Controller Cache &amp; Memory E-Series: Hardware Cache table Controller Cache &amp; Memory E-Series: Hardware Cache timeseries Top $TopResources Controllers by Processor Cache"},{"location":"eseries-metrics/#eseries_controller_read_data","title":"eseries_controller_read_data","text":"<p>Total number of bytes read by the controller</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>readBytesTotal</code> conf/eseriesperf/11.80.0/controller.yaml <p>The <code>eseries_controller_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Highlights timeseries Top $TopResources Controllers by Read Throughput"},{"location":"eseries-metrics/#eseries_controller_read_ops","title":"eseries_controller_read_ops","text":"<p>Total number of read IO operations serviced by the controller</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>readIopsTotal</code> conf/eseriesperf/11.80.0/controller.yaml <p>The <code>eseries_controller_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Highlights timeseries Top $TopResources Controllers by Read IOPS"},{"location":"eseries-metrics/#eseries_controller_total_cache_memory","title":"eseries_controller_total_cache_memory","text":"<p>Total cache memory on the controller in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>controllers.cacheMemorySize</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_controller_total_cache_memory</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Controller Details table Controller Cache &amp; Memory E-Series: Hardware Cache table Controller Cache &amp; Memory E-Series: Hardware Cache timeseries Top $TopResources Controllers by Data Cache Total"},{"location":"eseries-metrics/#eseries_controller_used_cache_memory","title":"eseries_controller_used_cache_memory","text":"<p>Used cache memory on the controller in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>controllers</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_controller_used_cache_memory</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Controller Details table Controller Cache &amp; Memory E-Series: Hardware Cache table Controller Cache &amp; Memory E-Series: Hardware Cache timeseries Top $TopResources Controllers by Data Cache Used"},{"location":"eseries-metrics/#eseries_controller_write_data","title":"eseries_controller_write_data","text":"<p>Total number of bytes written by the controller</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>writeBytesTotal</code> conf/eseriesperf/11.80.0/controller.yaml <p>The <code>eseries_controller_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Highlights timeseries Top $TopResources Controllers by Write Throughput"},{"location":"eseries-metrics/#eseries_controller_write_ops","title":"eseries_controller_write_ops","text":"<p>Total number of write IO operations serviced by the controller</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>writeIopsTotal</code> conf/eseriesperf/11.80.0/controller.yaml <p>The <code>eseries_controller_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Controller Highlights timeseries Top $TopResources Controllers by Write IOPS"},{"location":"eseries-metrics/#eseries_drive_block_size","title":"eseries_drive_block_size","text":"<p>Logical block size of the drive in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>drives.blkSize</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_drive_block_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Drives table Drives"},{"location":"eseries-metrics/#eseries_drive_block_size_physical","title":"eseries_drive_block_size_physical","text":"<p>Physical block size of the drive in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>drives.blkSizePhysical</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_drive_block_size_physical</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Drives table Drives"},{"location":"eseries-metrics/#eseries_drive_capacity","title":"eseries_drive_capacity","text":"<p>Raw capacity of the drive in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>drives.rawCapacity</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_drive_capacity</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Drives table Drives"},{"location":"eseries-metrics/#eseries_drive_labels","title":"eseries_drive_labels","text":"<p>This metric provides information about drives.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_drive_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Drives table Drives"},{"location":"eseries-metrics/#eseries_drive_percent_endurance_used","title":"eseries_drive_percent_endurance_used","text":"<p>Percentage of SSD endurance used for solid state drives</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>drives.ssdWearLife.percentEnduranceUsed</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_drive_percent_endurance_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Drives table Drives"},{"location":"eseries-metrics/#eseries_fan_labels","title":"eseries_fan_labels","text":"<p>This metric provides information about fans.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml <p>The <code>eseries_fan_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Fan table Fan"},{"location":"eseries-metrics/#eseries_host_labels","title":"eseries_host_labels","text":"<p>This metric provides information about hosts connected to the storage array.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hosts</code> <code>Harvest generated</code> conf/eseries/11.80.0/host.yaml <p>The <code>eseries_host_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Array Host table Host Configuration"},{"location":"eseries-metrics/#eseries_power_supply_labels","title":"eseries_power_supply_labels","text":"<p>This metric provides information about power supplies.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml <p>The <code>eseries_power_supply_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Power Supply table Power Supply"},{"location":"eseries-metrics/#eseries_sfp_labels","title":"eseries_sfp_labels","text":"<p>This metric provides information about SFP transceivers.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_sfp_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware SFP table SFP"},{"location":"eseries-metrics/#eseries_thermal_sensor_labels","title":"eseries_thermal_sensor_labels","text":"<p>This metric provides information about thermal sensors.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/hardware-inventory</code> <code>Harvest generated</code> conf/eseries/11.80.0/hardware.yaml (Hardware plugin) <p>The <code>eseries_thermal_sensor_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Hardware Thermal Sensor table Thermal Sensor"},{"location":"eseries-metrics/#eseries_volume_allocated_capacity","title":"eseries_volume_allocated_capacity","text":"<p>Allocated capacity of the volume in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/volumes</code> <code>totalSizeInBytes</code> conf/eseries/11.80.0/volume.yaml <p>The <code>eseries_volume_allocated_capacity</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Volume Table table Volumes"},{"location":"eseries-metrics/#eseries_volume_block_size","title":"eseries_volume_block_size","text":"<p>Block size of the volume in bytes</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/volumes</code> <code>blkSize</code> conf/eseries/11.80.0/volume.yaml <p>The <code>eseries_volume_block_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Volume Table table Volumes"},{"location":"eseries-metrics/#eseries_volume_labels","title":"eseries_volume_labels","text":"<p>This metric provides information about volumes.</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/volumes</code> <code>Harvest generated</code> conf/eseries/11.80.0/volume.yaml <p>The <code>eseries_volume_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Volume Table table Volumes"},{"location":"eseries-metrics/#eseries_volume_read_cache_hit_ratio","title":"eseries_volume_read_cache_hit_ratio","text":"<p>Volume read cache hit ratio calculated from read hit operations and total read operations</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>Harvest Generated</code> conf/eseriesperf/11.80.0/volume.yaml (CacheHitRatio plugin) <p>The <code>eseries_volume_read_cache_hit_ratio</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Cache timeseries Top $TopResources Volumes by Read Cache Hit Ratio"},{"location":"eseries-metrics/#eseries_volume_read_data","title":"eseries_volume_read_data","text":"<p>Volume read data throughput in bytes per second</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>readBytes</code> conf/eseriesperf/11.80.0/volume.yaml <p>The <code>eseries_volume_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Highlights timeseries Top $TopResources Volumes by Read Throughput"},{"location":"eseries-metrics/#eseries_volume_read_hit_ops","title":"eseries_volume_read_hit_ops","text":"<p>Number of read operations that hit cache</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>readHitOps</code> conf/eseriesperf/11.80.0/volume.yaml"},{"location":"eseries-metrics/#eseries_volume_read_latency","title":"eseries_volume_read_latency","text":"<p>Read response time average in microseconds</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>readTimeTotal</code> conf/eseriesperf/11.80.0/volume.yaml <p>The <code>eseries_volume_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Highlights timeseries Top $TopResources Volumes by Read Latency"},{"location":"eseries-metrics/#eseries_volume_read_ops","title":"eseries_volume_read_ops","text":"<p>Volume read I/O operations per second</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>readOps</code> conf/eseriesperf/11.80.0/volume.yaml <p>The <code>eseries_volume_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Highlights timeseries Top $TopResources Volumes by Read IOPs"},{"location":"eseries-metrics/#eseries_volume_reported_capacity","title":"eseries_volume_reported_capacity","text":"<p>The capacity in bytes of the volume</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/volumes</code> <code>capacity</code> conf/eseries/11.80.0/volume.yaml <p>The <code>eseries_volume_reported_capacity</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Volume Table table Volumes"},{"location":"eseries-metrics/#eseries_volume_total_cache_hit_ratio","title":"eseries_volume_total_cache_hit_ratio","text":"<p>Volume total cache hit ratio combining read and write cache hit operations</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>Harvest Generated</code> conf/eseriesperf/11.80.0/volume.yaml (CacheHitRatio plugin) <p>The <code>eseries_volume_total_cache_hit_ratio</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Cache timeseries Top $TopResources Volumes by Total Cache Hit Ratio"},{"location":"eseries-metrics/#eseries_volume_write_cache_hit_ratio","title":"eseries_volume_write_cache_hit_ratio","text":"<p>Volume write cache hit ratio calculated from write hit operations and total write operations</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>Harvest Generated</code> conf/eseriesperf/11.80.0/volume.yaml (CacheHitRatio plugin) <p>The <code>eseries_volume_write_cache_hit_ratio</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Cache timeseries Top $TopResources Volumes by Write Cache Hit Ratio"},{"location":"eseries-metrics/#eseries_volume_write_data","title":"eseries_volume_write_data","text":"<p>Volume write data throughput in bytes per second</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>writeBytes</code> conf/eseriesperf/11.80.0/volume.yaml <p>The <code>eseries_volume_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Highlights timeseries Top $TopResources Volumes by Write Throughput"},{"location":"eseries-metrics/#eseries_volume_write_hit_ops","title":"eseries_volume_write_hit_ops","text":"<p>Volume write cache hit operations per second</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>writeHitOps</code> conf/eseriesperf/11.80.0/volume.yaml"},{"location":"eseries-metrics/#eseries_volume_write_latency","title":"eseries_volume_write_latency","text":"<p>Write response time average in microseconds</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>writeTimeTotal</code> conf/eseriesperf/11.80.0/volume.yaml <p>The <code>eseries_volume_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Highlights timeseries Top $TopResources Volumes by Write Latency"},{"location":"eseries-metrics/#eseries_volume_write_ops","title":"eseries_volume_write_ops","text":"<p>Volume write I/O operations per second</p> API Endpoint Metric Template REST <code>storage-systems/{array_id}/live-statistics</code> <code>writeOps</code> conf/eseriesperf/11.80.0/volume.yaml <p>The <code>eseries_volume_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel E-Series: Volume Highlights timeseries Top $TopResources Volumes by Write IOPs"},{"location":"influxdb-exporter/","title":"InfluxDB Exporter","text":"InfluxDB Install <p>The information below describes how to setup Harvest's InfluxDB exporter.  If you need help installing or setting up InfluxDB, check  out their documentation.</p>"},{"location":"influxdb-exporter/#overview","title":"Overview","text":"<p>The InfluxDB Exporter will format metrics into the InfluxDB's line protocol and write it into a bucket. The Exporter is compatible with InfluxDB v2.0. For explanation about <code>bucket</code>, <code>org</code> and <code>precision</code>, see InfluxDB API documentation.</p> <p>If you are monitoring both CDOT and 7mode clusters, it is strongly recommended to use two different buckets.</p>"},{"location":"influxdb-exporter/#parameters","title":"Parameters","text":"<p>Overview of all parameters is provided below. Only one of <code>url</code> or <code>addr</code> should be provided and at least one of them is required. If <code>addr</code> is specified, it should be a valid TCP address or hostname of the InfluxDB server and should not include the scheme. When using <code>addr</code>, the <code>bucket</code>, <code>org</code>, and <code>token</code> key/values are required.</p> <p><code>addr</code> only works with HTTP. If you need to use HTTPS, you should use <code>url</code> instead.</p> <p>If <code>url</code> is specified, you must add all arguments to the url. Harvest will do no additional processing and use exactly what you specify. ( e.g. <code>url: https://influxdb.example.com:8086/write?db=netapp&amp;u=user&amp;p=pass&amp;precision=2</code>. When using <code>url</code>, the <code>bucket</code>, <code>org</code>, <code>port</code>, and <code>precision</code> fields will be ignored.</p> parameter type description default <code>url</code> string URL of the database, format: <code>SCHEME://HOST[:PORT]</code> <code>addr</code> string address of the database, format: <code>HOST</code> (HTTP only) <code>port</code> int, optional port of the database <code>8086</code> <code>bucket</code> string, required with <code>addr</code> InfluxDB bucket to write <code>org</code> string, required with <code>addr</code> InfluxDB organization name <code>precision</code> string, required with <code>addr</code> Preferred timestamp precision in seconds <code>2</code> <code>client_timeout</code> int, optional client timeout in seconds <code>5</code> <code>token</code> string token for authentication"},{"location":"influxdb-exporter/#example","title":"Example","text":"<p>snippet from <code>harvest.yml</code> using <code>addr</code>: (supports HTTP only))</p> <pre><code>Exporters:\n  my_influx:\n    exporter: InfluxDB\n    addr: localhost\n    bucket: harvest\n    org: harvest\n    token: ZTTrt%24@#WNFM2VZTTNNT25wZWUdtUmhBZEdVUmd3dl@# \n</code></pre> <p>snippet from <code>harvest.yml</code> using <code>url</code>: (supports both HTTP/HTTPS))</p> <pre><code>Exporters:\n  influx2:\n    exporter: InfluxDB\n    url: https://localhost:8086/api/v2/write?org=harvest&amp;bucket=harvest&amp;precision=s\n    token: my-token== \n</code></pre> <p>Notice: InfluxDB stores a token in <code>~/.influxdbv2/configs</code>, but you can also retrieve it from the UI (usually serving on <code>localhost:8086</code>): click on \"Data\" on the left task bar, then on \"Tokens\".</p>"},{"location":"license/","title":"License","text":"<p>Harvest's License</p>"},{"location":"manage-harvest/","title":"Manage Harvest Pollers","text":"<p>Coming Soon</p>"},{"location":"monitor-harvest/","title":"Monitor Harvest","text":""},{"location":"monitor-harvest/#harvest-metadata","title":"Harvest Metadata","text":"<p>Harvest publishes metadata metrics about the key components of Harvest. Many of these metrics are used in the <code>Harvest Metadata</code> dashboard.</p> <p>If you want to understand more about these metrics, read on!</p> <p>Metrics are published for:</p> <ul> <li>collectors</li> <li>pollers</li> <li>clusters being monitored</li> <li>exporters</li> </ul> <p>Here's a high-level summary of the metadata metrics Harvest publishes with details below.</p> Metric Description Units metadata_collector_api_time amount of time to collect data from monitored cluster object microseconds metadata_collector_instances number of objects collected from monitored cluster scalar metadata_collector_metrics number of counters collected from monitored cluster scalar metadata_collector_parse_time amount of time to parse XML, JSON, etc. for cluster object microseconds metadata_collector_plugin_time amount of time for all plugins to post-process metrics microseconds metadata_collector_poll_time amount of time it took for the poll to finish microseconds metadata_collector_task_time amount of time it took for each collector's subtasks to complete microseconds metadata_component_count number of metrics collected for each object scalar metadata_component_status status of the collector - 0 means running, 1 means standby, 2 means failed enum metadata_exporter_count number of metrics and labels exported scalar metadata_exporter_time amount of time it took to render, export, and serve exported data microseconds metadata_target_goroutines number of goroutines that exist within the poller scalar metadata_target_status status of the system being monitored. 0 means reachable, 1 means unreachable enum metadata_collector_calc_time amount of time it took to compute metrics between two successive polls, specifically using properties like raw, delta, rate, average, and percent. This metric is available for ZapiPerf/RestPerf collectors. microseconds metadata_collector_skips number of metrics that were not calculated between two successive polls. This metric is available for ZapiPerf/RestPerf collectors. scalar"},{"location":"monitor-harvest/#collector-metadata","title":"Collector Metadata","text":"<p>A poller publishes the metadata metrics for each collector and exporter associated with it.</p> <p>Let's say we start a poller with the <code>Zapi</code> collector and the out-of-the-box <code>default.yaml</code> exporting metrics to Prometheus. That means you will be monitoring 22 different objects (uncommented lines in <code>default.yaml</code> as of 23.02).</p> <p>When we start this poller, we expect it to export 23 <code>metadata_component_status</code> metrics.  One for each of the 22 objects, plus one for the Prometheus exporter.</p> <p>The following <code>curl</code> confirms there are 23 <code>metadata_component_status</code> metrics reported.</p> <pre><code>curl -s http://localhost:12990/metrics | grep -v \"#\" | grep metadata_component_status | wc -l\n      23\n</code></pre> <p>These metrics also tell us which collectors are in an standby or failed state.  For example, filtering on components not in the <code>running</code> state shows the following since this cluster doesn't have any <code>ClusterPeers</code>, <code>SecurityAuditDestinations</code>, or <code>SnapMirrors</code>. The <code>reason</code> is listed as <code>no instances</code> and the metric value is 1 which means standby. </p> <pre><code>curl -s http://localhost:12990/metrics | grep -v \"#\" | grep metadata_component_status | grep -Evo \"running\"\nmetadata_component_status{name=\"Zapi\", reason=\"no instances\",target=\"ClusterPeer\",type=\"collector\",version=\"23.04.1417\"} 1\nmetadata_component_status{name=\"Zapi\", reason=\"no instances\",target=\"SecurityAuditDestination\",type=\"collector\",version=\"23.04.1417\"} 1\nmetadata_component_status{name=\"Zapi\", reason=\"no instances\",target=\"SnapMirror\",type=\"collector\",version=\"23.04.1417\"} 1\n</code></pre> <p>The log files for the poller show a similar story. The poller starts with 22 collectors, but drops to 19 after three of the collectors go to standby because there are no instances to collect. </p> <pre><code>2023-04-17T13:14:18-04:00 INF ./poller.go:539 &gt; updated status, up collectors: 22 (of 22), up exporters: 1 (of 1) Poller=u2\n2023-04-17T13:14:18-04:00 INF collector/collector.go:342 &gt; no instances, entering standby Poller=u2 collector=Zapi:SecurityAuditDestination task=data\n2023-04-17T13:14:18-04:00 INF collector/collector.go:342 &gt; no instances, entering standby Poller=u2 collector=Zapi:ClusterPeer task=data\n2023-04-17T13:14:18-04:00 INF collector/collector.go:342 &gt; no instances, entering standby Poller=u2 collector=Zapi:SnapMirror task=data\n2023-04-17T13:15:18-04:00 INF ./poller.go:539 &gt; updated status, up collectors: 19 (of 22), up exporters: 1 (of 1) Poller=u2\n</code></pre>"},{"location":"ontap-metrics/","title":"ONTAP Metrics","text":"<p>This document describes how Harvest metrics relate to their relevant ONTAP ZAPI and REST mappings, including:</p> <ul> <li> <p>Details about which Harvest metrics each dashboard uses. These can be generated on demand by running <code>bin/harvest grafana metrics</code>. See #1577 for details.</p> </li> <li> <p>More information about ONTAP REST performance counters can be found here.</p> </li> </ul> <pre><code>Creation Date : 2026-Feb-20\nONTAP Version: 9.16.1\n</code></pre> Navigate to Grafana dashboards <p>Add your Grafana instance to the following form and save it. When you click on dashboard links on this page, a link to your dashboard will be opened. NAbox hosts Grafana on a subdomain like so: https://localhost/grafana/</p> <p> Grafana Host Save </p>"},{"location":"ontap-metrics/#understanding-the-structure","title":"Understanding the structure","text":"<p>Below is an annotated example of how to interpret the structure of each of the metrics.</p> <p>disk_io_queued Name of the metric exported by Harvest</p> <p>Number of I/Os queued to the disk but not yet issued Description of the ONTAP metric</p> <ul> <li>API will be one of REST or ZAPI depending on which collector is used to collect the metric</li> <li>Endpoint name of the REST or ZAPI API used to collect this metric</li> <li>Metric name of the ONTAP metric</li> <li>Template path of the template that collects the metric</li> </ul> <p>Performance related metrics also include:</p> <ul> <li>Unit the unit of the metric</li> <li>Type describes how to calculate a cooked metric from two consecutive ONTAP raw metrics</li> <li>Base some counters require a <code>base counter</code> for post-processing. When required, this property lists the <code>base counter</code></li> </ul> API Endpoint Metric Template REST <code>api/cluster/counter/tables/disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZAPI <code>perf-object-get-instances disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#metrics","title":"Metrics","text":""},{"location":"ontap-metrics/#aggr_disk_busy","title":"aggr_disk_busy","text":"<p>The utilization percent of the disk. aggr_disk_busy is disk_busy aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>disk_busy_percent</code>Unit: percentType: percentBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_busy</code>Unit: percentType: percentBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_disk_busy</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Disk Utilization table Top $TopResources Average Disk Utilization Per Aggregate ONTAP: Aggregate Disk Utilization timeseries Top $TopResources Average Disk Utilization Per Aggregate ONTAP: Cluster Throughput timeseries Average Disk Utilization by Aggregate ONTAP: Disk Highlights stat Raid Groups ONTAP: Disk Highlights stat Plexes"},{"location":"ontap-metrics/#aggr_disk_capacity","title":"aggr_disk_capacity","text":"<p>Disk capacity in MB. aggr_disk_capacity is disk_capacity aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>capacity</code>Unit: mbType: rawBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_capacity</code>Unit: mbType: rawBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_cp_read_chain","title":"aggr_disk_cp_read_chain","text":"<p>Average number of blocks transferred in each consistency point read operation during a CP. aggr_disk_cp_read_chain is disk_cp_read_chain aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_cp_read_latency","title":"aggr_disk_cp_read_latency","text":"<p>Average latency per block in microseconds for consistency point read operations. aggr_disk_cp_read_latency is disk_cp_read_latency aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_cp_reads","title":"aggr_disk_cp_reads","text":"<p>Number of disk read operations initiated each second for consistency point processing. aggr_disk_cp_reads is disk_cp_reads aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_io_pending","title":"aggr_disk_io_pending","text":"<p>Average number of I/Os issued to the disk for which we have not yet received the response. aggr_disk_io_pending is disk_io_pending aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_io_queued","title":"aggr_disk_io_queued","text":"<p>Number of I/Os queued to the disk but not yet issued. aggr_disk_io_queued is disk_io_queued aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_busy","title":"aggr_disk_max_busy","text":"<p>The utilization percent of the disk. aggr_disk_max_busy is the maximum of disk_busy for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>disk_busy_percent</code>Unit: percentType: percentBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_busy</code>Unit: percentType: percentBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_disk_max_busy</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Highlights table Top $TopResources Aggregates by Disk Utilization ONTAP: Disk Highlights timeseries Top $TopResources Aggregates by Max Disk Utilization ONTAP: MetroCluster Highlights gauge Max Disk Utilization Per Aggregate"},{"location":"ontap-metrics/#aggr_disk_max_capacity","title":"aggr_disk_max_capacity","text":"<p>Disk capacity in MB. aggr_disk_max_capacity is the maximum of disk_capacity for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>capacity</code>Unit: mbType: rawBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_capacity</code>Unit: mbType: rawBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_cp_read_chain","title":"aggr_disk_max_cp_read_chain","text":"<p>Average number of blocks transferred in each consistency point read operation during a CP. aggr_disk_max_cp_read_chain is the maximum of disk_cp_read_chain for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_cp_read_latency","title":"aggr_disk_max_cp_read_latency","text":"<p>Average latency per block in microseconds for consistency point read operations. aggr_disk_max_cp_read_latency is the maximum of disk_cp_read_latency for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_cp_reads","title":"aggr_disk_max_cp_reads","text":"<p>Number of disk read operations initiated each second for consistency point processing. aggr_disk_max_cp_reads is the maximum of disk_cp_reads for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_io_pending","title":"aggr_disk_max_io_pending","text":"<p>Average number of I/Os issued to the disk for which we have not yet received the response. aggr_disk_max_io_pending is the maximum of disk_io_pending for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_io_queued","title":"aggr_disk_max_io_queued","text":"<p>Number of I/Os queued to the disk but not yet issued. aggr_disk_max_io_queued is the maximum of disk_io_queued for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_total_data","title":"aggr_disk_max_total_data","text":"<p>Total throughput for user operations per second. aggr_disk_max_total_data is the maximum of disk_total_data for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_total_transfers","title":"aggr_disk_max_total_transfers","text":"<p>Total number of disk operations involving data transfer initiated per second. aggr_disk_max_total_transfers is the maximum of disk_total_transfers for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_transfer_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_transfers</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_disk_max_total_transfers</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Highlights table Top $TopResources Aggregates by Disk Utilization ONTAP: Disk Highlights timeseries Top $TopResources Aggregates by Disk Transfers"},{"location":"ontap-metrics/#aggr_disk_max_user_read_blocks","title":"aggr_disk_max_user_read_blocks","text":"<p>Number of blocks transferred for user read operations per second. aggr_disk_max_user_read_blocks is the maximum of disk_user_read_blocks for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_user_read_chain","title":"aggr_disk_max_user_read_chain","text":"<p>Average number of blocks transferred in each user read operation. aggr_disk_max_user_read_chain is the maximum of disk_user_read_chain for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_reads conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_disk_max_user_read_chain</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Highlights timeseries Top $TopResources Aggregates by User Read Chain Length"},{"location":"ontap-metrics/#aggr_disk_max_user_read_latency","title":"aggr_disk_max_user_read_latency","text":"<p>Average latency per block in microseconds for user read operations. aggr_disk_max_user_read_latency is the maximum of disk_user_read_latency for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_user_reads","title":"aggr_disk_max_user_reads","text":"<p>Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. aggr_disk_max_user_reads is the maximum of disk_user_reads for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_user_write_blocks","title":"aggr_disk_max_user_write_blocks","text":"<p>Number of blocks transferred for user write operations per second. aggr_disk_max_user_write_blocks is the maximum of disk_user_write_blocks for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_user_write_chain","title":"aggr_disk_max_user_write_chain","text":"<p>Average number of blocks transferred in each user write operation. aggr_disk_max_user_write_chain is the maximum of disk_user_write_chain for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_write_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_writes conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_disk_max_user_write_chain</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Highlights timeseries Top $TopResources Aggregates by User Write Chain Length"},{"location":"ontap-metrics/#aggr_disk_max_user_write_latency","title":"aggr_disk_max_user_write_latency","text":"<p>Average latency per block in microseconds for user write operations. aggr_disk_max_user_write_latency is the maximum of disk_user_write_latency for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_max_user_writes","title":"aggr_disk_max_user_writes","text":"<p>Number of disk write operations initiated each second for storing data or metadata associated with user requests. aggr_disk_max_user_writes is the maximum of disk_user_writes for label <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_writes</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_total_data","title":"aggr_disk_total_data","text":"<p>Total throughput for user operations per second. aggr_disk_total_data is disk_total_data aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_disk_total_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Aggregate table Aggregates"},{"location":"ontap-metrics/#aggr_disk_total_transfers","title":"aggr_disk_total_transfers","text":"<p>Total number of disk operations involving data transfer initiated per second. aggr_disk_total_transfers is disk_total_transfers aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_transfer_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_transfers</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_disk_total_transfers</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Aggregate table Aggregates"},{"location":"ontap-metrics/#aggr_disk_user_read_blocks","title":"aggr_disk_user_read_blocks","text":"<p>Number of blocks transferred for user read operations per second. aggr_disk_user_read_blocks is disk_user_read_blocks aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_user_read_chain","title":"aggr_disk_user_read_chain","text":"<p>Average number of blocks transferred in each user read operation. aggr_disk_user_read_chain is disk_user_read_chain aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_user_read_latency","title":"aggr_disk_user_read_latency","text":"<p>Average latency per block in microseconds for user read operations. aggr_disk_user_read_latency is disk_user_read_latency aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_user_reads","title":"aggr_disk_user_reads","text":"<p>Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. aggr_disk_user_reads is disk_user_reads aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_disk_user_reads</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Highlights timeseries Top $TopResources Aggregates by IOPS Per Power Consumed"},{"location":"ontap-metrics/#aggr_disk_user_write_blocks","title":"aggr_disk_user_write_blocks","text":"<p>Number of blocks transferred for user write operations per second. aggr_disk_user_write_blocks is disk_user_write_blocks aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_user_write_chain","title":"aggr_disk_user_write_chain","text":"<p>Average number of blocks transferred in each user write operation. aggr_disk_user_write_chain is disk_user_write_chain aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_write_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_writes conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_user_write_latency","title":"aggr_disk_user_write_latency","text":"<p>Average latency per block in microseconds for user write operations. aggr_disk_user_write_latency is disk_user_write_latency aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#aggr_disk_user_writes","title":"aggr_disk_user_writes","text":"<p>Number of disk write operations initiated each second for storing data or metadata associated with user requests. aggr_disk_user_writes is disk_user_writes aggregated by <code>aggr</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_writes</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_disk_user_writes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Highlights timeseries Top $TopResources Aggregates by IOPS Per Power Consumed"},{"location":"ontap-metrics/#aggr_efficiency_savings","title":"aggr_efficiency_savings","text":"<p>Space saved by storage efficiencies (logical_used - used)</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.efficiency.savings</code> conf/rest/9.12.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_efficiency_savings_wo_snapshots","title":"aggr_efficiency_savings_wo_snapshots","text":"<p>Space saved by storage efficiencies (logical_used - used)</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.efficiency_without_snapshots.savings</code> conf/rest/9.12.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_efficiency_savings_wo_snapshots_flexclones","title":"aggr_efficiency_savings_wo_snapshots_flexclones","text":"<p>Space saved by storage efficiencies (logical_used - used)</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.efficiency_without_snapshots_flexclones.savings</code> conf/rest/9.12.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_hybrid_cache_size_total","title":"aggr_hybrid_cache_size_total","text":"<p>Total usable space in bytes of SSD cache. Only provided when hybrid_cache.enabled is 'true'.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>block_storage.hybrid_cache.size</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.hybrid-cache-size-total</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_hybrid_disk_count","title":"aggr_hybrid_disk_count","text":"<p>Number of disks used in the cache tier of the aggregate. Only provided when hybrid_cache.enabled is 'true'.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>block_storage.hybrid_cache.disk_count</code> conf/rest/9.12.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_inode_files_private_used","title":"aggr_inode_files_private_used","text":"<p>Number of system metadata files used. If the referenced file system is restricted or offline, a value of 0 is returned.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the fields query parameter containing either footprint or **.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>inode_attributes.files_private_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-inode-attributes.files-private-used</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_inode_files_total","title":"aggr_inode_files_total","text":"<p>Maximum number of user-visible files that this referenced file system can currently hold. If the referenced file system is restricted or offline, a value of 0 is returned.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>inode_attributes.files_total</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-inode-attributes.files-total</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_inode_files_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Inodes Files"},{"location":"ontap-metrics/#aggr_inode_files_used","title":"aggr_inode_files_used","text":"<p>Number of user-visible files used in the referenced file system. If the referenced file system is restricted or offline, a value of 0 is returned.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>inode_attributes.files_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-inode-attributes.files-used</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_inode_files_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Inodes Files"},{"location":"ontap-metrics/#aggr_inode_inodefile_private_capacity","title":"aggr_inode_inodefile_private_capacity","text":"<p>Number of files that can currently be stored on disk for system metadata files. This number will dynamically increase as more system files are created.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the fields query parameter containing either footprint or **.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>inode_attributes.file_private_capacity</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-inode-attributes.inodefile-private-capacity</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_inode_inodefile_private_capacity</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Inode Capacity"},{"location":"ontap-metrics/#aggr_inode_inodefile_public_capacity","title":"aggr_inode_inodefile_public_capacity","text":"<p>Number of files that can currently be stored on disk for user-visible files.  This number will dynamically increase as more user-visible files are created.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the fields query parameter containing either footprint or **.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>inode_attributes.file_public_capacity</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-inode-attributes.inodefile-public-capacity</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_inode_inodefile_public_capacity</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Inode Capacity"},{"location":"ontap-metrics/#aggr_inode_maxfiles_available","title":"aggr_inode_maxfiles_available","text":"<p>The count of the maximum number of user-visible files currently allowable on the referenced file system.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>inode_attributes.max_files_available</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-inode-attributes.maxfiles-available</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_inode_maxfiles_possible","title":"aggr_inode_maxfiles_possible","text":"<p>The largest value to which the maxfiles-available parameter can be increased by reconfiguration, on the referenced file system.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>inode_attributes.max_files_possible</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-inode-attributes.maxfiles-possible</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_inode_maxfiles_used","title":"aggr_inode_maxfiles_used","text":"<p>The number of user-visible files currently in use on the referenced file system.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>inode_attributes.max_files_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-inode-attributes.maxfiles-used</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_inode_used_percent","title":"aggr_inode_used_percent","text":"<p>The percentage of disk space currently in use based on user-visible file count on the referenced file system.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>inode_attributes.used_percent</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-inode-attributes.percent-inode-used-capacity</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_inode_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Inodes Used %"},{"location":"ontap-metrics/#aggr_labels","title":"aggr_labels","text":"<p>This metric provides information about Aggregate</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>Harvest generated</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights stat Aggregates ONTAP: Aggregate Highlights table Aggregates ONTAP: Datacenter Highlights table Object Count ONTAP: StorageGrid FabricPool Highlights stat Aggregates ONTAP: StorageGrid FabricPool Highlights table Aggregates"},{"location":"ontap-metrics/#aggr_logical_used_wo_snapshots","title":"aggr_logical_used_wo_snapshots","text":"<p>Logical used</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.efficiency_without_snapshots.logical_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-efficiency-get-iter</code> <code>aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-logical-used-wo-snapshots</code> conf/zapi/cdot/9.9.0/aggr_efficiency.yaml <p>The <code>aggr_logical_used_wo_snapshots</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Storage Efficiency Ratios stat Data Reduction with FlexClones ONTAP: Aggregate Storage Efficiency Ratios timeseries Top $TopResources Aggregates by Logical Used with FlexClones ONTAP: Cluster Storage Efficiency Ratios stat Data Reduction with FlexClones ONTAP: Cluster Storage Efficiency Ratios timeseries Logical Used with FlexClones ONTAP: Datacenter Storage Efficiency stat Data Reduction with FlexClones ONTAP: Datacenter Storage Efficiency timeseries Top $TopResources Logical Used with FlexClones by Cluster"},{"location":"ontap-metrics/#aggr_logical_used_wo_snapshots_flexclones","title":"aggr_logical_used_wo_snapshots_flexclones","text":"<p>Logical used</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.efficiency_without_snapshots_flexclones.logical_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-efficiency-get-iter</code> <code>aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-logical-used-wo-snapshots-flexclones</code> conf/zapi/cdot/9.9.0/aggr_efficiency.yaml <p>The <code>aggr_logical_used_wo_snapshots_flexclones</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Storage Efficiency Ratios stat Data Reduction ONTAP: Aggregate Storage Efficiency Ratios timeseries Top $TopResources Aggregates by Logical Used ONTAP: Cluster Storage Efficiency Ratios stat Data Reduction ONTAP: Cluster Storage Efficiency Ratios timeseries Logical Used ONTAP: Datacenter Storage Efficiency stat Data Reduction ONTAP: Datacenter Storage Efficiency timeseries Top $TopResources Logical Used by Cluster"},{"location":"ontap-metrics/#aggr_new_status","title":"aggr_new_status","text":"<p>This metric indicates a value of 1 if the aggregate state is online (indicating the aggregate is operational) and a value of 0 for any other state.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_new_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights table Aggregates ONTAP: Node Highlights stat Aggregates ONTAP: StorageGrid FabricPool Highlights table Aggregates"},{"location":"ontap-metrics/#aggr_object_store_logical_used","title":"aggr_object_store_logical_used","text":"<p>Logical space usage of aggregates in the attached object store.</p> API Endpoint Metric Template REST <code>api/private/cli/aggr/show-space</code> <code>object_store_logical_used</code> conf/rest/9.12.0/aggr.yaml <p>The <code>aggr_object_store_logical_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by logical space usage in Object Store"},{"location":"ontap-metrics/#aggr_object_store_physical_used","title":"aggr_object_store_physical_used","text":"<p>Physical space usage of aggregates in the attached object store.</p> API Endpoint Metric Template REST <code>api/private/cli/aggr/show-space</code> <code>object_store_physical_used</code> conf/rest/9.12.0/aggr.yaml <p>The <code>aggr_object_store_physical_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by physical space usage in Object Store"},{"location":"ontap-metrics/#aggr_other_data","title":"aggr_other_data","text":"<p>Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.throughput_raw.other</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_other_latency","title":"aggr_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: aggr_statistics.iops_raw.other conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_other_ops","title":"aggr_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_physical_used_wo_snapshots","title":"aggr_physical_used_wo_snapshots","text":"<p>Total Data Reduction Physical Used Without Snapshots</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.efficiency_without_snapshots.logical_used, space.efficiency_without_snapshots.savings</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-efficiency-get-iter</code> <code>aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-physical-used-wo-snapshots</code> conf/zapi/cdot/9.9.0/aggr_efficiency.yaml <p>The <code>aggr_physical_used_wo_snapshots</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Storage Efficiency Ratios stat Data Reduction with FlexClones ONTAP: Aggregate Storage Efficiency Ratios timeseries Top $TopResources Aggregates by Physical Used with FlexClones ONTAP: Cluster Storage Efficiency Ratios stat Data Reduction with FlexClones ONTAP: Cluster Storage Efficiency Ratios timeseries Physical Used with FlexClones ONTAP: Datacenter Storage Efficiency stat Data Reduction with FlexClones ONTAP: Datacenter Storage Efficiency timeseries Top $TopResources Physical Used with FlexClones by Cluster"},{"location":"ontap-metrics/#aggr_physical_used_wo_snapshots_flexclones","title":"aggr_physical_used_wo_snapshots_flexclones","text":"<p>Total Data Reduction Physical Used without snapshots and flexclones</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.efficiency_without_snapshots_flexclones.logical_used, space.efficiency_without_snapshots_flexclones.savings</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-efficiency-get-iter</code> <code>aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-physical-used-wo-snapshots-flexclones</code> conf/zapi/cdot/9.9.0/aggr_efficiency.yaml <p>The <code>aggr_physical_used_wo_snapshots_flexclones</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Storage Efficiency Ratios stat Data Reduction ONTAP: Aggregate Storage Efficiency Ratios timeseries Top $TopResources Aggregates by Physical Used ONTAP: Cluster Storage Efficiency Ratios stat Data Reduction ONTAP: Cluster Storage Efficiency Ratios timeseries Physical Used ONTAP: Datacenter Storage Efficiency stat Data Reduction ONTAP: Datacenter Storage Efficiency timeseries Top $TopResources Physical Used by Cluster"},{"location":"ontap-metrics/#aggr_power","title":"aggr_power","text":"<p>Power consumed by aggregate in Watts.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>aggr_power</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Highlights timeseries Top $TopResources Aggregates by Power Consumed ONTAP: Power Highlights timeseries Aggregates Power by Disk Type ONTAP: Power Aggregate table Aggregates"},{"location":"ontap-metrics/#aggr_primary_disk_count","title":"aggr_primary_disk_count","text":"<p>Number of disks used in the aggregate. This includes parity disks, but excludes disks in the hybrid cache.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>block_storage.primary.disk_count</code> conf/rest/9.12.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_raid_disk_count","title":"aggr_raid_disk_count","text":"<p>Number of disks in the aggregate.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>block_storage.primary.disk_count, block_storage.hybrid_cache.disk_count</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-raid-attributes.disk-count</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_raid_disk_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights stat Disks ONTAP: Aggregate Highlights table Aggregates ONTAP: Disk Highlights stat Total Disks by Aggregate(s) ONTAP: Disk Highlights table Disk Capacity Per Aggregate ONTAP: StorageGrid FabricPool Highlights stat Disks ONTAP: StorageGrid FabricPool Highlights table Aggregates"},{"location":"ontap-metrics/#aggr_raid_plex_count","title":"aggr_raid_plex_count","text":"<p>Number of plexes in the aggregate</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>block_storage.plexes.#</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-raid-attributes.plex-count</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_raid_size","title":"aggr_raid_size","text":"<p>Option to specify the maximum number of disks that can be included in a RAID group.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>block_storage.primary.raid_size</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-raid-attributes.raid-size</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_read_data","title":"aggr_read_data","text":"<p>Performance metric for read I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_read_latency","title":"aggr_read_latency","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: aggr_statistics.iops_raw.read conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_read_ops","title":"aggr_read_ops","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_snapshot_files_total","title":"aggr_snapshot_files_total","text":"<p>Total files allowed in snapshots</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>snapshot.files_total</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.files-total</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_files_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Snapshot Files"},{"location":"ontap-metrics/#aggr_snapshot_files_used","title":"aggr_snapshot_files_used","text":"<p>Total files created in snapshots</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>snapshot.files_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.files-used</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_files_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Snapshot Files"},{"location":"ontap-metrics/#aggr_snapshot_inode_used_percent","title":"aggr_snapshot_inode_used_percent","text":"<p>The percentage of disk space currently in use based on user-visible file (inode) count on the referenced file system.</p> API Endpoint Metric Template ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.percent-inode-used-capacity</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_inode_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Snapshot Inodes Used %"},{"location":"ontap-metrics/#aggr_snapshot_maxfiles_available","title":"aggr_snapshot_maxfiles_available","text":"<p>Maximum files available for snapshots</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>snapshot.max_files_available</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.maxfiles-available</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_maxfiles_available</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Snapshot MaxFiles"},{"location":"ontap-metrics/#aggr_snapshot_maxfiles_possible","title":"aggr_snapshot_maxfiles_possible","text":"<p>The largest value to which the maxfiles-available parameter can be increased by reconfiguration, on the referenced file system.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>snapshot.max_files_available, snapshot.max_files_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.maxfiles-possible</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_maxfiles_possible</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Snapshot MaxFiles"},{"location":"ontap-metrics/#aggr_snapshot_maxfiles_used","title":"aggr_snapshot_maxfiles_used","text":"<p>Files in use by snapshots</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>snapshot.max_files_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.maxfiles-used</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_maxfiles_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Snapshot MaxFiles"},{"location":"ontap-metrics/#aggr_snapshot_reserve_percent","title":"aggr_snapshot_reserve_percent","text":"<p>Percentage of space reserved for snapshots</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.snapshot.reserve_percent</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.snapshot-reserve-percent</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_reserve_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Space Reserved for Snapshots %"},{"location":"ontap-metrics/#aggr_snapshot_size_available","title":"aggr_snapshot_size_available","text":"<p>Available space for snapshots in bytes</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.snapshot.available</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.size-available</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_size_available</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Space Used by Snapshots"},{"location":"ontap-metrics/#aggr_snapshot_size_total","title":"aggr_snapshot_size_total","text":"<p>Total space for snapshots in bytes</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.snapshot.total</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.size-total</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_snapshot_size_used","title":"aggr_snapshot_size_used","text":"<p>Space used by snapshots in bytes</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.snapshot.used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.size-used</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_size_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Space Used by Snapshots"},{"location":"ontap-metrics/#aggr_snapshot_used_percent","title":"aggr_snapshot_used_percent","text":"<p>Percentage of disk space used by snapshots</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.snapshot.used_percent</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-snapshot-attributes.percent-used-capacity</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_snapshot_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Space Used by Snapshots % ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Space Used by Snapshots %"},{"location":"ontap-metrics/#aggr_space_available","title":"aggr_space_available","text":"<p>Space available in bytes.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.available</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.size-available</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_available</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights stat Available Space ONTAP: Aggregate Highlights table Aggregates ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Space Available ONTAP: Cluster Highlights stat Available Space ONTAP: Datacenter Highlights stat Available Space ONTAP: Datacenter Highlights timeseries Top $TopResources Available Space by Cluster ONTAP: StorageGrid FabricPool Highlights stat Available Space ONTAP: StorageGrid FabricPool Highlights timeseries Space Available"},{"location":"ontap-metrics/#aggr_space_capacity_tier_used","title":"aggr_space_capacity_tier_used","text":"<p>Used space in bytes in the cloud store. Only applicable for aggregates with a cloud store tier.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.cloud_storage.used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.capacity-tier-used</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_capacity_tier_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Capacity Tier Used ONTAP: Aggregate FabricPool timeseries Top $TopResources Aggregates by Capacity Tier Footprint ONTAP: StorageGrid FabricPool Highlights timeseries Capacity Tier Used"},{"location":"ontap-metrics/#aggr_space_data_compacted_count","title":"aggr_space_data_compacted_count","text":"<p>Amount of compacted data in bytes.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.data_compacted_count</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.data-compacted-count</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_space_data_compaction_saved","title":"aggr_space_data_compaction_saved","text":"<p>Space saved in bytes by compacting the data.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.data_compaction_space_saved</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.data-compaction-space-saved</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_data_compaction_saved</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Data Compaction space saved"},{"location":"ontap-metrics/#aggr_space_data_compaction_saved_percent","title":"aggr_space_data_compaction_saved_percent","text":"<p>Percentage saved by compacting the data.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.data_compaction_space_saved_percent</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.data-compaction-space-saved-percent</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_space_performance_tier_inactive_user_data","title":"aggr_space_performance_tier_inactive_user_data","text":"<p>The size that is physically used in the block storage and has a cold temperature, in bytes. This property is only supported if the aggregate is either attached to a cloud store or can be attached to a cloud store.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the fields query parameter containing either block_storage.inactive_user_data or **.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.inactive_user_data</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.performance-tier-inactive-user-data</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_performance_tier_inactive_user_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Inactive Data"},{"location":"ontap-metrics/#aggr_space_performance_tier_inactive_user_data_percent","title":"aggr_space_performance_tier_inactive_user_data_percent","text":"<p>The percentage of inactive user data in the block storage. This property is only supported if the aggregate is either attached to a cloud store or can be attached to a cloud store.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the fields query parameter containing either block_storage.inactive_user_data_percent or **.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.inactive_user_data_percent</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.performance-tier-inactive-user-data-percent</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_performance_tier_inactive_user_data_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Inactive Data %"},{"location":"ontap-metrics/#aggr_space_performance_tier_used","title":"aggr_space_performance_tier_used","text":"<p>A summation of volume footprints (including volume guarantees), in bytes. This includes all of the volume footprints in the block_storage tier and the cloud_storage tier.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the fields query parameter containing either footprint or **.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.footprint</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-space-get-iter</code> <code>volume-footprints</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_performance_tier_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate FabricPool timeseries Top $TopResources Aggregates by Performance Tier Footprint"},{"location":"ontap-metrics/#aggr_space_performance_tier_used_percent","title":"aggr_space_performance_tier_used_percent","text":"<p>A summation of volume footprints inside the aggregate,as a percentage. A volume's footprint is the amount of space being used for the volume in the aggregate.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.footprint_percent</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-space-get-iter</code> <code>volume-footprints-percent</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_performance_tier_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate FabricPool timeseries Top $TopResources Aggregates by Performance Tier Footprint %"},{"location":"ontap-metrics/#aggr_space_physical_used","title":"aggr_space_physical_used","text":"<p>Total physical used size of an aggregate in bytes.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.physical_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.physical-used</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_physical_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Physical Space Used ONTAP: Aggregate Storage Efficiency Ratios timeseries Top $TopResources Aggregates by Physical Used with Snapshots &amp; FlexClones ONTAP: Cluster Storage Efficiency Ratios timeseries Physical Used with Snapshots &amp; FlexClones ONTAP: Datacenter Storage Efficiency timeseries Top $TopResources Physical Used with Snapshots &amp; FlexClones by Cluster ONTAP: StorageGrid FabricPool Highlights timeseries Physical Space Used"},{"location":"ontap-metrics/#aggr_space_physical_used_percent","title":"aggr_space_physical_used_percent","text":"<p>Physical used percentage.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.physical_used_percent</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.physical-used-percent</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_physical_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  Physical Space Used %"},{"location":"ontap-metrics/#aggr_space_reserved","title":"aggr_space_reserved","text":"<p>The total disk space in bytes that is reserved on the referenced file system. The reserved space is already counted in the used space, so this element can be used to see what portion of the used space represents space reserved for future use.</p> API Endpoint Metric Template ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.total-reserved-space</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_space_sis_saved","title":"aggr_space_sis_saved","text":"<p>Amount of space saved in bytes by storage efficiency.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.volume_deduplication_space_saved</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.sis-space-saved</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_sis_saved</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by  SIS space saved"},{"location":"ontap-metrics/#aggr_space_sis_saved_percent","title":"aggr_space_sis_saved_percent","text":"<p>Percentage of space saved by storage efficiency.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.volume_deduplication_space_saved_percent</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.sis-space-saved-percent</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_space_sis_shared_count","title":"aggr_space_sis_shared_count","text":"<p>Amount of shared bytes counted by storage efficiency.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.volume_deduplication_shared_count</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.sis-shared-count</code> conf/zapi/cdot/9.8.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_space_total","title":"aggr_space_total","text":"<p>Total usable space in bytes, not including WAFL reserve and aggregate snapshot reserve.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.size</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.size-total</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights stat Total Space ONTAP: Aggregate Highlights stat Space Used % ONTAP: Aggregate Highlights table Aggregates ONTAP: Aggregate Highlights timeseries Top $TopResources Aggregates by Total Space ONTAP: cDOT Capacity Metrics table Top $TopResources Aggregates by Capacity Used % ONTAP: cDOT Capacity Metrics timeseries Top $TopResources Aggregates by Capacity Used % ONTAP: Cluster Highlights stat Total Space ONTAP: Cluster Highlights stat Space Used % ONTAP: Cluster Nodes &amp; Subsystems - $Cluster bargauge Capacity used ONTAP: Datacenter Highlights stat Total Space ONTAP: Datacenter Highlights stat Space Used % ONTAP: Datacenter Highlights timeseries Top $TopResources Total Space by Cluster ONTAP: Datacenter Highlights timeseries Top $TopResources Space Used % by Cluster ONTAP: Disk Highlights table Disk Capacity Per Aggregate ONTAP: StorageGrid FabricPool Highlights stat Total Space ONTAP: StorageGrid FabricPool Highlights stat Space Used % ONTAP: StorageGrid FabricPool Highlights table Aggregates"},{"location":"ontap-metrics/#aggr_space_used","title":"aggr_space_used","text":"<p>Space used or reserved in bytes. Includes volume guarantees and aggregate metadata.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.size-used</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights stat Used and Reserved Space ONTAP: Aggregate Highlights stat Space Used % ONTAP: Aggregate Highlights table Aggregates ONTAP: cDOT Capacity Metrics table Top $TopResources Aggregates by Capacity Used % ONTAP: cDOT Capacity Metrics timeseries Top $TopResources Aggregates by Capacity Used % ONTAP: Cluster Highlights stat Used and Reserved Space ONTAP: Cluster Highlights stat Space Used % ONTAP: Cluster Nodes &amp; Subsystems - $Cluster bargauge Capacity used ONTAP: Datacenter Highlights stat Space Used % ONTAP: Datacenter Highlights stat Used and Reserved Space ONTAP: Datacenter Highlights timeseries Top $TopResources Used and Reserved Space by Cluster ONTAP: Datacenter Highlights timeseries Top $TopResources Space Used % by Cluster ONTAP: Datacenter Power and Temperature stat Average Power/Used_TB ONTAP: Power Highlights stat Average Power/Used_TB ONTAP: StorageGrid FabricPool Highlights stat Space Used % ONTAP: StorageGrid FabricPool Highlights table Aggregates"},{"location":"ontap-metrics/#aggr_space_used_percent","title":"aggr_space_used_percent","text":"<p>The percentage of disk space currently in use on the referenced file system</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.block_storage.used, space.block_storage.size</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-space-attributes.percent-used-capacity</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_space_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights table Aggregates ONTAP: Cluster Throughput timeseries Average Aggregate Space Used ONTAP: Disk Highlights table Disk Capacity Per Aggregate ONTAP: StorageGrid FabricPool Highlights table Aggregates"},{"location":"ontap-metrics/#aggr_total_data","title":"aggr_total_data","text":"<p>Performance metric aggregated over all types of I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_total_latency","title":"aggr_total_latency","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: aggr_statistics.iops_raw.total conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_total_logical_used","title":"aggr_total_logical_used","text":"<p>Logical used</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.efficiency.logical_used</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-efficiency-get-iter</code> <code>aggr-efficiency-info.aggr-efficiency-cumulative-info.total-logical-used</code> conf/zapi/cdot/9.9.0/aggr_efficiency.yaml <p>The <code>aggr_total_logical_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Storage Efficiency Ratios stat Data Reduction with Snapshots &amp; FlexClones ONTAP: Aggregate Storage Efficiency Ratios timeseries Top $TopResources Aggregates by Logical Used with Snapshots &amp; FlexClones ONTAP: Aggregate Growth Rate timeseries Top $TopResources Aggregates Per Growth Rate of Logical Used ONTAP: Aggregate Growth Rate table Top $TopResources Aggregates by Logical Usage: Delta Report ONTAP: Cluster Storage Efficiency Ratios stat Data Reduction with Snapshots &amp;FlexClones ONTAP: Cluster Storage Efficiency Ratios timeseries Logical Used with Snapshots &amp; FlexClones ONTAP: Datacenter Storage Efficiency stat Data Reduction with Snapshots &amp; FlexClones ONTAP: Datacenter Storage Efficiency timeseries Top $TopResources Logical Used with Snapshots &amp; FlexClones by Cluster"},{"location":"ontap-metrics/#aggr_total_ops","title":"aggr_total_ops","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_total_physical_used","title":"aggr_total_physical_used","text":"<p>Total Physical Used</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>space.efficiency.logical_used, space.efficiency.savings</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-efficiency-get-iter</code> <code>aggr-efficiency-info.aggr-efficiency-cumulative-info.total-physical-used</code> conf/zapi/cdot/9.9.0/aggr_efficiency.yaml <p>The <code>aggr_total_physical_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Storage Efficiency Ratios stat Data Reduction with Snapshots &amp; FlexClones ONTAP: Aggregate Growth Rate timeseries Top $TopResources Aggregates Per Growth Rate of Physical Used ONTAP: Aggregate Growth Rate table Top $TopResources Aggregates by Physical Usage: Delta Report ONTAP: Cluster Storage Efficiency Ratios stat Data Reduction with Snapshots &amp;FlexClones ONTAP: Datacenter Storage Efficiency stat Data Reduction with Snapshots &amp; FlexClones"},{"location":"ontap-metrics/#aggr_volume_count","title":"aggr_volume_count","text":"<p>The aggregate's volume count, which includes both FlexVols and FlexGroup constituents.</p> API Endpoint Metric Template REST <code>api/storage/aggregates</code> <code>volume_count</code> conf/rest/9.12.0/aggr.yaml ZAPI <code>aggr-get-iter</code> <code>aggr-attributes.aggr-volume-count-attributes.flexvol-count</code> conf/zapi/cdot/9.8.0/aggr.yaml <p>The <code>aggr_volume_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights stat Volumes ONTAP: Aggregate Highlights table Aggregates ONTAP: StorageGrid FabricPool Highlights stat Volumes ONTAP: StorageGrid FabricPool Highlights table Aggregates"},{"location":"ontap-metrics/#aggr_write_data","title":"aggr_write_data","text":"<p>Performance metric for write I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_write_latency","title":"aggr_write_latency","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.latency_raw.write</code>Unit: microsecType: averageBase: aggr_statistics.iops_raw.write conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#aggr_write_ops","title":"aggr_write_ops","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/aggregates</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/aggr.yaml"},{"location":"ontap-metrics/#audit_log","title":"audit_log","text":"<p>Captures the operations such as create, update, and delete attempts on volumes via REST or ONTAP CLI commands</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/audit_log.yaml <p>The <code>audit_log</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: AuditLog Highlights table Volume Changes"},{"location":"ontap-metrics/#availability_zone_space_available","title":"availability_zone_space_available","text":"API Endpoint Metric Template REST <code>api/storage/availability-zones</code> <code>space.available</code> conf/rest/asar2/9.16.0/availability_zone.yaml"},{"location":"ontap-metrics/#availability_zone_space_physical_used","title":"availability_zone_space_physical_used","text":"API Endpoint Metric Template REST <code>api/storage/availability-zones</code> <code>space.physical_used</code> conf/rest/asar2/9.16.0/availability_zone.yaml"},{"location":"ontap-metrics/#availability_zone_space_physical_used_percent","title":"availability_zone_space_physical_used_percent","text":"API Endpoint Metric Template REST <code>api/storage/availability-zones</code> <code>space.physical_used_percent</code> conf/rest/asar2/9.16.0/availability_zone.yaml"},{"location":"ontap-metrics/#availability_zone_space_size","title":"availability_zone_space_size","text":"API Endpoint Metric Template REST <code>api/storage/availability-zones</code> <code>space.size</code> conf/rest/asar2/9.16.0/availability_zone.yaml"},{"location":"ontap-metrics/#change_log","title":"change_log","text":"<p>Detect and track changes related to the creation, modification, and deletion of an object of Node, SVM and Volume</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/node.yaml REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.10.0/svm.yaml REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.14.0/volume.yaml <p>The <code>change_log</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Changelog Monitor Node Changes stat Create ONTAP: Changelog Monitor Node Changes stat Update ONTAP: Changelog Monitor Node Changes stat Delete ONTAP: Changelog Monitor Node Changes table Node Changes  ONTAP: Changelog Monitor SVM Changes stat Create ONTAP: Changelog Monitor SVM Changes stat Update ONTAP: Changelog Monitor SVM Changes stat Delete ONTAP: Changelog Monitor SVM Changes table SVM Changes  ONTAP: Changelog Monitor Volume Changes stat Create ONTAP: Changelog Monitor Volume Changes stat Update ONTAP: Changelog Monitor Volume Changes stat Delete ONTAP: Changelog Monitor Volume Changes table Volume Changes"},{"location":"ontap-metrics/#cifs_session_connection_count","title":"cifs_session_connection_count","text":"<p>A counter used to track requests that are sent to the volumes to the node.</p> API Endpoint Metric Template REST <code>api/protocols/cifs/sessions</code> <code>connection_count</code> conf/rest/9.8.0/cifs_session.yaml ZAPI <code>cifs-session-get-iter</code> <code>cifs-session.connection-count</code> conf/zapi/cdot/9.8.0/cifs_session.yaml <p>The <code>cifs_session_connection_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB Highlights timeseries Top $TopResources Connection Count ONTAP: SMB Highlights timeseries Connection Count By SMB version"},{"location":"ontap-metrics/#cifs_session_idle_duration","title":"cifs_session_idle_duration","text":"<p>Specifies an ISO-8601 format of date and time used to retrieve the idle time duration in hours, minutes, and seconds format.</p> API Endpoint Metric Template REST <code>api/protocols/cifs/sessions</code> <code>idle_duration</code> conf/rest/9.8.0/cifs_session.yaml <p>The <code>cifs_session_idle_duration</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB Highlights table CIFS Sessions"},{"location":"ontap-metrics/#cifs_session_labels","title":"cifs_session_labels","text":"<p>This metric provides information about CIFSSession</p> API Endpoint Metric Template REST <code>api/protocols/cifs/sessions</code> <code>Harvest generated</code> conf/rest/9.8.0/cifs_session.yaml ZAPI <code>cifs-session-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/cifs_session.yaml <p>The <code>cifs_session_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB Highlights table CIFS Sessions"},{"location":"ontap-metrics/#cifs_share_labels","title":"cifs_share_labels","text":"<p>This metric provides information about CIFSShare</p> API Endpoint Metric Template REST <code>api/private/cli/vserver/cifs/share</code> <code>Harvest generated</code> conf/rest/9.6.0/cifs_share.yaml ZAPI <code>cifs-share-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/cifs_share.yaml"},{"location":"ontap-metrics/#cloud_target_labels","title":"cloud_target_labels","text":"<p>This metric provides information about CloudTarget</p> API Endpoint Metric Template REST <code>api/cloud/targets</code> <code>Harvest generated</code> conf/rest/9.12.0/cloud_target.yaml ZAPI <code>aggr-object-store-config-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.10.0/aggr_object_store_config.yaml"},{"location":"ontap-metrics/#cloud_target_used","title":"cloud_target_used","text":"<p>The amount of cloud space used by all the aggregates attached to the target, in bytes. This field is only populated for FabricPool targets. The value is recalculated once every 5 minutes.</p> API Endpoint Metric Template REST <code>api/cloud/targets</code> <code>used</code> conf/rest/9.12.0/cloud_target.yaml ZAPI <code>aggr-object-store-config-get-iter</code> <code>aggr-object-store-config-info.used-space</code> conf/zapi/cdot/9.10.0/aggr_object_store_config.yaml"},{"location":"ontap-metrics/#cluster_new_status","title":"cluster_new_status","text":"<p>It is an indicator of the overall health status of the cluster, with a value of 1 indicating a healthy status and a value of 0 indicating an unhealthy status.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/status.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/status.yaml <p>The <code>cluster_new_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Nodes &amp; Subsystems - $Cluster table $Cluster ONTAP: Cluster Nodes &amp; Subsystems - $Cluster stat cluster health status ONTAP: Datacenter Health table Cluster Health ONTAP: Datacenter System Manager table System Manager"},{"location":"ontap-metrics/#cluster_other_data","title":"cluster_other_data","text":"<p>Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.throughput_raw.other</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_other_latency","title":"cluster_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: cluster_statistics.iops_raw.other conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_other_ops","title":"cluster_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_peer_labels","title":"cluster_peer_labels","text":"<p>This metric provides information about ClusterPeer</p> API Endpoint Metric Template REST <code>api/cluster/peers</code> <code>Harvest generated</code> conf/rest/9.12.0/clusterpeer.yaml ZAPI <code>cluster-peer-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/clusterpeer.yaml <p>The <code>cluster_peer_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Cluster Compliant % ONTAP: Security Highlights piechart Cluster Compliant ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#cluster_peer_non_encrypted","title":"cluster_peer_non_encrypted","text":"<p>This metric indicates a value of 1 if the cluster peer encryption state is none (indicating the connection is not encrypted) and a value of 0 for any other state.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/clusterpeer.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/clusterpeer.yaml <p>The <code>cluster_peer_non_encrypted</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#cluster_read_data","title":"cluster_read_data","text":"<p>Performance metric for read I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_read_latency","title":"cluster_read_latency","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: cluster_statistics.iops_raw.read conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_read_ops","title":"cluster_read_ops","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_schedule_labels","title":"cluster_schedule_labels","text":"<p>This metric provides information about ClusterSchedule</p> API Endpoint Metric Template REST <code>api/cluster/schedules</code> <code>Harvest generated</code> conf/rest/9.6.0/clusterschedule.yaml <p>The <code>cluster_schedule_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Local Policy table Schedules"},{"location":"ontap-metrics/#cluster_software_status","title":"cluster_software_status","text":"<p>Displays the software job with its status.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/clustersoftware.yaml <p>The <code>cluster_software_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Software table Cluster Software Status"},{"location":"ontap-metrics/#cluster_software_update","title":"cluster_software_update","text":"<p>Displays the software update phase with its status.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/clustersoftware.yaml <p>The <code>cluster_software_update</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Software table Cluster Software Update"},{"location":"ontap-metrics/#cluster_software_validation","title":"cluster_software_validation","text":"<p>Displays the software pre-validation checks with their status.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/clustersoftware.yaml <p>The <code>cluster_software_validation</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Software table Cluster Software Validation"},{"location":"ontap-metrics/#cluster_space_available","title":"cluster_space_available","text":"<p>Available space across the cluster.</p> API Endpoint Metric Template REST <code>api/storage/cluster</code> <code>block_storage.available</code> conf/rest/asar2/9.16.0/cluster.yaml <p>The <code>cluster_space_available</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Highlights stat Available"},{"location":"ontap-metrics/#cluster_subsystem_new_status","title":"cluster_subsystem_new_status","text":"<p>This metric indicates a value of 1 if the subsystem health is ok (indicating the subsystem is operational) and a value of 0 for any other health status.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/subsystem.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/subsystem.yaml <p>The <code>cluster_subsystem_new_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Nodes &amp; Subsystems - $Cluster table subsystems"},{"location":"ontap-metrics/#cluster_subsystem_outstanding_alerts","title":"cluster_subsystem_outstanding_alerts","text":"<p>Number of outstanding alerts</p> API Endpoint Metric Template REST <code>api/private/cli/system/health/subsystem</code> <code>outstanding_alert_count</code> conf/rest/9.12.0/subsystem.yaml ZAPI <code>diagnosis-subsystem-config-get-iter</code> <code>diagnosis-subsystem-config-info.outstanding-alert-count</code> conf/zapi/cdot/9.8.0/subsystem.yaml <p>The <code>cluster_subsystem_outstanding_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Nodes &amp; Subsystems - $Cluster table subsystems"},{"location":"ontap-metrics/#cluster_subsystem_suppressed_alerts","title":"cluster_subsystem_suppressed_alerts","text":"<p>Number of suppressed alerts</p> API Endpoint Metric Template REST <code>api/private/cli/system/health/subsystem</code> <code>suppressed_alert_count</code> conf/rest/9.12.0/subsystem.yaml ZAPI <code>diagnosis-subsystem-config-get-iter</code> <code>diagnosis-subsystem-config-info.suppressed-alert-count</code> conf/zapi/cdot/9.8.0/subsystem.yaml <p>The <code>cluster_subsystem_suppressed_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Nodes &amp; Subsystems - $Cluster table subsystems"},{"location":"ontap-metrics/#cluster_tags","title":"cluster_tags","text":"<p>Displays tags at the cluster level.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/status.yaml"},{"location":"ontap-metrics/#cluster_total_data","title":"cluster_total_data","text":"<p>Performance metric aggregated over all types of I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_total_latency","title":"cluster_total_latency","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: cluster_statistics.iops_raw.total conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_total_ops","title":"cluster_total_ops","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_write_data","title":"cluster_write_data","text":"<p>Performance metric for write I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_write_latency","title":"cluster_write_latency","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.latency_raw.write</code>Unit: microsecType: averageBase: cluster_statistics.iops_raw.write conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#cluster_write_ops","title":"cluster_write_ops","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/cluster</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/cluster.yaml"},{"location":"ontap-metrics/#copy_manager_bce_copy_count_curr","title":"copy_manager_bce_copy_count_curr","text":"<p>Current number of copy requests being processed by the Block Copy Engine.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/copy_manager</code> <code>block_copy_engine_current_copy_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/copy_manager.yaml ZapiPerf <code>perf-object-get-instances copy_manager</code> <code>bce_copy_count_curr</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/copy_manager.yaml"},{"location":"ontap-metrics/#copy_manager_kb_copied","title":"copy_manager_kb_copied","text":"<p>Sum of kilo-bytes copied.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/copy_manager</code> <code>KB_copied</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/copy_manager.yaml ZapiPerf <code>perf-object-get-instances copy_manager</code> <code>KB_copied</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/copy_manager.yaml <p>The <code>copy_manager_kb_copied</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Copy Offload timeseries Copy Offload Data Copied"},{"location":"ontap-metrics/#copy_manager_ocs_copy_count_curr","title":"copy_manager_ocs_copy_count_curr","text":"<p>Current number of copy requests being processed by the ONTAP copy subsystem.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/copy_manager</code> <code>ontap_copy_subsystem_current_copy_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/copy_manager.yaml ZapiPerf <code>perf-object-get-instances copy_manager</code> <code>ocs_copy_count_curr</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/copy_manager.yaml"},{"location":"ontap-metrics/#copy_manager_sce_copy_count_curr","title":"copy_manager_sce_copy_count_curr","text":"<p>Current number of copy requests being processed by the System Continuous Engineering.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/copy_manager</code> <code>system_continuous_engineering_current_copy_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/copy_manager.yaml ZapiPerf <code>perf-object-get-instances copy_manager</code> <code>sce_copy_count_curr</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/copy_manager.yaml"},{"location":"ontap-metrics/#copy_manager_spince_copy_count_curr","title":"copy_manager_spince_copy_count_curr","text":"<p>Current number of copy requests being processed by the SpinCE.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/copy_manager</code> <code>spince_current_copy_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/copy_manager.yaml ZapiPerf <code>perf-object-get-instances copy_manager</code> <code>spince_copy_count_curr</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/copy_manager.yaml"},{"location":"ontap-metrics/#disk_busy","title":"disk_busy","text":"<p>The utilization percent of the disk</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>disk_busy_percent</code>Unit: percentType: percentBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_busy</code>Unit: percentType: percentBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_bytes_per_sector","title":"disk_bytes_per_sector","text":"<p>Bytes per sector.</p> API Endpoint Metric Template REST <code>api/storage/disks</code> <code>bytes_per_sector</code> conf/rest/9.12.0/disk.yaml ZAPI <code>storage-disk-get-iter</code> <code>storage-disk-info.disk-inventory-info.bytes-per-sector</code> conf/zapi/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_capacity","title":"disk_capacity","text":"<p>Disk capacity in MB</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>capacity</code>Unit: mbType: rawBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_capacity</code>Unit: mbType: rawBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_cp_read_chain","title":"disk_cp_read_chain","text":"<p>Average number of blocks transferred in each consistency point read operation during a CP</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_cp_read_latency","title":"disk_cp_read_latency","text":"<p>Average latency per block in microseconds for consistency point read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_cp_reads","title":"disk_cp_reads","text":"<p>Number of disk read operations initiated each second for consistency point processing</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_io_pending","title":"disk_io_pending","text":"<p>Average number of I/Os issued to the disk for which we have not yet received the response</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_io_queued","title":"disk_io_queued","text":"<p>Number of I/Os queued to the disk but not yet issued</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_labels","title":"disk_labels","text":"<p>This metric provides information about Disk</p> API Endpoint Metric Template REST <code>api/storage/disks</code> <code>Harvest generated</code> conf/rest/9.12.0/disk.yaml ZAPI <code>storage-disk-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/disk.yaml <p>The <code>disk_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count ONTAP: Disk Highlights stat Total Disks ONTAP: Disk Highlights stat Failed Disks ONTAP: Disk List of Disks table Disks in Cluster ONTAP: Health Disks table Disks Issues"},{"location":"ontap-metrics/#disk_new_status","title":"disk_new_status","text":"<p>This metric indicates a value of 1 if the disk is not in an outage (i.e., the outage label is empty) and a value of 0 if the shelf is in an outage.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/disk.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_power_on_hours","title":"disk_power_on_hours","text":"<p>Hours powered on.</p> API Endpoint Metric Template REST <code>api/storage/disks</code> <code>stats.power_on_hours</code> conf/rest/9.12.0/disk.yaml"},{"location":"ontap-metrics/#disk_sectors","title":"disk_sectors","text":"<p>Number of sectors on the disk.</p> API Endpoint Metric Template REST <code>api/storage/disks</code> <code>sector_count</code> conf/rest/9.12.0/disk.yaml ZAPI <code>storage-disk-get-iter</code> <code>storage-disk-info.disk-inventory-info.capacity-sectors</code> conf/zapi/cdot/9.8.0/disk.yaml <p>The <code>disk_sectors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk List of Disks table Disks in Cluster"},{"location":"ontap-metrics/#disk_stats_average_latency","title":"disk_stats_average_latency","text":"<p>Average I/O latency across all active paths, in milliseconds.</p> API Endpoint Metric Template REST <code>api/storage/disks</code> <code>stats.average_latency</code> conf/rest/9.12.0/disk.yaml ZAPI <code>storage-disk-get-iter</code> <code>storage-disk-info.disk-stats-info.average-latency</code> conf/zapi/cdot/9.8.0/disk.yaml <p>The <code>disk_stats_average_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk List of Disks table Disks in Cluster"},{"location":"ontap-metrics/#disk_stats_io_kbps","title":"disk_stats_io_kbps","text":"<p>Total Disk Throughput in KBPS Across All Active Paths</p> API Endpoint Metric Template REST <code>api/private/cli/disk</code> <code>disk_io_kbps_total</code> conf/rest/9.12.0/disk.yaml ZAPI <code>storage-disk-get-iter</code> <code>storage-disk-info.disk-stats-info.disk-io-kbps</code> conf/zapi/cdot/9.8.0/disk.yaml <p>The <code>disk_stats_io_kbps</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk List of Disks table Disks in Cluster"},{"location":"ontap-metrics/#disk_stats_sectors_read","title":"disk_stats_sectors_read","text":"<p>Number of Sectors Read</p> API Endpoint Metric Template REST <code>api/private/cli/disk</code> <code>sectors_read</code> conf/rest/9.12.0/disk.yaml ZAPI <code>storage-disk-get-iter</code> <code>storage-disk-info.disk-stats-info.sectors-read</code> conf/zapi/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_stats_sectors_written","title":"disk_stats_sectors_written","text":"<p>Number of Sectors Written</p> API Endpoint Metric Template REST <code>api/private/cli/disk</code> <code>sectors_written</code> conf/rest/9.12.0/disk.yaml ZAPI <code>storage-disk-get-iter</code> <code>storage-disk-info.disk-stats-info.sectors-written</code> conf/zapi/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_total_data","title":"disk_total_data","text":"<p>Total throughput for user operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_total_transfers","title":"disk_total_transfers","text":"<p>Total number of disk operations involving data transfer initiated per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_transfer_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_transfers</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_uptime","title":"disk_uptime","text":"<p>Number of seconds the drive has been powered on</p> API Endpoint Metric Template REST <code>api/storage/disks</code> <code>stats.power_on_hours, 60, 60</code> conf/rest/9.12.0/disk.yaml ZAPI <code>storage-disk-get-iter</code> <code>storage-disk-info.disk-stats-info.power-on-time-interval</code> conf/zapi/cdot/9.8.0/disk.yaml <p>The <code>disk_uptime</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk List of Disks table Disks in Cluster"},{"location":"ontap-metrics/#disk_usable_size","title":"disk_usable_size","text":"<p>Usable size of each disk, in bytes.</p> API Endpoint Metric Template REST <code>api/storage/disks</code> <code>usable_size</code> conf/rest/9.12.0/disk.yaml"},{"location":"ontap-metrics/#disk_user_read_blocks","title":"disk_user_read_blocks","text":"<p>Number of blocks transferred for user read operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_user_read_chain","title":"disk_user_read_chain","text":"<p>Average number of blocks transferred in each user read operation</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_user_read_latency","title":"disk_user_read_latency","text":"<p>Average latency per block in microseconds for user read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_user_reads","title":"disk_user_reads","text":"<p>Number of disk read operations initiated each second for retrieving data or metadata associated with user requests</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_user_write_blocks","title":"disk_user_write_blocks","text":"<p>Number of blocks transferred for user write operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_user_write_chain","title":"disk_user_write_chain","text":"<p>Average number of blocks transferred in each user write operation</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_write_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_writes conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_user_write_latency","title":"disk_user_write_latency","text":"<p>Average latency per block in microseconds for user write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#disk_user_writes","title":"disk_user_writes","text":"<p>Number of disk write operations initiated each second for storing data or metadata associated with user requests</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_writes</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#ems_destination_labels","title":"ems_destination_labels","text":"<p>This metric provides information about EmsDestination</p> API Endpoint Metric Template REST <code>api/support/ems/destinations</code> <code>Harvest generated</code> conf/rest/9.12.0/ems_destination.yaml ZAPI <code>ems-event-notification-destination-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/ems_destination.yaml <p>The <code>ems_destination_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#ems_events","title":"ems_events","text":"<p>Indicates EMS events that have occurred in the ONTAP as configured in the ems.yaml.</p> API Endpoint Metric Template REST <code>api/support/ems/events</code> <code>Harvest generated</code> conf/ems/9.6.0/ems.yaml <p>The <code>ems_events</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues table Active Emergency EMS"},{"location":"ontap-metrics/#environment_sensor_average_ambient_temperature","title":"environment_sensor_average_ambient_temperature","text":"<p>Average temperature of all ambient sensors for node in Celsius.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_average_ambient_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#environment_sensor_average_fan_speed","title":"environment_sensor_average_fan_speed","text":"<p>Average fan speed for node in rpm.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_average_fan_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#environment_sensor_average_temperature","title":"environment_sensor_average_temperature","text":"<p>Average temperature of all non-ambient sensors for node in Celsius.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_average_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Highlights timeseries Top $TopResources Nodes by Average Temperature ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#environment_sensor_max_fan_speed","title":"environment_sensor_max_fan_speed","text":"<p>Maximum fan speed for node in rpm.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_max_fan_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Power and Temperature stat Max Node Fan Speed ONTAP: Power Highlights stat Max Node Fan Speed ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#environment_sensor_max_temperature","title":"environment_sensor_max_temperature","text":"<p>Maximum temperature of all non-ambient sensors for node in Celsius.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_max_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Power and Temperature stat Max Node Temp ONTAP: Power Highlights stat Max Node Temp ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#environment_sensor_min_ambient_temperature","title":"environment_sensor_min_ambient_temperature","text":"<p>Minimum temperature of all ambient sensors for node in Celsius.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_min_ambient_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#environment_sensor_min_fan_speed","title":"environment_sensor_min_fan_speed","text":"<p>Minimum fan speed for node in rpm.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_min_fan_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#environment_sensor_min_temperature","title":"environment_sensor_min_temperature","text":"<p>Minimum temperature of all non-ambient sensors for node in Celsius.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_min_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#environment_sensor_power","title":"environment_sensor_power","text":"<p>Power consumed by a node in Watts.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_power</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Power and Temperature stat Total Power ONTAP: Datacenter Power and Temperature stat Average Power/Used_TB ONTAP: Datacenter Power and Temperature stat Average IOPs/Watt ONTAP: Datacenter Power and Temperature timeseries Total Power Consumed ONTAP: Power Highlights stat Total Power ONTAP: Power Highlights stat Average Power/Used_TB ONTAP: Power Highlights stat Average IOPs/Watt ONTAP: Power Highlights timeseries Total Power Consumed ONTAP: Power Highlights timeseries Average Power Consumption (kWh) Over Last Hour ONTAP: Power Highlights timeseries Top $TopResources Nodes by Power Consumed ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#environment_sensor_status","title":"environment_sensor_status","text":"<p>This metric indicates a value of 1 if the sensor threshold state is normal (indicating the sensor is operating within normal parameters) and a value of 0 for any other state.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/sensor.yaml"},{"location":"ontap-metrics/#environment_sensor_threshold_value","title":"environment_sensor_threshold_value","text":"<p>Provides the sensor reading.</p> API Endpoint Metric Template REST <code>api/cluster/sensors</code> <code>value</code> conf/rest/9.12.0/sensor.yaml ZAPI <code>environment-sensors-get-iter</code> <code>environment-sensors-info.threshold-sensor-value</code> conf/zapi/cdot/9.8.0/sensor.yaml <p>The <code>environment_sensor_threshold_value</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Errors ONTAP: Health Highlights stat Total Errors ONTAP: Health Highlights piechart Errors ONTAP: Health Sensor table Sensor Issues ONTAP: Power Sensor Problems table Sensor Problems"},{"location":"ontap-metrics/#ethernet_switch_port_new_status","title":"ethernet_switch_port_new_status","text":"<p>Represent the status of the ethernet switch port</p> API Endpoint Metric Template Rest <code>NA</code> <code>Harvest generated</code> conf/rest/9.8.0/ethernet_switch_port.yaml <p>The <code>ethernet_switch_port_new_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Switch Highlights table Switch Details ONTAP: Switch Interfaces stat Down (Last 24h) ONTAP: Switch Interfaces table Down (Last 24h) ONTAP: Switch Interfaces timeseries Down (Last 24h)"},{"location":"ontap-metrics/#ethernet_switch_port_receive_discards","title":"ethernet_switch_port_receive_discards","text":"<p>Total number of discarded packets.</p> API Endpoint Metric Template KeyPerf <code>api/network/ethernet/switch/ports</code> <code>receive_raw.discards</code>Unit: Type: deltaBase: conf/keyperf/9.15.0/ethernet_switch_port.yaml <p>The <code>ethernet_switch_port_receive_discards</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Switch Traffic timeseries Top $TopResources Interface Drops"},{"location":"ontap-metrics/#ethernet_switch_port_receive_errors","title":"ethernet_switch_port_receive_errors","text":"<p>Number of packet errors.</p> API Endpoint Metric Template KeyPerf <code>api/network/ethernet/switch/ports</code> <code>receive_raw.errors</code>Unit: Type: deltaBase: conf/keyperf/9.15.0/ethernet_switch_port.yaml <p>The <code>ethernet_switch_port_receive_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Switch Traffic timeseries Top $TopResources Interface Errors"},{"location":"ontap-metrics/#ethernet_switch_port_receive_packets","title":"ethernet_switch_port_receive_packets","text":"<p>Total packet count.</p> API Endpoint Metric Template KeyPerf <code>api/network/ethernet/switch/ports</code> <code>receive_raw.packets</code>Unit: Type: deltaBase: conf/keyperf/9.15.0/ethernet_switch_port.yaml <p>The <code>ethernet_switch_port_receive_packets</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Switch Traffic timeseries Top $TopResources Interface Receive Packets"},{"location":"ontap-metrics/#ethernet_switch_port_transmit_discards","title":"ethernet_switch_port_transmit_discards","text":"<p>Total number of discarded packets.</p> API Endpoint Metric Template KeyPerf <code>api/network/ethernet/switch/ports</code> <code>transmit_raw.discards</code>Unit: Type: deltaBase: conf/keyperf/9.15.0/ethernet_switch_port.yaml <p>The <code>ethernet_switch_port_transmit_discards</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Switch Traffic timeseries Top $TopResources Interface Drops"},{"location":"ontap-metrics/#ethernet_switch_port_transmit_errors","title":"ethernet_switch_port_transmit_errors","text":"<p>Number of packet errors.</p> API Endpoint Metric Template KeyPerf <code>api/network/ethernet/switch/ports</code> <code>transmit_raw.errors</code>Unit: Type: deltaBase: conf/keyperf/9.15.0/ethernet_switch_port.yaml <p>The <code>ethernet_switch_port_transmit_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Switch Traffic timeseries Top $TopResources Interface Errors"},{"location":"ontap-metrics/#ethernet_switch_port_transmit_packets","title":"ethernet_switch_port_transmit_packets","text":"<p>Total packet count.</p> API Endpoint Metric Template KeyPerf <code>api/network/ethernet/switch/ports</code> <code>transmit_raw.packets</code>Unit: Type: deltaBase: conf/keyperf/9.15.0/ethernet_switch_port.yaml <p>The <code>ethernet_switch_port_transmit_packets</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Switch Traffic timeseries Top $TopResources Interface Transmit Packets"},{"location":"ontap-metrics/#export_rule_labels","title":"export_rule_labels","text":"<p>This metric provides information about ExportRule</p> API Endpoint Metric Template REST <code>api/private/cli/vserver/export-policy/rule</code> <code>Harvest generated</code> conf/rest/9.8.0/exports.yaml"},{"location":"ontap-metrics/#external_service_op_num_not_found_responses","title":"external_service_op_num_not_found_responses","text":"<p>Number of 'Not Found' responses for calls to this operation.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances external_service_op</code> <code>num_not_found_responses</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/external_service_operation.yaml <p>The <code>external_service_op_num_not_found_responses</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: External Service Operation Highlights timeseries Top $TopResources Number of 'Not Found' Responses Per Operation"},{"location":"ontap-metrics/#external_service_op_num_request_failures","title":"external_service_op_num_request_failures","text":"<p>A cumulative count of all request failures.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances external_service_op</code> <code>num_request_failures</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/external_service_operation.yaml <p>The <code>external_service_op_num_request_failures</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: External Service Operation Highlights timeseries Top $TopResources Number of Request Failures"},{"location":"ontap-metrics/#external_service_op_num_requests_sent","title":"external_service_op_num_requests_sent","text":"<p>Number of requests sent to this service.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances external_service_op</code> <code>num_requests_sent</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/external_service_operation.yaml <p>The <code>external_service_op_num_requests_sent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: External Service Operation Highlights timeseries Top $TopResources Number of Request Sent"},{"location":"ontap-metrics/#external_service_op_num_responses_received","title":"external_service_op_num_responses_received","text":"<p>Number of responses received from the server (does not include timeouts).</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances external_service_op</code> <code>num_responses_received</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/external_service_operation.yaml <p>The <code>external_service_op_num_responses_received</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: External Service Operation Highlights timeseries Top $TopResources Number of Responses Received"},{"location":"ontap-metrics/#external_service_op_num_successful_responses","title":"external_service_op_num_successful_responses","text":"<p>Number of successful responses to this operation.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances external_service_op</code> <code>num_successful_responses</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/external_service_operation.yaml <p>The <code>external_service_op_num_successful_responses</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: External Service Operation Highlights timeseries Top $TopResources Number of Successful Responses"},{"location":"ontap-metrics/#external_service_op_num_timeouts","title":"external_service_op_num_timeouts","text":"<p>Number of times requests to the server for this operation timed out, meaning no response was recevied in a given time period.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances external_service_op</code> <code>num_timeouts</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/external_service_operation.yaml <p>The <code>external_service_op_num_timeouts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: External Service Operation Highlights timeseries Top $TopResources Number of Timeouts"},{"location":"ontap-metrics/#external_service_op_request_latency","title":"external_service_op_request_latency","text":"<p>Average latency in microseconds of requests for operations of this type on this server.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances external_service_op</code> <code>request_latency</code>Unit: microsecType: averageBase: num_requests_sent conf/zapiperf/cdot/9.8.0/external_service_operation.yaml <p>The <code>external_service_op_request_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: External Service Operation Highlights timeseries Top $TopResources Request Latency to Server"},{"location":"ontap-metrics/#external_service_op_request_latency_hist","title":"external_service_op_request_latency_hist","text":"<p>This histogram holds the latency values for requests of this operation to the specified server.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances external_service_op</code> <code>request_latency_hist</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/external_service_operation.yaml"},{"location":"ontap-metrics/#fabricpool_average_latency","title":"fabricpool_average_latency","text":"<p>This counter is deprecated.Average latencies executed during various phases of command execution. The execution-start latency represents the average time taken to start executing an operation. The request-prepare latency represent the average time taken to prepare the commplete request that needs to be sent to the server. The send latency represents the average time taken to send requests to the server. The execution-start-to-send-complete represents the average time taken to send an operation out since its execution started. The execution-start-to-first-byte-received represent the average time taken to receive the first byte of a response since the command's request execution started. These counters can be used to identify performance bottlenecks within the object store client module.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances object_store_client_op</code> <code>average_latency</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml"},{"location":"ontap-metrics/#fabricpool_cloud_bin_op_latency_average","title":"fabricpool_cloud_bin_op_latency_average","text":"<p>Cloud bin operation latency average in milliseconds.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_comp_aggr_vol_bin</code> <code>cloud_bin_op_latency_average</code>Unit: millisecType: rawBase: conf/restperf/9.12.0/wafl_comp_aggr_vol_bin.yaml ZapiPerf <code>perf-object-get-instances wafl_comp_aggr_vol_bin</code> <code>cloud_bin_op_latency_average</code>Unit: millisecType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/wafl_comp_aggr_vol_bin.yaml <p>The <code>fabricpool_cloud_bin_op_latency_average</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Object Storage timeseries Top $TopResources Volumes by Object Storage GET Latency ONTAP: Volume Object Storage timeseries Top $TopResources Volumes by Object Storage PUT Latency"},{"location":"ontap-metrics/#fabricpool_cloud_bin_operation","title":"fabricpool_cloud_bin_operation","text":"<p>Cloud bin operation counters.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_comp_aggr_vol_bin</code> <code>cloud_bin_op</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/wafl_comp_aggr_vol_bin.yaml ZapiPerf <code>perf-object-get-instances wafl_comp_aggr_vol_bin</code> <code>cloud_bin_operation</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl_comp_aggr_vol_bin.yaml <p>The <code>fabricpool_cloud_bin_operation</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Object Storage timeseries Top $TopResources Volumes by Object Storage GET Request Count ONTAP: Volume Object Storage timeseries Top $TopResources Volumes by Object Storage PUT Request Count ONTAP: Volume Object Storage table Top $TopResources Volumes by Object Storage Requests"},{"location":"ontap-metrics/#fabricpool_get_throughput_bytes","title":"fabricpool_get_throughput_bytes","text":"<p>This counter is deprecated. Counter that indicates the throughput for GET command in bytes per second.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances object_store_client_op</code> <code>get_throughput_bytes</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml"},{"location":"ontap-metrics/#fabricpool_put_throughput_bytes","title":"fabricpool_put_throughput_bytes","text":"<p>This counter is deprecated. Counter that indicates the throughput for PUT command in bytes per second.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances object_store_client_op</code> <code>put_throughput_bytes</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml"},{"location":"ontap-metrics/#fabricpool_stats","title":"fabricpool_stats","text":"<p>This counter is deprecated. Counter that indicates the number of object store operations sent, and their success and failure counts. The objstore_client_op_name array indicate the operation name such as PUT, GET, etc. The objstore_client_op_stats_name array contain the total number of operations, their success and failure counter for each operation.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances object_store_client_op</code> <code>stats</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml"},{"location":"ontap-metrics/#fabricpool_throughput_ops","title":"fabricpool_throughput_ops","text":"<p>Counter that indicates the throughput for commands in ops per second.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances object_store_client_op</code> <code>throughput_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml"},{"location":"ontap-metrics/#fcp_avg_other_latency","title":"fcp_avg_other_latency","text":"<p>Average latency in microseconds for operations other than read and write</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>average_other_latency</code>Unit: microsecType: averageBase: other_ops conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>avg_other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.8.0/fcp.yaml"},{"location":"ontap-metrics/#fcp_avg_read_latency","title":"fcp_avg_read_latency","text":"<p>Average latency in microseconds for read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>average_read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>avg_read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_avg_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries Top $TopResources FCPs by Send Latency"},{"location":"ontap-metrics/#fcp_avg_write_latency","title":"fcp_avg_write_latency","text":"<p>Average latency in microseconds for write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>average_write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>avg_write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_avg_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries Top $TopResources FCPs by Receive Latency"},{"location":"ontap-metrics/#fcp_discarded_frames_count","title":"fcp_discarded_frames_count","text":"<p>Number of discarded frames.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>discarded_frames_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>discarded_frames_count</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_discarded_frames_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission errors"},{"location":"ontap-metrics/#fcp_fabric_connected_speed","title":"fcp_fabric_connected_speed","text":"<p>The negotiated data rate between the target FC port and the fabric in gigabits per second.</p> API Endpoint Metric Template REST <code>api/network/fc/ports</code> <code>fabric.connected_speed</code> conf/rest/9.6.0/fcp.yaml <p>The <code>fcp_fabric_connected_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel table FC ports with Fabric detail"},{"location":"ontap-metrics/#fcp_int_count","title":"fcp_int_count","text":"<p>Number of interrupts</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>interrupt_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>int_count</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_int_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission interrupts"},{"location":"ontap-metrics/#fcp_invalid_crc","title":"fcp_invalid_crc","text":"<p>Number of invalid cyclic redundancy checks (CRC count)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>invalid.crc</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>invalid_crc</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_invalid_crc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission interrupts"},{"location":"ontap-metrics/#fcp_invalid_transmission_word","title":"fcp_invalid_transmission_word","text":"<p>Number of invalid transmission words</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>invalid.transmission_word</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>invalid_transmission_word</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_invalid_transmission_word</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission interrupts"},{"location":"ontap-metrics/#fcp_isr_count","title":"fcp_isr_count","text":"<p>Number of interrupt responses</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>isr.count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>isr_count</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_isr_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission interrupts"},{"location":"ontap-metrics/#fcp_labels","title":"fcp_labels","text":"<p>This metric provides information about FCP</p> API Endpoint Metric Template REST <code>api/network/fc/ports</code> <code>Harvest generated</code> conf/rest/9.6.0/fcp.yaml <p>The <code>fcp_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count ONTAP: Network FibreChannel table FC ports with Fabric detail"},{"location":"ontap-metrics/#fcp_lif_avg_latency","title":"fcp_lif_avg_latency","text":"<p>Average latency in microseconds for FCP operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>average_latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>avg_latency</code>Unit: microsecType: averageBase: total_ops conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node FCP Frontend stat FCP Latency ONTAP: Node FCP Frontend timeseries FCP Average Latency by Port / LIF ONTAP: SVM FCP stat SVM FCP Average Latency ONTAP: SVM FCP timeseries SVM FCP Average Latency"},{"location":"ontap-metrics/#fcp_lif_avg_other_latency","title":"fcp_lif_avg_other_latency","text":"<p>Average latency in microseconds for operations other than read and write</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>average_other_latency</code>Unit: microsecType: averageBase: other_ops conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>avg_other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_avg_other_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM FCP timeseries SVM FCP Average Latency"},{"location":"ontap-metrics/#fcp_lif_avg_read_latency","title":"fcp_lif_avg_read_latency","text":"<p>Average latency in microseconds for read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>average_read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>avg_read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_avg_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM FCP stat SVM FCP Average Read Latency ONTAP: SVM FCP timeseries SVM FCP Average Latency ONTAP: SVM NVMe/FC stat SVM FCP Average Read Latency"},{"location":"ontap-metrics/#fcp_lif_avg_write_latency","title":"fcp_lif_avg_write_latency","text":"<p>Average latency in microseconds for write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>average_write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>avg_write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_avg_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM FCP stat SVM FCP Average Write Latency ONTAP: SVM FCP timeseries SVM FCP Average Latency ONTAP: SVM NVMe/FC stat SVM FCP Average Write Latency"},{"location":"ontap-metrics/#fcp_lif_other_ops","title":"fcp_lif_other_ops","text":"<p>Number of operations that are not read or write.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_other_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM FCP timeseries SVM FCP IOPs"},{"location":"ontap-metrics/#fcp_lif_read_data","title":"fcp_lif_read_data","text":"<p>Amount of data read from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM FCP stat SVM FCP Read Throughput ONTAP: SVM FCP timeseries SVM FCP Throughput ONTAP: SVM FCP timeseries Top $TopResources FCP LIFs by Send Throughput ONTAP: SVM NVMe/FC stat SVM FCP Read Throughput"},{"location":"ontap-metrics/#fcp_lif_read_ops","title":"fcp_lif_read_ops","text":"<p>Number of read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM FCP stat SVM FCP Read IOPs ONTAP: SVM FCP timeseries SVM FCP IOPs ONTAP: SVM NVMe/FC stat SVM FCP Read IOPs"},{"location":"ontap-metrics/#fcp_lif_total_ops","title":"fcp_lif_total_ops","text":"<p>Total number of operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node FCP Frontend stat FCP IOPs ONTAP: Node FCP Frontend timeseries FCP IOPs by Port / LIF ONTAP: SVM FCP stat SVM FCP IOPs"},{"location":"ontap-metrics/#fcp_lif_write_data","title":"fcp_lif_write_data","text":"<p>Amount of data written to the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node FCP Frontend timeseries FCP Throughput by Port / LIF ONTAP: SVM FCP stat SVM FCP Throughput ONTAP: SVM FCP stat SVM FCP Write Throughput ONTAP: SVM FCP timeseries SVM FCP Throughput ONTAP: SVM FCP timeseries Top $TopResources FCP LIFs by Receive Throughput ONTAP: SVM NVMe/FC stat SVM FCP Write Throughput"},{"location":"ontap-metrics/#fcp_lif_write_ops","title":"fcp_lif_write_ops","text":"<p>Number of write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp_lif</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp_lif.yaml ZapiPerf <code>perf-object-get-instances fcp_lif</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp_lif.yaml <p>The <code>fcp_lif_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM FCP stat SVM FCP Write IOPs ONTAP: SVM FCP timeseries SVM FCP IOPs ONTAP: SVM NVMe/FC stat SVM FCP Write IOPs"},{"location":"ontap-metrics/#fcp_link_down","title":"fcp_link_down","text":"<p>Number of times the Fibre Channel link was lost</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>link.down</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>link_down</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_link_down</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries Top $TopResources FCPs by Link Down"},{"location":"ontap-metrics/#fcp_link_failure","title":"fcp_link_failure","text":"<p>Number of link failures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>link_failure</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>link_failure</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_link_failure</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries Top $TopResources FCPs by Link Failure"},{"location":"ontap-metrics/#fcp_link_up","title":"fcp_link_up","text":"<p>Number of times the Fibre Channel link was established</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>link.up</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>link_up</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_loss_of_signal","title":"fcp_loss_of_signal","text":"<p>Number of times this port lost signal</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>loss_of_signal</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>loss_of_signal</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_loss_of_signal</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission errors"},{"location":"ontap-metrics/#fcp_loss_of_sync","title":"fcp_loss_of_sync","text":"<p>Number of times this port lost sync</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>loss_of_sync</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>loss_of_sync</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_loss_of_sync</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission errors"},{"location":"ontap-metrics/#fcp_max_speed","title":"fcp_max_speed","text":"<p>The maximum speed supported by the FC port in gigabits per second.</p> API Endpoint Metric Template REST <code>api/network/fc/ports</code> <code>speed.maximum</code> conf/rest/9.6.0/fcp.yaml <p>The <code>fcp_max_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel table FC ports with Fabric detail"},{"location":"ontap-metrics/#fcp_nvmf_avg_other_latency","title":"fcp_nvmf_avg_other_latency","text":"<p>Average latency in microseconds for operations other than read and write (FC-NVMe)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.average_other_latency</code>Unit: microsecType: averageBase: nvmf.other_ops conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_avg_other_latency</code>Unit: microsecType: averageBase: nvmf_other_ops conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_avg_read_latency","title":"fcp_nvmf_avg_read_latency","text":"<p>Average latency in microseconds for read operations (FC-NVMe)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.average_read_latency</code>Unit: microsecType: averageBase: nvmf.read_ops conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_avg_read_latency</code>Unit: microsecType: averageBase: nvmf_read_ops conf/zapiperf/cdot/9.10.1/fcp.yaml <p>The <code>fcp_nvmf_avg_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network NVMe/FC timeseries Top $TopResources FCP_NVMFs by Send Latency"},{"location":"ontap-metrics/#fcp_nvmf_avg_remote_other_latency","title":"fcp_nvmf_avg_remote_other_latency","text":"<p>Average latency in microseconds for remote operations other than read and write (FC-NVMe)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.average_remote_other_latency</code>Unit: microsecType: averageBase: nvmf_remote.other_ops conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_avg_remote_other_latency</code>Unit: microsecType: averageBase: nvmf_remote_other_ops conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_avg_remote_read_latency","title":"fcp_nvmf_avg_remote_read_latency","text":"<p>Average latency in microseconds for remote read operations (FC-NVMe)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.average_remote_read_latency</code>Unit: microsecType: averageBase: nvmf_remote.read_ops conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_avg_remote_read_latency</code>Unit: microsecType: averageBase: nvmf_remote_read_ops conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_avg_remote_write_latency","title":"fcp_nvmf_avg_remote_write_latency","text":"<p>Average latency in microseconds for remote write operations (FC-NVMe)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.average_remote_write_latency</code>Unit: microsecType: averageBase: nvmf_remote.write_ops conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_avg_remote_write_latency</code>Unit: microsecType: averageBase: nvmf_remote_write_ops conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_avg_write_latency","title":"fcp_nvmf_avg_write_latency","text":"<p>Average latency in microseconds for write operations (FC-NVMe)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.average_write_latency</code>Unit: microsecType: averageBase: nvmf.write_ops conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_avg_write_latency</code>Unit: microsecType: averageBase: nvmf_write_ops conf/zapiperf/cdot/9.10.1/fcp.yaml <p>The <code>fcp_nvmf_avg_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network NVMe/FC timeseries Top $TopResources FCP_NVMFs by Receive Latency"},{"location":"ontap-metrics/#fcp_nvmf_caw_data","title":"fcp_nvmf_caw_data","text":"<p>Amount of CAW data sent to the storage system (FC-NVMe) in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.caw_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_caw_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_caw_ops","title":"fcp_nvmf_caw_ops","text":"<p>Number of FC-NVMe CAW operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.caw_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_caw_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_command_slots","title":"fcp_nvmf_command_slots","text":"<p>Number of command slots that have been used by initiators logging into this port. This shows the command fan-in on the port.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.command_slots</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_command_slots</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_other_ops","title":"fcp_nvmf_other_ops","text":"<p>Number of NVMF operations that are not read or write.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_read_data","title":"fcp_nvmf_read_data","text":"<p>Amount of data read from the storage system (FC-NVMe) in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml <p>The <code>fcp_nvmf_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Highlights stat FC Read Throughput ONTAP: Network NVMe/FC table NVMe/FC ports ONTAP: Network NVMe/FC timeseries Top $TopResources FCP_NVMFs by Send Throughput ONTAP: Node Network Layer timeseries Top $TopResources FC Ports by Throughput"},{"location":"ontap-metrics/#fcp_nvmf_read_ops","title":"fcp_nvmf_read_ops","text":"<p>Number of FC-NVMe read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml <p>The <code>fcp_nvmf_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Highlights stat FC Read Throughput"},{"location":"ontap-metrics/#fcp_nvmf_remote_caw_data","title":"fcp_nvmf_remote_caw_data","text":"<p>Amount of remote CAW data sent to the storage system (FC-NVMe) in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf_remote.caw_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_remote_caw_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_remote_caw_ops","title":"fcp_nvmf_remote_caw_ops","text":"<p>Number of FC-NVMe remote CAW operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf_remote.caw_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_remote_caw_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_remote_other_ops","title":"fcp_nvmf_remote_other_ops","text":"<p>Number of NVMF remote operations that are not read or write.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf_remote.other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_remote_other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_remote_read_data","title":"fcp_nvmf_remote_read_data","text":"<p>Amount of remote data read from the storage system (FC-NVMe) in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf_remote.read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_remote_read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_remote_read_ops","title":"fcp_nvmf_remote_read_ops","text":"<p>Number of FC-NVMe remote read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf_remote.read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_remote_read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_remote_total_data","title":"fcp_nvmf_remote_total_data","text":"<p>Amount of remote FC-NVMe traffic to and from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf_remote.total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_remote_total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_remote_total_ops","title":"fcp_nvmf_remote_total_ops","text":"<p>Total number of remote FC-NVMe operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf_remote.total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_remote_total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_remote_write_data","title":"fcp_nvmf_remote_write_data","text":"<p>Amount of remote data written to the storage system (FC-NVMe) in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf_remote.write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_remote_write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_remote_write_ops","title":"fcp_nvmf_remote_write_ops","text":"<p>Number of FC-NVMe remote write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf_remote.write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_remote_write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml"},{"location":"ontap-metrics/#fcp_nvmf_total_data","title":"fcp_nvmf_total_data","text":"<p>Amount of FC-NVMe traffic to and from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml <p>The <code>fcp_nvmf_total_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Highlights stat FC Throughput"},{"location":"ontap-metrics/#fcp_nvmf_total_ops","title":"fcp_nvmf_total_ops","text":"<p>Total number of FC-NVMe operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml <p>The <code>fcp_nvmf_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Highlights stat FC Throughput"},{"location":"ontap-metrics/#fcp_nvmf_write_data","title":"fcp_nvmf_write_data","text":"<p>Amount of data written to the storage system (FC-NVMe) in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml <p>The <code>fcp_nvmf_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Highlights stat FC Write Throughput ONTAP: Network NVMe/FC table NVMe/FC ports ONTAP: Network NVMe/FC timeseries Top $TopResources FCP_NVMFs by Receive Throughput ONTAP: Node Network Layer timeseries Top $TopResources FC Ports by Throughput"},{"location":"ontap-metrics/#fcp_nvmf_write_ops","title":"fcp_nvmf_write_ops","text":"<p>Number of FC-NVMe write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>nvmf.write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>nvmf_write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/fcp.yaml <p>The <code>fcp_nvmf_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Highlights stat FC Write Throughput"},{"location":"ontap-metrics/#fcp_other_ops","title":"fcp_other_ops","text":"<p>Number of operations that are not read or write.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp.yaml"},{"location":"ontap-metrics/#fcp_prim_seq_err","title":"fcp_prim_seq_err","text":"<p>Number of primitive sequence errors</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>primitive_seq_err</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>prim_seq_err</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_prim_seq_err</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission errors"},{"location":"ontap-metrics/#fcp_queue_full","title":"fcp_queue_full","text":"<p>Number of times a queue full condition occurred.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>queue_full</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>queue_full</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_queue_full</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission errors"},{"location":"ontap-metrics/#fcp_read_data","title":"fcp_read_data","text":"<p>Amount of data read from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel table FC ports ONTAP: Network FibreChannel timeseries Top $TopResources FCPs by Send Throughput ONTAP: Node Network Layer timeseries Top $TopResources FC Ports by Throughput"},{"location":"ontap-metrics/#fcp_read_ops","title":"fcp_read_ops","text":"<p>Number of read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp.yaml"},{"location":"ontap-metrics/#fcp_reset_count","title":"fcp_reset_count","text":"<p>Number of physical port resets</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>reset_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>reset_count</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml"},{"location":"ontap-metrics/#fcp_shared_int_count","title":"fcp_shared_int_count","text":"<p>Number of shared interrupts</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>shared_interrupt_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>shared_int_count</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml"},{"location":"ontap-metrics/#fcp_spurious_int_count","title":"fcp_spurious_int_count","text":"<p>Number of spurious interrupts</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>spurious_interrupt_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>spurious_int_count</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_spurious_int_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission interrupts"},{"location":"ontap-metrics/#fcp_threshold_full","title":"fcp_threshold_full","text":"<p>Number of times the total number of outstanding commands on the port exceeds the threshold supported by this port.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>threshold_full</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>threshold_full</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_threshold_full</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel timeseries FCPs Transmission errors"},{"location":"ontap-metrics/#fcp_total_data","title":"fcp_total_data","text":"<p>Amount of FCP traffic to and from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp.yaml"},{"location":"ontap-metrics/#fcp_total_ops","title":"fcp_total_ops","text":"<p>Total number of FCP operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp.yaml"},{"location":"ontap-metrics/#fcp_util_percent","title":"fcp_util_percent","text":"<p>Represent the FCP utilization percentage</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/fcp.yaml <p>The <code>fcp_util_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel table FC ports ONTAP: Node Network Layer timeseries Top $TopResources FC Ports by Utilization %"},{"location":"ontap-metrics/#fcp_write_data","title":"fcp_write_data","text":"<p>Amount of data written to the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp.yaml <p>The <code>fcp_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network FibreChannel table FC ports ONTAP: Network FibreChannel timeseries Top $TopResources FCPs by Receive Throughput ONTAP: Node Network Layer timeseries Top $TopResources FC Ports by Throughput"},{"location":"ontap-metrics/#fcp_write_ops","title":"fcp_write_ops","text":"<p>Number of write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcp</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/fcp.yaml ZapiPerf <code>perf-object-get-instances fcp_port</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcp.yaml"},{"location":"ontap-metrics/#fcvi_firmware_invalid_crc_count","title":"fcvi_firmware_invalid_crc_count","text":"<p>Firmware reported invalid CRC count</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>firmware.invalid_crc_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>fw_invalid_crc</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_firmware_invalid_crc_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FCVI timeseries Invalid CRC Count"},{"location":"ontap-metrics/#fcvi_firmware_invalid_transmit_word_count","title":"fcvi_firmware_invalid_transmit_word_count","text":"<p>Firmware reported invalid transmit word count</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>firmware.invalid_transmit_word_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>fw_invalid_xmit_words</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_firmware_invalid_transmit_word_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FCVI timeseries Invalid Transmit Word Count"},{"location":"ontap-metrics/#fcvi_firmware_link_failure_count","title":"fcvi_firmware_link_failure_count","text":"<p>Firmware reported link failure count</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>firmware.link_failure_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>fw_link_failure</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_firmware_link_failure_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FCVI timeseries Link Failure Count"},{"location":"ontap-metrics/#fcvi_firmware_loss_of_signal_count","title":"fcvi_firmware_loss_of_signal_count","text":"<p>Firmware reported loss of signal count</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>firmware.loss_of_signal_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>fw_loss_of_signal</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_firmware_loss_of_signal_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FCVI timeseries Loss of Signal Count"},{"location":"ontap-metrics/#fcvi_firmware_loss_of_sync_count","title":"fcvi_firmware_loss_of_sync_count","text":"<p>Firmware reported loss of sync count</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>firmware.loss_of_sync_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>fw_loss_of_sync</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_firmware_loss_of_sync_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FCVI timeseries Loss of Sync Count"},{"location":"ontap-metrics/#fcvi_firmware_systat_discard_frames","title":"fcvi_firmware_systat_discard_frames","text":"<p>Firmware reported SyStatDiscardFrames value</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>firmware.systat.discard_frames</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>fw_SyStatDiscardFrames</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_firmware_systat_discard_frames</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FCVI timeseries SyStatDiscardFrames Value"},{"location":"ontap-metrics/#fcvi_hard_reset_count","title":"fcvi_hard_reset_count","text":"<p>Number of times hard reset of FCVI adapter got issued.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>hard_reset_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>hard_reset_cnt</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_hard_reset_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FCVI timeseries Hard Reset Count"},{"location":"ontap-metrics/#fcvi_rdma_write_avg_latency","title":"fcvi_rdma_write_avg_latency","text":"<p>Average RDMA write I/O latency in microseconds.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>rdma.write_average_latency</code>Unit: microsecType: averageBase: rdma.write_ops conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>rdma_write_avg_latency</code>Unit: microsecType: averageBase: rdma_write_ops conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_rdma_write_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster Highlights stat FCVI Write Latency ONTAP: MetroCluster MetroCluster FCVI timeseries Write Latency"},{"location":"ontap-metrics/#fcvi_rdma_write_ops","title":"fcvi_rdma_write_ops","text":"<p>Number of RDMA write I/Os issued per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>rdma.write_ops</code>Unit: noneType: rateBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>rdma_write_ops</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_rdma_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster Highlights stat FCVI Write IOPs ONTAP: MetroCluster MetroCluster FCVI timeseries Write IOPs"},{"location":"ontap-metrics/#fcvi_rdma_write_throughput","title":"fcvi_rdma_write_throughput","text":"<p>RDMA write throughput in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>rdma.write_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>rdma_write_throughput</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_rdma_write_throughput</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster Highlights stat FCVI Write Throughput ONTAP: MetroCluster MetroCluster FCVI timeseries Write Throughput"},{"location":"ontap-metrics/#fcvi_soft_reset_count","title":"fcvi_soft_reset_count","text":"<p>Number of times soft reset of FCVI adapter got issued.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/fcvi</code> <code>soft_reset_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/fcvi.yaml ZapiPerf <code>perf-object-get-instances fcvi</code> <code>soft_reset_cnt</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fcvi.yaml <p>The <code>fcvi_soft_reset_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FCVI timeseries Soft  Reset Count"},{"location":"ontap-metrics/#flashcache_accesses","title":"flashcache_accesses","text":"<p>External cache accesses per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>accesses</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>accesses</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_disk_reads_replaced","title":"flashcache_disk_reads_replaced","text":"<p>Estimated number of disk reads per second replaced by cache</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>disk_reads_replaced</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>disk_reads_replaced</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml <p>The <code>flashcache_disk_reads_replaced</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Disk Utilization timeseries Flash Cache"},{"location":"ontap-metrics/#flashcache_evicts","title":"flashcache_evicts","text":"<p>Number of blocks evicted from the external cache to make room for new blocks</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>evicts</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>evicts</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_hit","title":"flashcache_hit","text":"<p>Number of WAFL buffers served off the external cache</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>hit.total</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>hit</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_hit_directory","title":"flashcache_hit_directory","text":"<p>Number of directory buffers served off the external cache</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>hit.directory</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>hit_directory</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_hit_indirect","title":"flashcache_hit_indirect","text":"<p>Number of indirect file buffers served off the external cache</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>hit.indirect</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>hit_indirect</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_hit_metadata_file","title":"flashcache_hit_metadata_file","text":"<p>Number of metadata file buffers served off the external cache</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>hit.metadata_file</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>hit_metadata_file</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_hit_normal_lev0","title":"flashcache_hit_normal_lev0","text":"<p>Number of normal level 0 WAFL buffers served off the external cache</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>hit.normal_level_zero</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>hit_normal_lev0</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_hit_percent","title":"flashcache_hit_percent","text":"<p>External cache hit rate</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>hit.percent</code>Unit: percentType: averageBase: accesses conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>hit_percent</code>Unit: percentType: percentBase: accesses conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml <p>The <code>flashcache_hit_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Disk Utilization timeseries Flash Cache"},{"location":"ontap-metrics/#flashcache_inserts","title":"flashcache_inserts","text":"<p>Number of WAFL buffers inserted into the external cache</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>inserts</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>inserts</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_invalidates","title":"flashcache_invalidates","text":"<p>Number of blocks invalidated in the external cache</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>invalidates</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>invalidates</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_miss","title":"flashcache_miss","text":"<p>External cache misses</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>miss.total</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>miss</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_miss_directory","title":"flashcache_miss_directory","text":"<p>External cache misses accessing directory buffers</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>miss.directory</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>miss_directory</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_miss_indirect","title":"flashcache_miss_indirect","text":"<p>External cache misses accessing indirect file buffers</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>miss.indirect</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>miss_indirect</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_miss_metadata_file","title":"flashcache_miss_metadata_file","text":"<p>External cache misses accessing metadata file buffers</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>miss.metadata_file</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>miss_metadata_file</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_miss_normal_lev0","title":"flashcache_miss_normal_lev0","text":"<p>External cache misses accessing normal level 0 buffers</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>miss.normal_level_zero</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>miss_normal_lev0</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashcache_usage","title":"flashcache_usage","text":"<p>Percentage of blocks in external cache currently containing valid data</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/external_cache</code> <code>usage</code>Unit: percentType: rawBase: conf/restperf/9.12.0/ext_cache_obj.yaml ZapiPerf <code>perf-object-get-instances ext_cache_obj</code> <code>usage</code>Unit: percentType: rawBase: conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml"},{"location":"ontap-metrics/#flashpool_cache_stats","title":"flashpool_cache_stats","text":"<p>Automated Working-set Analyzer (AWA) per-interval pseudo cache statistics for the most recent intervals. The number of intervals defined as recent is CM_WAFL_HYAS_INT_DIS_CNT. This array is a table with fields corresponding to the enum type of hyas_cache_stat_type_t.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_sizer</code> <code>cache_stats</code>Unit: noneType: rawBase: conf/restperf/9.12.0/wafl_hya_sizer.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_sizer</code> <code>cache_stats</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/wafl_hya_sizer.yaml"},{"location":"ontap-metrics/#flashpool_evict_destage_rate","title":"flashpool_evict_destage_rate","text":"<p>Number of block destage per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>evict_destage_rate</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>evict_destage_rate</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_evict_destage_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by Cache Removals"},{"location":"ontap-metrics/#flashpool_evict_remove_rate","title":"flashpool_evict_remove_rate","text":"<p>Number of block free per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>evict_remove_rate</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>evict_remove_rate</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_evict_remove_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by Cache Removals"},{"location":"ontap-metrics/#flashpool_hya_read_hit_latency_average","title":"flashpool_hya_read_hit_latency_average","text":"<p>Average of RAID I/O latency on read hit.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>hya_read_hit_latency_average</code>Unit: millisecType: averageBase: hya_read_hit_latency_count conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>hya_read_hit_latency_average</code>Unit: millisecType: averageBase: hya_read_hit_latency_count conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_hya_read_hit_latency_average</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by SSD and HDD Latency"},{"location":"ontap-metrics/#flashpool_hya_read_miss_latency_average","title":"flashpool_hya_read_miss_latency_average","text":"<p>Average read miss latency.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>hya_read_miss_latency_average</code>Unit: millisecType: averageBase: hya_read_miss_latency_count conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>hya_read_miss_latency_average</code>Unit: millisecType: averageBase: hya_read_miss_latency_count conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_hya_read_miss_latency_average</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by SSD and HDD Latency"},{"location":"ontap-metrics/#flashpool_hya_write_hdd_latency_average","title":"flashpool_hya_write_hdd_latency_average","text":"<p>Average write latency to HDD.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>hya_write_hdd_latency_average</code>Unit: millisecType: averageBase: hya_write_hdd_latency_count conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>hya_write_hdd_latency_average</code>Unit: millisecType: averageBase: hya_write_hdd_latency_count conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml"},{"location":"ontap-metrics/#flashpool_hya_write_ssd_latency_average","title":"flashpool_hya_write_ssd_latency_average","text":"<p>Average of RAID I/O latency on write to SSD.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>hya_write_ssd_latency_average</code>Unit: millisecType: averageBase: hya_write_ssd_latency_count conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>hya_write_ssd_latency_average</code>Unit: millisecType: averageBase: hya_write_ssd_latency_count conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml"},{"location":"ontap-metrics/#flashpool_read_cache_ins_rate","title":"flashpool_read_cache_ins_rate","text":"<p>Cache insert rate blocks/sec.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>read_cache_insert_rate</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>read_cache_ins_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_read_cache_ins_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by Cache Inserts"},{"location":"ontap-metrics/#flashpool_read_ops_replaced","title":"flashpool_read_ops_replaced","text":"<p>Number of HDD read operations replaced by SSD reads per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>read_ops_replaced</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>read_ops_replaced</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_read_ops_replaced</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by Flash Pool Activity ONTAP: Disk Disk Utilization timeseries Flash Pool"},{"location":"ontap-metrics/#flashpool_read_ops_replaced_percent","title":"flashpool_read_ops_replaced_percent","text":"<p>Percentage of HDD read operations replace by SSD.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>read_ops_replaced_percent</code>Unit: percentType: percentBase: read_ops_total conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>read_ops_replaced_percent</code>Unit: percentType: percentBase: read_ops_total conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_read_ops_replaced_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by Flash Pool Activity ONTAP: Disk Disk Utilization timeseries Flash Pool"},{"location":"ontap-metrics/#flashpool_ssd_available","title":"flashpool_ssd_available","text":"<p>Total SSD blocks available.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>ssd_available</code>Unit: noneType: rawBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>ssd_available</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml"},{"location":"ontap-metrics/#flashpool_ssd_read_cached","title":"flashpool_ssd_read_cached","text":"<p>Total read cached SSD blocks.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>ssd_read_cached</code>Unit: noneType: rawBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>ssd_read_cached</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_ssd_read_cached</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by Flash Pool Capacity Used"},{"location":"ontap-metrics/#flashpool_ssd_total","title":"flashpool_ssd_total","text":"<p>Total SSD blocks.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>ssd_total</code>Unit: noneType: rawBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>ssd_total</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_ssd_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by Flash Pool Capacity Used"},{"location":"ontap-metrics/#flashpool_ssd_total_used","title":"flashpool_ssd_total_used","text":"<p>Total SSD blocks used.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>ssd_total_used</code>Unit: noneType: rawBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>ssd_total_used</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml"},{"location":"ontap-metrics/#flashpool_ssd_write_cached","title":"flashpool_ssd_write_cached","text":"<p>Total write cached SSD blocks.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>ssd_write_cached</code>Unit: noneType: rawBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>ssd_write_cached</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_ssd_write_cached</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by Flash Pool Capacity Used"},{"location":"ontap-metrics/#flashpool_wc_write_blks_total","title":"flashpool_wc_write_blks_total","text":"<p>Number of write-cache blocks written per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>wc_write_blocks_total</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>wc_write_blks_total</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml <p>The <code>flashpool_wc_write_blks_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Flash Pool timeseries Top $TopResources Aggregates by Cache Inserts"},{"location":"ontap-metrics/#flashpool_write_blks_replaced","title":"flashpool_write_blks_replaced","text":"<p>Number of HDD write blocks replaced by SSD writes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>write_blocks_replaced</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>write_blks_replaced</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml"},{"location":"ontap-metrics/#flashpool_write_blks_replaced_percent","title":"flashpool_write_blks_replaced_percent","text":"<p>Percentage of blocks overwritten to write-cache among all disk writes.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl_hya_per_aggregate</code> <code>write_blocks_replaced_percent</code>Unit: percentType: averageBase: estimated_write_blocks_total conf/restperf/9.12.0/wafl_hya_per_aggr.yaml ZapiPerf <code>perf-object-get-instances wafl_hya_per_aggr</code> <code>write_blks_replaced_percent</code>Unit: percentType: averageBase: est_write_blks_total conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml"},{"location":"ontap-metrics/#flexcache_blocks_requested_from_client","title":"flexcache_blocks_requested_from_client","text":"<p>Total blocks requested by the client.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.flexcache_raw.client_requested_blocks</code>Unit: Type: deltaBase: conf/keyperf/9.15.0/flexcache.yaml StatPerf <code>flexcache_per_volume</code> <code>blocks_requested_from_client</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>blocks_requested_from_client</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_blocks_requested_from_client</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Highlights timeseries Top $TopResources Blocks requested from Client"},{"location":"ontap-metrics/#flexcache_blocks_retrieved_from_origin","title":"flexcache_blocks_retrieved_from_origin","text":"<p>Blocks retrieved from origin in case of a cache miss. This can be divided by the raw client_requested_blocks and multiplied by 100 to calculate the cache miss percentage.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.flexcache_raw.cache_miss_blocks</code>Unit: Type: deltaBase: conf/keyperf/9.15.0/flexcache.yaml StatPerf <code>flexcache_per_volume</code> <code>blocks_retrieved_from_origin</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>blocks_retrieved_from_origin</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_blocks_retrieved_from_origin</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Highlights timeseries Top $TopResources Blocks requested from Origin"},{"location":"ontap-metrics/#flexcache_evict_rw_cache_skipped_reason_disconnected","title":"flexcache_evict_rw_cache_skipped_reason_disconnected","text":"<p>Total number of read-write cache evict operations skipped because cache is disconnected.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>evict_rw_cache_skipped_reason_disconnected</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>evict_rw_cache_skipped_reason_disconnected</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_evict_rw_cache_skipped_reason_disconnected</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Evict timeseries Top $TopResources Read-Write Cache Evictions Skipped Due to Cache Disconnection"},{"location":"ontap-metrics/#flexcache_evict_skipped_reason_config_noent","title":"flexcache_evict_skipped_reason_config_noent","text":"<p>Total number of evict operation is skipped because cache config is not available.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>evict_skipped_reason_config_noent</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>evict_skipped_reason_config_noent</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_evict_skipped_reason_config_noent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Evict timeseries Top $TopResources Evictions Skipped Due to Configuration Issues"},{"location":"ontap-metrics/#flexcache_evict_skipped_reason_disconnected","title":"flexcache_evict_skipped_reason_disconnected","text":"<p>Total number of evict operation is skipped because cache is disconnected.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>evict_skipped_reason_disconnected</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>evict_skipped_reason_disconnected</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_evict_skipped_reason_disconnected</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Evict timeseries Top $TopResources Evictions Skipped Due to Cache Disconnection"},{"location":"ontap-metrics/#flexcache_evict_skipped_reason_offline","title":"flexcache_evict_skipped_reason_offline","text":"<p>Total number of evict operation is skipped because cache volume is offline.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>evict_skipped_reason_offline</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>evict_skipped_reason_offline</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_evict_skipped_reason_offline</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Evict timeseries Top $TopResources Evictions Skipped When Cache is Offline ONTAP: FlexCache Invalidate timeseries Top $TopResources Invalidate Operations Skipped When Cache Volume is Offline"},{"location":"ontap-metrics/#flexcache_invalidate_skipped_reason_config_noent","title":"flexcache_invalidate_skipped_reason_config_noent","text":"<p>Total number of invalidate operation is skipped because cache config is not available.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>invalidate_skipped_reason_config_noent</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>invalidate_skipped_reason_config_noent</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_invalidate_skipped_reason_config_noent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Invalidate timeseries Top $TopResources Invalidate Operations Skipped Due to Unavailable Cache Configuration"},{"location":"ontap-metrics/#flexcache_invalidate_skipped_reason_disconnected","title":"flexcache_invalidate_skipped_reason_disconnected","text":"<p>Total number of invalidate operation is skipped because cache is disconnected.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>invalidate_skipped_reason_disconnected</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>invalidate_skipped_reason_disconnected</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_invalidate_skipped_reason_disconnected</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Invalidate timeseries Top $TopResources Invalidate Operations Skipped Due to Cache Disconnection"},{"location":"ontap-metrics/#flexcache_invalidate_skipped_reason_offline","title":"flexcache_invalidate_skipped_reason_offline","text":"<p>Total number of invalidate operation is skipped because cache volume is offline.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>invalidate_skipped_reason_offline</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>invalidate_skipped_reason_offline</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml"},{"location":"ontap-metrics/#flexcache_miss_percent","title":"flexcache_miss_percent","text":"<p>This metric represents the percentage of block requests from a client that resulted in a \"miss\" in the FlexCache. A \"miss\" occurs when the requested data is not found in the cache and has to be retrieved from the origin volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>blocks_retrieved_from_origin, blocks_requested_from_client</code>Unit: Type: Base: conf/keyperf/9.15.0/flexcache.yaml StatPerf <code>flexcache_per_volume</code> <code>blocks_retrieved_from_origin, blocks_requested_from_client</code>Unit: Type: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>flexcache_per_volume</code> <code>blocks_retrieved_from_origin, blocks_requested_from_client</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_miss_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Highlights timeseries Top $TopResources Cache Miss percent"},{"location":"ontap-metrics/#flexcache_nix_retry_skipped_reason_initiator_retrieve","title":"flexcache_nix_retry_skipped_reason_initiator_retrieve","text":"<p>Total retry nix operations skipped because the initiator is retrieve operation.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>nix_retry_skipped_reason_initiator_retrieve</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>nix_retry_skipped_reason_initiator_retrieve</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_nix_retry_skipped_reason_initiator_retrieve</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Nix timeseries Top $TopResources Retry Nix Operations Skipped Due to Retrieve Operation Initiator"},{"location":"ontap-metrics/#flexcache_nix_skipped_reason_config_noent","title":"flexcache_nix_skipped_reason_config_noent","text":"<p>Total number of nix operation is skipped because cache config is not available.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>nix_skipped_reason_config_noent</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>nix_skipped_reason_config_noent</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_nix_skipped_reason_config_noent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Nix timeseries Top $TopResources Nix Operations Skipped Due to Unavailable Cache Configuration"},{"location":"ontap-metrics/#flexcache_nix_skipped_reason_disconnected","title":"flexcache_nix_skipped_reason_disconnected","text":"<p>Total number of nix operation is skipped because cache is disconnected.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>nix_skipped_reason_disconnected</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>nix_skipped_reason_disconnected</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_nix_skipped_reason_disconnected</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Nix timeseries Top $TopResources Nix Operations Skipped Due to Cache Disconnection"},{"location":"ontap-metrics/#flexcache_nix_skipped_reason_in_progress","title":"flexcache_nix_skipped_reason_in_progress","text":"<p>Total nix operations skipped because of an in-progress nix.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>nix_skipped_reason_in_progress</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>nix_skipped_reason_in_progress</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_nix_skipped_reason_in_progress</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Nix timeseries Top $TopResources Nix Operations Skipped Due to In-Progress Nix Operation"},{"location":"ontap-metrics/#flexcache_nix_skipped_reason_offline","title":"flexcache_nix_skipped_reason_offline","text":"<p>Total number of nix operation is skipped because cache volume is offline.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>nix_skipped_reason_offline</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>nix_skipped_reason_offline</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_nix_skipped_reason_offline</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Nix timeseries Top $TopResources Nix Operations Skipped When Cache Volume is Offline"},{"location":"ontap-metrics/#flexcache_reconciled_data_entries","title":"flexcache_reconciled_data_entries","text":"<p>Total number of reconciled data entries at cache side.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>reconciled_data_entries</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>reconciled_data_entries</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_reconciled_data_entries</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Reconcile Metrics timeseries Top $TopResources Reconciled data entries"},{"location":"ontap-metrics/#flexcache_reconciled_lock_entries","title":"flexcache_reconciled_lock_entries","text":"<p>Total number of reconciled lock entries at cache side.</p> API Endpoint Metric Template StatPerf <code>flexcache_per_volume</code> <code>reconciled_lock_entries</code>Unit: noneType: Base: conf/statperf/9.8.0/flexcache.yaml ZapiPerf <code>perf-object-get-instances flexcache_per_volume</code> <code>reconciled_lock_entries</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_reconciled_lock_entries</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Reconcile Metrics timeseries Top $TopResources Reconciled Lock Entries"},{"location":"ontap-metrics/#flexcache_size","title":"flexcache_size","text":"<p>Physical size of the volume, in bytes. The minimum size for a FlexVol volume is 20MB and the minimum size for a FlexGroup volume is 200MB per constituent. The recommended size for a FlexGroup volume is a minimum of 100GB per constituent. For all volumes, the default size is equal to the minimum size.</p> API Endpoint Metric Template REST <code>api/storage/flexcache/flexcaches</code> <code>size</code> conf/rest/9.12.0/flexcache.yaml ZAPI <code>flexcache-get-iter</code> <code>flexcache-info.size</code> conf/zapi/cdot/9.8.0/flexcache.yaml <p>The <code>flexcache_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexCache Highlights table FlexCache Details"},{"location":"ontap-metrics/#fpolicy_aborted_requests","title":"fpolicy_aborted_requests","text":"<p>Number of screen requests aborted</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_policy</code> <code>aborted_requests</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fpolicy.yaml <p>The <code>fpolicy_aborted_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Highlights timeseries Top $TopResources Policy by Aborted Requests"},{"location":"ontap-metrics/#fpolicy_denied_requests","title":"fpolicy_denied_requests","text":"<p>Number of screen requests for which deny is received from fpolicy server</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_policy</code> <code>denied_requests</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fpolicy.yaml <p>The <code>fpolicy_denied_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Highlights timeseries Top $TopResources Policy by Denied Requests"},{"location":"ontap-metrics/#fpolicy_io_processing_latency","title":"fpolicy_io_processing_latency","text":"<p>Average IO processing latency in microseconds for screen request</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_policy</code> <code>io_processing_latency</code>Unit: microsecType: averageBase: io_processing_latency_base conf/zapiperf/cdot/9.8.0/fpolicy.yaml <p>The <code>fpolicy_io_processing_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Highlights timeseries Top $TopResources Policy by IO Processing Latency"},{"location":"ontap-metrics/#fpolicy_io_thread_wait_latency","title":"fpolicy_io_thread_wait_latency","text":"<p>Average IO thread wait latency in microseconds for the screen request</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_policy</code> <code>io_thread_wait_latency</code>Unit: microsecType: averageBase: io_thread_wait_latency_base conf/zapiperf/cdot/9.8.0/fpolicy.yaml <p>The <code>fpolicy_io_thread_wait_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Highlights timeseries Top $TopResources Policy by IO Thread Wait Latency"},{"location":"ontap-metrics/#fpolicy_processed_requests","title":"fpolicy_processed_requests","text":"<p>Number of screen requests went through fpolicy processing</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_policy</code> <code>processed_requests</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fpolicy.yaml <p>The <code>fpolicy_processed_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Highlights timeseries Top $TopResources Policy by Processed Requests"},{"location":"ontap-metrics/#fpolicy_processing_latency","title":"fpolicy_processing_latency","text":"<p>Average policy processing latency in microseconds for screen request</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_policy</code> <code>policy_processing_latency</code>Unit: microsecType: averageBase: policy_processing_latency_base conf/zapiperf/cdot/9.8.0/fpolicy.yaml <p>The <code>fpolicy_processing_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Highlights timeseries Top $TopResources Policy by Processing Latency"},{"location":"ontap-metrics/#fpolicy_server_cancelled_requests","title":"fpolicy_server_cancelled_requests","text":"<p>Number of screen requests whose processing was cancelled (cancel timeout)</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_server</code> <code>cancelled_requests</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml <p>The <code>fpolicy_server_cancelled_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Server timeseries Top $TopResources Servers by Cancelled Requests"},{"location":"ontap-metrics/#fpolicy_server_failed_requests","title":"fpolicy_server_failed_requests","text":"<p>Number of screen requests the node failed to send to fpolicy server</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_server</code> <code>failed_requests</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml <p>The <code>fpolicy_server_failed_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Server timeseries Top $TopResources Servers by Failed Requests"},{"location":"ontap-metrics/#fpolicy_server_max_request_latency","title":"fpolicy_server_max_request_latency","text":"<p>Maximum latency in microseconds for a screen request</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_server</code> <code>max_request_latency</code>Unit: microsecType: rawBase: conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml <p>The <code>fpolicy_server_max_request_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Server timeseries Top $TopResources Servers by Max Request Latency"},{"location":"ontap-metrics/#fpolicy_server_outstanding_requests","title":"fpolicy_server_outstanding_requests","text":"<p>Total number of screen requests waiting for response</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_server</code> <code>outstanding_requests</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml <p>The <code>fpolicy_server_outstanding_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Server timeseries Top $TopResources Servers by Outstanding Requests"},{"location":"ontap-metrics/#fpolicy_server_processed_requests","title":"fpolicy_server_processed_requests","text":"<p>Total number of screen requests processed(sync and async)</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_server</code> <code>processed_requests</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml <p>The <code>fpolicy_server_processed_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Server timeseries Top $TopResources Servers by Processed Requests"},{"location":"ontap-metrics/#fpolicy_server_request_latency","title":"fpolicy_server_request_latency","text":"<p>Average latency in microseconds for screen request</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy_server</code> <code>request_latency</code>Unit: microsecType: averageBase: request_latency_base conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml <p>The <code>fpolicy_server_request_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy Server timeseries Top $TopResources Servers by Request Latency"},{"location":"ontap-metrics/#fpolicy_svm_aborted_requests","title":"fpolicy_svm_aborted_requests","text":"<p>Number of screen requests aborted</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy</code> <code>aborted_requests</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml <p>The <code>fpolicy_svm_aborted_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy SVM timeseries Top $TopResources SVM by Aborted Requests"},{"location":"ontap-metrics/#fpolicy_svm_cifs_requests","title":"fpolicy_svm_cifs_requests","text":"<p>Number of cifs screen requests sent to fpolicy server</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy</code> <code>cifs_requests</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml <p>The <code>fpolicy_svm_cifs_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy SVM timeseries Top $TopResources SVM by Cifs Requests"},{"location":"ontap-metrics/#fpolicy_svm_failedop_notifications","title":"fpolicy_svm_failedop_notifications","text":"<p>Number of failed file operation notifications sent to fpolicy server</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy</code> <code>failedop_notifications</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml <p>The <code>fpolicy_svm_failedop_notifications</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy SVM timeseries Top $TopResources SVM by Failed File Operation"},{"location":"ontap-metrics/#fpolicy_svm_io_processing_latency","title":"fpolicy_svm_io_processing_latency","text":"<p>Average IO processing latency in microseconds for screen request</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy</code> <code>io_processing_latency</code>Unit: microsecType: averageBase: io_processing_latency_base conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml <p>The <code>fpolicy_svm_io_processing_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy SVM timeseries Top $TopResources SVM by IO Processing Latency"},{"location":"ontap-metrics/#fpolicy_svm_io_thread_wait_latency","title":"fpolicy_svm_io_thread_wait_latency","text":"<p>Average IO thread wait latency in microseconds for screen request</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances fpolicy</code> <code>io_thread_wait_latency</code>Unit: microsecType: averageBase: io_thread_wait_latency_base conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml <p>The <code>fpolicy_svm_io_thread_wait_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FPolicy SVM timeseries Top $TopResources SVM by IO Thread Wait Latency"},{"location":"ontap-metrics/#fru_status","title":"fru_status","text":"<p>This metric indicates a value of 1 if the FRU status is ok and a value of 0 for any other state.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/fru.yaml <p>The <code>fru_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Power Field Replaceable Unit (FRU) table Field Replaceable Unit (FRU)"},{"location":"ontap-metrics/#headroom_aggr_current_latency","title":"headroom_aggr_current_latency","text":"<p>This is the storage aggregate average latency in microseconds per message at the disk level.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>current_latency</code>Unit: microsecType: averageBase: current_ops conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>current_latency</code>Unit: microsecType: averageBase: current_ops conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml <p>The <code>headroom_aggr_current_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom Aggregate Headroom timeseries Current Latency"},{"location":"ontap-metrics/#headroom_aggr_current_ops","title":"headroom_aggr_current_ops","text":"<p>Total number of I/Os processed by the aggregate per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>current_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>current_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml <p>The <code>headroom_aggr_current_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom Highlights timeseries Available Ops: Aggregate ONTAP: Headroom Aggregate Headroom timeseries Current IOP/s"},{"location":"ontap-metrics/#headroom_aggr_current_utilization","title":"headroom_aggr_current_utilization","text":"<p>This is the storage aggregate average utilization of all the data disks in the aggregate.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>current_utilization</code>Unit: percentType: percentBase: current_utilization_denominator conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>current_utilization</code>Unit: percentType: percentBase: current_utilization_total conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml <p>The <code>headroom_aggr_current_utilization</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom Aggregate Headroom timeseries Current Utilization"},{"location":"ontap-metrics/#headroom_aggr_ewma_daily","title":"headroom_aggr_ewma_daily","text":"<p>Daily exponential weighted moving average.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>ewma.daily</code>Unit: noneType: rawBase: conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>ewma_daily</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml"},{"location":"ontap-metrics/#headroom_aggr_ewma_hourly","title":"headroom_aggr_ewma_hourly","text":"<p>Hourly exponential weighted moving average.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>ewma.hourly</code>Unit: noneType: rawBase: conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>ewma_hourly</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml"},{"location":"ontap-metrics/#headroom_aggr_ewma_monthly","title":"headroom_aggr_ewma_monthly","text":"<p>Monthly exponential weighted moving average.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>ewma.monthly</code>Unit: noneType: rawBase: conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>ewma_monthly</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml"},{"location":"ontap-metrics/#headroom_aggr_ewma_weekly","title":"headroom_aggr_ewma_weekly","text":"<p>Weekly exponential weighted moving average.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>ewma.weekly</code>Unit: noneType: rawBase: conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>ewma_weekly</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml"},{"location":"ontap-metrics/#headroom_aggr_optimal_point_confidence_factor","title":"headroom_aggr_optimal_point_confidence_factor","text":"<p>The confidence factor for the optimal point value based on the observed resource latency and utilization.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>optimal_point.confidence_factor</code>Unit: noneType: averageBase: optimal_point.samples conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>optimal_point_confidence_factor</code>Unit: noneType: averageBase: optimal_point_samples conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml"},{"location":"ontap-metrics/#headroom_aggr_optimal_point_latency","title":"headroom_aggr_optimal_point_latency","text":"<p>The latency in microseconds component of the optimal point of the latency/utilization curve.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>optimal_point.latency</code>Unit: microsecType: averageBase: optimal_point.samples conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>optimal_point_latency</code>Unit: microsecType: averageBase: optimal_point_samples conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml <p>The <code>headroom_aggr_optimal_point_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom Aggregate Headroom timeseries Optimal-Point Latency"},{"location":"ontap-metrics/#headroom_aggr_optimal_point_ops","title":"headroom_aggr_optimal_point_ops","text":"<p>The ops component of the optimal point derived from the latency/utilzation curve.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>optimal_point.ops</code>Unit: per_secType: averageBase: optimal_point.samples conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>optimal_point_ops</code>Unit: per_secType: averageBase: optimal_point_samples conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml <p>The <code>headroom_aggr_optimal_point_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom Highlights timeseries Available Ops: Aggregate ONTAP: Headroom Aggregate Headroom timeseries Optimal-Point IOP/s"},{"location":"ontap-metrics/#headroom_aggr_optimal_point_utilization","title":"headroom_aggr_optimal_point_utilization","text":"<p>The utilization component of the optimal point of the latency/utilization curve.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_aggregate</code> <code>optimal_point.utilization</code>Unit: noneType: averageBase: optimal_point.samples conf/restperf/9.12.0/resource_headroom_aggr.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_aggr</code> <code>optimal_point_utilization</code>Unit: noneType: averageBase: optimal_point_samples conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml <p>The <code>headroom_aggr_optimal_point_utilization</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom Aggregate Headroom timeseries Optimal-Point Utilization"},{"location":"ontap-metrics/#headroom_cpu_current_latency","title":"headroom_cpu_current_latency","text":"<p>Current operation latency in microseconds of the resource.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>current_latency</code>Unit: microsecType: averageBase: current_ops conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>current_latency</code>Unit: microsecType: averageBase: current_ops conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml <p>The <code>headroom_cpu_current_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom CPU Headroom timeseries Current Latency"},{"location":"ontap-metrics/#headroom_cpu_current_ops","title":"headroom_cpu_current_ops","text":"<p>Total number of operations per second (also referred to as dblade ops).</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>current_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>current_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml <p>The <code>headroom_cpu_current_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom Highlights timeseries Available Ops: CPU ONTAP: Headroom CPU Headroom timeseries Current CPU Ops ONTAP: NFS Troubleshooting Highlights table Headroom Overview (Average by Time Range)"},{"location":"ontap-metrics/#headroom_cpu_current_utilization","title":"headroom_cpu_current_utilization","text":"<p>Average processor utilization across all processors in the system.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>current_utilization</code>Unit: percentType: percentBase: elapsed_time conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>current_utilization</code>Unit: percentType: percentBase: current_utilization_total conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml <p>The <code>headroom_cpu_current_utilization</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom CPU Headroom timeseries Current Utilization ONTAP: NFS Troubleshooting Highlights table Headroom Overview (Average by Time Range)"},{"location":"ontap-metrics/#headroom_cpu_ewma_daily","title":"headroom_cpu_ewma_daily","text":"<p>Daily exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>ewma.daily</code>Unit: noneType: rawBase: conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>ewma_daily</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml <p>The <code>headroom_cpu_ewma_daily</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFS Troubleshooting Highlights timeseries Weighted Avg Daily (Headroom)"},{"location":"ontap-metrics/#headroom_cpu_ewma_hourly","title":"headroom_cpu_ewma_hourly","text":"<p>Hourly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>ewma.hourly</code>Unit: noneType: rawBase: conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>ewma_hourly</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml"},{"location":"ontap-metrics/#headroom_cpu_ewma_monthly","title":"headroom_cpu_ewma_monthly","text":"<p>Monthly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>ewma.monthly</code>Unit: noneType: rawBase: conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>ewma_monthly</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml"},{"location":"ontap-metrics/#headroom_cpu_ewma_weekly","title":"headroom_cpu_ewma_weekly","text":"<p>Weekly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>ewma.weekly</code>Unit: noneType: rawBase: conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>ewma_weekly</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml <p>The <code>headroom_cpu_ewma_weekly</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFS Troubleshooting Highlights timeseries Weighted Avg Weekly (Headroom)"},{"location":"ontap-metrics/#headroom_cpu_optimal_point_confidence_factor","title":"headroom_cpu_optimal_point_confidence_factor","text":"<p>Confidence factor for the optimal point value based on the observed resource latency and utilization. The possible values are: 0 - unknown, 1 - low, 2 - medium, 3 - high. This counter can provide an average confidence factor over a range of time.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>optimal_point.confidence_factor</code>Unit: noneType: averageBase: optimal_point.samples conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>optimal_point_confidence_factor</code>Unit: noneType: averageBase: optimal_point_samples conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml"},{"location":"ontap-metrics/#headroom_cpu_optimal_point_latency","title":"headroom_cpu_optimal_point_latency","text":"<p>Latency component of the optimal point of the latency in microseconds/utilization curve. This counter can provide an average latency over a range of time.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>optimal_point.latency</code>Unit: microsecType: averageBase: optimal_point.samples conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>optimal_point_latency</code>Unit: microsecType: averageBase: optimal_point_samples conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml <p>The <code>headroom_cpu_optimal_point_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom CPU Headroom timeseries Optimal-Point Latency"},{"location":"ontap-metrics/#headroom_cpu_optimal_point_ops","title":"headroom_cpu_optimal_point_ops","text":"<p>Ops component of the optimal point derived from the latency/utilization curve. This counter can provide an average ops over a range of time.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>optimal_point.ops</code>Unit: per_secType: averageBase: optimal_point.samples conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>optimal_point_ops</code>Unit: per_secType: averageBase: optimal_point_samples conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml <p>The <code>headroom_cpu_optimal_point_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom Highlights timeseries Available Ops: CPU ONTAP: Headroom CPU Headroom timeseries Optimal-Point Ops ONTAP: NFS Troubleshooting Highlights table Headroom Overview (Average by Time Range)"},{"location":"ontap-metrics/#headroom_cpu_optimal_point_utilization","title":"headroom_cpu_optimal_point_utilization","text":"<p>Utilization component of the optimal point of the latency/utilization curve. This counter can provide an average utilization over a range of time.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/headroom_cpu</code> <code>optimal_point.utilization</code>Unit: noneType: averageBase: optimal_point.samples conf/restperf/9.12.0/resource_headroom_cpu.yaml ZapiPerf <code>perf-object-get-instances resource_headroom_cpu</code> <code>optimal_point_utilization</code>Unit: noneType: averageBase: optimal_point_samples conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml <p>The <code>headroom_cpu_optimal_point_utilization</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Headroom CPU Headroom timeseries Optimal-Point Utilization ONTAP: NFS Troubleshooting Highlights table Headroom Overview (Average by Time Range)"},{"location":"ontap-metrics/#health_disk_alerts","title":"health_disk_alerts","text":"<p>Provides any issues related to Disks health check if disks are broken or unassigned. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_disk_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Errors ONTAP: Datacenter Issues piechart Warnings ONTAP: Health Highlights stat Total Errors ONTAP: Health Highlights stat Total Warnings ONTAP: Health Highlights piechart Errors ONTAP: Health Highlights piechart Warnings ONTAP: Health Disks table Disks Issues"},{"location":"ontap-metrics/#health_ems_alerts","title":"health_ems_alerts","text":"<p>The health_ems_alerts metric monitors EMS (Event Management System), providing a count based on their severity and other attributes. This metric includes labels such as node, message, source, and severity (e.g., emergency, alert, error). By default, it monitors alerts with emergency severity.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_ems_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Health Highlights stat Active Emergency EMS Alerts (Last 24 Hours) ONTAP: Health Highlights table Active Emergency EMS Alerts (Last 24 Hours) ONTAP: Health Emergency EMS table Active Emergency EMS Alerts (Last 24 Hours)"},{"location":"ontap-metrics/#health_ha_alerts","title":"health_ha_alerts","text":"<p>Provides any issues related to HA health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_ha_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Errors ONTAP: Health Highlights stat Total Errors ONTAP: Health Highlights piechart Errors ONTAP: Health HA table HA Issues"},{"location":"ontap-metrics/#health_license_alerts","title":"health_license_alerts","text":"<p>Provides any issues related to License health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_license_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Errors ONTAP: Health Highlights stat Total Errors ONTAP: Health Highlights piechart Errors ONTAP: Health License table Non Compliant License"},{"location":"ontap-metrics/#health_lif_alerts","title":"health_lif_alerts","text":"<p>Provides any issues related to LIF health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_lif_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Warnings ONTAP: Health Highlights stat Total Warnings ONTAP: Health Highlights piechart Warnings ONTAP: Health Lif table Lif not at home port"},{"location":"ontap-metrics/#health_network_ethernet_port_alerts","title":"health_network_ethernet_port_alerts","text":"<p>Provides any issues related to Network Ethernet Port health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_network_ethernet_port_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Errors ONTAP: Health Highlights stat Total Errors ONTAP: Health Highlights piechart Errors ONTAP: Health Network Port table Ethernet ports are down"},{"location":"ontap-metrics/#health_network_fc_port_alerts","title":"health_network_fc_port_alerts","text":"<p>Provides any issues related to Network FC Port health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_network_fc_port_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Errors ONTAP: Health Highlights stat Total Errors ONTAP: Health Highlights piechart Errors ONTAP: Health Network Port table FC ports are down"},{"location":"ontap-metrics/#health_node_alerts","title":"health_node_alerts","text":"<p>Provides any issues related to Node health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_node_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Errors ONTAP: Health Highlights stat Total Errors ONTAP: Health Highlights piechart Errors ONTAP: Health Node table Node Issues"},{"location":"ontap-metrics/#health_shelf_alerts","title":"health_shelf_alerts","text":"<p>Provides any issues related to Shelf health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_shelf_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Errors ONTAP: Datacenter Issues piechart Warnings ONTAP: Health Highlights stat Total Errors ONTAP: Health Highlights stat Total Warnings ONTAP: Health Highlights piechart Errors ONTAP: Health Highlights piechart Warnings ONTAP: Health Shelves table Storage Shelf Issues"},{"location":"ontap-metrics/#health_support_alerts","title":"health_support_alerts","text":"<p>Provides any issues related to Support health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_support_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Warnings ONTAP: Health Highlights stat Total Warnings ONTAP: Health Highlights piechart Warnings ONTAP: Health System Health Alerts table System Alerts"},{"location":"ontap-metrics/#health_volume_move_alerts","title":"health_volume_move_alerts","text":"<p>Provides any issues related to Volume Move health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_volume_move_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Warnings ONTAP: Health Highlights stat Total Warnings ONTAP: Health Highlights piechart Warnings ONTAP: Health Volume table Volumes Move Issues"},{"location":"ontap-metrics/#health_volume_ransomware_alerts","title":"health_volume_ransomware_alerts","text":"<p>Provides any issues related to Volume Ransomware health check. Value of 1 means issue is happening and 0 means that issue is resolved.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.6.0/health.yaml <p>The <code>health_volume_ransomware_alerts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Issues piechart Warnings ONTAP: Health Highlights stat Total Warnings ONTAP: Health Highlights piechart Warnings ONTAP: Health Volume table Volumes with Ransomware Issues (9.10+ Only)"},{"location":"ontap-metrics/#hostadapter_bytes_read","title":"hostadapter_bytes_read","text":"<p>Bytes read through a host adapter</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/host_adapter</code> <code>bytes_read</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/hostadapter.yaml ZapiPerf <code>perf-object-get-instances hostadapter</code> <code>bytes_read</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/hostadapter.yaml <p>The <code>hostadapter_bytes_read</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Disk Utilization timeseries Disk and Tape Drives Throughput by Node ONTAP: Disk Disk Utilization timeseries Top $TopResources Disk and Tape Drives Throughput by Host Adapter ONTAP: MetroCluster Disk and Tape Adapter timeseries Top $TopResources Adapters by Read Data"},{"location":"ontap-metrics/#hostadapter_bytes_written","title":"hostadapter_bytes_written","text":"<p>Bytes written through a host adapter</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/host_adapter</code> <code>bytes_written</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/hostadapter.yaml ZapiPerf <code>perf-object-get-instances hostadapter</code> <code>bytes_written</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/hostadapter.yaml <p>The <code>hostadapter_bytes_written</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Disk Utilization timeseries Disk and Tape Drives Throughput by Node ONTAP: Disk Disk Utilization timeseries Top $TopResources Disk and Tape Drives Throughput by Host Adapter ONTAP: MetroCluster Disk and Tape Adapter timeseries Top $TopResources Adapters by Write Data"},{"location":"ontap-metrics/#igroup_labels","title":"igroup_labels","text":"<p>Details of Igroups in the cluster.</p> API Endpoint Metric Template REST <code>api/protocols/san/igroups</code> <code>Harvest generated</code> conf/rest/asar2/9.16.0/igroup.yaml <p>The <code>igroup_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Highlights stat SCSI ASAr2: Overview Hosts table SAN initiator groups ONTAP: Hosts Highlights table SAN initiator groups"},{"location":"ontap-metrics/#iscsi_lif_avg_latency","title":"iscsi_lif_avg_latency","text":"<p>Average latency in microseconds for iSCSI operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>average_latency</code>Unit: microsecType: averageBase: cmd_transferred conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>avg_latency</code>Unit: microsecType: averageBase: cmd_transfered conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml <p>The <code>iscsi_lif_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node iSCSI Frontend stat iSCSI Latency ONTAP: Node iSCSI Frontend timeseries Average Latency by LIF ONTAP: SVM iSCSI stat SVM iSCSI Average Latency"},{"location":"ontap-metrics/#iscsi_lif_avg_other_latency","title":"iscsi_lif_avg_other_latency","text":"<p>Average latency in microseconds for operations other than read and write (for example, Inquiry, Report LUNs, SCSI Task Management Functions)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>average_other_latency</code>Unit: microsecType: averageBase: iscsi_other_ops conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>avg_other_latency</code>Unit: microsecType: averageBase: iscsi_other_ops conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml <p>The <code>iscsi_lif_avg_other_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM iSCSI timeseries SVM iSCSI Average Latency"},{"location":"ontap-metrics/#iscsi_lif_avg_read_latency","title":"iscsi_lif_avg_read_latency","text":"<p>Average latency in microseconds for read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>average_read_latency</code>Unit: microsecType: averageBase: iscsi_read_ops conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>avg_read_latency</code>Unit: microsecType: averageBase: iscsi_read_ops conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml <p>The <code>iscsi_lif_avg_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM iSCSI stat SVM iSCSI Average Read Latency ONTAP: SVM iSCSI timeseries SVM iSCSI Average Latency"},{"location":"ontap-metrics/#iscsi_lif_avg_write_latency","title":"iscsi_lif_avg_write_latency","text":"<p>Average latency in microseconds for write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>average_write_latency</code>Unit: microsecType: averageBase: iscsi_write_ops conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>avg_write_latency</code>Unit: microsecType: averageBase: iscsi_write_ops conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml <p>The <code>iscsi_lif_avg_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM iSCSI stat SVM iSCSI Average Write Latency ONTAP: SVM iSCSI timeseries SVM iSCSI Average Latency"},{"location":"ontap-metrics/#iscsi_lif_cmd_transfered","title":"iscsi_lif_cmd_transfered","text":"<p>Command transferred by this iSCSI connection</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>cmd_transferred</code>Unit: noneType: rateBase: conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>cmd_transfered</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml"},{"location":"ontap-metrics/#iscsi_lif_iscsi_other_ops","title":"iscsi_lif_iscsi_other_ops","text":"<p>iSCSI other operations per second on this logical interface (LIF)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>iscsi_other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>iscsi_other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml <p>The <code>iscsi_lif_iscsi_other_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node iSCSI Frontend stat iSCSI IOPs ONTAP: Node iSCSI Frontend timeseries IOPs by LIF ONTAP: SVM iSCSI stat SVM iSCSI IOPs"},{"location":"ontap-metrics/#iscsi_lif_iscsi_read_ops","title":"iscsi_lif_iscsi_read_ops","text":"<p>iSCSI read operations per second on this logical interface (LIF)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>iscsi_read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>iscsi_read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml <p>The <code>iscsi_lif_iscsi_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM iSCSI stat SVM iSCSI Read IOPs"},{"location":"ontap-metrics/#iscsi_lif_iscsi_write_ops","title":"iscsi_lif_iscsi_write_ops","text":"<p>iSCSI write operations per second on this logical interface (LIF)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>iscsi_write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>iscsi_write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml <p>The <code>iscsi_lif_iscsi_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM iSCSI stat SVM iSCSI Write IOPs"},{"location":"ontap-metrics/#iscsi_lif_protocol_errors","title":"iscsi_lif_protocol_errors","text":"<p>Number of protocol errors from iSCSI sessions on this logical interface (LIF)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>protocol_errors</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>protocol_errors</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml"},{"location":"ontap-metrics/#iscsi_lif_read_data","title":"iscsi_lif_read_data","text":"<p>Performance metric for ISCSI LIF read I/O operations in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml <p>The <code>iscsi_lif_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM LIF timeseries Top $TopResources iSCSI LIFs by Send Throughput ONTAP: SVM iSCSI stat SVM iSCSI Read Throughput ONTAP: SVM iSCSI timeseries Top $TopResources iSCSI LIFs by Send Throughput ONTAP: SVM iSCSI timeseries SVM iSCSI Throughput"},{"location":"ontap-metrics/#iscsi_lif_write_data","title":"iscsi_lif_write_data","text":"<p>Performance metric for ISCSI LIF write I/O operations in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iscsi_lif</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/iscsi_lif.yaml ZapiPerf <code>perf-object-get-instances iscsi_lif</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml <p>The <code>iscsi_lif_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node iSCSI Frontend timeseries Throughput by LIF ONTAP: SVM LIF timeseries Top $TopResources iSCSI LIFs by Receive Throughput ONTAP: SVM iSCSI stat SVM iSCSI Throughput ONTAP: SVM iSCSI stat SVM iSCSI Write Throughput ONTAP: SVM iSCSI timeseries Top $TopResources iSCSI LIFs by Receive Throughput ONTAP: SVM iSCSI timeseries SVM iSCSI Throughput"},{"location":"ontap-metrics/#iw_avg_latency","title":"iw_avg_latency","text":"<p>Average RDMA I/O latency in microseconds.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iwarp</code> <code>average_latency</code>Unit: microsecType: averageBase: ops conf/restperf/9.14.1/iwarp.yaml ZapiPerf <code>perf-object-get-instances iwarp</code> <code>iw_avg_latency</code>Unit: microsecType: averageBase: iw_ops conf/zapiperf/cdot/9.8.0/iwarp.yaml <p>The <code>iw_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Iwarp timeseries Average Latency"},{"location":"ontap-metrics/#iw_ops","title":"iw_ops","text":"<p>Number of RDMA I/Os issued.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iwarp</code> <code>ops</code>Unit: noneType: rateBase: conf/restperf/9.14.1/iwarp.yaml ZapiPerf <code>perf-object-get-instances iwarp</code> <code>iw_ops</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/iwarp.yaml"},{"location":"ontap-metrics/#iw_read_ops","title":"iw_read_ops","text":"<p>Number of RDMA read I/Os issued.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iwarp</code> <code>read_ops</code>Unit: noneType: rateBase: conf/restperf/9.14.1/iwarp.yaml ZapiPerf <code>perf-object-get-instances iwarp</code> <code>iw_read_ops</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/iwarp.yaml"},{"location":"ontap-metrics/#iw_write_ops","title":"iw_write_ops","text":"<p>Number of RDMA write I/Os issued.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/iwarp</code> <code>write_ops</code>Unit: noneType: rateBase: conf/restperf/9.14.1/iwarp.yaml ZapiPerf <code>perf-object-get-instances iwarp</code> <code>iw_write_ops</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/iwarp.yaml <p>The <code>iw_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Iwarp timeseries Write IOPs"},{"location":"ontap-metrics/#lif_labels","title":"lif_labels","text":"<p>This metric provides information about LIF</p> API Endpoint Metric Template REST <code>api/network/ip/interfaces</code> <code>Harvest generated</code> conf/rest/9.12.0/lif.yaml ZAPI <code>net-interface-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/lif.yaml <p>The <code>lif_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count ONTAP: Health Lif table Lif not at home port ONTAP: SVM LIF table LIF Details"},{"location":"ontap-metrics/#lif_recv_data","title":"lif_recv_data","text":"<p>Number of bytes received per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lif</code> <code>received_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/lif.yaml KeyPerf <code>api/network/ip/interfaces</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/lif.yaml ZapiPerf <code>perf-object-get-instances lif</code> <code>recv_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lif.yaml <p>The <code>lif_recv_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM LIF timeseries Top $TopResources NAS LIFs by Receive Throughput"},{"location":"ontap-metrics/#lif_recv_errors","title":"lif_recv_errors","text":"<p>Number of received Errors per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lif</code> <code>received_errors</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/lif.yaml ZapiPerf <code>perf-object-get-instances lif</code> <code>recv_errors</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lif.yaml"},{"location":"ontap-metrics/#lif_recv_packet","title":"lif_recv_packet","text":"<p>Number of packets received per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lif</code> <code>received_packets</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/lif.yaml ZapiPerf <code>perf-object-get-instances lif</code> <code>recv_packet</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lif.yaml"},{"location":"ontap-metrics/#lif_sent_data","title":"lif_sent_data","text":"<p>Number of bytes sent per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lif</code> <code>sent_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/lif.yaml KeyPerf <code>api/network/ip/interfaces</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/lif.yaml ZapiPerf <code>perf-object-get-instances lif</code> <code>sent_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lif.yaml <p>The <code>lif_sent_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM LIF timeseries Top $TopResources NAS LIFs by Send Throughput"},{"location":"ontap-metrics/#lif_sent_errors","title":"lif_sent_errors","text":"<p>Number of sent errors per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lif</code> <code>sent_errors</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/lif.yaml ZapiPerf <code>perf-object-get-instances lif</code> <code>sent_errors</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lif.yaml"},{"location":"ontap-metrics/#lif_sent_packet","title":"lif_sent_packet","text":"<p>Number of packets sent per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lif</code> <code>sent_packets</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/lif.yaml ZapiPerf <code>perf-object-get-instances lif</code> <code>sent_packet</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lif.yaml"},{"location":"ontap-metrics/#lif_total_data","title":"lif_total_data","text":"<p>Performance metric aggregated over all types of I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/network/ip/interfaces</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/lif.yaml"},{"location":"ontap-metrics/#lif_uptime","title":"lif_uptime","text":"<p>Interface up time</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lif</code> <code>up_time</code>Unit: millisecType: rawBase: conf/restperf/9.12.0/lif.yaml ZapiPerf <code>perf-object-get-instances lif</code> <code>up_time</code>Unit: millisecType: rawBase: conf/zapiperf/cdot/9.8.0/lif.yaml <p>The <code>lif_uptime</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM LIF table LIF Details"},{"location":"ontap-metrics/#lun_avg_read_latency","title":"lun_avg_read_latency","text":"<p>Average read latency in microseconds for all operations on the LUN</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>average_read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.12.0/lun.yaml KeyPerf <code>api/storage/luns</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: lun_statistics.iops_raw.read conf/keyperf/9.15.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>avg_read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_avg_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Highlights stat Top $TopResources Luns by Average Read Latency ONTAP: LUN LUN Table table Top $TopResources Luns by Read Latency ONTAP: LUN Top LUN Performance timeseries Top $TopResources Luns by Average Read Latency ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries Latency"},{"location":"ontap-metrics/#lun_avg_write_latency","title":"lun_avg_write_latency","text":"<p>Average write latency in microseconds for all operations on the LUN</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>average_write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.12.0/lun.yaml KeyPerf <code>api/storage/luns</code> <code>statistics.latency_raw.write</code>Unit: microsecType: averageBase: lun_statistics.iops_raw.write conf/keyperf/9.15.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>avg_write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_avg_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Highlights stat Top $TopResources Luns by Average Write Latency ONTAP: LUN LUN Table table Top $TopResources Luns by Write Latency ONTAP: LUN Top LUN Performance timeseries Top $TopResources Luns by Average Write Latency ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries Latency"},{"location":"ontap-metrics/#lun_avg_xcopy_latency","title":"lun_avg_xcopy_latency","text":"<p>Average latency in microseconds for xcopy requests</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>average_xcopy_latency</code>Unit: microsecType: averageBase: xcopy_requests conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>avg_xcopy_latency</code>Unit: microsecType: averageBase: xcopy_reqs conf/zapiperf/cdot/9.8.0/lun.yaml"},{"location":"ontap-metrics/#lun_block_size","title":"lun_block_size","text":"<p>Represents the block size being used</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>total_data, total_ops</code>Unit: Type: Base: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>lun</code> <code>total_data, total_ops</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_block_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN LUN Table table LUNS in Cluster ONTAP: LUN Top LUN Performance timeseries Top $TopResources Luns by Block Size"},{"location":"ontap-metrics/#lun_caw_reqs","title":"lun_caw_reqs","text":"<p>Number of compare and write requests</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>caw_requests</code>Unit: noneType: rateBase: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>caw_reqs</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_caw_reqs</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries vStorage Offload Operations"},{"location":"ontap-metrics/#lun_enospc","title":"lun_enospc","text":"<p>Number of operations receiving ENOSPC errors</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>enospc</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>enospc</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/lun.yaml"},{"location":"ontap-metrics/#lun_labels","title":"lun_labels","text":"<p>This metric provides information about Lun</p> API Endpoint Metric Template REST <code>api/private/cli/lun</code> <code>Harvest generated</code> conf/rest/9.12.0/lun.yaml ZAPI <code>lun-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/lun.yaml <p>The <code>lun_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count ONTAP: LUN LUN Table table LUNS in Cluster"},{"location":"ontap-metrics/#lun_new_status","title":"lun_new_status","text":"<p>This metric indicates a value of 1 if the LUN state is online (indicating the LUN is operational) and a value of 0 for any other state. and a value of 0 for any other state.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/lun.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/lun.yaml <p>The <code>lun_new_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN LUN Table table LUNS in Cluster"},{"location":"ontap-metrics/#lun_other_data","title":"lun_other_data","text":"<p>Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/luns</code> <code>statistics.throughput_raw.other</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/lun.yaml"},{"location":"ontap-metrics/#lun_other_latency","title":"lun_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/luns</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: lun_statistics.iops_raw.other conf/keyperf/9.15.0/lun.yaml"},{"location":"ontap-metrics/#lun_other_ops","title":"lun_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/luns</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/lun.yaml"},{"location":"ontap-metrics/#lun_queue_full","title":"lun_queue_full","text":"<p>Queue full responses</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>queue_full</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>queue_full</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml"},{"location":"ontap-metrics/#lun_read_align_histo","title":"lun_read_align_histo","text":"<p>Histogram of WAFL read alignment (number sectors off WAFL block start)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>read_align_histogram</code>Unit: percentType: percentBase: read_ops_sent conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>read_align_histo</code>Unit: percentType: percentBase: read_ops_sent conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_read_align_histo</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Top LUN Performance Efficiency timeseries Top $TopResources Luns by Read Misalignment Buckets"},{"location":"ontap-metrics/#lun_read_data","title":"lun_read_data","text":"<p>Performance metric for read I/O operations in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/lun.yaml KeyPerf <code>api/storage/luns</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Highlights stat Top $TopResources Luns by Read Throughput ONTAP: LUN LUN Table table Top $TopResources Luns by Read Throughput ONTAP: LUN Top LUN Performance timeseries Top $TopResources Luns by Read Throughput ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries Throughput"},{"location":"ontap-metrics/#lun_read_ops","title":"lun_read_ops","text":"<p>Number of read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/lun.yaml KeyPerf <code>api/storage/luns</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Highlights stat Top $TopResources Luns by Read IOPs ONTAP: LUN LUN Table table Top $TopResources Luns by Read IOPS ONTAP: LUN Top LUN Performance timeseries Top $TopResources Luns by Read IOPs ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries IOPs ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries IO Size"},{"location":"ontap-metrics/#lun_read_partial_blocks","title":"lun_read_partial_blocks","text":"<p>Percentage of reads whose size is not a multiple of WAFL block size</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>read_partial_blocks</code>Unit: percentType: percentBase: read_ops conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>read_partial_blocks</code>Unit: percentType: percentBase: read_ops conf/zapiperf/cdot/9.8.0/lun.yaml"},{"location":"ontap-metrics/#lun_remote_bytes","title":"lun_remote_bytes","text":"<p>I/O to or from a LUN which is not owned by the storage system handling the I/O in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>remote_bytes</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>remote_bytes</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_remote_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries Indirect Access"},{"location":"ontap-metrics/#lun_remote_ops","title":"lun_remote_ops","text":"<p>Number of operations received by a storage system that does not own the LUN targeted by the operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>remote_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>remote_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_remote_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Top LUN Performance Efficiency timeseries Top $TopResources Luns by Indirect Access IOPS ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries Indirect Access"},{"location":"ontap-metrics/#lun_size","title":"lun_size","text":"<p>The total provisioned size of the LUN. The LUN size can be increased but not decreased using the REST interface.The maximum and minimum sizes listed here are the absolute maximum and absolute minimum sizes, in bytes. The actual minimum and maximum sizes vary depending on the ONTAP version, ONTAP platform and the available space in the containing volume and aggregate.For more information, see Size properties in the docs section of the ONTAP REST API documentation.</p> API Endpoint Metric Template REST <code>api/private/cli/lun</code> <code>size</code> conf/rest/9.12.0/lun.yaml ZAPI <code>lun-get-iter</code> <code>lun-info.size</code> conf/zapi/cdot/9.8.0/lun.yaml <p>The <code>lun_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN LUN Table table LUNS in Cluster"},{"location":"ontap-metrics/#lun_size_used","title":"lun_size_used","text":"<p>The amount of space consumed by the main data stream of the LUN.This value is the total space consumed in the volume by the LUN, including filesystem overhead, but excluding prefix and suffix streams. Due to internal filesystem overhead and the many ways SAN filesystems and applications utilize blocks within a LUN, this value does not necessarily reflect actual consumption/availability from the perspective of the filesystem or application. Without specific knowledge of how the LUN blocks are utilized outside of ONTAP, this property should not be used as an indicator for an out-of-space condition.For more information, see Size properties in the docs section of the ONTAP REST API documentation.</p> API Endpoint Metric Template REST <code>api/private/cli/lun</code> <code>size_used</code> conf/rest/9.12.0/lun.yaml ZAPI <code>lun-get-iter</code> <code>lun-info.size-used</code> conf/zapi/cdot/9.8.0/lun.yaml <p>The <code>lun_size_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN LUN Table table LUNS in Cluster"},{"location":"ontap-metrics/#lun_size_used_percent","title":"lun_size_used_percent","text":"<p>This metric represents the percentage of a LUN that is currently being used.</p> API Endpoint Metric Template REST <code>api/private/cli/lun</code> <code>size_used, size</code> conf/rest/9.12.0/lun.yaml ZAPI <code>lun-get-iter</code> <code>size_used, size</code> conf/zapi/cdot/9.8.0/lun.yaml <p>The <code>lun_size_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Top Volume and LUN Capacity timeseries Top $TopResources LUNs by Percent Most Filled ONTAP: LUN Top Volume and LUN Capacity timeseries Top $TopResources LUNs by Percent Least Filled"},{"location":"ontap-metrics/#lun_total_data","title":"lun_total_data","text":"<p>Performance metric for read and write I/O operations in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>read_data, write_data</code>Unit: Type: Base: conf/restperf/9.12.0/lun.yaml KeyPerf <code>api/storage/luns</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/lun.yaml ZapiPerf <code>lun</code> <code>read_data, write_data</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/lun.yaml"},{"location":"ontap-metrics/#lun_total_latency","title":"lun_total_latency","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/luns</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: lun_statistics.iops_raw.total conf/keyperf/9.15.0/lun.yaml"},{"location":"ontap-metrics/#lun_total_ops","title":"lun_total_ops","text":"<p>Total number of read and write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>read_ops, write_ops</code>Unit: Type: Base: conf/restperf/9.12.0/lun.yaml KeyPerf <code>api/storage/luns</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/lun.yaml ZapiPerf <code>lun</code> <code>read_ops, write_ops</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/lun.yaml"},{"location":"ontap-metrics/#lun_unmap_reqs","title":"lun_unmap_reqs","text":"<p>Number of unmap command requests</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>unmap_requests</code>Unit: noneType: rateBase: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>unmap_reqs</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_unmap_reqs</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries vStorage Offload Operations"},{"location":"ontap-metrics/#lun_write_align_histo","title":"lun_write_align_histo","text":"<p>Histogram of WAFL write alignment (number of sectors off WAFL block start)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>write_align_histogram</code>Unit: percentType: percentBase: write_ops_sent conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>write_align_histo</code>Unit: percentType: percentBase: write_ops_sent conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_write_align_histo</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Top LUN Performance Efficiency timeseries Top $TopResources Luns by Write Misalignment Buckets"},{"location":"ontap-metrics/#lun_write_data","title":"lun_write_data","text":"<p>Performance metric for write I/O operations in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/lun.yaml KeyPerf <code>api/storage/luns</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Highlights stat Top $TopResources Luns by Write Throughput ONTAP: LUN LUN Table table Top $TopResources Luns by Write Throughput ONTAP: LUN Top LUN Performance timeseries Top $TopResources Luns by Write Throughput ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries Throughput"},{"location":"ontap-metrics/#lun_write_ops","title":"lun_write_ops","text":"<p>Number of write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/lun.yaml KeyPerf <code>api/storage/luns</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Highlights stat Top $TopResources Luns by Write IOPs ONTAP: LUN LUN Table table Top $TopResources Luns by Write IOPS ONTAP: LUN Top LUN Performance timeseries Top $TopResources Luns by Write IOPs ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries IOPs ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries IO Size"},{"location":"ontap-metrics/#lun_write_partial_blocks","title":"lun_write_partial_blocks","text":"<p>Percentage of writes whose size is not a multiple of WAFL block size</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>write_partial_blocks</code>Unit: percentType: percentBase: write_ops conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>write_partial_blocks</code>Unit: percentType: percentBase: write_ops conf/zapiperf/cdot/9.8.0/lun.yaml"},{"location":"ontap-metrics/#lun_writesame_reqs","title":"lun_writesame_reqs","text":"<p>Number of write same command requests</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>writesame_requests</code>Unit: noneType: rateBase: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>writesame_reqs</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_writesame_reqs</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries vStorage Offload Operations"},{"location":"ontap-metrics/#lun_writesame_unmap_reqs","title":"lun_writesame_unmap_reqs","text":"<p>Number of write same commands requests with unmap bit set</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>writesame_unmap_requests</code>Unit: noneType: rateBase: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>writesame_unmap_reqs</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_writesame_unmap_reqs</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries vStorage Offload Operations"},{"location":"ontap-metrics/#lun_xcopy_reqs","title":"lun_xcopy_reqs","text":"<p>Total number of xcopy operations on the LUN</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/lun</code> <code>xcopy_requests</code>Unit: noneType: rateBase: conf/restperf/9.12.0/lun.yaml ZapiPerf <code>perf-object-get-instances lun</code> <code>xcopy_reqs</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/lun.yaml <p>The <code>lun_xcopy_reqs</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Per LUN (Must Select Cluster/SVM/Volume/LUN) timeseries vStorage Offload Operations"},{"location":"ontap-metrics/#mav_request_approve_expiry_time","title":"mav_request_approve_expiry_time","text":"<p>Shows the deadline by which approved operations must be approved.</p> API Endpoint Metric Template REST <code>api/security/multi-admin-verify/requests</code> <code>approve_expiry_time</code> conf/rest/9.12.0/mav_request.yaml <p>The <code>mav_request_approve_expiry_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MAV Request Highlights table MAV Requests"},{"location":"ontap-metrics/#mav_request_approve_time","title":"mav_request_approve_time","text":"<p>Shows the date and time when requests were approved.</p> API Endpoint Metric Template REST <code>api/security/multi-admin-verify/requests</code> <code>approve_time</code> conf/rest/9.12.0/mav_request.yaml <p>The <code>mav_request_approve_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MAV Request Highlights table MAV Requests"},{"location":"ontap-metrics/#mav_request_create_time","title":"mav_request_create_time","text":"<p>Displays the date and time each MAV request was initiated.</p> API Endpoint Metric Template REST <code>api/security/multi-admin-verify/requests</code> <code>create_time</code> conf/rest/9.12.0/mav_request.yaml <p>The <code>mav_request_create_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MAV Request Highlights table MAV Requests"},{"location":"ontap-metrics/#mav_request_details","title":"mav_request_details","text":"<p>This metric provides information about MAV requests.</p> API Endpoint Metric Template REST <code>api/security/multi-admin-verify/requests</code> <code>Harvest generated.</code> conf/rest/9.12.0/mav_request.yaml <p>The <code>mav_request_details</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MAV Request Highlights table MAV Requests"},{"location":"ontap-metrics/#mav_request_execution_expiry_time","title":"mav_request_execution_expiry_time","text":"<p>Shows the deadline by which approved operations must be executed.</p> API Endpoint Metric Template REST <code>api/security/multi-admin-verify/requests</code> <code>execution_expiry_time</code> conf/rest/9.12.0/mav_request.yaml <p>The <code>mav_request_execution_expiry_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MAV Request Highlights table MAV Requests"},{"location":"ontap-metrics/#mediator_labels","title":"mediator_labels","text":"<p>This metric provides information about Mediator</p> API Endpoint Metric Template REST <code>api/cluster/mediators</code> <code>Harvest generated</code> conf/rest/9.12.0/mediator.yaml"},{"location":"ontap-metrics/#metadata_collector_api_time","title":"metadata_collector_api_time","text":"<p>amount of time to collect data from monitored cluster object</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA <p>The <code>metadata_collector_api_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Collectors timeseries API Time"},{"location":"ontap-metrics/#metadata_collector_bytesrx","title":"metadata_collector_bytesRx","text":"<p>The amount of data received by the collector from the monitored cluster.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: bytes NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: bytes NA"},{"location":"ontap-metrics/#metadata_collector_calc_time","title":"metadata_collector_calc_time","text":"<p>amount of time it took to compute metrics between two successive polls, specifically using properties like raw, delta, rate, average, and percent. This metric is available for ZapiPerf/RestPerf collectors.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA <p>The <code>metadata_collector_calc_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Collectors timeseries Postprocessing Time"},{"location":"ontap-metrics/#metadata_collector_instances","title":"metadata_collector_instances","text":"<p>number of objects collected from monitored cluster</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: scalar NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: scalar NA <p>The <code>metadata_collector_instances</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Collectors timeseries Instances Per Poll ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#metadata_collector_metrics","title":"metadata_collector_metrics","text":"<p>number of counters collected from monitored cluster</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: scalar NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: scalar NA <p>The <code>metadata_collector_metrics</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Collectors timeseries Data Points Per Poll"},{"location":"ontap-metrics/#metadata_collector_numcalls","title":"metadata_collector_numCalls","text":"<p>The number of API calls made by the collector to the monitored cluster.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: scalar NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: scalar NA"},{"location":"ontap-metrics/#metadata_collector_numpartials","title":"metadata_collector_numPartials","text":"<p>The number of partial responses received by the collector from the monitored cluster.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: scalar NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: scalar NA"},{"location":"ontap-metrics/#metadata_collector_parse_time","title":"metadata_collector_parse_time","text":"<p>amount of time to parse XML, JSON, etc. for cluster object</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA <p>The <code>metadata_collector_parse_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Collectors timeseries Parse Time"},{"location":"ontap-metrics/#metadata_collector_plugininstances","title":"metadata_collector_pluginInstances","text":"<p>The number of plugin instances generated by the collector.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: scalar NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: scalar NA"},{"location":"ontap-metrics/#metadata_collector_plugin_time","title":"metadata_collector_plugin_time","text":"<p>amount of time for all plugins to post-process metrics</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA"},{"location":"ontap-metrics/#metadata_collector_poll_time","title":"metadata_collector_poll_time","text":"<p>amount of time it took for the poll to finish</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA <p>The <code>metadata_collector_poll_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights timeseries Average Poll Time Per Poller Harvest Metadata Highlights timeseries Average Time Per Collector Harvest Metadata Collectors timeseries Time Per Data Poll"},{"location":"ontap-metrics/#metadata_collector_skips","title":"metadata_collector_skips","text":"<p>number of metrics that were not calculated between two successive polls. This metric is available for ZapiPerf/RestPerf collectors.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: scalar NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: scalar NA"},{"location":"ontap-metrics/#metadata_collector_task_time","title":"metadata_collector_task_time","text":"<p>amount of time it took for each collector's subtasks to complete</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA"},{"location":"ontap-metrics/#metadata_component_count","title":"metadata_component_count","text":"<p>number of metrics collected for each object</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: scalar NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: scalar NA <p>The <code>metadata_component_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights stat Collected/24h Harvest Metadata Highlights stat Collected/m Harvest Metadata Highlights stat Exported/m Harvest Metadata Prometheus timeseries Data Points Per Export"},{"location":"ontap-metrics/#metadata_component_status","title":"metadata_component_status","text":"<p>status of the collector - 0 means running, 1 means standby, 2 means failed</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: enum NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: enum NA <p>The <code>metadata_component_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights stat Total Object Count Across Collectors Harvest Metadata Highlights stat Failed Object Count Across Collectors Harvest Metadata Highlights stat Exporters Harvest Metadata Highlights table Collectors Harvest Metadata Highlights table Exporters"},{"location":"ontap-metrics/#metadata_exporter_count","title":"metadata_exporter_count","text":"<p>number of metrics and labels exported</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: scalar NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: scalar NA <p>The <code>metadata_exporter_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Prometheus timeseries Data Points Per Export"},{"location":"ontap-metrics/#metadata_exporter_time","title":"metadata_exporter_time","text":"<p>amount of time it took to render, export, and serve exported data</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: microseconds NA <p>The <code>metadata_exporter_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights timeseries Average Time Per Exporter Harvest Metadata Prometheus timeseries Average Time Per Export"},{"location":"ontap-metrics/#metadata_target_goroutines","title":"metadata_target_goroutines","text":"<p>number of goroutines that exist within the poller</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: scalar NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: scalar NA"},{"location":"ontap-metrics/#metadata_target_ping","title":"metadata_target_ping","text":"<p>The response time (in milliseconds) of the ping to the target system. If the ping is successful, the metric records the time it took for the ping to complete.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: milliseconds NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: milliseconds NA <p>The <code>metadata_target_ping</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights table Target Systems"},{"location":"ontap-metrics/#metadata_target_status","title":"metadata_target_status","text":"<p>status of the system being monitored. 0 means reachable, 1 means unreachable</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: enum NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: enum NA <p>The <code>metadata_target_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights stat Datacenters Harvest Metadata Highlights table Target Systems"},{"location":"ontap-metrics/#metrocluster_check_aggr_status","title":"metrocluster_check_aggr_status","text":"<p>Detail of the type of diagnostic operation run for the Aggregate with diagnostic operation result.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/metrocluster_check.yaml <p>The <code>metrocluster_check_aggr_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Diagnostics table Metrocluster Aggregate Diagnostics Check Details"},{"location":"ontap-metrics/#metrocluster_check_cluster_status","title":"metrocluster_check_cluster_status","text":"<p>Detail of the type of diagnostic operation run for the Cluster with diagnostic operation result.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/metrocluster_check.yaml <p>The <code>metrocluster_check_cluster_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Diagnostics table Metrocluster Cluster Diagnostics Check Details"},{"location":"ontap-metrics/#metrocluster_check_node_status","title":"metrocluster_check_node_status","text":"<p>Detail of the type of diagnostic operation run for the Node with diagnostic operation result.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/metrocluster_check.yaml <p>The <code>metrocluster_check_node_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Diagnostics table Metrocluster Node Diagnostics Check Details"},{"location":"ontap-metrics/#metrocluster_check_volume_status","title":"metrocluster_check_volume_status","text":"<p>Detail of the type of diagnostic operation run for the Volume with diagnostic operation result.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/metrocluster_check.yaml <p>The <code>metrocluster_check_volume_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Diagnostics table Metrocluster Volume Diagnostics Check Details"},{"location":"ontap-metrics/#namespace_avg_other_latency","title":"namespace_avg_other_latency","text":"<p>Average other ops latency in microseconds for all operations on the Namespace</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>average_other_latency</code>Unit: microsecType: averageBase: other_ops conf/restperf/9.12.0/namespace.yaml KeyPerf <code>api/storage/namespaces</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: namespace_statistics.iops_raw.other conf/keyperf/9.15.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>avg_other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.10.1/namespace.yaml"},{"location":"ontap-metrics/#namespace_avg_read_latency","title":"namespace_avg_read_latency","text":"<p>Average read latency in microseconds for all operations on the Namespace</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>average_read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.12.0/namespace.yaml KeyPerf <code>api/storage/namespaces</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: namespace_statistics.iops_raw.read conf/keyperf/9.15.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>avg_read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.10.1/namespace.yaml <p>The <code>namespace_avg_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NVMe Namespaces Highlights timeseries Top $TopResources NVMe Namespaces by Average Read Latency"},{"location":"ontap-metrics/#namespace_avg_total_latency","title":"namespace_avg_total_latency","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/namespaces</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: namespace_statistics.iops_raw.total conf/keyperf/9.15.0/namespace.yaml"},{"location":"ontap-metrics/#namespace_avg_write_latency","title":"namespace_avg_write_latency","text":"<p>Average write latency in microseconds for all operations on the Namespace</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>average_write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.12.0/namespace.yaml KeyPerf <code>api/storage/namespaces</code> <code>statistics.latency_raw.write</code>Unit: microsecType: averageBase: namespace_statistics.iops_raw.write conf/keyperf/9.15.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>avg_write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.10.1/namespace.yaml <p>The <code>namespace_avg_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NVMe Namespaces Highlights timeseries Top $TopResources NVMe Namespaces by Average Write Latency"},{"location":"ontap-metrics/#namespace_block_size","title":"namespace_block_size","text":"<p>The size of blocks in the namespace in bytes. The default for namespaces with an <code>os_type</code> of vmware is 512. All other namespaces default to 4096.Valid in POST when creating an NVMe namespace that is not a clone of another. Disallowed in POST when creating a namespace clone. Valid in POST.</p> API Endpoint Metric Template REST <code>api/storage/namespaces</code> <code>space.block_size</code> conf/rest/9.12.0/namespace.yaml ZAPI <code>nvme-namespace-get-iter</code> <code>nvme-namespace-info.block-size</code> conf/zapi/cdot/9.8.0/namespace.yaml"},{"location":"ontap-metrics/#namespace_labels","title":"namespace_labels","text":"<p>This metric provides information about Namespace</p> API Endpoint Metric Template REST <code>api/storage/namespaces</code> <code>Harvest generated</code> conf/rest/9.12.0/namespace.yaml ZAPI <code>nvme-namespace-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/namespace.yaml <p>The <code>namespace_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count ONTAP: NVMe Namespaces NVMe Namespaces Table table NVMe Namespaces"},{"location":"ontap-metrics/#namespace_other_ops","title":"namespace_other_ops","text":"<p>Number of other operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml KeyPerf <code>api/storage/namespaces</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml"},{"location":"ontap-metrics/#namespace_read_data","title":"namespace_read_data","text":"<p>Performance metric for namespace read I/O operations in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml KeyPerf <code>api/storage/namespaces</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml <p>The <code>namespace_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NVMe Namespaces Highlights timeseries Top $TopResources NVMe Namespaces by Read Throughput"},{"location":"ontap-metrics/#namespace_read_ops","title":"namespace_read_ops","text":"<p>Number of read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml KeyPerf <code>api/storage/namespaces</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml <p>The <code>namespace_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NVMe Namespaces Highlights timeseries Top $TopResources NVMe Namespaces by Read IOPs"},{"location":"ontap-metrics/#namespace_remote_other_ops","title":"namespace_remote_other_ops","text":"<p>Number of remote other operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>remote.other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>remote_other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml"},{"location":"ontap-metrics/#namespace_remote_read_data","title":"namespace_remote_read_data","text":"<p>Performance metric for namespace remote read I/O operations in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>remote.read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>remote_read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml"},{"location":"ontap-metrics/#namespace_remote_read_ops","title":"namespace_remote_read_ops","text":"<p>Number of remote read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>remote.read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>remote_read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml"},{"location":"ontap-metrics/#namespace_remote_write_data","title":"namespace_remote_write_data","text":"<p>Performance metric for namespace remote write I/O operations in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>remote.write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>remote_write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml"},{"location":"ontap-metrics/#namespace_remote_write_ops","title":"namespace_remote_write_ops","text":"<p>Number of remote write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>remote.write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>remote_write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml"},{"location":"ontap-metrics/#namespace_size","title":"namespace_size","text":"<p>The total provisioned size of the NVMe namespace. Valid in POST and PATCH. The NVMe namespace size can be increased but not be made smaller using the REST interface.The maximum and minimum sizes listed here are the absolute maximum and absolute minimum sizes in bytes. The maximum size is variable with respect to large NVMe namespace support in ONTAP. If large namespaces are supported, the maximum size is 128 TB (140737488355328 bytes) and if not supported, the maximum size is just under 16 TB (17557557870592 bytes). The minimum size supported is always 4096 bytes.For more information, see Size properties in the docs section of the ONTAP REST API documentation.</p> API Endpoint Metric Template REST <code>api/storage/namespaces</code> <code>space.size</code> conf/rest/9.12.0/namespace.yaml ZAPI <code>nvme-namespace-get-iter</code> <code>nvme-namespace-info.size</code> conf/zapi/cdot/9.8.0/namespace.yaml <p>The <code>namespace_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NVMe Namespaces NVMe Namespaces Table table NVMe Namespaces"},{"location":"ontap-metrics/#namespace_size_available","title":"namespace_size_available","text":"<p>This metric represents the amount of available space in a namespace.</p> API Endpoint Metric Template REST <code>api/storage/namespaces</code> <code>size, size_used</code> conf/rest/9.12.0/namespace.yaml ZAPI <code>nvme-namespace-get-iter</code> <code>size, size_used</code> conf/zapi/cdot/9.8.0/namespace.yaml"},{"location":"ontap-metrics/#namespace_size_available_percent","title":"namespace_size_available_percent","text":"<p>This metric represents the percentage of available space in a namespace.</p> API Endpoint Metric Template REST <code>api/storage/namespaces</code> <code>size_available, size</code> conf/rest/9.12.0/namespace.yaml ZAPI <code>nvme-namespace-get-iter</code> <code>size_available, size</code> conf/zapi/cdot/9.8.0/namespace.yaml <p>The <code>namespace_size_available_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NVMe Namespaces NVMe Namespaces Table table NVMe Namespaces"},{"location":"ontap-metrics/#namespace_size_used","title":"namespace_size_used","text":"<p>The amount of space consumed by the main data stream of the NVMe namespace.This value is the total space consumed in the volume by the NVMe namespace, including filesystem overhead, but excluding prefix and suffix streams. Due to internal filesystem overhead and the many ways NVMe filesystems and applications utilize blocks within a namespace, this value does not necessarily reflect actual consumption/availability from the perspective of the filesystem or application. Without specific knowledge of how the namespace blocks are utilized outside of ONTAP, this property should not be used and an indicator for an out-of-space condition.For more information, see Size properties in the docs section of the ONTAP REST API documentation.</p> API Endpoint Metric Template REST <code>api/storage/namespaces</code> <code>space.used</code> conf/rest/9.12.0/namespace.yaml ZAPI <code>nvme-namespace-get-iter</code> <code>nvme-namespace-info.size-used</code> conf/zapi/cdot/9.8.0/namespace.yaml <p>The <code>namespace_size_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NVMe Namespaces NVMe Namespaces Table table NVMe Namespaces"},{"location":"ontap-metrics/#namespace_total_data","title":"namespace_total_data","text":"<p>Performance metric aggregated over all types of I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/namespaces</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/namespace.yaml"},{"location":"ontap-metrics/#namespace_total_ops","title":"namespace_total_ops","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/namespaces</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/namespace.yaml"},{"location":"ontap-metrics/#namespace_write_data","title":"namespace_write_data","text":"<p>Performance metric for namespace write I/O operations in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml KeyPerf <code>api/storage/namespaces</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml <p>The <code>namespace_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NVMe Namespaces Highlights timeseries Top $TopResources NVMe Namespaces by Write Throughput"},{"location":"ontap-metrics/#namespace_write_ops","title":"namespace_write_ops","text":"<p>Number of write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/namespace</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/namespace.yaml KeyPerf <code>api/storage/namespaces</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/namespace.yaml ZapiPerf <code>perf-object-get-instances namespace</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/namespace.yaml <p>The <code>namespace_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NVMe Namespaces Highlights timeseries Top $TopResources NVMe Namespaces by Write IOPs"},{"location":"ontap-metrics/#ndmp_session_data_bytes_processed","title":"ndmp_session_data_bytes_processed","text":"<p>Indicates the NDMP data bytes processed.</p> API Endpoint Metric Template REST <code>api/protocols/ndmp/sessions</code> <code>data.bytes_processed</code> conf/rest/9.7.0/ndmp_session.yaml"},{"location":"ontap-metrics/#ndmp_session_mover_bytes_moved","title":"ndmp_session_mover_bytes_moved","text":"<p>Indicates the NDMP mover bytes moved.</p> API Endpoint Metric Template REST <code>api/protocols/ndmp/sessions</code> <code>mover.bytes_moved</code> conf/rest/9.7.0/ndmp_session.yaml"},{"location":"ontap-metrics/#net_connection_labels","title":"net_connection_labels","text":"<p>This metric provides information about NetConnections</p> API Endpoint Metric Template REST <code>api/private/cli/network/connections/active</code> <code>Harvest generated</code> conf/rest/9.12.0/netconnections.yaml"},{"location":"ontap-metrics/#net_port_mtu","title":"net_port_mtu","text":"<p>Maximum transmission unit, largest packet size on this network</p> API Endpoint Metric Template REST <code>api/network/ethernet/ports</code> <code>mtu</code> conf/rest/9.12.0/netport.yaml ZAPI <code>net-port-get-iter</code> <code>net-port-info.mtu</code> conf/zapi/cdot/9.8.0/netport.yaml <p>The <code>net_port_mtu</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet table Ethernet ports"},{"location":"ontap-metrics/#net_port_status","title":"net_port_status","text":"<p>This metric indicates a value of 1 if the port state is up and a value of 0 for any other state.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/netport.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/netport.yaml <p>The <code>net_port_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet table Ethernet ports"},{"location":"ontap-metrics/#net_route_labels","title":"net_route_labels","text":"<p>This metric provides information about NetRoute</p> API Endpoint Metric Template REST <code>api/network/ip/routes</code> <code>Harvest generated</code> conf/rest/9.8.0/netroute.yaml <p>The <code>net_route_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Routes table Routes"},{"location":"ontap-metrics/#netstat_bytes_recvd","title":"netstat_bytes_recvd","text":"<p>Number of bytes received by a TCP connection</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances netstat</code> <code>bytes_recvd</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/netstat.yaml"},{"location":"ontap-metrics/#netstat_bytes_sent","title":"netstat_bytes_sent","text":"<p>Number of bytes sent by a TCP connection</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances netstat</code> <code>bytes_sent</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/netstat.yaml"},{"location":"ontap-metrics/#netstat_cong_win","title":"netstat_cong_win","text":"<p>Congestion window of a TCP connection</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances netstat</code> <code>cong_win</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/netstat.yaml"},{"location":"ontap-metrics/#netstat_cong_win_th","title":"netstat_cong_win_th","text":"<p>Congestion window threshold of a TCP connection</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances netstat</code> <code>cong_win_th</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/netstat.yaml"},{"location":"ontap-metrics/#netstat_ooorcv_pkts","title":"netstat_ooorcv_pkts","text":"<p>Number of out-of-order packets received by this TCP connection</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances netstat</code> <code>ooorcv_pkts</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/netstat.yaml"},{"location":"ontap-metrics/#netstat_recv_window","title":"netstat_recv_window","text":"<p>Receive window size of a TCP connection</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances netstat</code> <code>recv_window</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/netstat.yaml"},{"location":"ontap-metrics/#netstat_rexmit_pkts","title":"netstat_rexmit_pkts","text":"<p>Number of packets retransmitted by this TCP connection</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances netstat</code> <code>rexmit_pkts</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/netstat.yaml"},{"location":"ontap-metrics/#netstat_send_window","title":"netstat_send_window","text":"<p>Send window size of a TCP connection</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances netstat</code> <code>send_window</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/netstat.yaml"},{"location":"ontap-metrics/#nfs_clients_idle_duration","title":"nfs_clients_idle_duration","text":"<p>Specifies an ISO-8601 format of date and time to retrieve the idle time duration in hours, minutes, and seconds format.</p> API Endpoint Metric Template REST <code>api/protocols/nfs/connected-clients</code> <code>idle_duration</code> conf/rest/9.7.0/nfs_clients.yaml <p>The <code>nfs_clients_idle_duration</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFS Clients Highlights stat Total NFS Connections ONTAP: NFS Clients Highlights piechart NFS Connections by Protocol ONTAP: NFS Clients Highlights table NFS Clients (active in the past 48 hours)"},{"location":"ontap-metrics/#nfs_diag_storepool_bytelockalloc","title":"nfs_diag_storePool_ByteLockAlloc","text":"<p>Current number of byte range lock objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.byte_lock_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_ByteLockAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_ByteLockAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries ByteLockAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_bytelockmax","title":"nfs_diag_storePool_ByteLockMax","text":"<p>Maximum number of byte range lock objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.byte_lock_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_ByteLockMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_ByteLockMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries ByteLockAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_clientalloc","title":"nfs_diag_storePool_ClientAlloc","text":"<p>Current number of client objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.client_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_ClientAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_ClientAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries ClientAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_clientmax","title":"nfs_diag_storePool_ClientMax","text":"<p>Maximum number of client objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.client_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_ClientMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_ClientMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries ClientAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_connectionparentsessionreferencealloc","title":"nfs_diag_storePool_ConnectionParentSessionReferenceAlloc","text":"<p>Current number of connection parent session reference objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.connection_parent_session_reference_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_ConnectionParentSessionReferenceAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_ConnectionParentSessionReferenceAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries ConnectionParentSessionReferenceAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_connectionparentsessionreferencemax","title":"nfs_diag_storePool_ConnectionParentSessionReferenceMax","text":"<p>Maximum number of connection parent session reference objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.connection_parent_session_reference_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_ConnectionParentSessionReferenceMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_ConnectionParentSessionReferenceMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries ConnectionParentSessionReferenceAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_copystatealloc","title":"nfs_diag_storePool_CopyStateAlloc","text":"<p>Current number of copy state objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.copy_state_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_CopyStateAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_CopyStateAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries CopyStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_copystatemax","title":"nfs_diag_storePool_CopyStateMax","text":"<p>Maximum number of copy state objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.copy_state_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_CopyStateMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_CopyStateMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries CopyStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_delegalloc","title":"nfs_diag_storePool_DelegAlloc","text":"<p>Current number of delegation lock objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.delegation_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_DelegAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_DelegAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries DelegAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_delegmax","title":"nfs_diag_storePool_DelegMax","text":"<p>Maximum number delegation lock objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.delegation_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_DelegMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_DelegMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries DelegAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_delegstatealloc","title":"nfs_diag_storePool_DelegStateAlloc","text":"<p>Current number of delegation state objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.delegation_state_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_DelegStateAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_DelegStateAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries DelegStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_delegstatemax","title":"nfs_diag_storePool_DelegStateMax","text":"<p>Maximum number of delegation state objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.delegation_state_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_DelegStateMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_DelegStateMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries DelegStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_layoutalloc","title":"nfs_diag_storePool_LayoutAlloc","text":"<p>Current number of layout objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.layout_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_LayoutAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_LayoutAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Lock timeseries LayoutAlloc"},{"location":"ontap-metrics/#nfs_diag_storepool_layoutmax","title":"nfs_diag_storePool_LayoutMax","text":"<p>Maximum number of layout objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.layout_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_LayoutMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_LayoutMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Lock timeseries LayoutAlloc"},{"location":"ontap-metrics/#nfs_diag_storepool_layoutstatealloc","title":"nfs_diag_storePool_LayoutStateAlloc","text":"<p>Current number of layout state objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.layout_state_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_LayoutStateAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_LayoutStateAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries LayoutStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_layoutstatemax","title":"nfs_diag_storePool_LayoutStateMax","text":"<p>Maximum number of layout state objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.layout_state_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_LayoutStateMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_LayoutStateMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries LayoutStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_lockalloc","title":"nfs_diag_storePool_LockAlloc","text":"<p>Represent the current number of lock objects allocated</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_LockAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_lockmax","title":"nfs_diag_storePool_LockMax","text":"<p>Represent the maximum number of lock objects</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_LockMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_lockstatealloc","title":"nfs_diag_storePool_LockStateAlloc","text":"<p>Current number of lock state objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.lock_state_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_LockStateAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_LockStateAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries LockStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_lockstatemax","title":"nfs_diag_storePool_LockStateMax","text":"<p>Maximum number of lock state objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.lock_state_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_LockStateMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_LockStateMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries LockStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_openalloc","title":"nfs_diag_storePool_OpenAlloc","text":"<p>Current number of share objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.open_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_OpenAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_OpenAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries OpenAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_openmax","title":"nfs_diag_storePool_OpenMax","text":"<p>Maximum number of share lock objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.open_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_OpenMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_OpenMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries OpenAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_openstatealloc","title":"nfs_diag_storePool_OpenStateAlloc","text":"<p>Current number of open state objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.openstate_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_OpenStateAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_OpenStateAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries OpenStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_openstatemax","title":"nfs_diag_storePool_OpenStateMax","text":"<p>Maximum number of open state objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.openstate_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_OpenStateMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_OpenStateMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries OpenStateAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_owneralloc","title":"nfs_diag_storePool_OwnerAlloc","text":"<p>Current number of owner objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.owner_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_OwnerAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_OwnerAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries OwnerAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_ownermax","title":"nfs_diag_storePool_OwnerMax","text":"<p>Maximum number of owner objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.owner_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_OwnerMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_OwnerMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries OwnerAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_sessionalloc","title":"nfs_diag_storePool_SessionAlloc","text":"<p>Current number of session objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.session_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_SessionAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_SessionAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries SessionAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_sessionconnectionholderalloc","title":"nfs_diag_storePool_SessionConnectionHolderAlloc","text":"<p>Current number of session connection holder objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.session_connection_holder_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_SessionConnectionHolderAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_SessionConnectionHolderAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries SessionConnectionHolderAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_sessionconnectionholdermax","title":"nfs_diag_storePool_SessionConnectionHolderMax","text":"<p>Maximum number of session connection holder objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.session_connection_holder_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_SessionConnectionHolderMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_SessionConnectionHolderMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries SessionConnectionHolderAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_sessionholderalloc","title":"nfs_diag_storePool_SessionHolderAlloc","text":"<p>Current number of session holder objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.session_holder_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_SessionHolderAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_SessionHolderAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries SessionHolderAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_sessionholdermax","title":"nfs_diag_storePool_SessionHolderMax","text":"<p>Maximum number of session holder objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.session_holder_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_SessionHolderMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_SessionHolderMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries SessionHolderAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_sessionmax","title":"nfs_diag_storePool_SessionMax","text":"<p>Maximum number of session objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.session_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_SessionMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_SessionMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries SessionAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_staterefhistoryalloc","title":"nfs_diag_storePool_StateRefHistoryAlloc","text":"<p>Current number of state reference callstack history objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.state_reference_history_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_StateRefHistoryAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_StateRefHistoryAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries StateRefHistoryAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_staterefhistorymax","title":"nfs_diag_storePool_StateRefHistoryMax","text":"<p>Maximum number of state reference callstack history objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.state_reference_history_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_StateRefHistoryMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_StateRefHistoryMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries StateRefHistoryAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_stringalloc","title":"nfs_diag_storePool_StringAlloc","text":"<p>Current number of string objects allocated.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.string_allocated</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_StringAlloc</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_StringAlloc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries StringAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nfs_diag_storepool_stringmax","title":"nfs_diag_storePool_StringMax","text":"<p>Maximum number of string objects.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nfs_v4_diag</code> <code>storepool.string_maximum</code>Unit: noneType: rawBase: conf/restperf/9.12.0/nfsv4_pool.yaml ZapiPerf <code>perf-object-get-instances nfsv4_diag</code> <code>storePool_StringMax</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml <p>The <code>nfs_diag_storePool_StringMax</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFSv4 StorePool Monitors Allocations over 50% timeseries Allocations over 50% ONTAP: NFSv4 StorePool Monitors Lock timeseries StringAlloc ONTAP: NFS Troubleshooting Highlights timeseries All nodes with 1% or more allocations in $Datacenter"},{"location":"ontap-metrics/#nic_ifgrp_rx_bytes","title":"nic_ifgrp_rx_bytes","text":"<p>Link Aggregation Group (LAG) Bytes received.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_ifgrp_rx_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Link Aggregation Group (LAG) table Link Aggregation Groups ONTAP: Network Link Aggregation Group (LAG) timeseries Top $TopResources LAGs by Receive Throughput"},{"location":"ontap-metrics/#nic_ifgrp_rx_perc","title":"nic_ifgrp_rx_perc","text":"<p>Link Aggregation Group (LAG) Bytes received percentage.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_ifgrp_rx_perc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Link Aggregation Group (LAG) table Link Aggregation Groups"},{"location":"ontap-metrics/#nic_ifgrp_tx_bytes","title":"nic_ifgrp_tx_bytes","text":"<p>Link Aggregation Group (LAG) Bytes sent.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_ifgrp_tx_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Link Aggregation Group (LAG) table Link Aggregation Groups ONTAP: Network Link Aggregation Group (LAG) timeseries Top $TopResources LAGs by Send Throughput"},{"location":"ontap-metrics/#nic_ifgrp_tx_perc","title":"nic_ifgrp_tx_perc","text":"<p>Link Aggregation Group (LAG) Bytes sent percentage.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_ifgrp_tx_perc</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Link Aggregation Group (LAG) table Link Aggregation Groups"},{"location":"ontap-metrics/#nic_labels","title":"nic_labels","text":"<p>This metric provides information about NicCommon</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nic_common.yaml ZAPI <code>nic_common</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet table NIC ports ONTAP: Network Ethernet table Ethernet port errors ONTAP: NFS Troubleshooting Network Port Table table Ethernet ports"},{"location":"ontap-metrics/#nic_link_up_to_downs","title":"nic_link_up_to_downs","text":"<p>Number of link state change from UP to DOWN.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>link_up_to_down</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>link_up_to_downs</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_link_up_to_downs</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet table Ethernet port errors ONTAP: NFS Troubleshooting Network Port Table table Ethernet ports"},{"location":"ontap-metrics/#nic_new_status","title":"nic_new_status","text":"<p>This metric indicates a value of 1 if the NIC state is up (indicating the NIC is operational) and a value of 0 for any other state.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_new_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet table NIC ports ONTAP: NFS Troubleshooting Network Port Table table Ethernet ports"},{"location":"ontap-metrics/#nic_rx_alignment_errors","title":"nic_rx_alignment_errors","text":"<p>Alignment errors detected on received packets</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>receive_alignment_errors</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>rx_alignment_errors</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_rx_alignment_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet timeseries NICs Receive Errors by Cluster ONTAP: Network Ethernet table Ethernet port errors"},{"location":"ontap-metrics/#nic_rx_bytes","title":"nic_rx_bytes","text":"<p>Received in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>receive_bytes</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>rx_bytes</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_rx_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Highlights stat Ethernet Throughput ONTAP: Network Highlights stat Ethernet Receive ONTAP: Network Ethernet table NIC ports ONTAP: Network Ethernet timeseries Top $TopResources NICs by Receive Throughput ONTAP: NFS Troubleshooting Network Port Table table Ethernet ports ONTAP: Node Network Layer timeseries Top $TopResources Ethernet Ports by Throughput"},{"location":"ontap-metrics/#nic_rx_crc_errors","title":"nic_rx_crc_errors","text":"<p>CRC errors detected on received packets</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>receive_crc_errors</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>rx_crc_errors</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_rx_crc_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet timeseries NICs Receive Errors by Cluster ONTAP: Network Ethernet table Ethernet port errors"},{"location":"ontap-metrics/#nic_rx_errors","title":"nic_rx_errors","text":"<p>Receive errors in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>receive_errors</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>rx_errors</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml"},{"location":"ontap-metrics/#nic_rx_length_errors","title":"nic_rx_length_errors","text":"<p>Length errors detected on received packets</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>receive_length_errors</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>rx_length_errors</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_rx_length_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet timeseries NICs Receive Errors by Cluster ONTAP: Network Ethernet table Ethernet port errors"},{"location":"ontap-metrics/#nic_rx_percent","title":"nic_rx_percent","text":"<p>Bytes received percentage.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_rx_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet table NIC ports"},{"location":"ontap-metrics/#nic_rx_total_errors","title":"nic_rx_total_errors","text":"<p>Total errors received</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>receive_total_errors</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>rx_total_errors</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_rx_total_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet timeseries NICs Receive Errors by Cluster ONTAP: Network Ethernet table Ethernet port errors ONTAP: NFS Troubleshooting Network Port Table table Ethernet ports"},{"location":"ontap-metrics/#nic_tx_bytes","title":"nic_tx_bytes","text":"<p>Sent in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>transmit_bytes</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>tx_bytes</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_tx_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Highlights stat Ethernet Send ONTAP: Network Ethernet table NIC ports ONTAP: Network Ethernet timeseries Top $TopResources NICs by Send Throughput ONTAP: NFS Troubleshooting Network Port Table table Ethernet ports ONTAP: Node Network Layer timeseries Top $TopResources Ethernet Ports by Throughput"},{"location":"ontap-metrics/#nic_tx_errors","title":"nic_tx_errors","text":"<p>Sent errors in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>transmit_errors</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>tx_errors</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml"},{"location":"ontap-metrics/#nic_tx_hw_errors","title":"nic_tx_hw_errors","text":"<p>Transmit errors reported by hardware</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>transmit_hw_errors</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>tx_hw_errors</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_tx_hw_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet timeseries NICs Send Errors by Cluster ONTAP: Network Ethernet table Ethernet port errors"},{"location":"ontap-metrics/#nic_tx_percent","title":"nic_tx_percent","text":"<p>Bytes sent percentage.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_tx_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet table NIC ports"},{"location":"ontap-metrics/#nic_tx_total_errors","title":"nic_tx_total_errors","text":"<p>Total errors sent</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nic_common</code> <code>transmit_total_errors</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>perf-object-get-instances nic_common</code> <code>tx_total_errors</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_tx_total_errors</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet timeseries NICs Send Errors by Cluster ONTAP: Network Ethernet table Ethernet port errors ONTAP: NFS Troubleshooting Network Port Table table Ethernet ports"},{"location":"ontap-metrics/#nic_util_percent","title":"nic_util_percent","text":"<p>Max of Bytes received percentage and Bytes sent percentage.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/nic_common.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/nic_common.yaml <p>The <code>nic_util_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Network Ethernet table NIC ports ONTAP: Network Ethernet timeseries Top $TopResources NICs by Port Utilization % ONTAP: NFS Troubleshooting Network Port Table table Ethernet ports ONTAP: Node Network Layer timeseries Top $TopResources Ethernet Ports by Utilization %"},{"location":"ontap-metrics/#node_avg_processor_busy","title":"node_avg_processor_busy","text":"<p>Average processor utilization across active processors in the system</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>average_processor_busy_percent</code>Unit: percentType: percentBase: cpu_elapsed_time conf/restperf/9.12.0/system_node.yaml KeyPerf <code>api/cluster/nodes</code> <code>statistics.processor_utilization_raw</code>Unit: statistics.processor_utilization_baseType: percentBase: conf/keyperf/9.15.0/system_node.yaml StatPerf <code>system:node</code> <code>avg_processor_busy</code>Unit: percentType: Base: cpu_elapsed_time conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>avg_processor_busy</code>Unit: percentType: percentBase: cpu_elapsed_time conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_avg_processor_busy</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT Cluster Metrics timeseries Top $TopResources Clusters by Average CPU Utilization ONTAP: Cluster Highlights table Top $TopResources Nodes by Average CPU Utilization ONTAP: Cluster Nodes &amp; Subsystems - $Cluster bargauge Average CPU Utilization ONTAP: Cluster Nodes &amp; Subsystems - $Cluster timeseries Node Average CPU Utilization ONTAP: Cluster Throughput timeseries Average CPU Utilization ONTAP: Datacenter Performance timeseries Top $TopResources Average CPU Utilization by Cluster ONTAP: MetroCluster Highlights gauge Average CPU Utilization ONTAP: Node Highlights bargauge Average CPU Utilization ONTAP: Node CPU Layer timeseries Average CPU Utilization"},{"location":"ontap-metrics/#node_cifs_connections","title":"node_cifs_connections","text":"<p>Number of connections</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>connections</code>Unit: noneType: rawBase: conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>connections</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/cifs_node.yaml <p>The <code>node_cifs_connections</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node CIFS Frontend timeseries CIFS Connections"},{"location":"ontap-metrics/#node_cifs_established_sessions","title":"node_cifs_established_sessions","text":"<p>Number of established SMB and SMB2 sessions</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>established_sessions</code>Unit: noneType: rawBase: conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>established_sessions</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/cifs_node.yaml <p>The <code>node_cifs_established_sessions</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node CIFS Frontend timeseries CIFS Connections"},{"location":"ontap-metrics/#node_cifs_latency","title":"node_cifs_latency","text":"<p>Average latency in microseconds for CIFS operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>latency</code>Unit: microsecType: averageBase: latency_base conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>cifs_latency</code>Unit: microsecType: averageBase: cifs_latency_base conf/zapiperf/cdot/9.8.0/cifs_node.yaml <p>The <code>node_cifs_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node CIFS Frontend stat CIFS Latency"},{"location":"ontap-metrics/#node_cifs_op_count","title":"node_cifs_op_count","text":"<p>Array of select CIFS operation counts</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>op_count</code>Unit: noneType: rateBase: conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>cifs_op_count</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/cifs_node.yaml <p>The <code>node_cifs_op_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node CIFS Frontend timeseries CIFS IOPs by Type"},{"location":"ontap-metrics/#node_cifs_open_files","title":"node_cifs_open_files","text":"<p>Number of open files over SMB and SMB2</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>open_files</code>Unit: noneType: rawBase: conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>open_files</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/cifs_node.yaml <p>The <code>node_cifs_open_files</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node CIFS Frontend timeseries CIFS Connections"},{"location":"ontap-metrics/#node_cifs_ops","title":"node_cifs_ops","text":"<p>Number of CIFS operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>cifs_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>cifs_ops</code>Unit: per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>cifs_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_cifs_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT Cluster Metrics timeseries Top $TopResources CIFS IOPs by Cluster ONTAP: Node Backend timeseries Protocol Backend IOPs ONTAP: Node CIFS Frontend stat CIFS IOPs"},{"location":"ontap-metrics/#node_cifs_read_latency","title":"node_cifs_read_latency","text":"<p>Average latency in microseconds for CIFS read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>average_read_latency</code>Unit: microsecType: averageBase: total_read_ops conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>cifs_read_latency</code>Unit: microsecType: averageBase: cifs_read_ops conf/zapiperf/cdot/9.8.0/cifs_node.yaml"},{"location":"ontap-metrics/#node_cifs_read_ops","title":"node_cifs_read_ops","text":"<p>Total number of CIFS read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>total_read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>cifs_read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/cifs_node.yaml"},{"location":"ontap-metrics/#node_cifs_total_ops","title":"node_cifs_total_ops","text":"<p>Total number of CIFS operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>cifs_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/cifs_node.yaml"},{"location":"ontap-metrics/#node_cifs_write_latency","title":"node_cifs_write_latency","text":"<p>Average latency in microseconds for CIFS write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>average_write_latency</code>Unit: microsecType: averageBase: total_write_ops conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>cifs_write_latency</code>Unit: microsecType: averageBase: cifs_write_ops conf/zapiperf/cdot/9.8.0/cifs_node.yaml"},{"location":"ontap-metrics/#node_cifs_write_ops","title":"node_cifs_write_ops","text":"<p>Total number of CIFS write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs:node</code> <code>total_write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/cifs_node.yaml ZapiPerf <code>perf-object-get-instances cifs:node</code> <code>cifs_write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/cifs_node.yaml"},{"location":"ontap-metrics/#node_cpu_busy","title":"node_cpu_busy","text":"<p>System CPU resource utilization. Returns a computed percentage for the default CPU field. Basically computes a 'cpu usage summary' value which indicates how 'busy' the system is based upon the most heavily utilized domain. The idea is to determine the amount of available CPU until we're limited by either a domain maxing out OR we exhaust all available idle CPU cycles, whichever occurs first.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>cpu_busy</code>Unit: percentType: percentBase: cpu_elapsed_time conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>cpu_busy</code>Unit: percentType: Base: cpu_elapsed_time conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>cpu_busy</code>Unit: percentType: percentBase: cpu_elapsed_time conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_cpu_busy</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT Cluster Metrics timeseries Top $TopResources Clusters by CPU busy ONTAP: Cluster Highlights table Top $TopResources Nodes by CPU busy ONTAP: Cluster Nodes &amp; Subsystems - $Cluster bargauge CPU busy ONTAP: Cluster Nodes &amp; Subsystems - $Cluster timeseries Node CPU Busy ONTAP: Cluster Throughput timeseries CPU Busy ONTAP: Datacenter Performance timeseries Top $TopResources CPU Busy by Cluster ONTAP: Node Highlights bargauge CPU Busy ONTAP: Node Backend timeseries System Utilization"},{"location":"ontap-metrics/#node_cpu_busytime","title":"node_cpu_busytime","text":"<p>The time (in hundredths of a second) that the CPU has been doing useful work since the last boot</p> API Endpoint Metric Template REST <code>api/private/cli/node</code> <code>cpu_busy_time</code> conf/rest/9.12.0/node.yaml ZAPI <code>system-node-get-iter</code> <code>node-details-info.cpu-busytime</code> conf/zapi/cdot/9.8.0/node.yaml"},{"location":"ontap-metrics/#node_cpu_domain_busy","title":"node_cpu_domain_busy","text":"<p>Array of processor time in percentage spent in various domains</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>domain_busy</code>Unit: percentType: percentBase: cpu_elapsed_time conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>domain_busy</code>Unit: percentType: arrayBase: cpu_elapsed_time conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>domain_busy</code>Unit: percentType: percentBase: cpu_elapsed_time conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_cpu_domain_busy</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node CPU Layer timeseries CPU Busy Domains ONTAP: Node Backend timeseries System Utilization"},{"location":"ontap-metrics/#node_cpu_elapsed_time","title":"node_cpu_elapsed_time","text":"<p>Elapsed time since boot</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>cpu_elapsed_time</code>Unit: microsecType: deltaBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>cpu_elapsed_time</code>Unit: noneType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>cpu_elapsed_time</code>Unit: noneType: delta,no-displayBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_disk_busy","title":"node_disk_busy","text":"<p>The utilization percent of the disk. node_disk_busy is disk_busy aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>disk_busy_percent</code>Unit: percentType: percentBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_busy</code>Unit: percentType: percentBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>node_disk_busy</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Highlights table Top $TopResources Nodes by Disk Utilization ONTAP: Cluster Nodes &amp; Subsystems - $Cluster bargauge Avg Disk Utilization by Cluster ONTAP: Node Backend timeseries System Utilization"},{"location":"ontap-metrics/#node_disk_capacity","title":"node_disk_capacity","text":"<p>Disk capacity in MB. node_disk_capacity is disk_capacity aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>capacity</code>Unit: mbType: rawBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_capacity</code>Unit: mbType: rawBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_cp_read_chain","title":"node_disk_cp_read_chain","text":"<p>Average number of blocks transferred in each consistency point read operation during a CP. node_disk_cp_read_chain is disk_cp_read_chain aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_cp_read_latency","title":"node_disk_cp_read_latency","text":"<p>Average latency per block in microseconds for consistency point read operations. node_disk_cp_read_latency is disk_cp_read_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_cp_reads","title":"node_disk_cp_reads","text":"<p>Number of disk read operations initiated each second for consistency point processing. node_disk_cp_reads is disk_cp_reads aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_data_read","title":"node_disk_data_read","text":"<p>Number of disk kilobytes (KB) read per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>disk_data_read</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>disk_data_read</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>disk_data_read</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_disk_data_read</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Disk Utilization timeseries Disk Throughput by Node"},{"location":"ontap-metrics/#node_disk_data_written","title":"node_disk_data_written","text":"<p>Number of disk kilobytes (KB) written per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>disk_data_written</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>disk_data_written</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>disk_data_written</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_disk_data_written</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Disk Utilization timeseries Disk Throughput by Node"},{"location":"ontap-metrics/#node_disk_io_pending","title":"node_disk_io_pending","text":"<p>Average number of I/Os issued to the disk for which we have not yet received the response. node_disk_io_pending is disk_io_pending aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_io_queued","title":"node_disk_io_queued","text":"<p>Number of I/Os queued to the disk but not yet issued. node_disk_io_queued is disk_io_queued aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_busy","title":"node_disk_max_busy","text":"<p>The utilization percent of the disk. node_disk_max_busy is the maximum of disk_busy for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>disk_busy_percent</code>Unit: percentType: percentBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_busy</code>Unit: percentType: percentBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>node_disk_max_busy</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Nodes &amp; Subsystems - $Cluster bargauge Max Disk Utilization by Cluster ONTAP: Node Highlights bargauge Max Disk Utilization"},{"location":"ontap-metrics/#node_disk_max_capacity","title":"node_disk_max_capacity","text":"<p>Disk capacity in MB. node_disk_max_capacity is the maximum of disk_capacity for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>capacity</code>Unit: mbType: rawBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_capacity</code>Unit: mbType: rawBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_cp_read_chain","title":"node_disk_max_cp_read_chain","text":"<p>Average number of blocks transferred in each consistency point read operation during a CP. node_disk_max_cp_read_chain is the maximum of disk_cp_read_chain for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_cp_read_latency","title":"node_disk_max_cp_read_latency","text":"<p>Average latency per block in microseconds for consistency point read operations. node_disk_max_cp_read_latency is the maximum of disk_cp_read_latency for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_cp_reads","title":"node_disk_max_cp_reads","text":"<p>Number of disk read operations initiated each second for consistency point processing. node_disk_max_cp_reads is the maximum of disk_cp_reads for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_io_pending","title":"node_disk_max_io_pending","text":"<p>Average number of I/Os issued to the disk for which we have not yet received the response. node_disk_max_io_pending is the maximum of disk_io_pending for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_io_queued","title":"node_disk_max_io_queued","text":"<p>Number of I/Os queued to the disk but not yet issued. node_disk_max_io_queued is the maximum of disk_io_queued for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_total_data","title":"node_disk_max_total_data","text":"<p>Total throughput for user operations per second. node_disk_max_total_data is the maximum of disk_total_data for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_total_transfers","title":"node_disk_max_total_transfers","text":"<p>Total number of disk operations involving data transfer initiated per second. node_disk_max_total_transfers is the maximum of disk_total_transfers for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_transfer_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_transfers</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_user_read_blocks","title":"node_disk_max_user_read_blocks","text":"<p>Number of blocks transferred for user read operations per second. node_disk_max_user_read_blocks is the maximum of disk_user_read_blocks for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_user_read_chain","title":"node_disk_max_user_read_chain","text":"<p>Average number of blocks transferred in each user read operation. node_disk_max_user_read_chain is the maximum of disk_user_read_chain for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_user_read_latency","title":"node_disk_max_user_read_latency","text":"<p>Average latency per block in microseconds for user read operations. node_disk_max_user_read_latency is the maximum of disk_user_read_latency for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_user_reads","title":"node_disk_max_user_reads","text":"<p>Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. node_disk_max_user_reads is the maximum of disk_user_reads for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_user_write_blocks","title":"node_disk_max_user_write_blocks","text":"<p>Number of blocks transferred for user write operations per second. node_disk_max_user_write_blocks is the maximum of disk_user_write_blocks for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_user_write_chain","title":"node_disk_max_user_write_chain","text":"<p>Average number of blocks transferred in each user write operation. node_disk_max_user_write_chain is the maximum of disk_user_write_chain for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_write_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_writes conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_user_write_latency","title":"node_disk_max_user_write_latency","text":"<p>Average latency per block in microseconds for user write operations. node_disk_max_user_write_latency is the maximum of disk_user_write_latency for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_max_user_writes","title":"node_disk_max_user_writes","text":"<p>Number of disk write operations initiated each second for storing data or metadata associated with user requests. node_disk_max_user_writes is the maximum of disk_user_writes for label <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_writes</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_total_data","title":"node_disk_total_data","text":"<p>Total throughput for user operations per second. node_disk_total_data is disk_total_data aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_total_transfers","title":"node_disk_total_transfers","text":"<p>Total number of disk operations involving data transfer initiated per second. node_disk_total_transfers is disk_total_transfers aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_transfer_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_transfers</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_user_read_blocks","title":"node_disk_user_read_blocks","text":"<p>Number of blocks transferred for user read operations per second. node_disk_user_read_blocks is disk_user_read_blocks aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_user_read_chain","title":"node_disk_user_read_chain","text":"<p>Average number of blocks transferred in each user read operation. node_disk_user_read_chain is disk_user_read_chain aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_user_read_latency","title":"node_disk_user_read_latency","text":"<p>Average latency per block in microseconds for user read operations. node_disk_user_read_latency is disk_user_read_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_user_reads","title":"node_disk_user_reads","text":"<p>Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. node_disk_user_reads is disk_user_reads aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_user_write_blocks","title":"node_disk_user_write_blocks","text":"<p>Number of blocks transferred for user write operations per second. node_disk_user_write_blocks is disk_user_write_blocks aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_user_write_chain","title":"node_disk_user_write_chain","text":"<p>Average number of blocks transferred in each user write operation. node_disk_user_write_chain is disk_user_write_chain aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_write_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_writes conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_user_write_latency","title":"node_disk_user_write_latency","text":"<p>Average latency per block in microseconds for user write operations. node_disk_user_write_latency is disk_user_write_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_disk_user_writes","title":"node_disk_user_writes","text":"<p>Number of disk write operations initiated each second for storing data or metadata associated with user requests. node_disk_user_writes is disk_user_writes aggregated by <code>node</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_writes</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#node_failed_fan","title":"node_failed_fan","text":"<p>Specifies a count of the number of chassis fans that are not operating within the recommended RPM range.</p> API Endpoint Metric Template REST <code>api/cluster/nodes</code> <code>controller.failed_fan.count</code> conf/rest/9.12.0/node.yaml ZAPI <code>system-node-get-iter</code> <code>node-details-info.env-failed-fan-count</code> conf/zapi/cdot/9.8.0/node.yaml <p>The <code>node_failed_fan</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Highlights table Node Details"},{"location":"ontap-metrics/#node_failed_power","title":"node_failed_power","text":"<p>Number of failed power supply units.</p> API Endpoint Metric Template REST <code>api/cluster/nodes</code> <code>controller.failed_power_supply.count</code> conf/rest/9.12.0/node.yaml ZAPI <code>system-node-get-iter</code> <code>node-details-info.env-failed-power-supply-count</code> conf/zapi/cdot/9.8.0/node.yaml <p>The <code>node_failed_power</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Highlights table Node Details"},{"location":"ontap-metrics/#node_fcp_data_recv","title":"node_fcp_data_recv","text":"<p>Number of FCP kilobytes (KB) received per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>fcp_data_received</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>fcp_data_recv</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>fcp_data_recv</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_fcp_data_sent","title":"node_fcp_data_sent","text":"<p>Number of FCP kilobytes (KB) sent per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>fcp_data_sent</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>fcp_data_sent</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>fcp_data_sent</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_fcp_ops","title":"node_fcp_ops","text":"<p>Number of FCP operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>fcp_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>fcp_ops</code>Unit: per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>fcp_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_fcp_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Backend timeseries Protocol Backend IOPs"},{"location":"ontap-metrics/#node_hdd_data_read","title":"node_hdd_data_read","text":"<p>Number of HDD Disk kilobytes (KB) read per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>hdd_data_read</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>hdd_data_read</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>hdd_data_read</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_hdd_data_written","title":"node_hdd_data_written","text":"<p>Number of HDD kilobytes (KB) written per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>hdd_data_written</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>hdd_data_written</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>hdd_data_written</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_iscsi_ops","title":"node_iscsi_ops","text":"<p>Number of iSCSI operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>iscsi_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>iscsi_ops</code>Unit: per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>iscsi_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_iscsi_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Backend timeseries Protocol Backend IOPs"},{"location":"ontap-metrics/#node_labels","title":"node_labels","text":"<p>This metric provides information about Node</p> API Endpoint Metric Template REST <code>api/cluster/nodes</code> <code>Harvest generated</code> conf/rest/9.12.0/node.yaml ZAPI <code>system-node-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/node.yaml <p>The <code>node_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Highlights table Aggregates ONTAP: Cluster Nodes &amp; Subsystems - $Cluster table $Cluster ONTAP: Datacenter Highlights table Object Count ONTAP: Datacenter Health table Node Health ONTAP: Datacenter Power and Temperature stat Average Power/Used_TB ONTAP: Health HA table HA Issues ONTAP: Health Node table Node Issues ONTAP: Node Highlights table Node Details ONTAP: Power Highlights stat Average Power/Used_TB ONTAP: Power Nodes table Storage Nodes"},{"location":"ontap-metrics/#node_memory","title":"node_memory","text":"<p>Total memory in megabytes (MB)</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>memory</code>Unit: noneType: rawBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>memory</code>Unit: noneType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>memory</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_net_data_recv","title":"node_net_data_recv","text":"<p>Number of network kilobytes (KB) received per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>network_data_received</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>net_data_recv</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>net_data_recv</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_net_data_sent","title":"node_net_data_sent","text":"<p>Number of network kilobytes (KB) sent per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>network_data_sent</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>net_data_sent</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>net_data_sent</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_new_status","title":"node_new_status","text":"<p>This metric indicates a value of 1 if the node is healthy (true or up, indicating the node is operational) and a value of 0 for any other state.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/node.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/node.yaml <p>The <code>node_new_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Nodes &amp; Subsystems - $Cluster table $Cluster ONTAP: Datacenter Health table Node Health ONTAP: Node Highlights stat Nodes"},{"location":"ontap-metrics/#node_nfs_access_avg_latency","title":"node_nfs_access_avg_latency","text":"<p>Average latency in microseconds of Access procedure requests. The counter keeps track of the average response time of Access requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>access.average_latency</code>Unit: microsecType: averageBase: access.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>access.average_latency</code>Unit: microsecType: averageBase: access.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>access.average_latency</code>Unit: microsecType: averageBase: access.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>access.average_latency</code>Unit: microsecType: averageBase: access.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>access_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: access_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>access_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: access_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>access_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: access_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>access_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: access_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_access_total","title":"node_nfs_access_total","text":"<p>Total number of Access procedure requests. It is the total number of access success and access error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>access.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>access.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>access.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>access.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>access_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>access_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>access_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>access_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_backchannel_ctl_avg_latency","title":"node_nfs_backchannel_ctl_avg_latency","text":"<p>Average latency in microseconds of BACKCHANNEL_CTL operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>backchannel_ctl.average_latency</code>Unit: microsecType: averageBase: backchannel_ctl.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>backchannel_ctl.average_latency</code>Unit: microsecType: averageBase: backchannel_ctl.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>backchannel_ctl_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: backchannel_ctl_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>backchannel_ctl_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: backchannel_ctl_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_backchannel_ctl_total","title":"node_nfs_backchannel_ctl_total","text":"<p>Total number of BACKCHANNEL_CTL operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>backchannel_ctl.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>backchannel_ctl.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>backchannel_ctl_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>backchannel_ctl_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_bind_conn_to_session_avg_latency","title":"node_nfs_bind_conn_to_session_avg_latency","text":"<p>Average latency in microseconds of BIND_CONN_TO_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>bind_connections_to_session.average_latency</code>Unit: microsecType: averageBase: bind_connections_to_session.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>bind_conn_to_session.average_latency</code>Unit: microsecType: averageBase: bind_conn_to_session.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>bind_conn_to_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: bind_conn_to_session_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>bind_conn_to_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: bind_conn_to_session_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_bind_conn_to_session_total","title":"node_nfs_bind_conn_to_session_total","text":"<p>Total number of BIND_CONN_TO_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>bind_connections_to_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>bind_conn_to_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>bind_conn_to_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>bind_conn_to_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_close_avg_latency","title":"node_nfs_close_avg_latency","text":"<p>Average latency in microseconds of CLOSE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>close.average_latency</code>Unit: microsecType: averageBase: close.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>close.average_latency</code>Unit: microsecType: averageBase: close.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>close.average_latency</code>Unit: microsecType: averageBase: close.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>close_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: close_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>close_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: close_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>close_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: close_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_close_total","title":"node_nfs_close_total","text":"<p>Total number of CLOSE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>close.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>close.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>close.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>close_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>close_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>close_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_commit_avg_latency","title":"node_nfs_commit_avg_latency","text":"<p>Average latency in microseconds of Commit procedure requests. The counter keeps track of the average response time of Commit requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>commit.average_latency</code>Unit: microsecType: averageBase: commit.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>commit.average_latency</code>Unit: microsecType: averageBase: commit.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>commit.average_latency</code>Unit: microsecType: averageBase: commit.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>commit.average_latency</code>Unit: microsecType: averageBase: commit.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>commit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: commit_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>commit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: commit_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>commit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: commit_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>commit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: commit_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_commit_total","title":"node_nfs_commit_total","text":"<p>Total number of Commit procedure requests. It is the total number of Commit success and Commit error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>commit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>commit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>commit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>commit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>commit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>commit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>commit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>commit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_create_avg_latency","title":"node_nfs_create_avg_latency","text":"<p>Average latency in microseconds of Create procedure requests. The counter keeps track of the average response time of Create requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>create.average_latency</code>Unit: microsecType: averageBase: create.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>create.average_latency</code>Unit: microsecType: averageBase: create.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>create.average_latency</code>Unit: microsecType: averageBase: create.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>create.average_latency</code>Unit: microsecType: averageBase: create.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>create_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>create_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>create_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>create_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_create_session_avg_latency","title":"node_nfs_create_session_avg_latency","text":"<p>Average latency in microseconds of CREATE_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>create_session.average_latency</code>Unit: microsecType: averageBase: create_session.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>create_session.average_latency</code>Unit: microsecType: averageBase: create_session.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>create_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_session_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>create_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_session_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_create_session_total","title":"node_nfs_create_session_total","text":"<p>Total number of CREATE_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>create_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>create_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>create_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>create_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_create_total","title":"node_nfs_create_total","text":"<p>Total number Create of procedure requests. It is the total number of create success and create error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>create.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>create.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>create.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>create.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>create_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>create_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>create_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>create_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_delegpurge_avg_latency","title":"node_nfs_delegpurge_avg_latency","text":"<p>Average latency in microseconds of DELEGPURGE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>delegpurge.average_latency</code>Unit: microsecType: averageBase: delegpurge.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>delegpurge.average_latency</code>Unit: microsecType: averageBase: delegpurge.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>delegpurge.average_latency</code>Unit: microsecType: averageBase: delegpurge.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>delegpurge_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegpurge_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>delegpurge_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegpurge_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>delegpurge_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegpurge_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_delegpurge_total","title":"node_nfs_delegpurge_total","text":"<p>Total number of DELEGPURGE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>delegpurge.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>delegpurge.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>delegpurge.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>delegpurge_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>delegpurge_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>delegpurge_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_delegreturn_avg_latency","title":"node_nfs_delegreturn_avg_latency","text":"<p>Average latency in microseconds of DELEGRETURN operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>delegreturn.average_latency</code>Unit: microsecType: averageBase: delegreturn.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>delegreturn.average_latency</code>Unit: microsecType: averageBase: delegreturn.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>delegreturn.average_latency</code>Unit: microsecType: averageBase: delegreturn.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>delegreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegreturn_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>delegreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegreturn_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>delegreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegreturn_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_delegreturn_total","title":"node_nfs_delegreturn_total","text":"<p>Total number of DELEGRETURN operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>delegreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>delegreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>delegreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>delegreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>delegreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>delegreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_destroy_clientid_avg_latency","title":"node_nfs_destroy_clientid_avg_latency","text":"<p>Average latency in microseconds of DESTROY_CLIENTID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>destroy_clientid.average_latency</code>Unit: microsecType: averageBase: destroy_clientid.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>destroy_clientid.average_latency</code>Unit: microsecType: averageBase: destroy_clientid.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>destroy_clientid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: destroy_clientid_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>destroy_clientid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: destroy_clientid_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_destroy_clientid_total","title":"node_nfs_destroy_clientid_total","text":"<p>Total number of DESTROY_CLIENTID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>destroy_clientid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>destroy_clientid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>destroy_clientid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>destroy_clientid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_destroy_session_avg_latency","title":"node_nfs_destroy_session_avg_latency","text":"<p>Average latency in microseconds of DESTROY_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>destroy_session.average_latency</code>Unit: microsecType: averageBase: destroy_session.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>destroy_session.average_latency</code>Unit: microsecType: averageBase: destroy_session.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>destroy_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: destroy_session_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>destroy_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: destroy_session_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_destroy_session_total","title":"node_nfs_destroy_session_total","text":"<p>Total number of DESTROY_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>destroy_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>destroy_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>destroy_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>destroy_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_exchange_id_avg_latency","title":"node_nfs_exchange_id_avg_latency","text":"<p>Average latency in microseconds of EXCHANGE_ID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>exchange_id.average_latency</code>Unit: microsecType: averageBase: exchange_id.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>exchange_id.average_latency</code>Unit: microsecType: averageBase: exchange_id.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>exchange_id_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: exchange_id_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>exchange_id_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: exchange_id_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_exchange_id_total","title":"node_nfs_exchange_id_total","text":"<p>Total number of EXCHANGE_ID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>exchange_id.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>exchange_id.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>exchange_id_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>exchange_id_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_free_stateid_avg_latency","title":"node_nfs_free_stateid_avg_latency","text":"<p>Average latency in microseconds of FREE_STATEID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>free_stateid.average_latency</code>Unit: microsecType: averageBase: free_stateid.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>free_stateid.average_latency</code>Unit: microsecType: averageBase: free_stateid.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>free_stateid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: free_stateid_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>free_stateid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: free_stateid_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_free_stateid_total","title":"node_nfs_free_stateid_total","text":"<p>Total number of FREE_STATEID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>free_stateid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>free_stateid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>free_stateid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>free_stateid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_fsinfo_avg_latency","title":"node_nfs_fsinfo_avg_latency","text":"<p>Average latency in microseconds of FSInfo procedure requests. The counter keeps track of the average response time of FSInfo requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>fsinfo.average_latency</code>Unit: microsecType: averageBase: fsinfo.total conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>fsinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: fsinfo_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_fsinfo_total","title":"node_nfs_fsinfo_total","text":"<p>Total number FSInfo of procedure requests. It is the total number of FSInfo success and FSInfo error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>fsinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>fsinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_fsstat_avg_latency","title":"node_nfs_fsstat_avg_latency","text":"<p>Average latency in microseconds of FSStat procedure requests. The counter keeps track of the average response time of FSStat requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>fsstat.average_latency</code>Unit: microsecType: averageBase: fsstat.total conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>fsstat_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: fsstat_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_fsstat_total","title":"node_nfs_fsstat_total","text":"<p>Total number FSStat of procedure requests. It is the total number of FSStat success and FSStat error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>fsstat.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>fsstat_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_get_dir_delegation_avg_latency","title":"node_nfs_get_dir_delegation_avg_latency","text":"<p>Average latency in microseconds of GET_DIR_DELEGATION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>get_dir_delegation.average_latency</code>Unit: microsecType: averageBase: get_dir_delegation.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>get_dir_delegation.average_latency</code>Unit: microsecType: averageBase: get_dir_delegation.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>get_dir_delegation_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: get_dir_delegation_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>get_dir_delegation_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: get_dir_delegation_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_get_dir_delegation_total","title":"node_nfs_get_dir_delegation_total","text":"<p>Total number of GET_DIR_DELEGATION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>get_dir_delegation.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>get_dir_delegation.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>get_dir_delegation_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>get_dir_delegation_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_getattr_avg_latency","title":"node_nfs_getattr_avg_latency","text":"<p>Average latency in microseconds of GetAttr procedure requests. This counter keeps track of the average response time of GetAttr requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>getattr.average_latency</code>Unit: microsecType: averageBase: getattr.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>getattr.average_latency</code>Unit: microsecType: averageBase: getattr.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>getattr.average_latency</code>Unit: microsecType: averageBase: getattr.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>getattr.average_latency</code>Unit: microsecType: averageBase: getattr.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>getattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getattr_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>getattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getattr_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>getattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getattr_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>getattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getattr_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_getattr_total","title":"node_nfs_getattr_total","text":"<p>Total number of Getattr procedure requests. It is the total number of getattr success and getattr error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>getattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>getattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>getattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>getattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>getattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>getattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>getattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>getattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_getdeviceinfo_avg_latency","title":"node_nfs_getdeviceinfo_avg_latency","text":"<p>Average latency in microseconds of GETDEVICEINFO operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>getdeviceinfo.average_latency</code>Unit: microsecType: averageBase: getdeviceinfo.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>getdeviceinfo.average_latency</code>Unit: microsecType: averageBase: getdeviceinfo.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>getdeviceinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getdeviceinfo_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>getdeviceinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getdeviceinfo_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_getdeviceinfo_total","title":"node_nfs_getdeviceinfo_total","text":"<p>Total number of GETDEVICEINFO operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>getdeviceinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>getdeviceinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>getdeviceinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>getdeviceinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_getdevicelist_avg_latency","title":"node_nfs_getdevicelist_avg_latency","text":"<p>Average latency in microseconds of GETDEVICELIST operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>getdevicelist.average_latency</code>Unit: microsecType: averageBase: getdevicelist.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>getdevicelist.average_latency</code>Unit: microsecType: averageBase: getdevicelist.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>getdevicelist_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getdevicelist_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>getdevicelist_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getdevicelist_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_getdevicelist_total","title":"node_nfs_getdevicelist_total","text":"<p>Total number of GETDEVICELIST operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>getdevicelist.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>getdevicelist.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>getdevicelist_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>getdevicelist_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_getfh_avg_latency","title":"node_nfs_getfh_avg_latency","text":"<p>Average latency in microseconds of GETFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>getfh.average_latency</code>Unit: microsecType: averageBase: getfh.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>getfh.average_latency</code>Unit: microsecType: averageBase: getfh.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>getfh.average_latency</code>Unit: microsecType: averageBase: getfh.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>getfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getfh_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>getfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getfh_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>getfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getfh_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_getfh_total","title":"node_nfs_getfh_total","text":"<p>Total number of GETFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>getfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>getfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>getfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>getfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>getfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>getfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_latency","title":"node_nfs_latency","text":"<p>Average latency in microseconds of NFSv3 requests. This counter keeps track of the average response time of NFSv3 requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>latency</code>Unit: microsecType: average,no-zero-valuesBase: total_ops conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>latency</code>Unit: microsecType: average,no-zero-valuesBase: total_ops conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>latency</code>Unit: microsecType: average,no-zero-valuesBase: total_ops conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>latency</code>Unit: microsecType: average,no-zero-valuesBase: total_ops conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml <p>The <code>node_nfs_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NFSv3 Frontend stat NFSv3 Avg Latency"},{"location":"ontap-metrics/#node_nfs_layoutcommit_avg_latency","title":"node_nfs_layoutcommit_avg_latency","text":"<p>Average latency in microseconds of LAYOUTCOMMIT operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>layoutcommit.average_latency</code>Unit: microsecType: averageBase: layoutcommit.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>layoutcommit.average_latency</code>Unit: microsecType: averageBase: layoutcommit.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>layoutcommit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutcommit_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>layoutcommit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutcommit_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_layoutcommit_total","title":"node_nfs_layoutcommit_total","text":"<p>Total number of LAYOUTCOMMIT operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>layoutcommit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>layoutcommit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>layoutcommit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>layoutcommit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_layoutget_avg_latency","title":"node_nfs_layoutget_avg_latency","text":"<p>Average latency in microseconds of LAYOUTGET operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>layoutget.average_latency</code>Unit: microsecType: averageBase: layoutget.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>layoutget.average_latency</code>Unit: microsecType: averageBase: layoutget.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>layoutget_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutget_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>layoutget_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutget_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_layoutget_total","title":"node_nfs_layoutget_total","text":"<p>Total number of LAYOUTGET operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>layoutget.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>layoutget.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>layoutget_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>layoutget_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_layoutreturn_avg_latency","title":"node_nfs_layoutreturn_avg_latency","text":"<p>Average latency in microseconds of LAYOUTRETURN operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>layoutreturn.average_latency</code>Unit: microsecType: averageBase: layoutreturn.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>layoutreturn.average_latency</code>Unit: microsecType: averageBase: layoutreturn.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>layoutreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutreturn_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>layoutreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutreturn_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_layoutreturn_total","title":"node_nfs_layoutreturn_total","text":"<p>Total number of LAYOUTRETURN operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>layoutreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>layoutreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>layoutreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>layoutreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_link_avg_latency","title":"node_nfs_link_avg_latency","text":"<p>Average latency in microseconds of Link procedure requests. The counter keeps track of the average response time of Link requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>link.average_latency</code>Unit: microsecType: averageBase: link.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>link.average_latency</code>Unit: microsecType: averageBase: link.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>link.average_latency</code>Unit: microsecType: averageBase: link.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>link.average_latency</code>Unit: microsecType: averageBase: link.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>link_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: link_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>link_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: link_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>link_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: link_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>link_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: link_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_link_total","title":"node_nfs_link_total","text":"<p>Total number Link of procedure requests. It is the total number of Link success and Link error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>link.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>link.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>link.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>link.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>link_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>link_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>link_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>link_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_lock_avg_latency","title":"node_nfs_lock_avg_latency","text":"<p>Average latency in microseconds of LOCK operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>lock.average_latency</code>Unit: microsecType: averageBase: lock.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>lock.average_latency</code>Unit: microsecType: averageBase: lock.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>lock.average_latency</code>Unit: microsecType: averageBase: lock.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>lock_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lock_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>lock_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lock_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>lock_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lock_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_lock_total","title":"node_nfs_lock_total","text":"<p>Total number of LOCK operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>lock.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>lock.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>lock.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>lock_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>lock_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>lock_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_lockt_avg_latency","title":"node_nfs_lockt_avg_latency","text":"<p>Average latency in microseconds of LOCKT operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>lockt.average_latency</code>Unit: microsecType: averageBase: lockt.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>lockt.average_latency</code>Unit: microsecType: averageBase: lockt.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>lockt.average_latency</code>Unit: microsecType: averageBase: lockt.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>lockt_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lockt_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>lockt_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lockt_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>lockt_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lockt_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_lockt_total","title":"node_nfs_lockt_total","text":"<p>Total number of LOCKT operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>lockt.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>lockt.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>lockt.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>lockt_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>lockt_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>lockt_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_locku_avg_latency","title":"node_nfs_locku_avg_latency","text":"<p>Average latency in microseconds of LOCKU operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>locku.average_latency</code>Unit: microsecType: averageBase: locku.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>locku.average_latency</code>Unit: microsecType: averageBase: locku.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>locku.average_latency</code>Unit: microsecType: averageBase: locku.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>locku_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: locku_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>locku_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: locku_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>locku_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: locku_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_locku_total","title":"node_nfs_locku_total","text":"<p>Total number of LOCKU operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>locku.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>locku.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>locku.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>locku_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>locku_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>locku_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_lookup_avg_latency","title":"node_nfs_lookup_avg_latency","text":"<p>Average latency in microseconds of LookUp procedure requests. This shows the average time it takes for the LookUp operation to reply to the request.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>lookup.average_latency</code>Unit: microsecType: averageBase: lookup.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>lookup.average_latency</code>Unit: microsecType: averageBase: lookup.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>lookup.average_latency</code>Unit: microsecType: averageBase: lookup.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>lookup.average_latency</code>Unit: microsecType: averageBase: lookup.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>lookup_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookup_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>lookup_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookup_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>lookup_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookup_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>lookup_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookup_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_lookup_total","title":"node_nfs_lookup_total","text":"<p>Total number of Lookup procedure requests. It is the total number of lookup success and lookup error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>lookup.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>lookup.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>lookup.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>lookup.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>lookup_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>lookup_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>lookup_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>lookup_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_lookupp_avg_latency","title":"node_nfs_lookupp_avg_latency","text":"<p>Average latency in microseconds of LOOKUPP operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>lookupp.average_latency</code>Unit: microsecType: averageBase: lookupp.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>lookupp.average_latency</code>Unit: microsecType: averageBase: lookupp.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>lookupp.average_latency</code>Unit: microsecType: averageBase: lookupp.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>lookupp_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookupp_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>lookupp_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookupp_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>lookupp_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookupp_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_lookupp_total","title":"node_nfs_lookupp_total","text":"<p>Total number of LOOKUPP operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>lookupp.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>lookupp.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>lookupp.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>lookupp_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>lookupp_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>lookupp_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_mkdir_avg_latency","title":"node_nfs_mkdir_avg_latency","text":"<p>Average latency in microseconds of MkDir procedure requests. The counter keeps track of the average response time of MkDir requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>mkdir.average_latency</code>Unit: microsecType: averageBase: mkdir.total conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>mkdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: mkdir_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_mkdir_total","title":"node_nfs_mkdir_total","text":"<p>Total number MkDir of procedure requests. It is the total number of MkDir success and MkDir error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>mkdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>mkdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_mknod_avg_latency","title":"node_nfs_mknod_avg_latency","text":"<p>Average latency in microseconds of MkNod procedure requests. The counter keeps track of the average response time of MkNod requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>mknod.average_latency</code>Unit: microsecType: averageBase: mknod.total conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>mknod_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: mknod_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_mknod_total","title":"node_nfs_mknod_total","text":"<p>Total number MkNod of procedure requests. It is the total number of MkNod success and MkNod error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>mknod.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>mknod_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_null_avg_latency","title":"node_nfs_null_avg_latency","text":"<p>Average latency in microseconds of Null procedure requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>null.average_latency</code>Unit: microsecType: averageBase: null.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>null.average_latency</code>Unit: microsecType: averageBase: null.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>null.average_latency</code>Unit: microsecType: averageBase: null.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>null.average_latency</code>Unit: microsecType: averageBase: null.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>null_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: null_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>null_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: null_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>null_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: null_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>null_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: null_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_null_total","title":"node_nfs_null_total","text":"<p>Total number of Null procedure requests. It is the total of null success and null error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>null.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>null.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>null.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>null.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>null_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>null_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>null_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>null_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_nverify_avg_latency","title":"node_nfs_nverify_avg_latency","text":"<p>Average latency in microseconds of NVERIFY operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>nverify.average_latency</code>Unit: microsecType: averageBase: nverify.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>nverify.average_latency</code>Unit: microsecType: averageBase: nverify.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>nverify.average_latency</code>Unit: microsecType: averageBase: nverify.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>nverify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: nverify_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>nverify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: nverify_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>nverify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: nverify_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_nverify_total","title":"node_nfs_nverify_total","text":"<p>Total number of NVERIFY operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>nverify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>nverify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>nverify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>nverify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>nverify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>nverify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_open_avg_latency","title":"node_nfs_open_avg_latency","text":"<p>Average latency in microseconds of OPEN operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>open.average_latency</code>Unit: microsecType: averageBase: open.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>open.average_latency</code>Unit: microsecType: averageBase: open.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>open.average_latency</code>Unit: microsecType: averageBase: open.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>open_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>open_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>open_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_open_confirm_avg_latency","title":"node_nfs_open_confirm_avg_latency","text":"<p>Average latency in microseconds of OPEN_CONFIRM procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>open_confirm.average_latency</code>Unit: microsecType: averageBase: open_confirm.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>open_confirm_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_confirm_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_open_confirm_total","title":"node_nfs_open_confirm_total","text":"<p>Total number of OPEN_CONFIRM procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>open_confirm.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>open_confirm_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_open_downgrade_avg_latency","title":"node_nfs_open_downgrade_avg_latency","text":"<p>Average latency in microseconds of OPEN_DOWNGRADE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>open_downgrade.average_latency</code>Unit: microsecType: averageBase: open_downgrade.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>open_downgrade.average_latency</code>Unit: microsecType: averageBase: open_downgrade.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>open_downgrade.average_latency</code>Unit: microsecType: averageBase: open_downgrade.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>open_downgrade_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_downgrade_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>open_downgrade_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_downgrade_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>open_downgrade_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_downgrade_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_open_downgrade_total","title":"node_nfs_open_downgrade_total","text":"<p>Total number of OPEN_DOWNGRADE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>open_downgrade.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>open_downgrade.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>open_downgrade.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>open_downgrade_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>open_downgrade_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>open_downgrade_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_open_total","title":"node_nfs_open_total","text":"<p>Total number of OPEN operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>open.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>open.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>open.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>open_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>open_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>open_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_openattr_avg_latency","title":"node_nfs_openattr_avg_latency","text":"<p>Average latency in microseconds of OPENATTR operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>openattr.average_latency</code>Unit: microsecType: averageBase: openattr.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>openattr.average_latency</code>Unit: microsecType: averageBase: openattr.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>openattr.average_latency</code>Unit: microsecType: averageBase: openattr.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>openattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: openattr_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>openattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: openattr_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>openattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: openattr_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_openattr_total","title":"node_nfs_openattr_total","text":"<p>Total number of OPENATTR operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>openattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>openattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>openattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>openattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>openattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>openattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_ops","title":"node_nfs_ops","text":"<p>Number of NFS operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>nfs_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>nfs_ops</code>Unit: per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>nfs_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_nfs_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT Cluster Metrics timeseries Top $TopResources NFS IOPs by Cluster ONTAP: Node Backend timeseries Protocol Backend IOPs ONTAP: Node NFSv3 Frontend table NFS Avg IOPS"},{"location":"ontap-metrics/#node_nfs_pathconf_avg_latency","title":"node_nfs_pathconf_avg_latency","text":"<p>Average latency in microseconds of PathConf procedure requests. The counter keeps track of the average response time of PathConf requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>pathconf.average_latency</code>Unit: microsecType: averageBase: pathconf.total conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>pathconf_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: pathconf_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_pathconf_total","title":"node_nfs_pathconf_total","text":"<p>Total number PathConf of procedure requests. It is the total number of PathConf success and PathConf error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>pathconf.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>pathconf_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_putfh_avg_latency","title":"node_nfs_putfh_avg_latency","text":"<p>The number of successful PUTPUBFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>putfh.average_latency</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>putfh.average_latency</code>Unit: microsecType: averageBase: putfh.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>putfh.average_latency</code>Unit: microsecType: averageBase: putfh.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>putfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putfh_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>putfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putfh_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>putfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putfh_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_putfh_total","title":"node_nfs_putfh_total","text":"<p>Total number of PUTFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>putfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>putfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>putfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>putfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>putfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>putfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_putpubfh_avg_latency","title":"node_nfs_putpubfh_avg_latency","text":"<p>Average latency in microseconds of PUTPUBFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>putpubfh.average_latency</code>Unit: microsecType: averageBase: putpubfh.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>putpubfh.average_latency</code>Unit: microsecType: averageBase: putpubfh.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>putpubfh.average_latency</code>Unit: microsecType: averageBase: putpubfh.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>putpubfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putpubfh_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>putpubfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putpubfh_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>putpubfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putpubfh_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_putpubfh_total","title":"node_nfs_putpubfh_total","text":"<p>Total number of PUTPUBFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>putpubfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>putpubfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>putpubfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>putpubfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>putpubfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>putpubfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_putrootfh_avg_latency","title":"node_nfs_putrootfh_avg_latency","text":"<p>Average latency in microseconds of PUTROOTFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>putrootfh.average_latency</code>Unit: microsecType: averageBase: putrootfh.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>putrootfh.average_latency</code>Unit: microsecType: averageBase: putrootfh.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>putrootfh.average_latency</code>Unit: microsecType: averageBase: putrootfh.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>putrootfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putrootfh_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>putrootfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putrootfh_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>putrootfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putrootfh_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_putrootfh_total","title":"node_nfs_putrootfh_total","text":"<p>Total number of PUTROOTFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>putrootfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>putrootfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>putrootfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>putrootfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>putrootfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>putrootfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_read_avg_latency","title":"node_nfs_read_avg_latency","text":"<p>Average latency in microseconds of Read procedure requests. The counter keeps track of the average response time of Read requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>read.average_latency</code>Unit: microsecType: averageBase: read.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>read.average_latency</code>Unit: microsecType: averageBase: read.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>read.average_latency</code>Unit: microsecType: averageBase: read.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>read.average_latency</code>Unit: microsecType: averageBase: read.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>read_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>read_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>read_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>read_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml <p>The <code>node_nfs_read_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NFSv3 Frontend stat NFSv3 Avg Read Latency ONTAP: Node NFSv3 Frontend timeseries NFSv3 Read and Write Latency"},{"location":"ontap-metrics/#node_nfs_read_ops","title":"node_nfs_read_ops","text":"<p>Total observed NFSv3 read operations per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>nfsv3_read_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml <p>The <code>node_nfs_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NFSv3 Frontend table NFS Avg IOPS ONTAP: Node NFSv3 Frontend timeseries NFSv3 Read and Write IOPs"},{"location":"ontap-metrics/#node_nfs_read_symlink_avg_latency","title":"node_nfs_read_symlink_avg_latency","text":"<p>Average latency in microseconds of ReadSymLink procedure requests. The counter keeps track of the average response time of ReadSymLink requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>read_symlink.average_latency</code>Unit: microsecType: averageBase: read_symlink.total conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>read_symlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_symlink_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_read_symlink_total","title":"node_nfs_read_symlink_total","text":"<p>Total number of ReadSymLink procedure requests. It is the total number of read symlink success and read symlink error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>read_symlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>read_symlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_read_throughput","title":"node_nfs_read_throughput","text":"<p>Rate of NFSv3 read data transfers in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>read_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>total.read_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>total.read_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>total.read_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>nfsv3_read_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>nfs41_read_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>nfs42_read_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>nfs4_read_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml <p>The <code>node_nfs_read_throughput</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NFSv3 Frontend table NFSv3 Avg Throughput ONTAP: Node NFSv3 Frontend timeseries NFSv3 Read and Write Throughput"},{"location":"ontap-metrics/#node_nfs_read_total","title":"node_nfs_read_total","text":"<p>Total number Read of procedure requests. It is the total number of read success and read error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>read.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>read.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>read.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>read.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>read_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>read_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>read_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>read_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_readdir_avg_latency","title":"node_nfs_readdir_avg_latency","text":"<p>Average latency in microseconds of ReadDir procedure requests. The counter keeps track of the average response time of ReadDir requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>readdir.average_latency</code>Unit: microsecType: averageBase: readdir.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>readdir.average_latency</code>Unit: microsecType: averageBase: readdir.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>readdir.average_latency</code>Unit: microsecType: averageBase: readdir.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>readdir.average_latency</code>Unit: microsecType: averageBase: readdir.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>readdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdir_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>readdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdir_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>readdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdir_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>readdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdir_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_readdir_total","title":"node_nfs_readdir_total","text":"<p>Total number ReadDir of procedure requests. It is the total number of ReadDir success and ReadDir error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>readdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>readdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>readdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>readdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>readdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>readdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>readdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>readdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_readdirplus_avg_latency","title":"node_nfs_readdirplus_avg_latency","text":"<p>Average latency in microseconds of ReadDirPlus procedure requests. The counter keeps track of the average response time of ReadDirPlus requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>readdirplus.average_latency</code>Unit: microsecType: averageBase: readdirplus.total conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>readdirplus_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdirplus_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_readdirplus_total","title":"node_nfs_readdirplus_total","text":"<p>Total number ReadDirPlus of procedure requests. It is the total number of ReadDirPlus success and ReadDirPlus error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>readdirplus.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>readdirplus_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_readlink_avg_latency","title":"node_nfs_readlink_avg_latency","text":"<p>Average latency in microseconds of READLINK operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>readlink.average_latency</code>Unit: microsecType: averageBase: readlink.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>readlink.average_latency</code>Unit: microsecType: averageBase: readlink.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>readlink.average_latency</code>Unit: microsecType: averageBase: readlink.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>readlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readlink_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>readlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readlink_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>readlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readlink_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_readlink_total","title":"node_nfs_readlink_total","text":"<p>Total number of READLINK operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>readlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>readlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>readlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>readlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>readlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>readlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_reclaim_complete_avg_latency","title":"node_nfs_reclaim_complete_avg_latency","text":"<p>Average latency in microseconds of RECLAIM_COMPLETE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>reclaim_complete.average_latency</code>Unit: microsecType: averageBase: reclaim_complete.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>reclaim_complete.average_latency</code>Unit: microsecType: averageBase: reclaim_complete.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>reclaim_complete_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: reclaim_complete_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>reclaim_complete_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: reclaim_complete_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_reclaim_complete_total","title":"node_nfs_reclaim_complete_total","text":"<p>Total number of RECLAIM_COMPLETE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>reclaim_complete.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>reclaim_complete.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>reclaim_complete_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>reclaim_complete_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_release_lock_owner_avg_latency","title":"node_nfs_release_lock_owner_avg_latency","text":"<p>Average Latency of RELEASE_LOCKOWNER procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>release_lock_owner.average_latency</code>Unit: microsecType: averageBase: release_lock_owner.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>release_lock_owner_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: release_lock_owner_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_release_lock_owner_total","title":"node_nfs_release_lock_owner_total","text":"<p>Total number of RELEASE_LOCKOWNER procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>release_lock_owner.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>release_lock_owner_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_remove_avg_latency","title":"node_nfs_remove_avg_latency","text":"<p>Average latency in microseconds of Remove procedure requests. The counter keeps track of the average response time of Remove requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>remove.average_latency</code>Unit: microsecType: averageBase: remove.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>remove.average_latency</code>Unit: microsecType: averageBase: remove.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>remove.average_latency</code>Unit: microsecType: averageBase: remove.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>remove.average_latency</code>Unit: microsecType: averageBase: remove.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>remove_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: remove_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>remove_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: remove_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>remove_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: remove_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>remove_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: remove_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_remove_total","title":"node_nfs_remove_total","text":"<p>Total number Remove of procedure requests. It is the total number of Remove success and Remove error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>remove.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>remove.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>remove.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>remove.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>remove_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>remove_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>remove_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>remove_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_rename_avg_latency","title":"node_nfs_rename_avg_latency","text":"<p>Average latency in microseconds of Rename procedure requests. The counter keeps track of the average response time of Rename requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>rename.average_latency</code>Unit: microsecType: averageBase: rename.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>rename.average_latency</code>Unit: microsecType: averageBase: rename.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>rename.average_latency</code>Unit: microsecType: averageBase: rename.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>rename.average_latency</code>Unit: microsecType: averageBase: rename.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>rename_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rename_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>rename_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rename_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>rename_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rename_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>rename_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rename_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_rename_total","title":"node_nfs_rename_total","text":"<p>Total number Rename of procedure requests. It is the total number of Rename success and Rename error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>rename.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>rename.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>rename.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>rename.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>rename_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>rename_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>rename_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>rename_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_renew_avg_latency","title":"node_nfs_renew_avg_latency","text":"<p>Average latency in microseconds of RENEW procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>renew.average_latency</code>Unit: microsecType: averageBase: renew.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>renew_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: renew_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_renew_total","title":"node_nfs_renew_total","text":"<p>Total number of RENEW procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>renew.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>renew_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_restorefh_avg_latency","title":"node_nfs_restorefh_avg_latency","text":"<p>Average latency in microseconds of RESTOREFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>restorefh.average_latency</code>Unit: microsecType: averageBase: restorefh.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>restorefh.average_latency</code>Unit: microsecType: averageBase: restorefh.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>restorefh.average_latency</code>Unit: microsecType: averageBase: restorefh.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>restorefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: restorefh_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>restorefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: restorefh_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>restorefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: restorefh_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_restorefh_total","title":"node_nfs_restorefh_total","text":"<p>Total number of RESTOREFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>restorefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>restorefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>restorefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>restorefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>restorefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>restorefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_rmdir_avg_latency","title":"node_nfs_rmdir_avg_latency","text":"<p>Average latency in microseconds of RmDir procedure requests. The counter keeps track of the average response time of RmDir requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>rmdir.average_latency</code>Unit: microsecType: averageBase: rmdir.total conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>rmdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rmdir_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_rmdir_total","title":"node_nfs_rmdir_total","text":"<p>Total number RmDir of procedure requests. It is the total number of RmDir success and RmDir error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>rmdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>rmdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_savefh_avg_latency","title":"node_nfs_savefh_avg_latency","text":"<p>Average latency in microseconds of SAVEFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>savefh.average_latency</code>Unit: microsecType: averageBase: savefh.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>savefh.average_latency</code>Unit: microsecType: averageBase: savefh.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>savefh.average_latency</code>Unit: microsecType: averageBase: savefh.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>savefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: savefh_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>savefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: savefh_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>savefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: savefh_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_savefh_total","title":"node_nfs_savefh_total","text":"<p>Total number of SAVEFH operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>savefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>savefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>savefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>savefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>savefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>savefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_secinfo_avg_latency","title":"node_nfs_secinfo_avg_latency","text":"<p>Average latency in microseconds of SECINFO operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>secinfo.average_latency</code>Unit: microsecType: averageBase: secinfo.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>secinfo.average_latency</code>Unit: microsecType: averageBase: secinfo.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>secinfo.average_latency</code>Unit: microsecType: averageBase: secinfo.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>secinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>secinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>secinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_secinfo_no_name_avg_latency","title":"node_nfs_secinfo_no_name_avg_latency","text":"<p>Average latency in microseconds of SECINFO_NO_NAME operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>secinfo_no_name.average_latency</code>Unit: microsecType: averageBase: secinfo_no_name.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>secinfo_no_name.average_latency</code>Unit: microsecType: averageBase: secinfo_no_name.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>secinfo_no_name_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_no_name_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>secinfo_no_name_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_no_name_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_secinfo_no_name_total","title":"node_nfs_secinfo_no_name_total","text":"<p>Total number of SECINFO_NO_NAME operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>secinfo_no_name.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>secinfo_no_name.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>secinfo_no_name_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>secinfo_no_name_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_secinfo_total","title":"node_nfs_secinfo_total","text":"<p>Total number of SECINFO operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>secinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>secinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>secinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>secinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>secinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>secinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_sequence_avg_latency","title":"node_nfs_sequence_avg_latency","text":"<p>Average latency in microseconds of SEQUENCE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>sequence.average_latency</code>Unit: microsecType: averageBase: sequence.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>sequence.average_latency</code>Unit: microsecType: averageBase: sequence.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>sequence_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: sequence_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>sequence_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: sequence_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_sequence_total","title":"node_nfs_sequence_total","text":"<p>Total number of SEQUENCE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>sequence.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>sequence.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>sequence_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>sequence_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_set_ssv_avg_latency","title":"node_nfs_set_ssv_avg_latency","text":"<p>Average latency in microseconds of SET_SSV operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>set_ssv.average_latency</code>Unit: microsecType: averageBase: set_ssv.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>set_ssv.average_latency</code>Unit: microsecType: averageBase: set_ssv.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>set_ssv_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: set_ssv_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>set_ssv_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: set_ssv_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_set_ssv_total","title":"node_nfs_set_ssv_total","text":"<p>Total number of SET_SSV operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>set_ssv.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>set_ssv.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>set_ssv_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>set_ssv_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_setattr_avg_latency","title":"node_nfs_setattr_avg_latency","text":"<p>Average latency in microseconds of SetAttr procedure requests. The counter keeps track of the average response time of SetAttr requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>setattr.average_latency</code>Unit: microsecType: averageBase: setattr.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>setattr.average_latency</code>Unit: microsecType: averageBase: setattr.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>setattr.average_latency</code>Unit: microsecType: averageBase: setattr.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>setattr.average_latency</code>Unit: microsecType: averageBase: setattr.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>setattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setattr_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>setattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setattr_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>setattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setattr_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>setattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setattr_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_setattr_total","title":"node_nfs_setattr_total","text":"<p>Total number of Setattr procedure requests. It is the total number of Setattr success and setattr error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>setattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>setattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>setattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>setattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>setattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>setattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>setattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>setattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_setclientid_avg_latency","title":"node_nfs_setclientid_avg_latency","text":"<p>Average latency in microseconds of SETCLIENTID procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>setclientid.average_latency</code>Unit: microsecType: averageBase: setclientid.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>setclientid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setclientid_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_setclientid_confirm_avg_latency","title":"node_nfs_setclientid_confirm_avg_latency","text":"<p>Average latency in microseconds of SETCLIENTID_CONFIRM procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>setclientid_confirm.average_latency</code>Unit: microsecType: averageBase: setclientid_confirm.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>setclientid_confirm_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setclientid_confirm_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_setclientid_confirm_total","title":"node_nfs_setclientid_confirm_total","text":"<p>Total number of SETCLIENTID_CONFIRM procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>setclientid_confirm.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>setclientid_confirm_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_setclientid_total","title":"node_nfs_setclientid_total","text":"<p>Total number of SETCLIENTID procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>setclientid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>setclientid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_symlink_avg_latency","title":"node_nfs_symlink_avg_latency","text":"<p>Average latency in microseconds of SymLink procedure requests. The counter keeps track of the average response time of SymLink requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>symlink.average_latency</code>Unit: microsecType: averageBase: symlink.total conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>symlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: symlink_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_symlink_total","title":"node_nfs_symlink_total","text":"<p>Total number SymLink of procedure requests. It is the total number of SymLink success and create SymLink requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>symlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>symlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml"},{"location":"ontap-metrics/#node_nfs_test_stateid_avg_latency","title":"node_nfs_test_stateid_avg_latency","text":"<p>Average latency in microseconds of TEST_STATEID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>test_stateid.average_latency</code>Unit: microsecType: averageBase: test_stateid.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>test_stateid.average_latency</code>Unit: microsecType: averageBase: test_stateid.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>test_stateid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: test_stateid_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>test_stateid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: test_stateid_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_test_stateid_total","title":"node_nfs_test_stateid_total","text":"<p>Total number of TEST_STATEID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>test_stateid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>test_stateid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>test_stateid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>test_stateid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_throughput","title":"node_nfs_throughput","text":"<p>Rate of NFSv3 data transfers in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>total.throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>total.throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>total.throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>nfsv3_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>nfs41_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>nfs42_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>nfs4_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml <p>The <code>node_nfs_throughput</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NFSv3 Frontend table NFSv3 Avg Throughput"},{"location":"ontap-metrics/#node_nfs_total_ops","title":"node_nfs_total_ops","text":"<p>Total number of NFSv3 procedure requests per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>nfsv3_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>total_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>total_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>total_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml <p>The <code>node_nfs_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NFSv3 Frontend timeseries NFSv3 Read and Write IOPs"},{"location":"ontap-metrics/#node_nfs_verify_avg_latency","title":"node_nfs_verify_avg_latency","text":"<p>Average latency in microseconds of VERIFY operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>verify.average_latency</code>Unit: microsecType: averageBase: verify.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>verify.average_latency</code>Unit: microsecType: averageBase: verify.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>verify.average_latency</code>Unit: microsecType: averageBase: verify.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>verify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: verify_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>verify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: verify_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>verify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: verify_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_verify_total","title":"node_nfs_verify_total","text":"<p>Total number of VERIFY operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>verify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>verify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>verify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>verify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>verify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>verify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nfs_want_delegation_avg_latency","title":"node_nfs_want_delegation_avg_latency","text":"<p>Average latency in microseconds of WANT_DELEGATION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>want_delegation.average_latency</code>Unit: microsecType: averageBase: want_delegation.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>want_delegation.average_latency</code>Unit: microsecType: averageBase: want_delegation.total conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>want_delegation_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: want_delegation_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>want_delegation_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: want_delegation_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_want_delegation_total","title":"node_nfs_want_delegation_total","text":"<p>Total number of WANT_DELEGATION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>want_delegation.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>want_delegation.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>want_delegation_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>want_delegation_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml"},{"location":"ontap-metrics/#node_nfs_write_avg_latency","title":"node_nfs_write_avg_latency","text":"<p>Average latency in microseconds of Write procedure requests. The counter keeps track of the average response time of Write requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>write.average_latency</code>Unit: microsecType: averageBase: write.total conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>write.average_latency</code>Unit: microsecType: averageBase: write.total conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>write.average_latency</code>Unit: microsecType: averageBase: write.total conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>write.average_latency</code>Unit: microsecType: averageBase: write.total conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>write_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: write_total conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>write_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: write_total conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>write_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: write_total conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>write_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: write_total conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml <p>The <code>node_nfs_write_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NFSv3 Frontend stat NFSv3 Avg Write Latency ONTAP: Node NFSv3 Frontend timeseries NFSv3 Read and Write Latency"},{"location":"ontap-metrics/#node_nfs_write_ops","title":"node_nfs_write_ops","text":"<p>Total observed NFSv3 write operations per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>nfsv3_write_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml <p>The <code>node_nfs_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NFSv3 Frontend table NFS Avg IOPS ONTAP: Node NFSv3 Frontend timeseries NFSv3 Read and Write IOPs"},{"location":"ontap-metrics/#node_nfs_write_throughput","title":"node_nfs_write_throughput","text":"<p>Rate of NFSv3 write data transfers in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>write_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>total.write_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>total.write_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>total.write_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>nfsv3_write_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>nfs41_write_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>nfs42_write_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>nfs4_write_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml <p>The <code>node_nfs_write_throughput</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NFSv3 Frontend table NFSv3 Avg Throughput ONTAP: Node NFSv3 Frontend timeseries NFSv3 Read and Write Throughput"},{"location":"ontap-metrics/#node_nfs_write_total","title":"node_nfs_write_total","text":"<p>Total number of Write procedure requests. It is the total number of write success and write error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3:node</code> <code>write.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41:node</code> <code>write.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42:node</code> <code>write.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2_node.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4:node</code> <code>write.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_node.yaml ZapiPerf <code>perf-object-get-instances nfsv3:node</code> <code>write_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1:node</code> <code>write_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2:node</code> <code>write_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml ZapiPerf <code>perf-object-get-instances nfsv4:node</code> <code>write_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml"},{"location":"ontap-metrics/#node_nvme_fc_data_recv","title":"node_nvme_fc_data_recv","text":"<p>NVMe/FC kilobytes (KB) received per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>nvme_fc_data_received</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>nvme_fc_data_recv</code>Unit: kb_per_secType: Base: conf/statperf/9.15.1/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>nvme_fc_data_recv</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.15.1/system_node.yaml"},{"location":"ontap-metrics/#node_nvme_fc_data_sent","title":"node_nvme_fc_data_sent","text":"<p>NVMe/FC kilobytes (KB) sent per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>nvme_fc_data_sent</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>nvme_fc_data_sent</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>nvme_fc_data_sent</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.15.1/system_node.yaml"},{"location":"ontap-metrics/#node_nvme_fc_ops","title":"node_nvme_fc_ops","text":"<p>NVMe/FC operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>nvme_fc_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>nvme_fc_ops</code>Unit: per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>nvme_fc_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.15.1/system_node.yaml"},{"location":"ontap-metrics/#node_nvmf_data_recv","title":"node_nvmf_data_recv","text":"<p>NVMe/FC kilobytes (KB) received per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>nvme_fc_data_received, 1</code>Unit: Type: Base: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>nvme_fc_data_recv, 1</code>Unit: Type: Base: conf/statperf/9.15.1/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>nvmf_data_recv</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_nvmf_data_sent","title":"node_nvmf_data_sent","text":"<p>NVMe/FC kilobytes (KB) sent per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>nvme_fc_data_sent, 1</code>Unit: Type: Base: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>nvme_fc_data_sent, 1</code>Unit: Type: Base: conf/statperf/9.15.1/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>nvmf_data_sent</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_nvmf_ops","title":"node_nvmf_ops","text":"<p>NVMe/FC operations per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>nvme_fc_ops, 1</code>Unit: Type: Base: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>nvme_fc_ops, 1</code>Unit: Type: Base: conf/statperf/9.15.1/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>nvmf_ops</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_nvmf_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Backend timeseries Protocol Backend IOPs"},{"location":"ontap-metrics/#node_other_data","title":"node_other_data","text":"<p>Other throughput in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>other_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>other_data</code>Unit: b_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>other_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_other_latency","title":"node_other_latency","text":"<p>Average latency for all other operations in the system in microseconds</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>other_latency</code>Unit: microsecType: averageBase: other_ops conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>other_latency</code>Unit: microsecType: Base: other_ops conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_other_ops","title":"node_other_ops","text":"<p>All other operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>other_ops</code>Unit: per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_read_data","title":"node_read_data","text":"<p>Read throughput in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>read_data</code>Unit: b_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_read_latency","title":"node_read_latency","text":"<p>Average latency for all read operations in the system in microseconds</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>read_latency</code>Unit: microsecType: Base: read_ops conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_read_ops","title":"node_read_ops","text":"<p>Read operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>read_ops</code>Unit: per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_ssd_data_read","title":"node_ssd_data_read","text":"<p>Number of SSD Disk kilobytes (KB) read per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>ssd_data_read</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>ssd_data_read</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>ssd_data_read</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_ssd_data_written","title":"node_ssd_data_written","text":"<p>Number of SSD Disk kilobytes (KB) written per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>ssd_data_written</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>ssd_data_written</code>Unit: kb_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>ssd_data_written</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_total_data","title":"node_total_data","text":"<p>Performance metric for total data throughput in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>total_data</code>Unit: b_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_total_latency","title":"node_total_latency","text":"<p>Average latency for all operations in the system in microseconds</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>total_latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>total_latency</code>Unit: microsecType: Base: total_ops conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>total_latency</code>Unit: microsecType: averageBase: total_ops conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_total_ops","title":"node_total_ops","text":"<p>Total number of operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>total_ops</code>Unit: per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml <p>The <code>node_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Power and Temperature stat Average IOPs/Watt ONTAP: Power Highlights stat Average IOPs/Watt"},{"location":"ontap-metrics/#node_uptime","title":"node_uptime","text":"<p>The total time, in seconds, that the node has been up.</p> API Endpoint Metric Template REST <code>api/cluster/nodes</code> <code>uptime</code> conf/rest/9.12.0/node.yaml ZAPI <code>system-node-get-iter</code> <code>node-details-info.node-uptime</code> conf/zapi/cdot/9.8.0/node.yaml <p>The <code>node_uptime</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Highlights table Node Details"},{"location":"ontap-metrics/#node_volume_avg_latency","title":"node_volume_avg_latency","text":"<p>Performance metric aggregated over all types of I/O operations. node_volume_avg_latency in microseconds is volume_avg_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.total conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>avg_latency</code>Unit: microsecType: averageBase: total_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT Cluster Metrics timeseries Top $TopResources Clusters by Max Node Latency ONTAP: Cluster Highlights timeseries Top $TopResources Nodes by Latency ONTAP: Node Highlights stat Average Latency ONTAP: Node Highlights timeseries Latency"},{"location":"ontap-metrics/#node_volume_nfs_access_latency","title":"node_volume_nfs_access_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_access_latency is volume_nfs_access_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.access.total_time</code>Unit: statistics.nfs_ops_raw.access.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_access_latency</code>Unit: microsecType: averageBase: nfs_access_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_access_ops","title":"node_volume_nfs_access_ops","text":"<p>Number of operations of the given type performed on this volume. node_volume_nfs_access_ops is volume_nfs_access_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.access.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_access_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_getattr_latency","title":"node_volume_nfs_getattr_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_getattr_latency is volume_nfs_getattr_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.getattr.total_time</code>Unit: statistics.nfs_ops_raw.getattr.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_getattr_latency</code>Unit: microsecType: averageBase: nfs_getattr_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_getattr_ops","title":"node_volume_nfs_getattr_ops","text":"<p>Number of operations of the given type performed on this volume. node_volume_nfs_getattr_ops is volume_nfs_getattr_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.getattr.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_getattr_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_lookup_latency","title":"node_volume_nfs_lookup_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_lookup_latency is volume_nfs_lookup_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.lookup.total_time</code>Unit: statistics.nfs_ops_raw.lookup.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_lookup_latency</code>Unit: microsecType: averageBase: nfs_lookup_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_lookup_ops","title":"node_volume_nfs_lookup_ops","text":"<p>Number of operations of the given type performed on this volume. node_volume_nfs_lookup_ops is volume_nfs_lookup_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.lookup.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_lookup_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_other_latency","title":"node_volume_nfs_other_latency","text":"<p>Average time for the WAFL filesystem to process other NFS operations to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency in microseconds (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_other_latency is volume_nfs_other_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_other_latency</code>Unit: microsecType: averageBase: nfs_other_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_other_ops","title":"node_volume_nfs_other_ops","text":"<p>Number of other NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_other_ops is volume_nfs_other_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_punch_hole_latency","title":"node_volume_nfs_punch_hole_latency","text":"<p>Average time for the WAFL filesystem to process NFS protocol hole-punch requests to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_punch_hole_latency in microseconds is volume_nfs_punch_hole_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_punch_hole_latency</code>Unit: microsecType: averageBase: nfs_punch_hole_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_punch_hole_ops","title":"node_volume_nfs_punch_hole_ops","text":"<p>Number of NFS hole-punch requests per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_punch_hole_ops is volume_nfs_punch_hole_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_punch_hole_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_read_latency","title":"node_volume_nfs_read_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_read_latency is volume_nfs_read_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.read.total_time</code>Unit: statistics.nfs_ops_raw.read.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_read_latency</code>Unit: microsecType: averageBase: nfs_read_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_read_ops","title":"node_volume_nfs_read_ops","text":"<p>Number of operations of the given type performed on this volume. node_volume_nfs_read_ops is volume_nfs_read_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.read.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_setattr_latency","title":"node_volume_nfs_setattr_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_setattr_latency is volume_nfs_setattr_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.setattr.total_time</code>Unit: statistics.nfs_ops_raw.setattr.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_setattr_latency</code>Unit: microsecType: averageBase: nfs_setattr_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_setattr_ops","title":"node_volume_nfs_setattr_ops","text":"<p>Number of operations of the given type performed on this volume. node_volume_nfs_setattr_ops is volume_nfs_setattr_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.setattr.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_setattr_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_total_ops","title":"node_volume_nfs_total_ops","text":"<p>Number of total NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_total_ops is volume_nfs_total_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_write_latency","title":"node_volume_nfs_write_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_write_latency is volume_nfs_write_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.write.total_time</code>Unit: statistics.nfs_ops_raw.write.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_write_latency</code>Unit: microsecType: averageBase: nfs_write_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_nfs_write_ops","title":"node_volume_nfs_write_ops","text":"<p>Number of operations of the given type performed on this volume. node_volume_nfs_write_ops is volume_nfs_write_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.write.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_other_data","title":"node_volume_other_data","text":"<p>Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on. node_volume_other_data is volume_other_data aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.other</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml"},{"location":"ontap-metrics/#node_volume_other_latency","title":"node_volume_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on. node_volume_other_latency in microseconds is volume_other_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.other conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_other_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Backend timeseries Average Latency"},{"location":"ontap-metrics/#node_volume_other_ops","title":"node_volume_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on. node_volume_other_ops is volume_other_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_other_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Backend timeseries IOPs"},{"location":"ontap-metrics/#node_volume_read_data","title":"node_volume_read_data","text":"<p>Performance metric for read I/O operations in bytes per seconds. node_volume_read_data is volume_read_data aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Highlights table Top $TopResources Nodes by Throughput ONTAP: Cluster Highlights timeseries Top $TopResources Nodes by Throughput ONTAP: Node Highlights timeseries Throughput ONTAP: Node Backend timeseries Throughput"},{"location":"ontap-metrics/#node_volume_read_latency","title":"node_volume_read_latency","text":"<p>Performance metric for read I/O operations. node_volume_read_latency in microseconds is volume_read_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.read conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Highlights table Top $TopResources Nodes by Read Latency ONTAP: Node Backend timeseries Average Latency"},{"location":"ontap-metrics/#node_volume_read_ops","title":"node_volume_read_ops","text":"<p>Performance metric for read I/O operations. node_volume_read_ops is volume_read_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Backend timeseries IOPs"},{"location":"ontap-metrics/#node_volume_total_data","title":"node_volume_total_data","text":"<p>Performance metric for the aggregated total data throughput in bytes per second across all volumes on a node. This metric is calculated by Harvest by summing the data read from and written to each volume on the node.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/volume.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_total_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Highlights stat Throughput"},{"location":"ontap-metrics/#node_volume_total_ops","title":"node_volume_total_ops","text":"<p>Performance metric aggregated over all types of I/O operations. node_volume_total_ops is volume_total_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Highlights table Top $TopResources Nodes by IOPs ONTAP: Cluster Highlights timeseries Top $TopResources Nodes by IOPs ONTAP: Node Highlights stat IOPs ONTAP: Node Highlights timeseries Top Average IOPs"},{"location":"ontap-metrics/#node_volume_write_data","title":"node_volume_write_data","text":"<p>Performance metric for write I/O operations in bytes per seconds. node_volume_write_data is volume_write_data aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Highlights table Top $TopResources Nodes by Throughput ONTAP: Cluster Highlights timeseries Top $TopResources Nodes by Throughput ONTAP: Node Highlights timeseries Throughput ONTAP: Node Backend timeseries Throughput"},{"location":"ontap-metrics/#node_volume_write_latency","title":"node_volume_write_latency","text":"<p>Performance metric for write I/O operations. node_volume_write_latency in microseconds is volume_write_latency aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.write</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.write conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Highlights table Top $TopResources Nodes by Write Latency ONTAP: Disk Disk Utilization timeseries Write Latency by Node ONTAP: Node Backend timeseries Average Latency"},{"location":"ontap-metrics/#node_volume_write_ops","title":"node_volume_write_ops","text":"<p>Performance metric for write I/O operations. node_volume_write_ops is volume_write_ops aggregated by <code>node</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>node_volume_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Backend timeseries IOPs"},{"location":"ontap-metrics/#node_write_data","title":"node_write_data","text":"<p>Write throughput in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>write_data</code>Unit: b_per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_write_latency","title":"node_write_latency","text":"<p>Average latency for all write operations in the system in microseconds</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>write_latency</code>Unit: microsecType: Base: write_ops conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#node_write_ops","title":"node_write_ops","text":"<p>Write operations per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/system:node</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/system_node.yaml StatPerf <code>system:node</code> <code>write_ops</code>Unit: per_secType: Base: conf/statperf/9.8.0/system_node.yaml ZapiPerf <code>perf-object-get-instances system:node</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/system_node.yaml"},{"location":"ontap-metrics/#ntpserver_labels","title":"ntpserver_labels","text":"<p>This metric provides information about NtpServer</p> API Endpoint Metric Template REST <code>api/cluster/ntp/servers</code> <code>Harvest generated</code> conf/rest/9.12.0/ntpserver.yaml ZAPI <code>ntp-server-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/ntpserver.yaml <p>The <code>ntpserver_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Cluster Compliant % ONTAP: Security Highlights piechart Cluster Compliant ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#nvm_mirror_write_throughput","title":"nvm_mirror_write_throughput","text":"<p>Mirror throughput in bytes per second.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances nvm_mirror</code> <code>write_throughput</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvm_mirror.yaml <p>The <code>nvm_mirror_write_throughput</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster NVM Mirror timeseries Write Throughput"},{"location":"ontap-metrics/#nvme_lif_avg_latency","title":"nvme_lif_avg_latency","text":"<p>Average latency in microseconds for NVMF operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>average_latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>avg_latency</code>Unit: microsecType: averageBase: total_ops conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NVMe/FC Frontend stat NVMe/FC Latency ONTAP: Node NVMe/FC Frontend timeseries NVMe/FC Average Latency by Port / LIF ONTAP: SVM NVMe/FC stat SVM NVMe/FC Average Latency ONTAP: SVM NVMe/FC timeseries SVM NVMe/FC Average Latency"},{"location":"ontap-metrics/#nvme_lif_avg_other_latency","title":"nvme_lif_avg_other_latency","text":"<p>Average latency in microseconds for operations other than read, write, compare or compare-and-write.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>average_other_latency</code>Unit: microsecType: averageBase: other_ops conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>avg_other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_avg_other_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NVMe/FC timeseries SVM NVMe/FC Average Latency"},{"location":"ontap-metrics/#nvme_lif_avg_read_latency","title":"nvme_lif_avg_read_latency","text":"<p>Average latency in microseconds for read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>average_read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>avg_read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_avg_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NVMe/FC timeseries SVM NVMe/FC Average Latency"},{"location":"ontap-metrics/#nvme_lif_avg_write_latency","title":"nvme_lif_avg_write_latency","text":"<p>Average latency in microseconds for write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>average_write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>avg_write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_avg_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NVMe/FC timeseries SVM NVMe/FC Average Latency"},{"location":"ontap-metrics/#nvme_lif_other_ops","title":"nvme_lif_other_ops","text":"<p>Number of operations that are not read, write, compare or compare-and-write.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_other_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NVMe/FC timeseries SVM NVMe/FC IOPs"},{"location":"ontap-metrics/#nvme_lif_read_data","title":"nvme_lif_read_data","text":"<p>Amount of data read from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM LIF timeseries Top $TopResources NVMe/FC LIFs by Send Throughput ONTAP: SVM NVMe/FC timeseries SVM NVMe/FC Throughput ONTAP: SVM NVMe/FC timeseries Top $TopResources SVM NVMe/FC LIFs by Send Throughput"},{"location":"ontap-metrics/#nvme_lif_read_ops","title":"nvme_lif_read_ops","text":"<p>Number of read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NVMe/FC timeseries SVM NVMe/FC IOPs"},{"location":"ontap-metrics/#nvme_lif_total_ops","title":"nvme_lif_total_ops","text":"<p>Total number of operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NVMe/FC Frontend stat NVMe/FC IOPs ONTAP: Node NVMe/FC Frontend timeseries NVMe/FC IOPs by Port / LIF ONTAP: SVM NVMe/FC stat SVM NVMe/FC IOPs"},{"location":"ontap-metrics/#nvme_lif_write_data","title":"nvme_lif_write_data","text":"<p>Amount of data written to the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node NVMe/FC Frontend timeseries NVMe/FC Throughput by Port / LIF ONTAP: SVM LIF timeseries Top $TopResources NVMe/FC LIFs by Receive Throughput ONTAP: SVM NVMe/FC stat SVM NVMe/FC Throughput ONTAP: SVM NVMe/FC timeseries SVM NVMe/FC Throughput ONTAP: SVM NVMe/FC timeseries Top $TopResources SVM NVMe/FC LIFs by Receive Throughput"},{"location":"ontap-metrics/#nvme_lif_write_ops","title":"nvme_lif_write_ops","text":"<p>Number of write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_lif</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nvmf_lif.yaml ZapiPerf <code>perf-object-get-instances nvmf_fc_lif</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml <p>The <code>nvme_lif_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NVMe/FC timeseries SVM NVMe/FC IOPs"},{"location":"ontap-metrics/#nvmf_rdma_port_avg_latency","title":"nvmf_rdma_port_avg_latency","text":"<p>Average latency in microseconds for NVMF operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>average_latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>avg_latency</code>Unit: microsecType: averageBase: total_ops conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_avg_other_latency","title":"nvmf_rdma_port_avg_other_latency","text":"<p>Average latency in microseconds for operations other than read, write, compare or compare-and-write</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>average_other_latency</code>Unit: microsecType: averageBase: other_ops conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>avg_other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_avg_read_latency","title":"nvmf_rdma_port_avg_read_latency","text":"<p>Average latency in microseconds for read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>average_read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>avg_read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_avg_write_latency","title":"nvmf_rdma_port_avg_write_latency","text":"<p>Average latency in microseconds for write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>average_write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>avg_write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_other_ops","title":"nvmf_rdma_port_other_ops","text":"<p>Number of operations that are not read, write, compare or compare-and-right.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_read_data","title":"nvmf_rdma_port_read_data","text":"<p>Amount of data read from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_read_ops","title":"nvmf_rdma_port_read_ops","text":"<p>Number of read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_total_data","title":"nvmf_rdma_port_total_data","text":"<p>Amount of NVMF traffic to and from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_total_ops","title":"nvmf_rdma_port_total_ops","text":"<p>Total number of operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_write_data","title":"nvmf_rdma_port_write_data","text":"<p>Amount of data written to the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_rdma_port_write_ops","title":"nvmf_rdma_port_write_ops","text":"<p>Number of write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_rdma_port</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/nvmf_rdma_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_rdma_port</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_avg_latency","title":"nvmf_tcp_port_avg_latency","text":"<p>Average latency in microseconds for NVMF operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>average_latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>avg_latency</code>Unit: microsecType: averageBase: total_ops conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_avg_other_latency","title":"nvmf_tcp_port_avg_other_latency","text":"<p>Average latency in microseconds for operations other than read, write, compare or compare-and-write</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>average_other_latency</code>Unit: microsecType: averageBase: other_ops conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>avg_other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_avg_read_latency","title":"nvmf_tcp_port_avg_read_latency","text":"<p>Average latency in microseconds for read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>average_read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>avg_read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_avg_write_latency","title":"nvmf_tcp_port_avg_write_latency","text":"<p>Average latency in microseconds for write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>average_write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>avg_write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_other_ops","title":"nvmf_tcp_port_other_ops","text":"<p>Number of operations that are not read, write, compare or compare-and-write.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_read_data","title":"nvmf_tcp_port_read_data","text":"<p>Amount of data read from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_read_ops","title":"nvmf_tcp_port_read_ops","text":"<p>Number of read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_total_data","title":"nvmf_tcp_port_total_data","text":"<p>Amount of NVMF traffic to and from the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_total_ops","title":"nvmf_tcp_port_total_ops","text":"<p>Total number of operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_write_data","title":"nvmf_tcp_port_write_data","text":"<p>Amount of data written to the storage system in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#nvmf_tcp_port_write_ops","title":"nvmf_tcp_port_write_ops","text":"<p>Number of write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/nvmf_tcp_port</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/nvmf_tcp_port.yaml ZapiPerf <code>perf-object-get-instances nvmf_tcp_port</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml"},{"location":"ontap-metrics/#ontaps3_labels","title":"ontaps3_labels","text":"<p>This metric provides information about OntapS3</p> API Endpoint Metric Template REST <code>api/protocols/s3/buckets</code> <code>Harvest generated</code> conf/rest/9.7.0/ontap_s3.yaml <p>The <code>ontaps3_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Bucket protection stat Total Buckets ONTAP: Data Protection Bucket protection stat Unprotected Buckets ONTAP: Data Protection Bucket protection stat Not Backed up to Cloud ONTAP: Data Protection Bucket protection table Buckets ONTAP: Datacenter Highlights table Object Count ONTAP: S3 Object Storage Highlights table Bucket Overview"},{"location":"ontap-metrics/#ontaps3_logical_used_size","title":"ontaps3_logical_used_size","text":"<p>Specifies the bucket logical used size up to this point. This field cannot be specified using a POST or PATCH method.</p> API Endpoint Metric Template REST <code>api/protocols/s3/buckets</code> <code>logical_used_size</code> conf/rest/9.7.0/ontap_s3.yaml <p>The <code>ontaps3_logical_used_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Bucket protection table Buckets ONTAP: S3 Object Storage Highlights table Bucket Overview ONTAP: S3 Object Storage Highlights timeseries Top $TopResources Buckets by Used Size"},{"location":"ontap-metrics/#ontaps3_object_count","title":"ontaps3_object_count","text":"API Endpoint Metric Template REST <code>api/private/cli/vserver/object-store-server/bucket</code> <code>object_count</code> conf/rest/9.7.0/ontap_s3.yaml <p>The <code>ontaps3_object_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage Highlights table Bucket Overview"},{"location":"ontap-metrics/#ontaps3_policy_labels","title":"ontaps3_policy_labels","text":"<p>This metric provides information about OntapS3Policy</p> API Endpoint Metric Template REST <code>api/private/cli/vserver/object-store-server/bucket/policy</code> <code>Harvest generated</code> conf/rest/9.7.0/ontap_s3_policy.yaml <p>The <code>ontaps3_policy_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage Highlights table Bucket Permission"},{"location":"ontap-metrics/#ontaps3_size","title":"ontaps3_size","text":"<p>Specifies the bucket size in bytes; ranges from 190MB to 62PB.</p> API Endpoint Metric Template REST <code>api/protocols/s3/buckets</code> <code>size</code> conf/rest/9.7.0/ontap_s3.yaml <p>The <code>ontaps3_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Bucket protection table Buckets ONTAP: S3 Object Storage Highlights table Bucket Overview"},{"location":"ontap-metrics/#ontaps3_svm_abort_multipart_upload_failed","title":"ontaps3_svm_abort_multipart_upload_failed","text":"<p>Number of failed Abort Multipart Upload operations. ontaps3_svm_abort_multipart_upload_failed is ontaps3_svm_abort_multipart_upload_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>abort_multipart_upload_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>abort_multipart_upload_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_abort_multipart_upload_failed_client_close","title":"ontaps3_svm_abort_multipart_upload_failed_client_close","text":"<p>Number of times Abort Multipart Upload operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_abort_multipart_upload_failed_client_close is ontaps3_svm_abort_multipart_upload_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>abort_multipart_upload_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>abort_multipart_upload_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_abort_multipart_upload_latency","title":"ontaps3_svm_abort_multipart_upload_latency","text":"<p>Average latency in microseconds for Abort Multipart Upload operations. ontaps3_svm_abort_multipart_upload_latency is ontaps3_svm_abort_multipart_upload_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>abort_multipart_upload_latency</code>Unit: microsecType: averageBase: abort_multipart_upload_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>abort_multipart_upload_latency</code>Unit: microsecType: average,no-zero-valuesBase: abort_multipart_upload_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_abort_multipart_upload_rate","title":"ontaps3_svm_abort_multipart_upload_rate","text":"<p>Number of Abort Multipart Upload operations per second. ontaps3_svm_abort_multipart_upload_rate is ontaps3_svm_abort_multipart_upload_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>abort_multipart_upload_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>abort_multipart_upload_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_abort_multipart_upload_total","title":"ontaps3_svm_abort_multipart_upload_total","text":"<p>Number of Abort Multipart Upload operations. ontaps3_svm_abort_multipart_upload_total is ontaps3_svm_abort_multipart_upload_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>abort_multipart_upload_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>abort_multipart_upload_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_allow_access","title":"ontaps3_svm_allow_access","text":"<p>Number of times access was allowed. ontaps3_svm_allow_access is ontaps3_svm_allow_access aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>allow_access</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>allow_access</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_anonymous_access","title":"ontaps3_svm_anonymous_access","text":"<p>Number of times anonymous access was allowed. ontaps3_svm_anonymous_access is ontaps3_svm_anonymous_access aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>anonymous_access</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>anonymous_access</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_anonymous_deny_access","title":"ontaps3_svm_anonymous_deny_access","text":"<p>Number of times anonymous access was denied. ontaps3_svm_anonymous_deny_access is ontaps3_svm_anonymous_deny_access aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>anonymous_deny_access</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>anonymous_deny_access</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_authentication_failures","title":"ontaps3_svm_authentication_failures","text":"<p>Number of authentication failures. ontaps3_svm_authentication_failures is ontaps3_svm_authentication_failures aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>authentication_failures</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>authentication_failures</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_chunked_upload_reqs","title":"ontaps3_svm_chunked_upload_reqs","text":"<p>Total number of object store server chunked object upload requests. ontaps3_svm_chunked_upload_reqs is ontaps3_svm_chunked_upload_reqs aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>chunked_upload_requests</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>chunked_upload_reqs</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_complete_multipart_upload_failed","title":"ontaps3_svm_complete_multipart_upload_failed","text":"<p>Number of failed Complete Multipart Upload operations. ontaps3_svm_complete_multipart_upload_failed is ontaps3_svm_complete_multipart_upload_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>complete_multipart_upload_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>complete_multipart_upload_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_complete_multipart_upload_failed_client_close","title":"ontaps3_svm_complete_multipart_upload_failed_client_close","text":"<p>Number of times Complete Multipart Upload operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_complete_multipart_upload_failed_client_close is ontaps3_svm_complete_multipart_upload_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>complete_multipart_upload_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>complete_multipart_upload_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_complete_multipart_upload_latency","title":"ontaps3_svm_complete_multipart_upload_latency","text":"<p>Average latency in microseconds for Complete Multipart Upload operations. ontaps3_svm_complete_multipart_upload_latency is ontaps3_svm_complete_multipart_upload_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>complete_multipart_upload_latency</code>Unit: microsecType: averageBase: complete_multipart_upload_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>complete_multipart_upload_latency</code>Unit: microsecType: average,no-zero-valuesBase: complete_multipart_upload_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_complete_multipart_upload_rate","title":"ontaps3_svm_complete_multipart_upload_rate","text":"<p>Number of Complete Multipart Upload operations per second. ontaps3_svm_complete_multipart_upload_rate is ontaps3_svm_complete_multipart_upload_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>complete_multipart_upload_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>complete_multipart_upload_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_complete_multipart_upload_total","title":"ontaps3_svm_complete_multipart_upload_total","text":"<p>Number of Complete Multipart Upload operations. ontaps3_svm_complete_multipart_upload_total is ontaps3_svm_complete_multipart_upload_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>complete_multipart_upload_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>complete_multipart_upload_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_connected_connections","title":"ontaps3_svm_connected_connections","text":"<p>Number of object store server connections currently established. ontaps3_svm_connected_connections is ontaps3_svm_connected_connections aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>connected_connections</code>Unit: noneType: rawBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>connected_connections</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_connections","title":"ontaps3_svm_connections","text":"<p>Total number of object store server connections. ontaps3_svm_connections is ontaps3_svm_connections aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>connections</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>connections</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_connections</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM table Requests &amp; Connections stats"},{"location":"ontap-metrics/#ontaps3_svm_create_bucket_failed","title":"ontaps3_svm_create_bucket_failed","text":"<p>Number of failed Create Bucket operations. ontaps3_svm_create_bucket_failed is ontaps3_svm_create_bucket_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>create_bucket_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>create_bucket_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_create_bucket_failed_client_close","title":"ontaps3_svm_create_bucket_failed_client_close","text":"<p>Number of times Create Bucket operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_create_bucket_failed_client_close is ontaps3_svm_create_bucket_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>create_bucket_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>create_bucket_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_create_bucket_latency","title":"ontaps3_svm_create_bucket_latency","text":"<p>Average latency in microseconds for Create Bucket operations. ontaps3_svm_create_bucket_latency is ontaps3_svm_create_bucket_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>create_bucket_latency</code>Unit: microsecType: averageBase: create_bucket_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>create_bucket_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_bucket_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_create_bucket_rate","title":"ontaps3_svm_create_bucket_rate","text":"<p>Number of Create Bucket operations per second. ontaps3_svm_create_bucket_rate is ontaps3_svm_create_bucket_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>create_bucket_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>create_bucket_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_create_bucket_total","title":"ontaps3_svm_create_bucket_total","text":"<p>Number of Create Bucket operations. ontaps3_svm_create_bucket_total is ontaps3_svm_create_bucket_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>create_bucket_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>create_bucket_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_default_deny_access","title":"ontaps3_svm_default_deny_access","text":"<p>Number of times access was denied by default and not through any policy statement. ontaps3_svm_default_deny_access is ontaps3_svm_default_deny_access aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>default_deny_access</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>default_deny_access</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_bucket_failed","title":"ontaps3_svm_delete_bucket_failed","text":"<p>Number of failed Delete Bucket operations. ontaps3_svm_delete_bucket_failed is ontaps3_svm_delete_bucket_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_bucket_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_bucket_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_bucket_failed_client_close","title":"ontaps3_svm_delete_bucket_failed_client_close","text":"<p>Number of times Delete Bucket operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_delete_bucket_failed_client_close is ontaps3_svm_delete_bucket_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_bucket_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_bucket_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_bucket_latency","title":"ontaps3_svm_delete_bucket_latency","text":"<p>Average latency in microseconds for Delete Bucket operations. ontaps3_svm_delete_bucket_latency is ontaps3_svm_delete_bucket_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_bucket_latency</code>Unit: microsecType: averageBase: delete_bucket_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_bucket_latency</code>Unit: microsecType: average,no-zero-valuesBase: delete_bucket_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_bucket_rate","title":"ontaps3_svm_delete_bucket_rate","text":"<p>Number of Delete Bucket operations per second. ontaps3_svm_delete_bucket_rate is ontaps3_svm_delete_bucket_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_bucket_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_bucket_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_bucket_total","title":"ontaps3_svm_delete_bucket_total","text":"<p>Number of Delete Bucket operations. ontaps3_svm_delete_bucket_total is ontaps3_svm_delete_bucket_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_bucket_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_bucket_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_failed","title":"ontaps3_svm_delete_object_failed","text":"<p>Number of failed DELETE object operations. ontaps3_svm_delete_object_failed is ontaps3_svm_delete_object_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_failed_client_close","title":"ontaps3_svm_delete_object_failed_client_close","text":"<p>Number of times DELETE object operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_delete_object_failed_client_close is ontaps3_svm_delete_object_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_latency","title":"ontaps3_svm_delete_object_latency","text":"<p>Average latency in microseconds for DELETE object operations. ontaps3_svm_delete_object_latency is ontaps3_svm_delete_object_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_latency</code>Unit: microsecType: averageBase: delete_object_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_latency</code>Unit: microsecType: average,no-zero-valuesBase: delete_object_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_delete_object_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Latency"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_rate","title":"ontaps3_svm_delete_object_rate","text":"<p>Number of DELETE object operations per second. ontaps3_svm_delete_object_rate is ontaps3_svm_delete_object_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_delete_object_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Rate"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_tagging_failed","title":"ontaps3_svm_delete_object_tagging_failed","text":"<p>Number of failed DELETE object tagging operations. ontaps3_svm_delete_object_tagging_failed is ontaps3_svm_delete_object_tagging_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_tagging_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_tagging_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_tagging_failed_client_close","title":"ontaps3_svm_delete_object_tagging_failed_client_close","text":"<p>Number of times DELETE object tagging operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_delete_object_tagging_failed_client_close is ontaps3_svm_delete_object_tagging_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_tagging_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_tagging_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_tagging_latency","title":"ontaps3_svm_delete_object_tagging_latency","text":"<p>Average latency in microseconds for DELETE object tagging operations. ontaps3_svm_delete_object_tagging_latency is ontaps3_svm_delete_object_tagging_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_tagging_latency</code>Unit: microsecType: averageBase: delete_object_tagging_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_tagging_latency</code>Unit: microsecType: average,no-zero-valuesBase: delete_object_tagging_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_tagging_rate","title":"ontaps3_svm_delete_object_tagging_rate","text":"<p>Number of DELETE object tagging operations per second. ontaps3_svm_delete_object_tagging_rate is ontaps3_svm_delete_object_tagging_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_tagging_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_tagging_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_tagging_total","title":"ontaps3_svm_delete_object_tagging_total","text":"<p>Number of DELETE object tagging operations. ontaps3_svm_delete_object_tagging_total is ontaps3_svm_delete_object_tagging_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_tagging_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_tagging_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_delete_object_total","title":"ontaps3_svm_delete_object_total","text":"<p>Number of DELETE object operations. ontaps3_svm_delete_object_total is ontaps3_svm_delete_object_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>delete_object_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>delete_object_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_delete_object_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Operations"},{"location":"ontap-metrics/#ontaps3_svm_explicit_deny_access","title":"ontaps3_svm_explicit_deny_access","text":"<p>Number of times access was denied explicitly by a policy statement. ontaps3_svm_explicit_deny_access is ontaps3_svm_explicit_deny_access aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>explicit_deny_access</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>explicit_deny_access</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_bucket_acl_failed","title":"ontaps3_svm_get_bucket_acl_failed","text":"<p>Number of failed GET Bucket ACL operations. ontaps3_svm_get_bucket_acl_failed is ontaps3_svm_get_bucket_acl_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_bucket_acl_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_bucket_acl_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_bucket_acl_total","title":"ontaps3_svm_get_bucket_acl_total","text":"<p>Number of GET Bucket ACL operations. ontaps3_svm_get_bucket_acl_total is ontaps3_svm_get_bucket_acl_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_bucket_acl_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_bucket_acl_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_bucket_versioning_failed","title":"ontaps3_svm_get_bucket_versioning_failed","text":"<p>Number of failed Get Bucket Versioning operations. ontaps3_svm_get_bucket_versioning_failed is ontaps3_svm_get_bucket_versioning_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_bucket_versioning_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_bucket_versioning_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_bucket_versioning_total","title":"ontaps3_svm_get_bucket_versioning_total","text":"<p>Number of Get Bucket Versioning operations. ontaps3_svm_get_bucket_versioning_total is ontaps3_svm_get_bucket_versioning_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_bucket_versioning_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_bucket_versioning_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_data","title":"ontaps3_svm_get_data","text":"<p>Rate of GET object data transfers in bytes per second. ontaps3_svm_get_data is ontaps3_svm_get_data aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_data</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_get_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Data Transfer"},{"location":"ontap-metrics/#ontaps3_svm_get_object_acl_failed","title":"ontaps3_svm_get_object_acl_failed","text":"<p>Number of failed GET Object ACL operations. ontaps3_svm_get_object_acl_failed is ontaps3_svm_get_object_acl_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_acl_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_acl_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_acl_total","title":"ontaps3_svm_get_object_acl_total","text":"<p>Number of GET Object ACL operations. ontaps3_svm_get_object_acl_total is ontaps3_svm_get_object_acl_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_acl_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_acl_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_failed","title":"ontaps3_svm_get_object_failed","text":"<p>Number of failed GET object operations. ontaps3_svm_get_object_failed is ontaps3_svm_get_object_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_failed_client_close","title":"ontaps3_svm_get_object_failed_client_close","text":"<p>Number of times GET object operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_get_object_failed_client_close is ontaps3_svm_get_object_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_lastbyte_latency","title":"ontaps3_svm_get_object_lastbyte_latency","text":"<p>Average last-byte latency in microseconds for GET object operations. ontaps3_svm_get_object_lastbyte_latency is ontaps3_svm_get_object_lastbyte_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_lastbyte_latency</code>Unit: microsecType: averageBase: get_object_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_lastbyte_latency</code>Unit: microsecType: average,no-zero-valuesBase: get_object_lastbyte_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_latency","title":"ontaps3_svm_get_object_latency","text":"<p>Average first-byte latency in microseconds for GET object operations. ontaps3_svm_get_object_latency is ontaps3_svm_get_object_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_latency</code>Unit: microsecType: averageBase: get_object_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_latency</code>Unit: microsecType: average,no-zero-valuesBase: get_object_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_get_object_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Latency"},{"location":"ontap-metrics/#ontaps3_svm_get_object_rate","title":"ontaps3_svm_get_object_rate","text":"<p>Number of GET object operations per second. ontaps3_svm_get_object_rate is ontaps3_svm_get_object_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_get_object_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Rate"},{"location":"ontap-metrics/#ontaps3_svm_get_object_tagging_failed","title":"ontaps3_svm_get_object_tagging_failed","text":"<p>Number of failed GET object tagging operations. ontaps3_svm_get_object_tagging_failed is ontaps3_svm_get_object_tagging_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_tagging_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_tagging_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_tagging_failed_client_close","title":"ontaps3_svm_get_object_tagging_failed_client_close","text":"<p>Number of times GET object tagging operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_get_object_tagging_failed_client_close is ontaps3_svm_get_object_tagging_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_tagging_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_tagging_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_tagging_latency","title":"ontaps3_svm_get_object_tagging_latency","text":"<p>Average latency in microseconds for GET object tagging operations. ontaps3_svm_get_object_tagging_latency is ontaps3_svm_get_object_tagging_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_tagging_latency</code>Unit: microsecType: averageBase: get_object_tagging_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_tagging_latency</code>Unit: microsecType: average,no-zero-valuesBase: get_object_tagging_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_tagging_rate","title":"ontaps3_svm_get_object_tagging_rate","text":"<p>Number of GET object tagging operations per second. ontaps3_svm_get_object_tagging_rate is ontaps3_svm_get_object_tagging_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_tagging_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_tagging_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_tagging_total","title":"ontaps3_svm_get_object_tagging_total","text":"<p>Number of GET object tagging operations. ontaps3_svm_get_object_tagging_total is ontaps3_svm_get_object_tagging_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_tagging_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_tagging_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_get_object_total","title":"ontaps3_svm_get_object_total","text":"<p>Number of GET object operations. ontaps3_svm_get_object_total is ontaps3_svm_get_object_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>get_object_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>get_object_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_get_object_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Operations"},{"location":"ontap-metrics/#ontaps3_svm_group_policy_evaluated","title":"ontaps3_svm_group_policy_evaluated","text":"<p>Number of times group policies were evaluated. ontaps3_svm_group_policy_evaluated is ontaps3_svm_group_policy_evaluated aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>group_policy_evaluated</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>group_policy_evaluated</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_head_bucket_failed","title":"ontaps3_svm_head_bucket_failed","text":"<p>Number of failed HEAD bucket operations. ontaps3_svm_head_bucket_failed is ontaps3_svm_head_bucket_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_bucket_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_bucket_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_head_bucket_failed_client_close","title":"ontaps3_svm_head_bucket_failed_client_close","text":"<p>Number of times HEAD bucket operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_head_bucket_failed_client_close is ontaps3_svm_head_bucket_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_bucket_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_bucket_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_head_bucket_latency","title":"ontaps3_svm_head_bucket_latency","text":"<p>Average latency in microseconds for HEAD bucket operations. ontaps3_svm_head_bucket_latency is ontaps3_svm_head_bucket_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_bucket_latency</code>Unit: microsecType: averageBase: head_bucket_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_bucket_latency</code>Unit: microsecType: average,no-zero-valuesBase: head_bucket_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_head_bucket_rate","title":"ontaps3_svm_head_bucket_rate","text":"<p>Number of HEAD bucket operations per second. ontaps3_svm_head_bucket_rate is ontaps3_svm_head_bucket_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_bucket_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_bucket_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_head_bucket_total","title":"ontaps3_svm_head_bucket_total","text":"<p>Number of HEAD bucket operations. ontaps3_svm_head_bucket_total is ontaps3_svm_head_bucket_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_bucket_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_bucket_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_head_object_failed","title":"ontaps3_svm_head_object_failed","text":"<p>Number of failed HEAD Object operations. ontaps3_svm_head_object_failed is ontaps3_svm_head_object_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_object_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_object_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_head_object_failed_client_close","title":"ontaps3_svm_head_object_failed_client_close","text":"<p>Number of times HEAD object operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_head_object_failed_client_close is ontaps3_svm_head_object_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_object_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_object_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_head_object_latency","title":"ontaps3_svm_head_object_latency","text":"<p>Average latency in microseconds for HEAD object operations. ontaps3_svm_head_object_latency is ontaps3_svm_head_object_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_object_latency</code>Unit: microsecType: averageBase: head_object_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_object_latency</code>Unit: microsecType: average,no-zero-valuesBase: head_object_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_head_object_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Latency"},{"location":"ontap-metrics/#ontaps3_svm_head_object_rate","title":"ontaps3_svm_head_object_rate","text":"<p>Number of HEAD Object operations per second. ontaps3_svm_head_object_rate is ontaps3_svm_head_object_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_object_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_object_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_head_object_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Rate"},{"location":"ontap-metrics/#ontaps3_svm_head_object_total","title":"ontaps3_svm_head_object_total","text":"<p>Number of HEAD Object operations. ontaps3_svm_head_object_total is ontaps3_svm_head_object_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>head_object_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>head_object_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_head_object_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Operations"},{"location":"ontap-metrics/#ontaps3_svm_initiate_multipart_upload_failed","title":"ontaps3_svm_initiate_multipart_upload_failed","text":"<p>Number of failed Initiate Multipart Upload operations. ontaps3_svm_initiate_multipart_upload_failed is ontaps3_svm_initiate_multipart_upload_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>initiate_multipart_upload_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>initiate_multipart_upload_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_initiate_multipart_upload_failed_client_close","title":"ontaps3_svm_initiate_multipart_upload_failed_client_close","text":"<p>Number of times Initiate Multipart Upload operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_initiate_multipart_upload_failed_client_close is ontaps3_svm_initiate_multipart_upload_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>initiate_multipart_upload_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>initiate_multipart_upload_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_initiate_multipart_upload_latency","title":"ontaps3_svm_initiate_multipart_upload_latency","text":"<p>Average latency in microseconds for Initiate Multipart Upload operations. ontaps3_svm_initiate_multipart_upload_latency is ontaps3_svm_initiate_multipart_upload_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>initiate_multipart_upload_latency</code>Unit: microsecType: averageBase: initiate_multipart_upload_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>initiate_multipart_upload_latency</code>Unit: microsecType: average,no-zero-valuesBase: initiate_multipart_upload_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_initiate_multipart_upload_rate","title":"ontaps3_svm_initiate_multipart_upload_rate","text":"<p>Number of Initiate Multipart Upload operations per second. ontaps3_svm_initiate_multipart_upload_rate is ontaps3_svm_initiate_multipart_upload_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>initiate_multipart_upload_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>initiate_multipart_upload_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_initiate_multipart_upload_total","title":"ontaps3_svm_initiate_multipart_upload_total","text":"<p>Number of Initiate Multipart Upload operations. ontaps3_svm_initiate_multipart_upload_total is ontaps3_svm_initiate_multipart_upload_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>initiate_multipart_upload_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>initiate_multipart_upload_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_input_flow_control_entry","title":"ontaps3_svm_input_flow_control_entry","text":"<p>Number of times input flow control was entered. ontaps3_svm_input_flow_control_entry is ontaps3_svm_input_flow_control_entry aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>input_flow_control_entry</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>input_flow_control_entry</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_input_flow_control_exit","title":"ontaps3_svm_input_flow_control_exit","text":"<p>Number of times input flow control was exited. ontaps3_svm_input_flow_control_exit is ontaps3_svm_input_flow_control_exit aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>input_flow_control_exit</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>input_flow_control_exit</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_buckets_failed","title":"ontaps3_svm_list_buckets_failed","text":"<p>Number of failed LIST Buckets operations. ontaps3_svm_list_buckets_failed is ontaps3_svm_list_buckets_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_buckets_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_buckets_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_buckets_failed_client_close","title":"ontaps3_svm_list_buckets_failed_client_close","text":"<p>Number of times LIST Bucket operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_list_buckets_failed_client_close is ontaps3_svm_list_buckets_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_buckets_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_buckets_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_buckets_latency","title":"ontaps3_svm_list_buckets_latency","text":"<p>Average latency in microseconds for LIST Buckets operations. ontaps3_svm_list_buckets_latency is ontaps3_svm_list_buckets_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_buckets_latency</code>Unit: microsecType: averageBase: list_buckets_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_buckets_latency</code>Unit: microsecType: average,no-zero-valuesBase: head_object_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_buckets_rate","title":"ontaps3_svm_list_buckets_rate","text":"<p>Number of LIST Buckets operations per second. ontaps3_svm_list_buckets_rate is ontaps3_svm_list_buckets_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_buckets_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_buckets_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_buckets_total","title":"ontaps3_svm_list_buckets_total","text":"<p>Number of LIST Buckets operations. ontaps3_svm_list_buckets_total is ontaps3_svm_list_buckets_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_buckets_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_buckets_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_object_versions_failed","title":"ontaps3_svm_list_object_versions_failed","text":"<p>Number of failed LIST object versions operations. ontaps3_svm_list_object_versions_failed is ontaps3_svm_list_object_versions_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_object_versions_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_object_versions_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_object_versions_failed_client_close","title":"ontaps3_svm_list_object_versions_failed_client_close","text":"<p>Number of times LIST object versions operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_list_object_versions_failed_client_close is ontaps3_svm_list_object_versions_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_object_versions_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_object_versions_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_object_versions_latency","title":"ontaps3_svm_list_object_versions_latency","text":"<p>Average latency in microseconds for LIST Object versions operations. ontaps3_svm_list_object_versions_latency is ontaps3_svm_list_object_versions_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_object_versions_latency</code>Unit: microsecType: averageBase: list_object_versions_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_object_versions_latency</code>Unit: microsecType: average,no-zero-valuesBase: list_object_versions_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_object_versions_rate","title":"ontaps3_svm_list_object_versions_rate","text":"<p>Number of LIST Object Versions operations per second. ontaps3_svm_list_object_versions_rate is ontaps3_svm_list_object_versions_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_object_versions_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_object_versions_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_object_versions_total","title":"ontaps3_svm_list_object_versions_total","text":"<p>Number of LIST Object Versions operations. ontaps3_svm_list_object_versions_total is ontaps3_svm_list_object_versions_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_object_versions_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_object_versions_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_objects_failed","title":"ontaps3_svm_list_objects_failed","text":"<p>Number of failed LIST objects operations. ontaps3_svm_list_objects_failed is ontaps3_svm_list_objects_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_objects_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_objects_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_objects_failed_client_close","title":"ontaps3_svm_list_objects_failed_client_close","text":"<p>Number of times LIST objects operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_list_objects_failed_client_close is ontaps3_svm_list_objects_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_objects_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_objects_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_objects_latency","title":"ontaps3_svm_list_objects_latency","text":"<p>Average latency in microseconds for LIST Objects operations. ontaps3_svm_list_objects_latency is ontaps3_svm_list_objects_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_objects_latency</code>Unit: microsecType: averageBase: list_objects_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_objects_latency</code>Unit: microsecType: average,no-zero-valuesBase: list_objects_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_objects_rate","title":"ontaps3_svm_list_objects_rate","text":"<p>Number of LIST Objects operations per second. ontaps3_svm_list_objects_rate is ontaps3_svm_list_objects_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_objects_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_objects_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_objects_total","title":"ontaps3_svm_list_objects_total","text":"<p>Number of LIST Objects operations. ontaps3_svm_list_objects_total is ontaps3_svm_list_objects_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_objects_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_objects_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_uploads_failed","title":"ontaps3_svm_list_uploads_failed","text":"<p>Number of failed LIST Upload operations. ontaps3_svm_list_uploads_failed is ontaps3_svm_list_uploads_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_uploads_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_uploads_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_uploads_failed_client_close","title":"ontaps3_svm_list_uploads_failed_client_close","text":"<p>Number of times LIST Upload operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_list_uploads_failed_client_close is ontaps3_svm_list_uploads_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_uploads_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_uploads_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_uploads_latency","title":"ontaps3_svm_list_uploads_latency","text":"<p>Average latency in microseconds for LIST Upload operations. ontaps3_svm_list_uploads_latency is ontaps3_svm_list_uploads_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_uploads_latency</code>Unit: microsecType: averageBase: list_uploads_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_uploads_latency</code>Unit: microsecType: average,no-zero-valuesBase: list_uploads_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_uploads_rate","title":"ontaps3_svm_list_uploads_rate","text":"<p>Number of LIST Upload operations per second. ontaps3_svm_list_uploads_rate is ontaps3_svm_list_uploads_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_uploads_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_uploads_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_list_uploads_total","title":"ontaps3_svm_list_uploads_total","text":"<p>Number of LIST Upload operations. ontaps3_svm_list_uploads_total is ontaps3_svm_list_uploads_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>list_uploads_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>list_uploads_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_max_cmds_per_connection","title":"ontaps3_svm_max_cmds_per_connection","text":"<p>Maximum commands pipelined at any instance on a connection. ontaps3_svm_max_cmds_per_connection is ontaps3_svm_max_cmds_per_connection aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>maximum_commands_per_connection</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>max_cmds_per_connection</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_max_cmds_per_connection</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM table Requests &amp; Connections stats"},{"location":"ontap-metrics/#ontaps3_svm_max_connected_connections","title":"ontaps3_svm_max_connected_connections","text":"<p>Maximum number of object store server connections established at one time. ontaps3_svm_max_connected_connections is ontaps3_svm_max_connected_connections aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>maximum_connected_connections</code>Unit: noneType: rawBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>max_connected_connections</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_max_connected_connections</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM table Requests &amp; Connections stats"},{"location":"ontap-metrics/#ontaps3_svm_max_requests_outstanding","title":"ontaps3_svm_max_requests_outstanding","text":"<p>Maximum number of object store server requests in process at one time. ontaps3_svm_max_requests_outstanding is ontaps3_svm_max_requests_outstanding aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>maximum_requests_outstanding</code>Unit: noneType: rawBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>max_requests_outstanding</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_max_requests_outstanding</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM table Requests &amp; Connections stats"},{"location":"ontap-metrics/#ontaps3_svm_multi_delete_reqs","title":"ontaps3_svm_multi_delete_reqs","text":"<p>Total number of object store server multiple object delete requests. ontaps3_svm_multi_delete_reqs is ontaps3_svm_multi_delete_reqs aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>multiple_delete_requests</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>multi_delete_reqs</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_output_flow_control_entry","title":"ontaps3_svm_output_flow_control_entry","text":"<p>Number of output flow control was entered. ontaps3_svm_output_flow_control_entry is ontaps3_svm_output_flow_control_entry aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>output_flow_control_entry</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>output_flow_control_entry</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_output_flow_control_exit","title":"ontaps3_svm_output_flow_control_exit","text":"<p>Number of times output flow control was exited. ontaps3_svm_output_flow_control_exit is ontaps3_svm_output_flow_control_exit aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>output_flow_control_exit</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>output_flow_control_exit</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_presigned_url_reqs","title":"ontaps3_svm_presigned_url_reqs","text":"<p>Total number of presigned object store server URL requests. ontaps3_svm_presigned_url_reqs is ontaps3_svm_presigned_url_reqs aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>presigned_url_requests</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>presigned_url_reqs</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_bucket_versioning_failed","title":"ontaps3_svm_put_bucket_versioning_failed","text":"<p>Number of failed Put Bucket Versioning operations. ontaps3_svm_put_bucket_versioning_failed is ontaps3_svm_put_bucket_versioning_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_bucket_versioning_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_bucket_versioning_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_bucket_versioning_total","title":"ontaps3_svm_put_bucket_versioning_total","text":"<p>Number of Put Bucket Versioning operations. ontaps3_svm_put_bucket_versioning_total is ontaps3_svm_put_bucket_versioning_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_bucket_versioning_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_bucket_versioning_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_data","title":"ontaps3_svm_put_data","text":"<p>Rate of PUT object data transfers in bytes per second. ontaps3_svm_put_data is ontaps3_svm_put_data aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_data</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_put_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Data Transfer"},{"location":"ontap-metrics/#ontaps3_svm_put_object_failed","title":"ontaps3_svm_put_object_failed","text":"<p>Number of failed PUT object operations. ontaps3_svm_put_object_failed is ontaps3_svm_put_object_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_object_failed_client_close","title":"ontaps3_svm_put_object_failed_client_close","text":"<p>Number of times PUT object operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_put_object_failed_client_close is ontaps3_svm_put_object_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_object_latency","title":"ontaps3_svm_put_object_latency","text":"<p>Average latency in microseconds for PUT object operations. ontaps3_svm_put_object_latency is ontaps3_svm_put_object_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_latency</code>Unit: microsecType: averageBase: put_object_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_latency</code>Unit: microsecType: average,no-zero-valuesBase: put_object_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_put_object_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Latency"},{"location":"ontap-metrics/#ontaps3_svm_put_object_rate","title":"ontaps3_svm_put_object_rate","text":"<p>Number of PUT object operations per second. ontaps3_svm_put_object_rate is ontaps3_svm_put_object_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_put_object_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Rate"},{"location":"ontap-metrics/#ontaps3_svm_put_object_tagging_failed","title":"ontaps3_svm_put_object_tagging_failed","text":"<p>Number of failed PUT object tagging operations. ontaps3_svm_put_object_tagging_failed is ontaps3_svm_put_object_tagging_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_tagging_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_tagging_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_object_tagging_failed_client_close","title":"ontaps3_svm_put_object_tagging_failed_client_close","text":"<p>Number of times PUT object tagging operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_put_object_tagging_failed_client_close is ontaps3_svm_put_object_tagging_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_tagging_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_tagging_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_object_tagging_latency","title":"ontaps3_svm_put_object_tagging_latency","text":"<p>Average latency in microseconds for PUT object tagging operations. ontaps3_svm_put_object_tagging_latency is ontaps3_svm_put_object_tagging_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_tagging_latency</code>Unit: microsecType: averageBase: put_object_tagging_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_tagging_latency</code>Unit: microsecType: average,no-zero-valuesBase: put_object_tagging_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_object_tagging_rate","title":"ontaps3_svm_put_object_tagging_rate","text":"<p>Number of PUT object tagging operations per second. ontaps3_svm_put_object_tagging_rate is ontaps3_svm_put_object_tagging_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_tagging_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_tagging_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_object_tagging_total","title":"ontaps3_svm_put_object_tagging_total","text":"<p>Number of PUT object tagging operations. ontaps3_svm_put_object_tagging_total is ontaps3_svm_put_object_tagging_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_tagging_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_tagging_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_put_object_total","title":"ontaps3_svm_put_object_total","text":"<p>Number of PUT object operations. ontaps3_svm_put_object_total is ontaps3_svm_put_object_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>put_object_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>put_object_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_put_object_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM timeseries Top $TopResources SVMs by Operations"},{"location":"ontap-metrics/#ontaps3_svm_request_parse_errors","title":"ontaps3_svm_request_parse_errors","text":"<p>Number of request parser errors due to malformed requests. ontaps3_svm_request_parse_errors is ontaps3_svm_request_parse_errors aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>request_parse_errors</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>request_parse_errors</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_requests","title":"ontaps3_svm_requests","text":"<p>Total number of object store server requests. ontaps3_svm_requests is ontaps3_svm_requests aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>requests</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>requests</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml <p>The <code>ontaps3_svm_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage S3 Object Storage SVM table Requests &amp; Connections stats"},{"location":"ontap-metrics/#ontaps3_svm_requests_outstanding","title":"ontaps3_svm_requests_outstanding","text":"<p>Number of object store server requests in process. ontaps3_svm_requests_outstanding is ontaps3_svm_requests_outstanding aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>requests_outstanding</code>Unit: noneType: rawBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>requests_outstanding</code>Unit: noneType: raw,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_root_user_access","title":"ontaps3_svm_root_user_access","text":"<p>Number of times access was done by root user. ontaps3_svm_root_user_access is ontaps3_svm_root_user_access aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>root_user_access</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>root_user_access</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_server_connection_close","title":"ontaps3_svm_server_connection_close","text":"<p>Number of connection closes triggered by server due to fatal errors. ontaps3_svm_server_connection_close is ontaps3_svm_server_connection_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>server_connection_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>server_connection_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_signature_v2_reqs","title":"ontaps3_svm_signature_v2_reqs","text":"<p>Total number of object store server signature V2 requests. ontaps3_svm_signature_v2_reqs is ontaps3_svm_signature_v2_reqs aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>signature_v2_requests</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>signature_v2_reqs</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_signature_v4_reqs","title":"ontaps3_svm_signature_v4_reqs","text":"<p>Total number of object store server signature V4 requests. ontaps3_svm_signature_v4_reqs is ontaps3_svm_signature_v4_reqs aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>signature_v4_requests</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>signature_v4_reqs</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_tagging","title":"ontaps3_svm_tagging","text":"<p>Number of requests with tagging specified. ontaps3_svm_tagging is ontaps3_svm_tagging aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>tagging</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>tagging</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_upload_part_failed","title":"ontaps3_svm_upload_part_failed","text":"<p>Number of failed Upload Part operations. ontaps3_svm_upload_part_failed is ontaps3_svm_upload_part_failed aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>upload_part_failed</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>upload_part_failed</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_upload_part_failed_client_close","title":"ontaps3_svm_upload_part_failed_client_close","text":"<p>Number of times Upload Part operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_upload_part_failed_client_close is ontaps3_svm_upload_part_failed_client_close aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>upload_part_failed_client_close</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>upload_part_failed_client_close</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_upload_part_latency","title":"ontaps3_svm_upload_part_latency","text":"<p>Average latency in microseconds for Upload Part operations. ontaps3_svm_upload_part_latency is ontaps3_svm_upload_part_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>upload_part_latency</code>Unit: microsecType: averageBase: upload_part_total conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>upload_part_latency</code>Unit: microsecType: average,no-zero-valuesBase: upload_part_latency_base conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_upload_part_rate","title":"ontaps3_svm_upload_part_rate","text":"<p>Number of Upload Part operations per second. ontaps3_svm_upload_part_rate is ontaps3_svm_upload_part_rate aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>upload_part_rate</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>upload_part_rate</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_svm_upload_part_total","title":"ontaps3_svm_upload_part_total","text":"<p>Number of Upload Part operations. ontaps3_svm_upload_part_total is ontaps3_svm_upload_part_total aggregated by <code>svm</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/object_store_server</code> <code>upload_part_total</code>Unit: noneType: deltaBase: conf/restperf/9.14.1/ontap_s3_svm.yaml ZapiPerf <code>perf-object-get-instances object_store_server</code> <code>upload_part_total</code>Unit: noneType: delta,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml"},{"location":"ontap-metrics/#ontaps3_used_percent","title":"ontaps3_used_percent","text":"<p>The used_percent metric the percentage of a bucket's total capacity that is currently being used.</p> API Endpoint Metric Template REST <code>api/protocols/s3/buckets</code> <code>logical_used_size, size</code> conf/rest/9.7.0/ontap_s3.yaml <p>The <code>ontaps3_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: S3 Object Storage Highlights table Bucket Overview ONTAP: S3 Object Storage Highlights timeseries Top $TopResources Buckets by Used Size Percent"},{"location":"ontap-metrics/#path_read_data","title":"path_read_data","text":"<p>The average read throughput in kilobytes per second read from the indicated target port by the controller.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/path</code> <code>read_data</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/path.yaml ZapiPerf <code>perf-object-get-instances path</code> <code>read_data</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/path.yaml <p>The <code>path_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FibreBridge/Array timeseries Read Data from FibreBridge/Array WWPN"},{"location":"ontap-metrics/#path_read_iops","title":"path_read_iops","text":"<p>The number of I/O read operations sent from the initiator port to the indicated target port.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/path</code> <code>read_iops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/path.yaml ZapiPerf <code>perf-object-get-instances path</code> <code>read_iops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/path.yaml <p>The <code>path_read_iops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FibreBridge/Array timeseries Read IOPs from FibreBridge/Array WWPN"},{"location":"ontap-metrics/#path_read_latency","title":"path_read_latency","text":"<p>The average latency in microseconds of I/O read operations sent from this controller to the indicated target port.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/path</code> <code>read_latency</code>Unit: microsecType: averageBase: read_iops conf/restperf/9.12.0/path.yaml ZapiPerf <code>perf-object-get-instances path</code> <code>read_latency</code>Unit: microsecType: averageBase: read_iops conf/zapiperf/cdot/9.8.0/path.yaml <p>The <code>path_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FibreBridge/Array timeseries Read Latency from FibreBridge/Array WWPN"},{"location":"ontap-metrics/#path_total_data","title":"path_total_data","text":"<p>The average throughput in kilobytes per second read and written from/to the indicated target port by the controller.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/path</code> <code>total_data</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/path.yaml ZapiPerf <code>perf-object-get-instances path</code> <code>total_data</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/path.yaml"},{"location":"ontap-metrics/#path_total_iops","title":"path_total_iops","text":"<p>The number of total read/write I/O operations sent from the initiator port to the indicated target port.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/path</code> <code>total_iops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/path.yaml ZapiPerf <code>perf-object-get-instances path</code> <code>total_iops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/path.yaml"},{"location":"ontap-metrics/#path_write_data","title":"path_write_data","text":"<p>The average write throughput in kilobytes per second written to the indicated target port by the controller.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/path</code> <code>write_data</code>Unit: kb_per_secType: rateBase: conf/restperf/9.12.0/path.yaml ZapiPerf <code>perf-object-get-instances path</code> <code>write_data</code>Unit: kb_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/path.yaml <p>The <code>path_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FibreBridge/Array timeseries Write Data to FibreBridge/Array WWPN"},{"location":"ontap-metrics/#path_write_iops","title":"path_write_iops","text":"<p>The number of I/O write operations sent from the initiator port to the indicated target port.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/path</code> <code>write_iops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/path.yaml ZapiPerf <code>perf-object-get-instances path</code> <code>write_iops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/path.yaml <p>The <code>path_write_iops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FibreBridge/Array timeseries Write IOPs to FibreBridge/Array WWPN"},{"location":"ontap-metrics/#path_write_latency","title":"path_write_latency","text":"<p>The average latency in microseconds of I/O write operations sent from this controller to the indicated target port.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/path</code> <code>write_latency</code>Unit: microsecType: averageBase: write_iops conf/restperf/9.12.0/path.yaml ZapiPerf <code>perf-object-get-instances path</code> <code>write_latency</code>Unit: microsecType: averageBase: write_iops conf/zapiperf/cdot/9.8.0/path.yaml <p>The <code>path_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster FibreBridge/Array timeseries Write Latency to FibreBridge/Array WWPN"},{"location":"ontap-metrics/#plex_disk_busy","title":"plex_disk_busy","text":"<p>The utilization percent of the disk. plex_disk_busy is disk_busy aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>disk_busy_percent</code>Unit: percentType: percentBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_busy</code>Unit: percentType: percentBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>plex_disk_busy</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Disk timeseries Top $TopResources Plexes by Disk Utilization ONTAP: MetroCluster MetroCluster Disk table Top $TopResources Plexes by Disk Utilization"},{"location":"ontap-metrics/#plex_disk_capacity","title":"plex_disk_capacity","text":"<p>Disk capacity in MB. plex_disk_capacity is disk_capacity aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>capacity</code>Unit: mbType: rawBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_capacity</code>Unit: mbType: rawBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_cp_read_chain","title":"plex_disk_cp_read_chain","text":"<p>Average number of blocks transferred in each consistency point read operation during a CP. plex_disk_cp_read_chain is disk_cp_read_chain aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_cp_read_latency","title":"plex_disk_cp_read_latency","text":"<p>Average latency per block in microseconds for consistency point read operations. plex_disk_cp_read_latency is disk_cp_read_latency aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_cp_reads","title":"plex_disk_cp_reads","text":"<p>Number of disk read operations initiated each second for consistency point processing. plex_disk_cp_reads is disk_cp_reads aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_io_pending","title":"plex_disk_io_pending","text":"<p>Average number of I/Os issued to the disk for which we have not yet received the response. plex_disk_io_pending is disk_io_pending aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_io_queued","title":"plex_disk_io_queued","text":"<p>Number of I/Os queued to the disk but not yet issued. plex_disk_io_queued is disk_io_queued aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_total_data","title":"plex_disk_total_data","text":"<p>Total throughput for user operations per second. plex_disk_total_data is disk_total_data aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_total_transfers","title":"plex_disk_total_transfers","text":"<p>Total number of disk operations involving data transfer initiated per second. plex_disk_total_transfers is disk_total_transfers aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_transfer_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_transfers</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_user_read_blocks","title":"plex_disk_user_read_blocks","text":"<p>Number of blocks transferred for user read operations per second. plex_disk_user_read_blocks is disk_user_read_blocks aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_user_read_chain","title":"plex_disk_user_read_chain","text":"<p>Average number of blocks transferred in each user read operation. plex_disk_user_read_chain is disk_user_read_chain aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_user_read_latency","title":"plex_disk_user_read_latency","text":"<p>Average latency per block in microseconds for user read operations. plex_disk_user_read_latency is disk_user_read_latency aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>plex_disk_user_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Disk timeseries Top $TopResources Plexes by User Read Latency per 4KB IO"},{"location":"ontap-metrics/#plex_disk_user_reads","title":"plex_disk_user_reads","text":"<p>Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. plex_disk_user_reads is disk_user_reads aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>plex_disk_user_reads</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Disk timeseries Top $TopResources Plexes by User Reads"},{"location":"ontap-metrics/#plex_disk_user_write_blocks","title":"plex_disk_user_write_blocks","text":"<p>Number of blocks transferred for user write operations per second. plex_disk_user_write_blocks is disk_user_write_blocks aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_user_write_chain","title":"plex_disk_user_write_chain","text":"<p>Average number of blocks transferred in each user write operation. plex_disk_user_write_chain is disk_user_write_chain aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_write_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_writes conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#plex_disk_user_write_latency","title":"plex_disk_user_write_latency","text":"<p>Average latency per block in microseconds for user write operations. plex_disk_user_write_latency is disk_user_write_latency aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_blocks conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>plex_disk_user_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Disk timeseries Top $TopResources Plexes by User Write Latency per 4KB IO"},{"location":"ontap-metrics/#plex_disk_user_writes","title":"plex_disk_user_writes","text":"<p>Number of disk write operations initiated each second for storing data or metadata associated with user requests. plex_disk_user_writes is disk_user_writes aggregated by <code>plex</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_writes</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>plex_disk_user_writes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: MetroCluster MetroCluster Disk timeseries Top $TopResources Plexes by User Writes"},{"location":"ontap-metrics/#poller_concurrent_collectors","title":"poller_concurrent_collectors","text":"<p>Tracks the number of concurrent collectors running.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: bytes NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: bytes NA"},{"location":"ontap-metrics/#poller_cpu_percent","title":"poller_cpu_percent","text":"<p>Tracks the percentage of cpu usage of concurrent collectors running.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> NA <p>The <code>poller_cpu_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights timeseries % CPU Used"},{"location":"ontap-metrics/#poller_memory","title":"poller_memory","text":"<p>Tracks the memory usage of the poller process, including Resident Set Size (RSS), swap memory, and Virtual Memory Size (VMS).</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: bytes NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: bytes NA <p>The <code>poller_memory</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights table Pollers Harvest Metadata Highlights timeseries Poller RSS Memory"},{"location":"ontap-metrics/#poller_memory_percent","title":"poller_memory_percent","text":"<p>Indicates the percentage of memory used by the poller process relative to the total available memory.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code>Unit: percent NA ZAPI <code>NA</code> <code>Harvest generated</code>Unit: percent NA <p>The <code>poller_memory_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights timeseries % Memory Used"},{"location":"ontap-metrics/#poller_status","title":"poller_status","text":"<p>Indicates the operational status of the poller process, where 1 means operational and 0 means not operational.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> NA ZAPI <code>NA</code> <code>Harvest generated</code> NA <p>The <code>poller_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel Harvest Metadata Highlights stat Pollers Harvest Metadata Highlights table Pollers"},{"location":"ontap-metrics/#qos_concurrency","title":"qos_concurrency","text":"<p>This is the average number of concurrent requests for the workload.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>concurrency</code>Unit: noneType: rateBase: conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>concurrency</code>Unit: noneType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_concurrency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Concurrency"},{"location":"ontap-metrics/#qos_latency","title":"qos_latency","text":"<p>This is the average response time for requests that were initiated by the workload.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>latency</code>Unit: microsecType: averageBase: ops conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>latency</code>Unit: microsecType: average,no-zero-valuesBase: ops conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group stat Average QoS Latency ONTAP: Volume QoS stat Top $TopResources QoS Volumes Latency ONTAP: Volume QoS timeseries Top $TopResources QoS Volumes by Latency ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Average Latency"},{"location":"ontap-metrics/#qos_ops","title":"qos_ops","text":"<p>This field is the workload's rate of operations that completed during the measurement interval; measured per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group stat QoS IOPs ONTAP: Volume QoS stat Top $TopResources QoS Volumes by Total IOPs ONTAP: Volume QoS timeseries Top $TopResources QoS Volumes by Total IOPs ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Total IOPS ONTAP: Workload Fixed QoS Shared Policy Utilization timeseries Top $TopResources Fixed QoS Shared Policy IOPs Utilization (%) ONTAP: Workload Fixed QoS Shared Policy Utilization table Fixed QoS Shared Policy IOPs Utilization (%) ONTAP: Workload Fixed QoS Workload Utilization timeseries Top $TopResources Fixed QoS Workload IOPs Utilization (%) ONTAP: Workload Fixed QoS Workload Utilization table Fixed QoS Workload IOPs Utilization (%) ONTAP: Workload Adaptive QoS Workload Utilization timeseries Top $TopResources Adaptive QoS Workload IOPs Utilization (%) ONTAP: Workload Adaptive QoS Workload Utilization table Adaptive QoS Workload IOPs Utilization (%)"},{"location":"ontap-metrics/#qos_other_ops","title":"qos_other_ops","text":"<p>This is the rate of this workload's other operations that completed during the measurement interval.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/workload.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>other_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_other_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Other IOPS ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Other IOPS"},{"location":"ontap-metrics/#qos_policy_adaptive_absolute_min_iops","title":"qos_policy_adaptive_absolute_min_iops","text":"<p>Specifies the absolute minimum IOPS that is used as an override when the expected_iops is less than this value.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_policy_adaptive.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_policy_adaptive.yaml <p>The <code>qos_policy_adaptive_absolute_min_iops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Adaptive QoS Workload Utilization table Adaptive QoS Workload IOPs Utilization (%)"},{"location":"ontap-metrics/#qos_policy_adaptive_expected_iops","title":"qos_policy_adaptive_expected_iops","text":"<p>Specifies the size to be used to calculate expected IOPS per TB.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_policy_adaptive.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_policy_adaptive.yaml"},{"location":"ontap-metrics/#qos_policy_adaptive_labels","title":"qos_policy_adaptive_labels","text":"<p>This metric provides information about QosPolicyAdaptive</p> API Endpoint Metric Template REST <code>api/private/cli/qos/adaptive-policy-group</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_policy_adaptive.yaml ZAPI <code>qos-adaptive-policy-group-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_policy_adaptive.yaml"},{"location":"ontap-metrics/#qos_policy_adaptive_peak_iops","title":"qos_policy_adaptive_peak_iops","text":"<p>Specifies the maximum possible IOPS per TB allocated based on the storage object allocated size or the storage object used size.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_policy_adaptive.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_policy_adaptive.yaml <p>The <code>qos_policy_adaptive_peak_iops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Adaptive QoS Workload Utilization table Adaptive QoS Workload IOPs Utilization (%)"},{"location":"ontap-metrics/#qos_policy_fixed_labels","title":"qos_policy_fixed_labels","text":"<p>This metric provides information about QosPolicyFixed</p> API Endpoint Metric Template REST <code>api/private/cli/qos/policy-group</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_policy_fixed.yaml ZAPI <code>qos-policy-group-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml <p>The <code>qos_policy_fixed_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Fixed QoS Shared Policy Utilization timeseries Top $TopResources Fixed QoS Shared Policy IOPs Utilization (%) ONTAP: Workload Fixed QoS Shared Policy Utilization timeseries Top $TopResources Fixed QoS Shared Policy Bandwidth Utilization (%) ONTAP: Workload Fixed QoS Shared Policy Utilization table Fixed QoS Shared Policy IOPs Utilization (%) ONTAP: Workload Fixed QoS Shared Policy Utilization table Fixed QoS Shared Policy Bandwidth Utilization (%) ONTAP: Workload Fixed QoS Workload Utilization table Fixed QoS Workload IOPs Utilization (%) ONTAP: Workload Fixed QoS Workload Utilization table Fixed QoS Workload Bandwidth Utilization (%)"},{"location":"ontap-metrics/#qos_policy_fixed_max_throughput_iops","title":"qos_policy_fixed_max_throughput_iops","text":"<p>Maximum throughput defined by this policy. It is specified in terms of IOPS. 0 means no maximum throughput is enforced.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_policy_fixed.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml <p>The <code>qos_policy_fixed_max_throughput_iops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Fixed QoS Shared Policy Utilization timeseries Top $TopResources Fixed QoS Shared Policy IOPs Utilization (%) ONTAP: Workload Fixed QoS Shared Policy Utilization table Fixed QoS Shared Policy IOPs Utilization (%) ONTAP: Workload Fixed QoS Workload Utilization timeseries Top $TopResources Fixed QoS Workload IOPs Utilization (%)"},{"location":"ontap-metrics/#qos_policy_fixed_max_throughput_mbps","title":"qos_policy_fixed_max_throughput_mbps","text":"<p>Maximum throughput defined by this policy. It is specified in terms of Mbps. 0 means no maximum throughput is enforced.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_policy_fixed.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml <p>The <code>qos_policy_fixed_max_throughput_mbps</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Fixed QoS Shared Policy Utilization timeseries Top $TopResources Fixed QoS Shared Policy Bandwidth Utilization (%) ONTAP: Workload Fixed QoS Shared Policy Utilization table Fixed QoS Shared Policy Bandwidth Utilization (%) ONTAP: Workload Fixed QoS Workload Utilization timeseries Top $TopResources Fixed QoS Workload Bandwidth Utilization (%)"},{"location":"ontap-metrics/#qos_policy_fixed_min_throughput_iops","title":"qos_policy_fixed_min_throughput_iops","text":"<p>Minimum throughput defined by this policy. It is specified in terms of IOPS. 0 means no minimum throughput is enforced. These floors are not guaranteed on non-AFF platforms or when FabricPool tiering policies are set.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_policy_fixed.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml"},{"location":"ontap-metrics/#qos_policy_fixed_min_throughput_mbps","title":"qos_policy_fixed_min_throughput_mbps","text":"<p>Minimum throughput defined by this policy. It is specified in terms of Mbps. 0 means no minimum throughput is enforced.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_policy_fixed.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml"},{"location":"ontap-metrics/#qos_read_data","title":"qos_read_data","text":"<p>This is the amount of data read in bytes per second from the filer by the workload.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>read_data</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group stat QoS Read Throughput ONTAP: SVM QoS Policy Group timeseries Top $TopResources SVMs by QoS Throughput ONTAP: Volume QoS stat Top $TopResources Qos Volumes Total Throughput ONTAP: Volume QoS timeseries Top $TopResources QoS Volumes by Average Throughput ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Read Throughput ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Read Throughput"},{"location":"ontap-metrics/#qos_read_io_type","title":"qos_read_io_type","text":"<p>This is the percentage of read requests served from various components (such as buffer cache, ext_cache, disk, etc.).</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>read_io_type_percent</code>Unit: percentType: percentBase: read_io_type_base conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>read_io_type</code>Unit: percentType: percentBase: read_io_type_base conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_read_io_type</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type bamboo_ssd ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type cache ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type cloud ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type cloud_s2c ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type disk ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type ext_cache ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type fc_miss ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type hya_cache ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type hya_hdd ONTAP: Workload Read IO Type timeseries Top $TopResources Workloads by Read IO Type hya_non_cache"},{"location":"ontap-metrics/#qos_read_latency","title":"qos_read_latency","text":"<p>This is the average response time for read requests that were initiated by the workload.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>read_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_ops conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group stat Average QoS Read Latency ONTAP: SVM QoS Policy Group timeseries Top $TopResources SVMs by QoS Latency ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Read Latency ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Average Read Latency"},{"location":"ontap-metrics/#qos_read_ops","title":"qos_read_ops","text":"<p>This is the rate of this workload's read operations that completed during the measurement interval.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>read_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group stat QoS Read IOPs ONTAP: SVM QoS Policy Group timeseries Top $TopResources SVMs by QoS IOPs ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Read IOPS ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Read IOPS"},{"location":"ontap-metrics/#qos_sequential_reads","title":"qos_sequential_reads","text":"<p>This is the percentage of reads, performed on behalf of the workload, that were sequential.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>sequential_reads_percent</code>Unit: percentType: percentBase: sequential_reads_base conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>sequential_reads</code>Unit: percentType: percent,no-zero-valuesBase: sequential_reads_base conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_sequential_reads</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group timeseries Top $TopResources Workloads by Sequential Reads (%) ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Sequential Reads ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Sequential Reads (%)"},{"location":"ontap-metrics/#qos_sequential_writes","title":"qos_sequential_writes","text":"<p>This is the percentage of writes, performed on behalf of the workload, that were sequential. This counter is only available on platforms with more than 4GB of NVRAM.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>sequential_writes_percent</code>Unit: percentType: percentBase: sequential_writes_base conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>sequential_writes</code>Unit: percentType: percent,no-zero-valuesBase: sequential_writes_base conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_sequential_writes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group timeseries Top $TopResources Workloads by Sequential Writes (%) ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Sequential Writes ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Sequential Writes (%)"},{"location":"ontap-metrics/#qos_total_data","title":"qos_total_data","text":"<p>This is the total amount of data read/written in bytes per second from/to the filer by the workload.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>total_data</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_total_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Fixed QoS Shared Policy Utilization timeseries Top $TopResources Fixed QoS Shared Policy Bandwidth Utilization (%) ONTAP: Workload Fixed QoS Shared Policy Utilization table Fixed QoS Shared Policy Bandwidth Utilization (%) ONTAP: Workload Fixed QoS Workload Utilization timeseries Top $TopResources Fixed QoS Workload Bandwidth Utilization (%) ONTAP: Workload Fixed QoS Workload Utilization table Fixed QoS Workload Bandwidth Utilization (%) ONTAP: Workload Adaptive QoS Workload Utilization timeseries Top $TopResources Adaptive QoS Workload Bandwidth Utilization (%) ONTAP: Workload Adaptive QoS Workload Utilization table Adaptive QoS Workload Bandwidth Utilization (%)"},{"location":"ontap-metrics/#qos_workload_labels","title":"qos_workload_labels","text":"<p>This metric provides information about QosWorkload</p> API Endpoint Metric Template REST <code>api/private/cli/qos/workload</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_workload.yaml ZAPI <code>qos-workload-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_workload.yaml <p>The <code>qos_workload_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count ONTAP: Workload Adaptive QoS Workload Utilization table Adaptive QoS Workload IOPs Utilization (%) ONTAP: Workload Adaptive QoS Workload Utilization table Adaptive QoS Workload Bandwidth Utilization (%)"},{"location":"ontap-metrics/#qos_workload_max_throughput_iops","title":"qos_workload_max_throughput_iops","text":"<p>Maximum throughput IOPs allowed for the workload.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_workload.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_workload.yaml <p>The <code>qos_workload_max_throughput_iops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Adaptive QoS Workload Utilization timeseries Top $TopResources Adaptive QoS Workload IOPs Utilization (%) ONTAP: Workload Adaptive QoS Workload Utilization table Adaptive QoS Workload IOPs Utilization (%)"},{"location":"ontap-metrics/#qos_workload_max_throughput_mbps","title":"qos_workload_max_throughput_mbps","text":"<p>Maximum throughput Mbps allowed for the workload.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_workload.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_workload.yaml <p>The <code>qos_workload_max_throughput_mbps</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Adaptive QoS Workload Utilization timeseries Top $TopResources Adaptive QoS Workload Bandwidth Utilization (%) ONTAP: Workload Adaptive QoS Workload Utilization table Adaptive QoS Workload Bandwidth Utilization (%)"},{"location":"ontap-metrics/#qos_workload_min_throughput_iops","title":"qos_workload_min_throughput_iops","text":"<p>Minimum throughput IOPs allowed for the workload.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_workload.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_workload.yaml <p>The <code>qos_workload_min_throughput_iops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Workload Adaptive QoS Workload Utilization table Adaptive QoS Workload IOPs Utilization (%)"},{"location":"ontap-metrics/#qos_workload_min_throughput_mbps","title":"qos_workload_min_throughput_mbps","text":"<p>Minimum throughput Mbps allowed for the workload.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/qos_workload.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qos_workload.yaml"},{"location":"ontap-metrics/#qos_write_data","title":"qos_write_data","text":"<p>This is the amount of data written in bytes per second to the filer by the workload.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>write_data</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group stat QoS Throughput ONTAP: SVM QoS Policy Group stat QoS Write Throughput ONTAP: SVM QoS Policy Group timeseries Top $TopResources SVMs by QoS Throughput ONTAP: Volume QoS stat Top $TopResources Qos Volumes Total Throughput ONTAP: Volume QoS timeseries Top $TopResources QoS Volumes by Average Throughput ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Write Throughput ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Write Throughput"},{"location":"ontap-metrics/#qos_write_latency","title":"qos_write_latency","text":"<p>This is the average response time for write requests that were initiated by the workload.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>write_latency</code>Unit: microsecType: average,no-zero-valuesBase: write_ops conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group stat Average QoS Write Latency ONTAP: SVM QoS Policy Group timeseries Top $TopResources SVMs by QoS Latency ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Write Latency ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Average Write Latency"},{"location":"ontap-metrics/#qos_write_ops","title":"qos_write_ops","text":"<p>This is the workload's write operations that completed during the measurement interval; measured per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qos_volume</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/workload_volume.yaml ZapiPerf <code>perf-object-get-instances workload_volume</code> <code>write_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/workload_volume.yaml <p>The <code>qos_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM QoS Policy Group stat QoS Write IOPs ONTAP: SVM QoS Policy Group timeseries Top $TopResources SVMs by QoS IOPs ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Write IOPS ONTAP: Workload Highlights timeseries Top $TopResources Workloads by Write IOPS"},{"location":"ontap-metrics/#qtree_cifs_ops","title":"qtree_cifs_ops","text":"<p>Number of CIFS operations per second to the qtree</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qtree</code> <code>cifs_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/qtree.yaml ZapiPerf <code>perf-object-get-instances qtree</code> <code>cifs_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/qtree.yaml <p>The <code>qtree_cifs_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Qtree Highlights timeseries Top $TopResources CIFS by IOPs"},{"location":"ontap-metrics/#qtree_id","title":"qtree_id","text":"<p>The identifier for the qtree, unique within the qtree's volume.</p> API Endpoint Metric Template REST <code>api/storage/qtrees</code> <code>id</code> conf/rest/9.12.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_internal_ops","title":"qtree_internal_ops","text":"<p>Number of internal operations generated by activites such as snapmirror and backup per second to the qtree</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qtree</code> <code>internal_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/qtree.yaml ZapiPerf <code>perf-object-get-instances qtree</code> <code>internal_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/qtree.yaml <p>The <code>qtree_internal_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Qtree Highlights timeseries Top $TopResources Qtrees by Internal IOPs"},{"location":"ontap-metrics/#qtree_labels","title":"qtree_labels","text":"<p>This metric provides information about Qtree</p> API Endpoint Metric Template REST <code>api/storage/qtrees</code> <code>Harvest generated</code> conf/rest/9.12.0/qtree.yaml ZAPI <code>qtree-list-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/qtree.yaml <p>The <code>qtree_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count"},{"location":"ontap-metrics/#qtree_nfs_ops","title":"qtree_nfs_ops","text":"<p>Number of NFS operations per second to the qtree</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qtree</code> <code>nfs_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/qtree.yaml ZapiPerf <code>perf-object-get-instances qtree</code> <code>nfs_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/qtree.yaml <p>The <code>qtree_nfs_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Qtree Highlights timeseries Top $TopResources NFSs by IOPs"},{"location":"ontap-metrics/#qtree_other_data","title":"qtree_other_data","text":"<p>Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.throughput_raw.other</code>Unit: b_per_secType: rateBase: conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_other_latency","title":"qtree_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: qtree_statistics.iops_raw.other conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_other_ops","title":"qtree_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_read_data","title":"qtree_read_data","text":"<p>Performance metric for read I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_read_latency","title":"qtree_read_latency","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: qtree_statistics.iops_raw.read conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_read_ops","title":"qtree_read_ops","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_total_data","title":"qtree_total_data","text":"<p>Performance metric aggregated over all types of I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_total_latency","title":"qtree_total_latency","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: qtree_statistics.iops_raw.total conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_total_ops","title":"qtree_total_ops","text":"<p>Summation of NFS ops, CIFS ops, CSS ops and internal ops</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/qtree</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/qtree.yaml KeyPerf <code>api/storage/qtrees</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.16.0/qtree.yaml ZapiPerf <code>perf-object-get-instances qtree</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/qtree.yaml <p>The <code>qtree_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Qtree Highlights timeseries Top $TopResources Qtrees by IOPs ONTAP: Volume Deep Dive Highlights timeseries Qtrees by IOPs"},{"location":"ontap-metrics/#qtree_write_data","title":"qtree_write_data","text":"<p>Performance metric for write I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_write_latency","title":"qtree_write_latency","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.latency_raw.write</code>Unit: microsecType: averageBase: qtree_statistics.iops_raw.write conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#qtree_write_ops","title":"qtree_write_ops","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/qtrees</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.16.0/qtree.yaml"},{"location":"ontap-metrics/#quota_disk_limit","title":"quota_disk_limit","text":"<p>Maximum amount of disk space, in kilobytes, allowed for the quota target (hard disk space limit). The value is -1 if the limit is unlimited.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>space.hard_limit</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>disk-limit</code> conf/zapi/cdot/9.8.0/qtree.yaml <p>The <code>quota_disk_limit</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count ONTAP: Quota Highlights table Reports"},{"location":"ontap-metrics/#quota_disk_used","title":"quota_disk_used","text":"<p>Current amount of disk space, in kilobytes, used by the quota target.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>space.used.total</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>disk-used</code> conf/zapi/cdot/9.8.0/qtree.yaml <p>The <code>quota_disk_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Qtree Usage timeseries Top $TopResources Qtrees by Disk Used ONTAP: Qtree Usage timeseries Top $TopResources Qtrees by Disk Used Growth ONTAP: Quota Highlights table Reports ONTAP: Quota Space Usage timeseries Top $TopResources Quotas by Space Used"},{"location":"ontap-metrics/#quota_disk_used_pct_disk_limit","title":"quota_disk_used_pct_disk_limit","text":"<p>Current disk space used expressed as a percentage of hard disk limit.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>space.used.hard_limit_percent</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>disk-used-pct-disk-limit</code> conf/zapi/cdot/9.8.0/qtree.yaml <p>The <code>quota_disk_used_pct_disk_limit</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Quota Highlights table Reports ONTAP: Quota Space Usage timeseries Top $TopResources Quotas by Space Used %"},{"location":"ontap-metrics/#quota_disk_used_pct_soft_disk_limit","title":"quota_disk_used_pct_soft_disk_limit","text":"<p>Current disk space used expressed as a percentage of soft disk limit.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>space.used.soft_limit_percent</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>disk-used-pct-soft-disk-limit</code> conf/zapi/cdot/9.8.0/qtree.yaml"},{"location":"ontap-metrics/#quota_disk_used_pct_threshold","title":"quota_disk_used_pct_threshold","text":"<p>Current disk space used expressed as a percentage of threshold.</p> API Endpoint Metric Template ZAPI <code>quota-report-iter</code> <code>disk-used-pct-threshold</code> conf/zapi/cdot/9.8.0/qtree.yaml"},{"location":"ontap-metrics/#quota_file_limit","title":"quota_file_limit","text":"<p>Maximum number of files allowed for the quota target (hard files limit). The value is -1 if the limit is unlimited.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>files.hard_limit</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>file-limit</code> conf/zapi/cdot/9.8.0/qtree.yaml <p>The <code>quota_file_limit</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Quota Highlights table Reports"},{"location":"ontap-metrics/#quota_files_used","title":"quota_files_used","text":"<p>Current number of files used by the quota target.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>files.used.total</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>files-used</code> conf/zapi/cdot/9.8.0/qtree.yaml <p>The <code>quota_files_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Qtree Usage timeseries Top $TopResources Qtrees by Files Used ONTAP: Quota Highlights table Reports ONTAP: Quota Space Usage timeseries Top $TopResources Quotas by Files Used"},{"location":"ontap-metrics/#quota_files_used_pct_file_limit","title":"quota_files_used_pct_file_limit","text":"<p>Current number of files used expressed as a percentage of hard file limit.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>files.used.hard_limit_percent</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>files-used-pct-file-limit</code> conf/zapi/cdot/9.8.0/qtree.yaml <p>The <code>quota_files_used_pct_file_limit</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Quota Highlights table Reports ONTAP: Quota Space Usage timeseries Top $TopResources Quotas by Files Used %"},{"location":"ontap-metrics/#quota_files_used_pct_soft_file_limit","title":"quota_files_used_pct_soft_file_limit","text":"<p>Current number of files used expressed as a percentage of soft file limit.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>files.used.soft_limit_percent</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>files-used-pct-soft-file-limit</code> conf/zapi/cdot/9.8.0/qtree.yaml"},{"location":"ontap-metrics/#quota_soft_disk_limit","title":"quota_soft_disk_limit","text":"<p>soft disk space limit, in kilobytes, for the quota target. The value is -1 if the limit is unlimited.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>space.soft_limit</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>soft-disk-limit</code> conf/zapi/cdot/9.8.0/qtree.yaml <p>The <code>quota_soft_disk_limit</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Quota Highlights table Reports"},{"location":"ontap-metrics/#quota_soft_file_limit","title":"quota_soft_file_limit","text":"<p>Soft file limit, in number of files, for the quota target. The value is -1 if the limit is unlimited.</p> API Endpoint Metric Template REST <code>api/storage/quota/reports</code> <code>files.soft_limit</code> conf/rest/9.12.0/quota.yaml ZAPI <code>quota-report-iter</code> <code>soft-file-limit</code> conf/zapi/cdot/9.8.0/qtree.yaml <p>The <code>quota_soft_file_limit</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Quota Highlights table Reports"},{"location":"ontap-metrics/#quota_threshold","title":"quota_threshold","text":"<p>Disk space threshold, in kilobytes, for the quota target. The value is -1 if the limit is unlimited.</p> API Endpoint Metric Template ZAPI <code>quota-report-iter</code> <code>threshold</code> conf/zapi/cdot/9.8.0/qtree.yaml REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/quota.yaml"},{"location":"ontap-metrics/#raid_disk_busy","title":"raid_disk_busy","text":"<p>The utilization percent of the disk. raid_disk_busy is disk_busy aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>disk_busy_percent</code>Unit: percentType: percentBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_busy</code>Unit: percentType: percentBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>raid_disk_busy</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Top Disks: Raid-level Overview timeseries Top $TopResources Disks by Disk Busy"},{"location":"ontap-metrics/#raid_disk_capacity","title":"raid_disk_capacity","text":"<p>Disk capacity in MB. raid_disk_capacity is disk_capacity aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>capacity</code>Unit: mbType: rawBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>disk_capacity</code>Unit: mbType: rawBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_cp_read_chain","title":"raid_disk_cp_read_chain","text":"<p>Average number of blocks transferred in each consistency point read operation during a CP. raid_disk_cp_read_chain is disk_cp_read_chain aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_chain</code>Unit: noneType: averageBase: cp_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_cp_read_latency","title":"raid_disk_cp_read_latency","text":"<p>Average latency per block in microseconds for consistency point read operations. raid_disk_cp_read_latency is disk_cp_read_latency aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_read_latency</code>Unit: microsecType: averageBase: cp_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_cp_reads","title":"raid_disk_cp_reads","text":"<p>Number of disk read operations initiated each second for consistency point processing. raid_disk_cp_reads is disk_cp_reads aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>cp_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>cp_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_io_pending","title":"raid_disk_io_pending","text":"<p>Average number of I/Os issued to the disk for which we have not yet received the response. raid_disk_io_pending is disk_io_pending aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_pending</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_io_queued","title":"raid_disk_io_queued","text":"<p>Number of I/Os queued to the disk but not yet issued. raid_disk_io_queued is disk_io_queued aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>io_queued</code>Unit: noneType: averageBase: base_for_disk_busy conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_total_data","title":"raid_disk_total_data","text":"<p>Total throughput for user operations per second. raid_disk_total_data is disk_total_data aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_total_transfers","title":"raid_disk_total_transfers","text":"<p>Total number of disk operations involving data transfer initiated per second. raid_disk_total_transfers is disk_total_transfers aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>total_transfer_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>total_transfers</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>raid_disk_total_transfers</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Top Disks: Raid-level Overview timeseries Top $TopResources Disks by Total Transfers"},{"location":"ontap-metrics/#raid_disk_user_read_blocks","title":"raid_disk_user_read_blocks","text":"<p>Number of blocks transferred for user read operations per second. raid_disk_user_read_blocks is disk_user_read_blocks aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_user_read_chain","title":"raid_disk_user_read_chain","text":"<p>Average number of blocks transferred in each user read operation. raid_disk_user_read_chain is disk_user_read_chain aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_read_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_chain</code>Unit: noneType: averageBase: user_reads conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_user_read_latency","title":"raid_disk_user_read_latency","text":"<p>Average latency per block in microseconds for user read operations. raid_disk_user_read_latency is disk_user_read_latency aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_read_latency</code>Unit: microsecType: averageBase: user_read_blocks conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>raid_disk_user_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Top Disks: Raid-level Overview timeseries Top $TopResources Disks by User Read Latency"},{"location":"ontap-metrics/#raid_disk_user_reads","title":"raid_disk_user_reads","text":"<p>Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. raid_disk_user_reads is disk_user_reads aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_read_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_reads</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_user_write_blocks","title":"raid_disk_user_write_blocks","text":"<p>Number of blocks transferred for user write operations per second. raid_disk_user_write_blocks is disk_user_write_blocks aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_block_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_blocks</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_user_write_chain","title":"raid_disk_user_write_chain","text":"<p>Average number of blocks transferred in each user write operation. raid_disk_user_write_chain is disk_user_write_chain aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_write_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_chain</code>Unit: noneType: averageBase: user_writes conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#raid_disk_user_write_latency","title":"raid_disk_user_write_latency","text":"<p>Average latency per block in microseconds for user write operations. raid_disk_user_write_latency is disk_user_write_latency aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_block_count conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_write_latency</code>Unit: microsecType: averageBase: user_write_blocks conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>raid_disk_user_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Top Disks: Raid-level Overview timeseries Top $TopResources Disks by User Write Latency"},{"location":"ontap-metrics/#raid_disk_user_writes","title":"raid_disk_user_writes","text":"<p>Number of disk write operations initiated each second for storing data or metadata associated with user requests. raid_disk_user_writes is disk_user_writes aggregated by <code>raid</code>.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/disk:constituent</code> <code>user_write_count</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>perf-object-get-instances disk:constituent</code> <code>user_writes</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#rw_ctx_cifs_giveups","title":"rw_ctx_cifs_giveups","text":"<p>Array of number of give-ups of CIFS ops because they rewind more than a certain threshold, categorized by their rewind reasons.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/rewind_context</code> <code>cifs_give_ups</code>Unit: noneType: deltaBase: conf/restperf/9.16.0/rwctx.yaml ZapiPerf <code>perf-object-get-instances rw_ctx</code> <code>cifs_giveups</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/rwctx.yaml <p>The <code>rw_ctx_cifs_giveups</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Rewind View timeseries Top $TopResources Metric by CIFS Giveups"},{"location":"ontap-metrics/#rw_ctx_cifs_rewinds","title":"rw_ctx_cifs_rewinds","text":"<p>Array of number of rewinds for CIFS ops based on their reasons.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/rewind_context</code> <code>cifs_rewinds</code>Unit: noneType: deltaBase: conf/restperf/9.16.0/rwctx.yaml ZapiPerf <code>perf-object-get-instances rw_ctx</code> <code>cifs_rewinds</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/rwctx.yaml <p>The <code>rw_ctx_cifs_rewinds</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Rewind View timeseries Top $TopResources Metric by CIFS Rewinds"},{"location":"ontap-metrics/#rw_ctx_nfs_giveups","title":"rw_ctx_nfs_giveups","text":"<p>Array of number of give-ups of NFS ops because they rewind more than a certain threshold, categorized by their rewind reasons.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/rewind_context</code> <code>nfs_give_ups</code>Unit: noneType: deltaBase: conf/restperf/9.16.0/rwctx.yaml ZapiPerf <code>perf-object-get-instances rw_ctx</code> <code>nfs_giveups</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/rwctx.yaml <p>The <code>rw_ctx_nfs_giveups</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Rewind View timeseries Top $TopResources Metric by NFS Giveups"},{"location":"ontap-metrics/#rw_ctx_nfs_rewinds","title":"rw_ctx_nfs_rewinds","text":"<p>Array of number of rewinds for NFS ops based on their reasons.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/rewind_context</code> <code>nfs_rewinds</code>Unit: noneType: deltaBase: conf/restperf/9.16.0/rwctx.yaml ZapiPerf <code>perf-object-get-instances rw_ctx</code> <code>nfs_rewinds</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/rwctx.yaml <p>The <code>rw_ctx_nfs_rewinds</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Rewind View timeseries Top $TopResources Metric by NFS Rewinds"},{"location":"ontap-metrics/#rw_ctx_qos_flowcontrol","title":"rw_ctx_qos_flowcontrol","text":"<p>The number of times QoS limiting has enabled stream flowcontrol.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances rw_ctx</code> <code>qos_flowcontrol</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/rwctx.yaml <p>The <code>rw_ctx_qos_flowcontrol</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Rewind View timeseries Top $TopResources Node by QoS Flowcontrol"},{"location":"ontap-metrics/#rw_ctx_qos_rewinds","title":"rw_ctx_qos_rewinds","text":"<p>The number of restarts after a rewind because of QoS limiting.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances rw_ctx</code> <code>qos_rewinds</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/rwctx.yaml <p>The <code>rw_ctx_qos_rewinds</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Rewind View timeseries Top $TopResources Node by QoS Rewinds"},{"location":"ontap-metrics/#security_account_activediruser","title":"security_account_activediruser","text":"<p>Represent the Active directory user in security account</p> API Endpoint Metric Template Rest <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/security_account.yaml <p>The <code>security_account_activediruser</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat AD/LDAP ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_account_certificateuser","title":"security_account_certificateuser","text":"<p>Represent the Certificate user in security account</p> API Endpoint Metric Template Rest <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/security_account.yaml <p>The <code>security_account_certificateuser</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Certificate ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_account_labels","title":"security_account_labels","text":"<p>This metric provides information about SecurityAccount</p> API Endpoint Metric Template REST <code>api/security/accounts</code> <code>Harvest generated</code> conf/rest/9.12.0/security_account.yaml ZAPI <code>security-login-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/security_account.yaml <p>The <code>security_account_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Cluster Compliant % ONTAP: Security Highlights piechart Cluster Compliant ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_account_ldapuser","title":"security_account_ldapuser","text":"<p>Represent the LDAP user in security account</p> API Endpoint Metric Template Rest <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/security_account.yaml <p>The <code>security_account_ldapuser</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat AD/LDAP ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_account_localuser","title":"security_account_localuser","text":"<p>Represent the Local user in security account</p> API Endpoint Metric Template Rest <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/security_account.yaml <p>The <code>security_account_localuser</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Local ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_account_samluser","title":"security_account_samluser","text":"<p>Represent the SAML user in security account</p> API Endpoint Metric Template Rest <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/security_account.yaml <p>The <code>security_account_samluser</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat SAML ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_audit_destination_port","title":"security_audit_destination_port","text":"<p>The destination port used to forward the message.</p> API Endpoint Metric Template ZAPI <code>cluster-log-forward-get-iter</code> <code>cluster-log-forward-info.port</code> conf/zapi/cdot/9.8.0/security_audit_dest.yaml"},{"location":"ontap-metrics/#security_audit_destination_status","title":"security_audit_destination_status","text":"<p>Represent the security audit protocol in security audit destinations</p> API Endpoint Metric Template Rest <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/security_audit_dest.yaml <p>The <code>security_audit_destination_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Cluster Compliant % ONTAP: Security Highlights piechart Cluster Compliant ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_certificate_expiry_time","title":"security_certificate_expiry_time","text":"API Endpoint Metric Template REST <code>api/private/cli/security/certificate</code> <code>expiration</code> conf/rest/9.12.0/security_certificate.yaml ZAPI <code>security-certificate-get-iter</code> <code>certificate-info.expiration-date</code> conf/zapi/cdot/9.8.0/security_certificate.yaml <p>The <code>security_certificate_expiry_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights table SSL Certificates Expiration"},{"location":"ontap-metrics/#security_certificate_labels","title":"security_certificate_labels","text":"<p>This metric provides information about SecurityCert</p> API Endpoint Metric Template REST <code>api/private/cli/security/certificate</code> <code>Harvest generated</code> conf/rest/9.12.0/security_certificate.yaml ZAPI <code>security-certificate-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/security_certificate.yaml <p>The <code>security_certificate_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Expiring in &lt; 60 days ONTAP: Security Highlights stat Expired ONTAP: Security Highlights table SSL Certificates Expiration ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_labels","title":"security_labels","text":"<p>This metric provides information about Security</p> API Endpoint Metric Template REST <code>api/security</code> <code>Harvest generated</code> conf/rest/9.12.0/security.yaml ZAPI <code>cluster-identity-get</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/security.yaml <p>The <code>security_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Cluster Compliant % ONTAP: Security Highlights piechart Cluster Compliant ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_login_labels","title":"security_login_labels","text":"<p>This metric provides information about SecurityLogin</p> API Endpoint Metric Template REST <code>api/security/login/messages</code> <code>Harvest generated</code> conf/rest/9.12.0/security_login.yaml ZAPI <code>vserver-login-banner-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/security_login.yaml <p>The <code>security_login_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Cluster Compliant % ONTAP: Security Highlights stat SVM Compliant % ONTAP: Security Highlights piechart Cluster Compliant ONTAP: Security Highlights piechart SVM Compliant ONTAP: Security Cluster Compliance table Cluster Compliance ONTAP: Security SVM Compliance table SVM Compliance"},{"location":"ontap-metrics/#security_ssh_labels","title":"security_ssh_labels","text":"<p>This metric provides information about SecuritySsh</p> API Endpoint Metric Template REST <code>api/security/ssh</code> <code>Harvest generated</code> conf/rest/9.12.0/security_ssh.yaml <p>The <code>security_ssh_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Cluster Compliant % ONTAP: Security Highlights piechart Cluster Compliant ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#security_ssh_max_instances","title":"security_ssh_max_instances","text":"<p>Maximum possible simultaneous connections.</p> API Endpoint Metric Template REST <code>api/security/ssh</code> <code>max_instances</code> conf/rest/9.12.0/security_ssh.yaml"},{"location":"ontap-metrics/#shelf_average_ambient_temperature","title":"shelf_average_ambient_temperature","text":"<p>Average temperature of all ambient sensors for shelf in Celsius.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_average_ambient_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_average_fan_speed","title":"shelf_average_fan_speed","text":"<p>Average fan speed for shelf in rpm.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_average_fan_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_average_temperature","title":"shelf_average_temperature","text":"<p>Average temperature of all non-ambient sensors for shelf in Celsius.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_average_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Highlights timeseries Top $TopResources Shelves by Average Temperature ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights timeseries Top $TopResources Shelves by Average Temperature ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_disk_count","title":"shelf_disk_count","text":"<p>Disk count in a shelf.</p> API Endpoint Metric Template REST <code>api/storage/shelves</code> <code>disk_count</code> conf/rest/9.12.0/shelf.yaml ZAPI <code>storage-shelf-info-get-iter</code> <code>storage-shelf-info.disk-count</code> conf/zapi/cdot/9.8.0/shelf.yaml <p>The <code>shelf_disk_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_fan_labels","title":"shelf_fan_labels","text":"<p>This metric provides information about shelf fans.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#shelf_fan_rpm","title":"shelf_fan_rpm","text":"<p>Fan Rotation Per Minute.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_fan_rpm</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Shelf Highlights stat Fan RPM Avg ONTAP: Shelf Temperature Sensors bargauge Cooling Sensors"},{"location":"ontap-metrics/#shelf_fan_status","title":"shelf_fan_status","text":"<p>Fan Operational Status.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#shelf_labels","title":"shelf_labels","text":"<p>This metric provides information about Shelf</p> API Endpoint Metric Template REST <code>api/storage/shelves</code> <code>Harvest generated</code> conf/rest/9.12.0/shelf.yaml ZAPI <code>storage-shelf-info-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/shelf.yaml <p>The <code>shelf_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count ONTAP: Datacenter Power and Temperature stat Total Power ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Highlights stat Total Power ONTAP: Power Highlights stat Average Power/Used_TB ONTAP: Power Highlights stat Average IOPs/Watt ONTAP: Power Highlights timeseries Total Power Consumed ONTAP: Power Highlights timeseries Average Power Consumption (kWh) Over Last Hour ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights stat Shelves ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_max_fan_speed","title":"shelf_max_fan_speed","text":"<p>Maximum fan speed for shelf in rpm.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_max_fan_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Power and Temperature stat Max Shelf Fan Speed ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Highlights stat Max Shelf Fan Speed ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights stat Max Shelf Fan Speed ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_max_temperature","title":"shelf_max_temperature","text":"<p>Maximum temperature of all non-ambient sensors for shelf in Celsius.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_max_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Power and Temperature stat Max Shelf Temp ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Highlights stat Max Shelf Temp ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights stat Max Shelf temp ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_min_ambient_temperature","title":"shelf_min_ambient_temperature","text":"<p>Minimum temperature of all ambient sensors for shelf in Celsius.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_min_ambient_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_min_fan_speed","title":"shelf_min_fan_speed","text":"<p>Minimum fan speed for shelf in rpm.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_min_fan_speed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_min_temperature","title":"shelf_min_temperature","text":"<p>Minimum temperature of all non-ambient sensors for shelf in Celsius.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_min_temperature</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_module_labels","title":"shelf_module_labels","text":"<p>This metric provides information about shelf module.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_module_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Shelf Module table Storage Shelf Modules"},{"location":"ontap-metrics/#shelf_module_status","title":"shelf_module_status","text":"<p>Displays the shelf module labels with their status.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#shelf_new_status","title":"shelf_new_status","text":"<p>This metric indicates a value of 1 if the shelf state is online or ok (indicating the shelf is operational) and a value of 0 for any other state (indicating the shelf is not operational).</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/shelf.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/shelf.yaml <p>The <code>shelf_new_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_power","title":"shelf_power","text":"<p>Power consumed by shelf in Watts.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_power</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Power and Temperature stat Total Power ONTAP: Datacenter Power and Temperature timeseries Total Power Consumed ONTAP: Health Shelves table Storage Shelf Issues ONTAP: Power Highlights stat Total Power ONTAP: Power Highlights stat Average Power/Used_TB ONTAP: Power Highlights stat Average IOPs/Watt ONTAP: Power Highlights timeseries Total Power Consumed ONTAP: Power Highlights timeseries Average Power Consumption (kWh) Over Last Hour ONTAP: Power Highlights timeseries Top $TopResources Shelves by Power Consumed ONTAP: Power Shelves table Storage Shelves ONTAP: Shelf Highlights stat Total Power (Shelf) ONTAP: Shelf Highlights timeseries Top $TopResources Shelves by Power Consumed ONTAP: Shelf Highlights table Storage Shelves"},{"location":"ontap-metrics/#shelf_psu_labels","title":"shelf_psu_labels","text":"<p>This metric provides information about shelf psu.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_psu_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Shelf PSU table Storage Shelf PSUs"},{"location":"ontap-metrics/#shelf_psu_power_drawn","title":"shelf_psu_power_drawn","text":"<p>Power Drawn From PSU In Watts.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_psu_power_drawn</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Shelf PSU table Storage Shelf PSUs ONTAP: Shelf Voltage &amp; PSU Sensors bargauge Energy Drawn from PSUs"},{"location":"ontap-metrics/#shelf_psu_power_rating","title":"shelf_psu_power_rating","text":"<p>Power Supply Power Ratings In Watts.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_psu_power_rating</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Shelf PSU table Storage Shelf PSUs ONTAP: Shelf Voltage &amp; PSU Sensors bargauge Power Rating from PSUs"},{"location":"ontap-metrics/#shelf_psu_status","title":"shelf_psu_status","text":"<p>Operational Status.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#shelf_sensor_labels","title":"shelf_sensor_labels","text":"<p>This metric provides information about shelf sensor.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#shelf_sensor_reading","title":"shelf_sensor_reading","text":"<p>Current Sensor Reading.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_sensor_reading</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Shelf Highlights stat Custom Sensor Avg ONTAP: Shelf Custom Sensors bargauge Custom Sensors - Shelf"},{"location":"ontap-metrics/#shelf_sensor_status","title":"shelf_sensor_status","text":"<p>Operational Status.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#shelf_temperature_labels","title":"shelf_temperature_labels","text":"<p>This metric provides information about shelf temperature.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#shelf_temperature_reading","title":"shelf_temperature_reading","text":"<p>Temperature Reading.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_temperature_reading</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Shelf Highlights stat Temperature Avg ONTAP: Shelf Temperature Sensors bargauge Temperature Sensors"},{"location":"ontap-metrics/#shelf_temperature_status","title":"shelf_temperature_status","text":"<p>Operational Status.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#shelf_voltage_labels","title":"shelf_voltage_labels","text":"<p>This metric provides information about shelf voltage.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#shelf_voltage_reading","title":"shelf_voltage_reading","text":"<p>Voltage Current Reading.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml <p>The <code>shelf_voltage_reading</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Shelf Voltage &amp; PSU Sensors bargauge Voltage Sensors"},{"location":"ontap-metrics/#shelf_voltage_status","title":"shelf_voltage_status","text":"<p>Operational Status.</p> API Endpoint Metric Template RestPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/restperf/9.12.0/disk.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/disk.yaml"},{"location":"ontap-metrics/#smb2_close_latency","title":"smb2_close_latency","text":"<p>Average latency in microseconds for SMB2_COM_CLOSE operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>close_latency</code>Unit: microsecType: averageBase: close_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>close_latency</code>Unit: microsecType: averageBase: close_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_close_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_close_ops","title":"smb2_close_ops","text":"<p>Number of SMB2_COM_CLOSE operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>close_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>close_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_close_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_create_latency","title":"smb2_create_latency","text":"<p>Average latency in microseconds for SMB2_COM_CREATE operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>create_latency</code>Unit: microsecType: averageBase: create_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>create_latency</code>Unit: microsecType: averageBase: create_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_create_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_create_ops","title":"smb2_create_ops","text":"<p>Number of SMB2_COM_CREATE operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>create_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>create_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_create_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_lock_latency","title":"smb2_lock_latency","text":"<p>Average latency in microseconds for SMB2_COM_LOCK operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>lock_latency</code>Unit: microsecType: averageBase: lock_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>lock_latency</code>Unit: microsecType: averageBase: lock_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_lock_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_lock_ops","title":"smb2_lock_ops","text":"<p>Number of SMB2_COM_LOCK operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>lock_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>lock_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_lock_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_negotiate_latency","title":"smb2_negotiate_latency","text":"<p>Average latency in microseconds for SMB2_COM_NEGOTIATE operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>negotiate_latency</code>Unit: microsecType: averageBase: negotiate_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>negotiate_latency</code>Unit: microsecType: averageBase: negotiate_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_negotiate_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_negotiate_ops","title":"smb2_negotiate_ops","text":"<p>Number of SMB2_COM_NEGOTIATE operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>negotiate_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>negotiate_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_negotiate_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_oplock_break_latency","title":"smb2_oplock_break_latency","text":"<p>Average latency in microseconds for SMB2_COM_OPLOCK_BREAK operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>oplock_break_latency</code>Unit: microsecType: averageBase: oplock_break_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>oplock_break_latency</code>Unit: microsecType: averageBase: oplock_break_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_oplock_break_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_oplock_break_ops","title":"smb2_oplock_break_ops","text":"<p>Number of SMB2_COM_OPLOCK_BREAK operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>oplock_break_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>oplock_break_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_oplock_break_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_query_directory_latency","title":"smb2_query_directory_latency","text":"<p>Average latency in microseconds for SMB2_COM_QUERY_DIRECTORY operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>query_directory_latency</code>Unit: microsecType: averageBase: query_directory_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>query_directory_latency</code>Unit: microsecType: averageBase: query_directory_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_query_directory_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_query_directory_ops","title":"smb2_query_directory_ops","text":"<p>Number of SMB2_COM_QUERY_DIRECTORY operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>query_directory_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>query_directory_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_query_directory_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_query_info_latency","title":"smb2_query_info_latency","text":"<p>Average latency in microseconds for SMB2_COM_QUERY_INFO operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>query_info_latency</code>Unit: microsecType: averageBase: query_info_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>query_info_latency</code>Unit: microsecType: averageBase: query_info_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_query_info_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_query_info_ops","title":"smb2_query_info_ops","text":"<p>Number of SMB2_COM_QUERY_INFO operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>query_info_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>query_info_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_query_info_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_read_latency","title":"smb2_read_latency","text":"<p>Average latency in microseconds for SMB2_COM_READ operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>read_latency</code>Unit: microsecType: averageBase: read_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Read / Write Latency"},{"location":"ontap-metrics/#smb2_read_ops","title":"smb2_read_ops","text":"<p>Number of SMB2_COM_READ operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Read / Write IOPS"},{"location":"ontap-metrics/#smb2_session_setup_latency","title":"smb2_session_setup_latency","text":"<p>Average latency in microseconds for SMB2_COM_SESSION_SETUP operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>session_setup_latency</code>Unit: microsecType: averageBase: session_setup_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>session_setup_latency</code>Unit: microsecType: averageBase: session_setup_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_session_setup_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_session_setup_ops","title":"smb2_session_setup_ops","text":"<p>Number of SMB2_COM_SESSION_SETUP operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>session_setup_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>session_setup_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_session_setup_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_set_info_latency","title":"smb2_set_info_latency","text":"<p>Average latency in microseconds for SMB2_COM_SET_INFO operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>set_info_latency</code>Unit: microsecType: averageBase: set_info_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>set_info_latency</code>Unit: microsecType: averageBase: set_info_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_set_info_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_set_info_ops","title":"smb2_set_info_ops","text":"<p>Number of SMB2_COM_SET_INFO operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>set_info_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>set_info_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_set_info_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_tree_connect_latency","title":"smb2_tree_connect_latency","text":"<p>Average latency in microseconds for SMB2_COM_TREE_CONNECT operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>tree_connect_latency</code>Unit: microsecType: averageBase: tree_connect_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>tree_connect_latency</code>Unit: microsecType: averageBase: tree_connect_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_tree_connect_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other Latency"},{"location":"ontap-metrics/#smb2_tree_connect_ops","title":"smb2_tree_connect_ops","text":"<p>Number of SMB2_COM_TREE_CONNECT operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>tree_connect_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>tree_connect_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_tree_connect_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Other IOPS"},{"location":"ontap-metrics/#smb2_write_latency","title":"smb2_write_latency","text":"<p>Average latency in microseconds for SMB2_COM_WRITE operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>write_latency</code>Unit: microsecType: averageBase: write_ops conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>write_latency</code>Unit: microsecType: averageBase: write_latency_base conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Read / Write Latency"},{"location":"ontap-metrics/#smb2_write_ops","title":"smb2_write_ops","text":"<p>Number of SMB2_COM_WRITE operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/smb2</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.14.1/smb2.yaml ZapiPerf <code>perf-object-get-instances smb2</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/smb2.yaml <p>The <code>smb2_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SMB SMB Performance timeseries Read / Write IOPS"},{"location":"ontap-metrics/#snapmirror_break_failed_count","title":"snapmirror_break_failed_count","text":"<p>The number of failed SnapMirror break operations for the relationship</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>break_failed_count</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.break-failed-count</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_break_failed_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SnapMirror Sources Highlights stat Number of Failed SnapMirror Transfers"},{"location":"ontap-metrics/#snapmirror_break_successful_count","title":"snapmirror_break_successful_count","text":"<p>The number of successful SnapMirror break operations for the relationship</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>break_successful_count</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.break-successful-count</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_break_successful_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SnapMirror Sources Highlights stat Number of Successful SnapMirror Transfers"},{"location":"ontap-metrics/#snapmirror_labels","title":"snapmirror_labels","text":"<p>This metric provides information about SnapMirror</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>Harvest generated</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Datacenter Highlights table Object Count ONTAP: SnapMirror Sources Highlights stat Unhealthy Relationships ONTAP: SnapMirror Sources Highlights table Relationships ONTAP: SnapMirror Sources Policy and Lag details piechart Relationships by Protection Policy ONTAP: SnapMirror Sources Policy and Lag details table Protection Policy and Lag detail ONTAP: SnapMirror Destinations Highlights stat Unhealthy ONTAP: SnapMirror Destinations Highlights piechart Relationships by Protection Policy ONTAP: SnapMirror Destinations Highlights stat Healthy ONTAP: SnapMirror Destinations Highlights table Relationships ONTAP: SnapMirror Destinations Highlights timeseries Top $TopResources Destination Volumes by Average Throughput ONTAP: SnapMirror Destinations Consistency Group Data Protection stat Unhealthy ONTAP: SnapMirror Destinations Consistency Group Data Protection piechart Consistency Group relationships by relationship type ONTAP: SnapMirror Destinations Consistency Group Data Protection stat Healthy ONTAP: SnapMirror Destinations Consistency Group Data Protection table Consistency Group Relationships"},{"location":"ontap-metrics/#snapmirror_lag_time","title":"snapmirror_lag_time","text":"<p>Amount of time since the last snapmirror transfer in seconds</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>lag_time</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.lag-time</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_lag_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SnapMirror Sources Highlights table Relationships ONTAP: SnapMirror Sources Highlights timeseries Top $TopResources Relationships by Lag Time ONTAP: SnapMirror Sources Policy and Lag details piechart Relationships by Lag time ONTAP: SnapMirror Sources Policy and Lag details table Protection Policy and Lag detail ONTAP: SnapMirror Destinations Highlights table Relationships ONTAP: SnapMirror Destinations Highlights timeseries Top $TopResources Relationships by Lag Time"},{"location":"ontap-metrics/#snapmirror_last_transfer_duration","title":"snapmirror_last_transfer_duration","text":"<p>Duration of the last SnapMirror transfer in seconds</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>last_transfer_duration</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.last-transfer-duration</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_last_transfer_duration</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SnapMirror Sources Highlights table Relationships ONTAP: SnapMirror Sources Highlights timeseries Top $TopResources Relationships by Transfer Duration ONTAP: SnapMirror Destinations Highlights table Relationships ONTAP: SnapMirror Destinations Highlights timeseries Top $TopResources Relationships by Transfer Duration"},{"location":"ontap-metrics/#snapmirror_last_transfer_end_timestamp","title":"snapmirror_last_transfer_end_timestamp","text":"<p>The Timestamp of the end of the last transfer</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>last_transfer_end_timestamp</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.last-transfer-end-timestamp</code> conf/zapi/cdot/9.8.0/snapmirror.yaml"},{"location":"ontap-metrics/#snapmirror_last_transfer_size","title":"snapmirror_last_transfer_size","text":"<p>Size in kilobytes (1024 bytes) of the last transfer</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>last_transfer_size</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.last-transfer-size</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_last_transfer_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SnapMirror Sources Highlights table Relationships ONTAP: SnapMirror Sources Highlights timeseries Top $TopResources Relationships by Transfer Data ONTAP: SnapMirror Destinations Highlights table Relationships ONTAP: SnapMirror Destinations Highlights timeseries Top $TopResources Relationships by Transfer Data"},{"location":"ontap-metrics/#snapmirror_newest_snapshot_timestamp","title":"snapmirror_newest_snapshot_timestamp","text":"<p>The timestamp of the newest Snapshot copy on the destination volume</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>newest_snapshot_timestamp</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.newest-snapshot-timestamp</code> conf/zapi/cdot/9.8.0/snapmirror.yaml"},{"location":"ontap-metrics/#snapmirror_policy_labels","title":"snapmirror_policy_labels","text":"<p>This metric provides information about SnapMirrorPolicy</p> API Endpoint Metric Template REST <code>api/snapmirror/policies</code> <code>Harvest generated</code> conf/rest/9.6.0/snapmirrorpolicy.yaml <p>The <code>snapmirror_policy_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Local Policy table Protection policies"},{"location":"ontap-metrics/#snapmirror_resync_failed_count","title":"snapmirror_resync_failed_count","text":"<p>The number of failed SnapMirror resync operations for the relationship</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>resync_failed_count</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.resync-failed-count</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_resync_failed_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SnapMirror Sources Highlights stat Number of Failed SnapMirror Transfers"},{"location":"ontap-metrics/#snapmirror_resync_successful_count","title":"snapmirror_resync_successful_count","text":"<p>The number of successful SnapMirror resync operations for the relationship</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>resync_successful_count</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.resync-successful-count</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_resync_successful_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SnapMirror Sources Highlights stat Number of Successful SnapMirror Transfers"},{"location":"ontap-metrics/#snapmirror_total_transfer_bytes","title":"snapmirror_total_transfer_bytes","text":"<p>Cumulative bytes transferred for the relationship</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>total_transfer_bytes</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.total-transfer-bytes</code> conf/zapi/cdot/9.8.0/snapmirror.yaml"},{"location":"ontap-metrics/#snapmirror_total_transfer_time_secs","title":"snapmirror_total_transfer_time_secs","text":"<p>Cumulative total transfer time in seconds for the relationship</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>total_transfer_time_secs</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.total-transfer-time-secs</code> conf/zapi/cdot/9.8.0/snapmirror.yaml"},{"location":"ontap-metrics/#snapmirror_update_failed_count","title":"snapmirror_update_failed_count","text":"<p>The number of successful SnapMirror update operations for the relationship</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>update_failed_count</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.update-failed-count</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_update_failed_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SnapMirror Sources Highlights stat Number of Failed SnapMirror Transfers"},{"location":"ontap-metrics/#snapmirror_update_successful_count","title":"snapmirror_update_successful_count","text":"<p>Number of Successful Updates</p> API Endpoint Metric Template REST <code>api/private/cli/snapmirror</code> <code>update_successful_count</code> conf/rest/9.12.0/snapmirror.yaml ZAPI <code>snapmirror-get-iter</code> <code>snapmirror-info.update-successful-count</code> conf/zapi/cdot/9.8.0/snapmirror.yaml <p>The <code>snapmirror_update_successful_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SnapMirror Sources Highlights stat Number of Successful SnapMirror Transfers"},{"location":"ontap-metrics/#snapshot_create_time","title":"snapshot_create_time","text":"<p>Creation time of the snapshot. It is the volume access time when the snapshot was created.</p> API Endpoint Metric Template REST <code>api/private/cli/snapshot</code> <code>create_time</code> conf/rest/9.6.0/snapshot.yaml <p>The <code>snapshot_create_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Volume Encryption &amp; Autonomous Ransomware Protection table Anti-ransomware Snapshots"},{"location":"ontap-metrics/#snapshot_labels","title":"snapshot_labels","text":"<p>This metric provides information about Snapshot</p> API Endpoint Metric Template REST <code>api/private/cli/snapshot</code> <code>Harvest generated</code> conf/rest/9.6.0/snapshot.yaml <p>The <code>snapshot_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Volume Encryption &amp; Autonomous Ransomware Protection table Anti-ransomware Snapshots"},{"location":"ontap-metrics/#snapshot_policy_labels","title":"snapshot_policy_labels","text":"<p>This metric provides information about SnapshotPolicy</p> API Endpoint Metric Template REST <code>api/storage/snapshot-policies</code> <code>Harvest generated</code> conf/rest/9.12.0/snapshotpolicy.yaml ZAPI <code>snapshot-policy-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/snapshotpolicy.yaml <p>The <code>snapshot_policy_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Snapshot Copies stat &lt;10 Copies  ONTAP: Data Protection Snapshot Copies stat 10-100 Copies ONTAP: Data Protection Snapshot Copies stat 101-500 Copies ONTAP: Data Protection Snapshot Copies stat &gt;500 Copies ONTAP: Data Protection Snapshot Copies table Volume count by the number of Snapshot copies ONTAP: Data Protection Local Policy table Snapshot policies ONTAP: Datacenter Snapshots piechart Snapshot Copies"},{"location":"ontap-metrics/#snapshot_restore_size","title":"snapshot_restore_size","text":"<p>Size of the active file system at the time the snapshot is captured. The actual size of the snapshot also includes those blocks trapped by other snapshots.</p> API Endpoint Metric Template REST <code>api/private/cli/snapshot</code> <code>afs_used</code> conf/rest/9.6.0/snapshot.yaml <p>The <code>snapshot_restore_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Volume Encryption &amp; Autonomous Ransomware Protection table Anti-ransomware Snapshots"},{"location":"ontap-metrics/#snapshot_volume_violation_count","title":"snapshot_volume_violation_count","text":"<p>This metric represents the total number of snapshots that exist on volumes without being created through an applied snapshot policy.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/snapshotpolicy.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/snapshotpolicy.yaml <p>The <code>snapshot_volume_violation_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Snapshot Policy Violations stat Total Snapshots Violations ONTAP: Data Protection Snapshot Policy Violations table Snapshot Violation Details"},{"location":"ontap-metrics/#snapshot_volume_violation_total_size","title":"snapshot_volume_violation_total_size","text":"<p>This metric captures the total size of all snapshots that were not created through an applied snapshot policy.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/snapshotpolicy.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/snapshotpolicy.yaml <p>The <code>snapshot_volume_violation_total_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Snapshot Policy Violations stat Snapshot Violations Total Size ONTAP: Data Protection Snapshot Policy Violations table Snapshot Violation Details"},{"location":"ontap-metrics/#storage_unit_avg_latency","title":"storage_unit_avg_latency","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: storage_unit_statistics.iops_raw.total conf/keyperf/asar2/9.16.0/storage_unit.yaml <p>The <code>storage_unit_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Storage Units table Storage Units in Cluster"},{"location":"ontap-metrics/#storage_unit_labels","title":"storage_unit_labels","text":"<p>This metric provides information about StorageUnit</p> API Endpoint Metric Template REST <code>api/storage/storage-units</code> <code>Harvest generated</code> conf/rest/asar2/9.16.0/storage_unit.yaml <p>The <code>storage_unit_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Highlights stat Storage Units ASAr2: Overview Highlights stat Online ASAr2: Overview Highlights stat Offline ASAr2: Overview Storage Units table Storage Units in Cluster"},{"location":"ontap-metrics/#storage_unit_other_data","title":"storage_unit_other_data","text":"<p>Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.throughput_raw.other</code>Unit: b_per_secType: rateBase: conf/keyperf/asar2/9.16.0/storage_unit.yaml"},{"location":"ontap-metrics/#storage_unit_other_latency","title":"storage_unit_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: storage_unit_statistics.iops_raw.other conf/keyperf/asar2/9.16.0/storage_unit.yaml"},{"location":"ontap-metrics/#storage_unit_other_ops","title":"storage_unit_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/asar2/9.16.0/storage_unit.yaml"},{"location":"ontap-metrics/#storage_unit_read_data","title":"storage_unit_read_data","text":"<p>Performance metric for read I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/asar2/9.16.0/storage_unit.yaml"},{"location":"ontap-metrics/#storage_unit_read_latency","title":"storage_unit_read_latency","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: storage_unit_statistics.iops_raw.read conf/keyperf/asar2/9.16.0/storage_unit.yaml"},{"location":"ontap-metrics/#storage_unit_read_ops","title":"storage_unit_read_ops","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/asar2/9.16.0/storage_unit.yaml"},{"location":"ontap-metrics/#storage_unit_space_efficiency_ratio","title":"storage_unit_space_efficiency_ratio","text":"API Endpoint Metric Template REST <code>api/storage/storage-units</code> <code>space.efficiency_ratio</code> conf/rest/asar2/9.16.0/storage_unit.yaml <p>The <code>storage_unit_space_efficiency_ratio</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Storage Units table Storage Units in Cluster"},{"location":"ontap-metrics/#storage_unit_space_physical_used","title":"storage_unit_space_physical_used","text":"<p>The number of bytes consumed on the disk by the storage unit, excluding snapshots.</p> API Endpoint Metric Template REST <code>api/storage/storage-units</code> <code>space.physical_used</code> conf/rest/asar2/9.16.0/storage_unit.yaml <p>The <code>storage_unit_space_physical_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Storage Units table Storage Units in Cluster"},{"location":"ontap-metrics/#storage_unit_space_size","title":"storage_unit_space_size","text":"API Endpoint Metric Template REST <code>api/storage/storage-units</code> <code>space.size</code> conf/rest/asar2/9.16.0/storage_unit.yaml <p>The <code>storage_unit_space_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Storage Units table Storage Units in Cluster"},{"location":"ontap-metrics/#storage_unit_space_used","title":"storage_unit_space_used","text":"API Endpoint Metric Template REST <code>api/storage/storage-units</code> <code>space.used</code> conf/rest/asar2/9.16.0/storage_unit.yaml <p>The <code>storage_unit_space_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Storage Units table Storage Units in Cluster"},{"location":"ontap-metrics/#storage_unit_total_data","title":"storage_unit_total_data","text":"<p>Performance metric aggregated over all types of I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/asar2/9.16.0/storage_unit.yaml <p>The <code>storage_unit_total_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Storage Units table Storage Units in Cluster"},{"location":"ontap-metrics/#storage_unit_total_ops","title":"storage_unit_total_ops","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/asar2/9.16.0/storage_unit.yaml <p>The <code>storage_unit_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ASAr2: Overview Storage Units table Storage Units in Cluster"},{"location":"ontap-metrics/#storage_unit_write_data","title":"storage_unit_write_data","text":"<p>Performance metric for write I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/asar2/9.16.0/storage_unit.yaml"},{"location":"ontap-metrics/#storage_unit_write_latency","title":"storage_unit_write_latency","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.latency_raw.write</code>Unit: microsec conf/rest/asar2/9.16.0/storage_unit.yaml"},{"location":"ontap-metrics/#storage_unit_write_ops","title":"storage_unit_write_ops","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/storage-units</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/asar2/9.16.0/storage_unit.yaml"},{"location":"ontap-metrics/#support_auto_update_labels","title":"support_auto_update_labels","text":"<p>This metric provides information about SupportAutoUpdate</p> API Endpoint Metric Template REST <code>api/support/auto-update</code> <code>Harvest generated</code> conf/rest/9.12.0/support_auto_update.yaml <p>The <code>support_auto_update_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#support_labels","title":"support_labels","text":"<p>This metric provides information about Support</p> API Endpoint Metric Template REST <code>api/support/autosupport</code> <code>Harvest generated</code> conf/rest/9.12.0/support.yaml ZAPI <code>autosupport-config-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/support.yaml <p>The <code>support_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Highlights stat Cluster Compliant % ONTAP: Security Highlights piechart Cluster Compliant ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#svm_cifs_connections","title":"svm_cifs_connections","text":"<p>Number of connections</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>connections</code>Unit: noneType: rawBase: conf/restperf/9.12.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>connections</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml <p>The <code>svm_cifs_connections</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS timeseries SVM CIFS Connections and Open Files"},{"location":"ontap-metrics/#svm_cifs_established_sessions","title":"svm_cifs_established_sessions","text":"<p>Number of established SMB and SMB2 sessions</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>established_sessions</code>Unit: noneType: rawBase: conf/restperf/9.12.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>established_sessions</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml"},{"location":"ontap-metrics/#svm_cifs_latency","title":"svm_cifs_latency","text":"<p>Average latency in microseconds for CIFS operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>latency</code>Unit: microsecType: averageBase: latency_base conf/restperf/9.12.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>cifs_latency</code>Unit: microsecType: averageBase: cifs_latency_base conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml <p>The <code>svm_cifs_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS stat SVM CIFS Average Latency ONTAP: SVM CIFS timeseries SVM CIFS Latency"},{"location":"ontap-metrics/#svm_cifs_op_count","title":"svm_cifs_op_count","text":"<p>Array of select CIFS operation counts</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>op_count</code>Unit: noneType: rateBase: conf/restperf/9.12.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>cifs_op_count</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml <p>The <code>svm_cifs_op_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS stat SVM CIFS IOPs ONTAP: SVM CIFS timeseries SVM CIFS IOPs ONTAP: SVM CIFS timeseries SVM CIFS IOP by Type"},{"location":"ontap-metrics/#svm_cifs_open_files","title":"svm_cifs_open_files","text":"<p>Number of open files over SMB and SMB2</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>open_files</code>Unit: noneType: rawBase: conf/restperf/9.12.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>open_files</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml <p>The <code>svm_cifs_open_files</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS timeseries SVM CIFS Connections and Open Files"},{"location":"ontap-metrics/#svm_cifs_ops","title":"svm_cifs_ops","text":"<p>Total number of CIFS operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>cifs_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml"},{"location":"ontap-metrics/#svm_cifs_other_latency","title":"svm_cifs_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: svm_cifs_statistics.iops_raw.other conf/keyperf/9.15.0/cifs_vserver.yaml"},{"location":"ontap-metrics/#svm_cifs_other_ops","title":"svm_cifs_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/cifs_vserver.yaml"},{"location":"ontap-metrics/#svm_cifs_read_data","title":"svm_cifs_read_data","text":"<p>Performance metric for read I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/cifs_vserver.yaml"},{"location":"ontap-metrics/#svm_cifs_read_latency","title":"svm_cifs_read_latency","text":"<p>Average latency in microseconds for CIFS read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>average_read_latency</code>Unit: microsecType: averageBase: total_read_ops conf/restperf/9.12.0/cifs_vserver.yaml KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: svm_cifs_statistics.iops_raw.read conf/keyperf/9.15.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>cifs_read_latency</code>Unit: microsecType: averageBase: cifs_read_ops conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml <p>The <code>svm_cifs_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS stat SVM CIFS Average Read Latency ONTAP: SVM CIFS timeseries SVM CIFS Latency"},{"location":"ontap-metrics/#svm_cifs_read_ops","title":"svm_cifs_read_ops","text":"<p>Total number of CIFS read operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>total_read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/cifs_vserver.yaml KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>cifs_read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml <p>The <code>svm_cifs_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS stat SVM CIFS Read IOPs ONTAP: SVM CIFS timeseries SVM CIFS IOPs"},{"location":"ontap-metrics/#svm_cifs_signed_sessions","title":"svm_cifs_signed_sessions","text":"<p>Number of signed SMB and SMB2 sessions.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>signed_sessions</code>Unit: noneType: rawBase: conf/restperf/9.12.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>signed_sessions</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml"},{"location":"ontap-metrics/#svm_cifs_total_latency","title":"svm_cifs_total_latency","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: svm_cifs_statistics.iops_raw.total conf/keyperf/9.15.0/cifs_vserver.yaml"},{"location":"ontap-metrics/#svm_cifs_total_ops","title":"svm_cifs_total_ops","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/cifs_vserver.yaml"},{"location":"ontap-metrics/#svm_cifs_write_data","title":"svm_cifs_write_data","text":"<p>Performance metric for write I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/cifs_vserver.yaml"},{"location":"ontap-metrics/#svm_cifs_write_latency","title":"svm_cifs_write_latency","text":"<p>Average latency in microseconds for CIFS write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>average_write_latency</code>Unit: microsecType: averageBase: total_write_ops conf/restperf/9.12.0/cifs_vserver.yaml KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.latency_raw.write</code>Unit: microsecType: averageBase: svm_cifs_statistics.iops_raw.write conf/keyperf/9.15.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>cifs_write_latency</code>Unit: microsecType: averageBase: cifs_write_ops conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml <p>The <code>svm_cifs_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS stat SVM CIFS Average Write Latency ONTAP: SVM CIFS timeseries SVM CIFS Latency"},{"location":"ontap-metrics/#svm_cifs_write_ops","title":"svm_cifs_write_ops","text":"<p>Total number of CIFS write operations</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_cifs</code> <code>total_write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/cifs_vserver.yaml KeyPerf <code>api/protocols/cifs/services</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/cifs_vserver.yaml ZapiPerf <code>perf-object-get-instances cifs:vserver</code> <code>cifs_write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml <p>The <code>svm_cifs_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS stat SVM CIFS Write IOPs ONTAP: SVM CIFS timeseries SVM CIFS IOPs"},{"location":"ontap-metrics/#svm_labels","title":"svm_labels","text":"<p>This metric provides information about SVM</p> API Endpoint Metric Template REST <code>api/private/cli/vserver</code> <code>Harvest generated</code> conf/rest/9.9.0/svm.yaml ZAPI <code>vserver-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/svm.yaml <p>The <code>svm_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT Capacity Metrics table Top $TopResources SVMs by Capacity Used % ONTAP: cDOT Capacity Metrics timeseries Top $TopResources SVMs by Capacity Used % ONTAP: cDOT SVM Metrics timeseries Top $TopResources Average Throughput by SVMs ONTAP: Datacenter Highlights table Object Count ONTAP: Security Highlights stat Cluster Compliant % ONTAP: Security Highlights stat SVM Compliant % ONTAP: Security Highlights stat SVM Autonomous Ransomware Protection % ONTAP: Security Highlights piechart Cluster Compliant ONTAP: Security Highlights piechart SVM Compliant ONTAP: Security Highlights piechart SVM Autonomous Ransomware Protection ONTAP: Security Cluster Compliance table Cluster Compliance ONTAP: Security SVM Compliance table SVM Compliance"},{"location":"ontap-metrics/#svm_ldap_encrypted","title":"svm_ldap_encrypted","text":"<p>This metric indicates a LDAP session security has been sealed</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.10.0/svm.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/svm.yaml <p>The <code>svm_ldap_encrypted</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security SVM Compliance table SVM Compliance"},{"location":"ontap-metrics/#svm_ldap_signed","title":"svm_ldap_signed","text":"<p>This metric indicates a LDAP session security has been signed</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.10.0/svm.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/svm.yaml <p>The <code>svm_ldap_signed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security SVM Compliance table SVM Compliance"},{"location":"ontap-metrics/#svm_new_status","title":"svm_new_status","text":"<p>This metric indicates a value of 1 if the SVM state is online (indicating the SVM is operational) and a value of 0 for any other state.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.10.0/svm.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/svm.yaml"},{"location":"ontap-metrics/#svm_nfs_access_avg_latency","title":"svm_nfs_access_avg_latency","text":"<p>Average latency in microseconds of Access procedure requests. The counter keeps track of the average response time of Access requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>access.average_latency</code>Unit: microsecType: averageBase: access.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>access.average_latency</code>Unit: microsecType: averageBase: access.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>access.average_latency</code>Unit: microsecType: averageBase: access.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>access.average_latency</code>Unit: microsecType: averageBase: access.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>access_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: access_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>access_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: access_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>access_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: access_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>access_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: access_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_access_total","title":"svm_nfs_access_total","text":"<p>Total number of Access procedure requests. It is the total number of access success and access error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>access.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>access.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>access.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>access.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>access_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>access_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>access_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>access_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_backchannel_ctl_avg_latency","title":"svm_nfs_backchannel_ctl_avg_latency","text":"<p>Average latency in microseconds of BACKCHANNEL_CTL operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>backchannel_ctl.average_latency</code>Unit: microsecType: averageBase: backchannel_ctl.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>backchannel_ctl.average_latency</code>Unit: microsecType: averageBase: backchannel_ctl.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>backchannel_ctl_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: backchannel_ctl_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>backchannel_ctl_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: backchannel_ctl_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_backchannel_ctl_total","title":"svm_nfs_backchannel_ctl_total","text":"<p>Total number of BACKCHANNEL_CTL operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>backchannel_ctl.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>backchannel_ctl.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>backchannel_ctl_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>backchannel_ctl_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_bind_conn_to_session_avg_latency","title":"svm_nfs_bind_conn_to_session_avg_latency","text":"<p>Average latency in microseconds of BIND_CONN_TO_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>bind_connections_to_session.average_latency</code>Unit: microsecType: averageBase: bind_connections_to_session.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>bind_conn_to_session.average_latency</code>Unit: microsecType: averageBase: bind_conn_to_session.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>bind_conn_to_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: bind_conn_to_session_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>bind_conn_to_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: bind_conn_to_session_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_bind_conn_to_session_total","title":"svm_nfs_bind_conn_to_session_total","text":"<p>Total number of BIND_CONN_TO_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>bind_connections_to_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>bind_conn_to_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>bind_conn_to_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>bind_conn_to_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_close_avg_latency","title":"svm_nfs_close_avg_latency","text":"<p>Average latency in microseconds of CLOSE procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>close.average_latency</code>Unit: microsecType: averageBase: close.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>close.average_latency</code>Unit: microsecType: averageBase: close.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>close.average_latency</code>Unit: microsecType: averageBase: close.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>close_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: close_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>close_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: close_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>close_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: close_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_close_total","title":"svm_nfs_close_total","text":"<p>Total number of CLOSE procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>close.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>close.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>close.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>close_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>close_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>close_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_commit_avg_latency","title":"svm_nfs_commit_avg_latency","text":"<p>Average latency in microseconds of Commit procedure requests. The counter keeps track of the average response time of Commit requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>commit.average_latency</code>Unit: microsecType: averageBase: commit.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>commit.average_latency</code>Unit: microsecType: averageBase: commit.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>commit.average_latency</code>Unit: microsecType: averageBase: commit.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>commit.average_latency</code>Unit: microsecType: averageBase: commit.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>commit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: commit_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>commit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: commit_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>commit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: commit_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>commit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: commit_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_commit_total","title":"svm_nfs_commit_total","text":"<p>Total number of Commit procedure requests. It is the total number of Commit success and Commit error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>commit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>commit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>commit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>commit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>commit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>commit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>commit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>commit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_create_avg_latency","title":"svm_nfs_create_avg_latency","text":"<p>Average latency in microseconds of Create procedure requests. The counter keeps track of the average response time of Create requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>create.average_latency</code>Unit: microsecType: averageBase: create.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>create.average_latency</code>Unit: microsecType: averageBase: create.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>create.average_latency</code>Unit: microsecType: averageBase: create.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>create.average_latency</code>Unit: microsecType: averageBase: create.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>create_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>create_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>create_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>create_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_create_session_avg_latency","title":"svm_nfs_create_session_avg_latency","text":"<p>Average latency in microseconds of CREATE_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>create_session.average_latency</code>Unit: microsecType: averageBase: create_session.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>create_session.average_latency</code>Unit: microsecType: averageBase: create_session.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>create_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_session_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>create_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: create_session_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_create_session_total","title":"svm_nfs_create_session_total","text":"<p>Total number of CREATE_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>create_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>create_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>create_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>create_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_create_total","title":"svm_nfs_create_total","text":"<p>Total number Create of procedure requests. It is the total number of create success and create error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>create.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>create.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>create.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>create.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>create_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>create_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>create_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>create_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_delegpurge_avg_latency","title":"svm_nfs_delegpurge_avg_latency","text":"<p>Average latency in microseconds of DELEGPURGE procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>delegpurge.average_latency</code>Unit: microsecType: averageBase: delegpurge.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>delegpurge.average_latency</code>Unit: microsecType: averageBase: delegpurge.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>delegpurge.average_latency</code>Unit: microsecType: averageBase: delegpurge.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>delegpurge_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegpurge_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>delegpurge_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegpurge_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>delegpurge_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegpurge_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_delegpurge_total","title":"svm_nfs_delegpurge_total","text":"<p>Total number of DELEGPURGE procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>delegpurge.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>delegpurge.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>delegpurge.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>delegpurge_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>delegpurge_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>delegpurge_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_delegreturn_avg_latency","title":"svm_nfs_delegreturn_avg_latency","text":"<p>Average latency in microseconds of DELEGRETURN procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>delegreturn.average_latency</code>Unit: microsecType: averageBase: delegreturn.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>delegreturn.average_latency</code>Unit: microsecType: averageBase: delegreturn.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>delegreturn.average_latency</code>Unit: microsecType: averageBase: delegreturn.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>delegreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegreturn_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>delegreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegreturn_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>delegreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: delegreturn_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_delegreturn_total","title":"svm_nfs_delegreturn_total","text":"<p>Total number of DELEGRETURN procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>delegreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>delegreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>delegreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>delegreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>delegreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>delegreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_destroy_clientid_avg_latency","title":"svm_nfs_destroy_clientid_avg_latency","text":"<p>Average latency in microseconds of DESTROY_CLIENTID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>destroy_clientid.average_latency</code>Unit: microsecType: averageBase: destroy_clientid.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>destroy_clientid.average_latency</code>Unit: microsecType: averageBase: destroy_clientid.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>destroy_clientid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: destroy_clientid_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>destroy_clientid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: destroy_clientid_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_destroy_clientid_total","title":"svm_nfs_destroy_clientid_total","text":"<p>Total number of DESTROY_CLIENTID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>destroy_clientid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>destroy_clientid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>destroy_clientid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>destroy_clientid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_destroy_session_avg_latency","title":"svm_nfs_destroy_session_avg_latency","text":"<p>Average latency in microseconds of DESTROY_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>destroy_session.average_latency</code>Unit: microsecType: averageBase: destroy_session.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>destroy_session.average_latency</code>Unit: microsecType: averageBase: destroy_session.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>destroy_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: destroy_session_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>destroy_session_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: destroy_session_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_destroy_session_total","title":"svm_nfs_destroy_session_total","text":"<p>Total number of DESTROY_SESSION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>destroy_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>destroy_session.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>destroy_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>destroy_session_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_exchange_id_avg_latency","title":"svm_nfs_exchange_id_avg_latency","text":"<p>Average latency in microseconds of EXCHANGE_ID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>exchange_id.average_latency</code>Unit: microsecType: averageBase: exchange_id.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>exchange_id.average_latency</code>Unit: microsecType: averageBase: exchange_id.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>exchange_id_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: exchange_id_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>exchange_id_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: exchange_id_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_exchange_id_total","title":"svm_nfs_exchange_id_total","text":"<p>Total number of EXCHANGE_ID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>exchange_id.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>exchange_id.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>exchange_id_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>exchange_id_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_free_stateid_avg_latency","title":"svm_nfs_free_stateid_avg_latency","text":"<p>Average latency in microseconds of FREE_STATEID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>free_stateid.average_latency</code>Unit: microsecType: averageBase: free_stateid.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>free_stateid.average_latency</code>Unit: microsecType: averageBase: free_stateid.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>free_stateid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: free_stateid_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>free_stateid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: free_stateid_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_free_stateid_total","title":"svm_nfs_free_stateid_total","text":"<p>Total number of FREE_STATEID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>free_stateid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>free_stateid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>free_stateid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>free_stateid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_fsinfo_avg_latency","title":"svm_nfs_fsinfo_avg_latency","text":"<p>Average latency in microseconds of FSInfo procedure requests. The counter keeps track of the average response time of FSInfo requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>fsinfo.average_latency</code>Unit: microsecType: averageBase: fsinfo.total conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>fsinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: fsinfo_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_fsinfo_total","title":"svm_nfs_fsinfo_total","text":"<p>Total number FSInfo of procedure requests. It is the total number of FSInfo success and FSInfo error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>fsinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>fsinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_fsstat_avg_latency","title":"svm_nfs_fsstat_avg_latency","text":"<p>Average latency in microseconds of FSStat procedure requests. The counter keeps track of the average response time of FSStat requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>fsstat.average_latency</code>Unit: microsecType: averageBase: fsstat.total conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>fsstat_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: fsstat_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_fsstat_total","title":"svm_nfs_fsstat_total","text":"<p>Total number FSStat of procedure requests. It is the total number of FSStat success and FSStat error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>fsstat.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>fsstat_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_get_dir_delegation_avg_latency","title":"svm_nfs_get_dir_delegation_avg_latency","text":"<p>Average latency in microseconds of GET_DIR_DELEGATION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>get_dir_delegation.average_latency</code>Unit: microsecType: averageBase: get_dir_delegation.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>get_dir_delegation.average_latency</code>Unit: microsecType: averageBase: get_dir_delegation.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>get_dir_delegation_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: get_dir_delegation_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>get_dir_delegation_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: get_dir_delegation_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_get_dir_delegation_total","title":"svm_nfs_get_dir_delegation_total","text":"<p>Total number of GET_DIR_DELEGATION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>get_dir_delegation.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>get_dir_delegation.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>get_dir_delegation_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>get_dir_delegation_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_getattr_avg_latency","title":"svm_nfs_getattr_avg_latency","text":"<p>Average latency in microseconds of GetAttr procedure requests. This counter keeps track of the average response time of GetAttr requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>getattr.average_latency</code>Unit: microsecType: averageBase: getattr.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>getattr.average_latency</code>Unit: microsecType: averageBase: getattr.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>getattr.average_latency</code>Unit: microsecType: averageBase: getattr.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>getattr.average_latency</code>Unit: microsecType: averageBase: getattr.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>getattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getattr_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>getattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getattr_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>getattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getattr_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>getattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getattr_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_getattr_total","title":"svm_nfs_getattr_total","text":"<p>Total number of Getattr procedure requests. It is the total number of getattr success and getattr error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>getattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>getattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>getattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>getattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>getattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>getattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>getattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>getattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_getdeviceinfo_avg_latency","title":"svm_nfs_getdeviceinfo_avg_latency","text":"<p>Average latency in microseconds of GETDEVICEINFO operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>getdeviceinfo.average_latency</code>Unit: microsecType: averageBase: getdeviceinfo.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>getdeviceinfo.average_latency</code>Unit: microsecType: averageBase: getdeviceinfo.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>getdeviceinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getdeviceinfo_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>getdeviceinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getdeviceinfo_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_getdeviceinfo_total","title":"svm_nfs_getdeviceinfo_total","text":"<p>Total number of GETDEVICEINFO operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>getdeviceinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>getdeviceinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>getdeviceinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>getdeviceinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_getdevicelist_avg_latency","title":"svm_nfs_getdevicelist_avg_latency","text":"<p>Average latency in microseconds of GETDEVICELIST operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>getdevicelist.average_latency</code>Unit: microsecType: averageBase: getdevicelist.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>getdevicelist.average_latency</code>Unit: microsecType: averageBase: getdevicelist.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>getdevicelist_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getdevicelist_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>getdevicelist_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getdevicelist_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_getdevicelist_total","title":"svm_nfs_getdevicelist_total","text":"<p>Total number of GETDEVICELIST operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>getdevicelist.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>getdevicelist.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>getdevicelist_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>getdevicelist_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_getfh_avg_latency","title":"svm_nfs_getfh_avg_latency","text":"<p>Average latency in microseconds of GETFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>getfh.average_latency</code>Unit: microsecType: averageBase: getfh.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>getfh.average_latency</code>Unit: microsecType: averageBase: getfh.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>getfh.average_latency</code>Unit: microsecType: averageBase: getfh.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>getfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getfh_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>getfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getfh_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>getfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: getfh_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_getfh_total","title":"svm_nfs_getfh_total","text":"<p>Total number of GETFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>getfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>getfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>getfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>getfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>getfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>getfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_latency","title":"svm_nfs_latency","text":"<p>Average latency in microseconds of NFSv3 requests. This counter keeps track of the average response time of NFSv3 requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.latency_raw.total</code>Unit: microsecType: averageBase: svm_nfs_statistics.v3.iops_raw.total conf/keyperf/9.15.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.latency_raw.total</code>Unit: microsecType: averageBase: svm_nfs_statistics.v4.iops_raw.total conf/keyperf/9.15.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/nfsv4_1.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.latency_raw.total</code>Unit: microsecType: averageBase: svm_nfs_statistics.v41.iops_raw.total conf/keyperf/9.15.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>latency</code>Unit: microsecType: averageBase: total_ops conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>latency</code>Unit: microsecType: average,no-zero-valuesBase: total_ops conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>latency</code>Unit: microsecType: average,no-zero-valuesBase: total_ops conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>latency</code>Unit: microsecType: average,no-zero-valuesBase: total_ops conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>latency</code>Unit: microsecType: average,no-zero-valuesBase: total_ops conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml <p>The <code>svm_nfs_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv3 stat NFSv3 Avg Latency ONTAP: SVM NFSv4 stat NFSv4 Avg Latency ONTAP: SVM NFSv4.1 stat NFSv4.1 Avg Latency ONTAP: SVM NFSv4.2 stat NFSv4.2 Avg Latency"},{"location":"ontap-metrics/#svm_nfs_layoutcommit_avg_latency","title":"svm_nfs_layoutcommit_avg_latency","text":"<p>Average latency in microseconds of LAYOUTCOMMIT operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>layoutcommit.average_latency</code>Unit: microsecType: averageBase: layoutcommit.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>layoutcommit.average_latency</code>Unit: microsecType: averageBase: layoutcommit.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>layoutcommit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutcommit_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>layoutcommit_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutcommit_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_layoutcommit_total","title":"svm_nfs_layoutcommit_total","text":"<p>Total number of LAYOUTCOMMIT operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>layoutcommit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>layoutcommit.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>layoutcommit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>layoutcommit_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_layoutget_avg_latency","title":"svm_nfs_layoutget_avg_latency","text":"<p>Average latency in microseconds of LAYOUTGET operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>layoutget.average_latency</code>Unit: microsecType: averageBase: layoutget.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>layoutget.average_latency</code>Unit: microsecType: averageBase: layoutget.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>layoutget_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutget_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>layoutget_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutget_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_layoutget_total","title":"svm_nfs_layoutget_total","text":"<p>Total number of LAYOUTGET operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>layoutget.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>layoutget.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>layoutget_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>layoutget_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_layoutreturn_avg_latency","title":"svm_nfs_layoutreturn_avg_latency","text":"<p>Average latency in microseconds of LAYOUTRETURN operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>layoutreturn.average_latency</code>Unit: microsecType: averageBase: layoutreturn.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>layoutreturn.average_latency</code>Unit: microsecType: averageBase: layoutreturn.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>layoutreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutreturn_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>layoutreturn_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: layoutreturn_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_layoutreturn_total","title":"svm_nfs_layoutreturn_total","text":"<p>Total number of LAYOUTRETURN operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>layoutreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>layoutreturn.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>layoutreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>layoutreturn_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_link_avg_latency","title":"svm_nfs_link_avg_latency","text":"<p>Average latency in microseconds of Link procedure requests. The counter keeps track of the average response time of Link requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>link.average_latency</code>Unit: microsecType: averageBase: link.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>link.average_latency</code>Unit: microsecType: averageBase: link.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>link.average_latency</code>Unit: microsecType: averageBase: link.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>link.average_latency</code>Unit: microsecType: averageBase: link.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>link_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: link_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>link_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: link_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>link_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: link_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>link_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: link_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_link_total","title":"svm_nfs_link_total","text":"<p>Total number Link of procedure requests. It is the total number of Link success and Link error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>link.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>link.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>link.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>link.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>link_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>link_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>link_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>link_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_lock_avg_latency","title":"svm_nfs_lock_avg_latency","text":"<p>Average latency in microseconds of LOCK procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>lock.average_latency</code>Unit: microsecType: averageBase: lock.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>lock.average_latency</code>Unit: microsecType: averageBase: lock.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>lock.average_latency</code>Unit: microsecType: averageBase: lock.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>lock_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lock_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>lock_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lock_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>lock_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lock_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_lock_total","title":"svm_nfs_lock_total","text":"<p>Total number of LOCK procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>lock.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>lock.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>lock.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>lock_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>lock_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>lock_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_lockt_avg_latency","title":"svm_nfs_lockt_avg_latency","text":"<p>Average latency in microseconds of LOCKT procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>lockt.average_latency</code>Unit: microsecType: averageBase: lockt.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>lockt.average_latency</code>Unit: microsecType: averageBase: lockt.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>lockt.average_latency</code>Unit: microsecType: averageBase: lockt.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>lockt_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lockt_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>lockt_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lockt_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>lockt_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lockt_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_lockt_total","title":"svm_nfs_lockt_total","text":"<p>Total number of LOCKT procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>lockt.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>lockt.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>lockt.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>lockt_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>lockt_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>lockt_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_locku_avg_latency","title":"svm_nfs_locku_avg_latency","text":"<p>Average latency in microseconds of LOCKU procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>locku.average_latency</code>Unit: microsecType: averageBase: locku.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>locku.average_latency</code>Unit: microsecType: averageBase: locku.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>locku.average_latency</code>Unit: microsecType: averageBase: locku.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>locku_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: locku_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>locku_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: locku_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>locku_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: locku_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_locku_total","title":"svm_nfs_locku_total","text":"<p>Total number of LOCKU procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>locku.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>locku.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>locku.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>locku_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>locku_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>locku_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_lookup_avg_latency","title":"svm_nfs_lookup_avg_latency","text":"<p>Average latency in microseconds of LookUp procedure requests. This shows the average time it takes for the LookUp operation to reply to the request.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>lookup.average_latency</code>Unit: microsecType: averageBase: lookup.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>lookup.average_latency</code>Unit: microsecType: averageBase: lookup.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>lookup.average_latency</code>Unit: microsecType: averageBase: lookup.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>lookup.average_latency</code>Unit: microsecType: averageBase: lookup.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>lookup_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookup_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>lookup_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookup_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>lookup_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookup_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>lookup_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookup_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_lookup_total","title":"svm_nfs_lookup_total","text":"<p>Total number of Lookup procedure requests. It is the total number of lookup success and lookup error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>lookup.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>lookup.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>lookup.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>lookup.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>lookup_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>lookup_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>lookup_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>lookup_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_lookupp_avg_latency","title":"svm_nfs_lookupp_avg_latency","text":"<p>Average latency in microseconds of LOOKUPP procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>lookupp.average_latency</code>Unit: microsecType: averageBase: lookupp.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>lookupp.average_latency</code>Unit: microsecType: averageBase: lookupp.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>lookupp.average_latency</code>Unit: microsecType: averageBase: lookupp.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>lookupp_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookupp_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>lookupp_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookupp_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>lookupp_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: lookupp_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_lookupp_total","title":"svm_nfs_lookupp_total","text":"<p>Total number of LOOKUPP procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>lookupp.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>lookupp.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>lookupp.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>lookupp_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>lookupp_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>lookupp_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_mkdir_avg_latency","title":"svm_nfs_mkdir_avg_latency","text":"<p>Average latency in microseconds of MkDir procedure requests. The counter keeps track of the average response time of MkDir requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>mkdir.average_latency</code>Unit: microsecType: averageBase: mkdir.total conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>mkdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: mkdir_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_mkdir_total","title":"svm_nfs_mkdir_total","text":"<p>Total number MkDir of procedure requests. It is the total number of MkDir success and MkDir error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>mkdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>mkdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_mknod_avg_latency","title":"svm_nfs_mknod_avg_latency","text":"<p>Average latency in microseconds of MkNod procedure requests. The counter keeps track of the average response time of MkNod requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>mknod.average_latency</code>Unit: microsecType: averageBase: mknod.total conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>mknod_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: mknod_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_mknod_total","title":"svm_nfs_mknod_total","text":"<p>Total number MkNod of procedure requests. It is the total number of MkNod success and MkNod error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>mknod.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>mknod_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_null_avg_latency","title":"svm_nfs_null_avg_latency","text":"<p>Average latency in microseconds of Null procedure requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>null.average_latency</code>Unit: microsecType: averageBase: null.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>null.average_latency</code>Unit: microsecType: averageBase: null.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>null.average_latency</code>Unit: microsecType: averageBase: null.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>null.average_latency</code>Unit: microsecType: averageBase: null.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>null_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: null_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>null_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: null_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>null_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: null_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>null_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: null_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_null_total","title":"svm_nfs_null_total","text":"<p>Total number of Null procedure requests. It is the total of null success and null error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>null.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>null.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>null.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>null.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>null_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>null_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>null_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>null_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_nverify_avg_latency","title":"svm_nfs_nverify_avg_latency","text":"<p>Average latency in microseconds of NVERIFY procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>nverify.average_latency</code>Unit: microsecType: averageBase: nverify.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>nverify.average_latency</code>Unit: microsecType: averageBase: nverify.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>nverify.average_latency</code>Unit: microsecType: averageBase: nverify.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>nverify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: nverify_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>nverify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: nverify_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>nverify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: nverify_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_nverify_total","title":"svm_nfs_nverify_total","text":"<p>Total number of NVERIFY procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>nverify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>nverify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>nverify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>nverify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>nverify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>nverify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_open_avg_latency","title":"svm_nfs_open_avg_latency","text":"<p>Average latency in microseconds of OPEN procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>open.average_latency</code>Unit: microsecType: averageBase: open.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>open.average_latency</code>Unit: microsecType: averageBase: open.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>open.average_latency</code>Unit: microsecType: averageBase: open.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>open_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>open_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>open_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_open_confirm_avg_latency","title":"svm_nfs_open_confirm_avg_latency","text":"<p>Average latency in microseconds of OPEN_CONFIRM procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>open_confirm.average_latency</code>Unit: microsecType: averageBase: open_confirm.total conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>open_confirm_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_confirm_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_open_confirm_total","title":"svm_nfs_open_confirm_total","text":"<p>Total number of OPEN_CONFIRM procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>open_confirm.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>open_confirm_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_open_downgrade_avg_latency","title":"svm_nfs_open_downgrade_avg_latency","text":"<p>Average latency in microseconds of OPEN_DOWNGRADE procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>open_downgrade.average_latency</code>Unit: microsecType: averageBase: open_downgrade.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>open_downgrade.average_latency</code>Unit: microsecType: averageBase: open_downgrade.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>open_downgrade.average_latency</code>Unit: microsecType: averageBase: open_downgrade.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>open_downgrade_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_downgrade_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>open_downgrade_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_downgrade_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>open_downgrade_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: open_downgrade_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_open_downgrade_total","title":"svm_nfs_open_downgrade_total","text":"<p>Total number of OPEN_DOWNGRADE procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>open_downgrade.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>open_downgrade.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>open_downgrade.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>open_downgrade_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>open_downgrade_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>open_downgrade_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_open_total","title":"svm_nfs_open_total","text":"<p>Total number of OPEN procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>open.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>open.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>open.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>open_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>open_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>open_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_openattr_avg_latency","title":"svm_nfs_openattr_avg_latency","text":"<p>Average latency in microseconds of OPENATTR procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>openattr.average_latency</code>Unit: microsecType: averageBase: openattr.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>openattr.average_latency</code>Unit: microsecType: averageBase: openattr.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>openattr.average_latency</code>Unit: microsecType: averageBase: openattr.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>openattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: openattr_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>openattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: openattr_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>openattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: openattr_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_openattr_total","title":"svm_nfs_openattr_total","text":"<p>Total number of OPENATTR procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>openattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>openattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>openattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>openattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>openattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>openattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_ops","title":"svm_nfs_ops","text":"<p>Total number of NFSv3 procedure requests per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>nfsv3_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>total_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>total_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>total_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml <p>The <code>svm_nfs_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv3 stat NFSv3 IOPs ONTAP: SVM NFSv3 timeseries Top $TopResources NFSv3 SVMs by IOPs ONTAP: SVM NFSv4 stat NFSv4 IOPs ONTAP: SVM NFSv4 timeseries Top $TopResources NFSv4 SVMs by IOPs ONTAP: SVM NFSv4.1 stat NFSv4.1 IOPs ONTAP: SVM NFSv4.1 timeseries Top $TopResources NFSv4.1 SVMs by IOPs ONTAP: SVM NFSv4.2 stat NFSv4.2 IOPs ONTAP: SVM NFSv4.2 timeseries Top $TopResources NFSv4.2 SVMs by IOPs"},{"location":"ontap-metrics/#svm_nfs_other_latency","title":"svm_nfs_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.latency_raw.other</code>Unit: microsecType: averageBase: svm_nfs_statistics.v3.iops_raw.other conf/keyperf/9.15.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.latency_raw.other</code>Unit: microsecType: averageBase: svm_nfs_statistics.v4.iops_raw.other conf/keyperf/9.15.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.latency_raw.other</code>Unit: microsecType: averageBase: svm_nfs_statistics.v41.iops_raw.other conf/keyperf/9.15.0/nfsv4_1.yaml"},{"location":"ontap-metrics/#svm_nfs_other_ops","title":"svm_nfs_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv4_1.yaml"},{"location":"ontap-metrics/#svm_nfs_pathconf_avg_latency","title":"svm_nfs_pathconf_avg_latency","text":"<p>Average latency in microseconds of PathConf procedure requests. The counter keeps track of the average response time of PathConf requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>pathconf.average_latency</code>Unit: microsecType: averageBase: pathconf.total conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>pathconf_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: pathconf_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_pathconf_total","title":"svm_nfs_pathconf_total","text":"<p>Total number PathConf of procedure requests. It is the total number of PathConf success and PathConf error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>pathconf.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>pathconf_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_putfh_avg_latency","title":"svm_nfs_putfh_avg_latency","text":"<p>Average latency in microseconds of PUTFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>putfh.average_latency</code>Unit: microsecType: averageBase: putfh.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>putfh.average_latency</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>putfh.average_latency</code>Unit: microsecType: averageBase: putfh.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>putfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putfh_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>putfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putfh_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>putfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putfh_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_putfh_total","title":"svm_nfs_putfh_total","text":"<p>Total number of PUTFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>putfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>putfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>putfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>putfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>putfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>putfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_putpubfh_avg_latency","title":"svm_nfs_putpubfh_avg_latency","text":"<p>Average latency in microseconds of PUTPUBFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>putpubfh.average_latency</code>Unit: microsecType: averageBase: putpubfh.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>putpubfh.average_latency</code>Unit: microsecType: averageBase: putpubfh.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>putpubfh.average_latency</code>Unit: microsecType: averageBase: putpubfh.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>putpubfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putpubfh_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>putpubfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putpubfh_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>putpubfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putpubfh_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_putpubfh_total","title":"svm_nfs_putpubfh_total","text":"<p>Total number of PUTPUBFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>putpubfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>putpubfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>putpubfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>putpubfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>putpubfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>putpubfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_putrootfh_avg_latency","title":"svm_nfs_putrootfh_avg_latency","text":"<p>Average latency in microseconds of PUTROOTFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>putrootfh.average_latency</code>Unit: microsecType: averageBase: putrootfh.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>putrootfh.average_latency</code>Unit: microsecType: averageBase: putrootfh.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>putrootfh.average_latency</code>Unit: microsecType: averageBase: putrootfh.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>putrootfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putrootfh_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>putrootfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putrootfh_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>putrootfh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: putrootfh_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_putrootfh_total","title":"svm_nfs_putrootfh_total","text":"<p>Total number of PUTROOTFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>putrootfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>putrootfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>putrootfh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>putrootfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>putrootfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>putrootfh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_read_avg_latency","title":"svm_nfs_read_avg_latency","text":"<p>Average latency in microseconds of Read procedure requests. The counter keeps track of the average response time of Read requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>read.average_latency</code>Unit: microsecType: averageBase: read.total conf/restperf/9.12.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.latency_raw.read</code>Unit: microsecType: averageBase: svm_nfs_statistics.v3.iops_raw.read conf/keyperf/9.15.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>read.average_latency</code>Unit: microsecType: averageBase: read.total conf/restperf/9.12.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.latency_raw.read</code>Unit: microsecType: averageBase: svm_nfs_statistics.v4.iops_raw.read conf/keyperf/9.15.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>read.average_latency</code>Unit: microsecType: averageBase: read.total conf/restperf/9.12.0/nfsv4_1.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.latency_raw.read</code>Unit: microsecType: averageBase: svm_nfs_statistics.v41.iops_raw.read conf/keyperf/9.15.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>read.average_latency</code>Unit: microsecType: averageBase: read.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>read_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>read_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>read_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>read_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml <p>The <code>svm_nfs_read_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv3 stat NFSv3 Read Latency ONTAP: SVM NFSv3 timeseries Top $TopResources NFSv3 SVMs by Read and Write Latency ONTAP: SVM NFSv4 stat NFSv4 Read Latency ONTAP: SVM NFSv4 timeseries Top $TopResources NFSv4 SVMs by Read and Write Latency ONTAP: SVM NFSv4.1 stat NFSv4.1 Read Latency ONTAP: SVM NFSv4.1 timeseries Top $TopResources NFSv4.1 SVMs by Read and Write Latency ONTAP: SVM NFSv4.2 stat NFSv4.2 Read Latency ONTAP: SVM NFSv4.2 timeseries Top $TopResources NFSv4.2 SVMs by Read and Write Latency"},{"location":"ontap-metrics/#svm_nfs_read_ops","title":"svm_nfs_read_ops","text":"<p>Total observed NFSv3 read operations per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>nfsv3_read_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml <p>The <code>svm_nfs_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv3 stat NFSv3 Read IOPs ONTAP: SVM NFSv3 timeseries Top $TopResources NFSv3 SVMs by IOPs"},{"location":"ontap-metrics/#svm_nfs_read_symlink_avg_latency","title":"svm_nfs_read_symlink_avg_latency","text":"<p>Average latency in microseconds of ReadSymLink procedure requests. The counter keeps track of the average response time of ReadSymLink requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>read_symlink.average_latency</code>Unit: microsecType: averageBase: read_symlink.total conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>read_symlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: read_symlink_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_read_symlink_total","title":"svm_nfs_read_symlink_total","text":"<p>Total number of ReadSymLink procedure requests. It is the total number of read symlink success and read symlink error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>read_symlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>read_symlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_read_throughput","title":"svm_nfs_read_throughput","text":"<p>Rate of NFSv3 read data transfers in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>read_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>total.read_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>total.read_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>total.read_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>nfsv3_read_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>nfs4_read_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>nfs41_read_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>nfs42_read_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml <p>The <code>svm_nfs_read_throughput</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv3 stat NFSv3 Read Throughput ONTAP: SVM NFSv3 timeseries Top $TopResources NFSv3 SVMs by Throughput ONTAP: SVM NFSv4 stat NFSv4 Read Throughput ONTAP: SVM NFSv4 timeseries Top $TopResources NFSv4 SVMs by Throughput ONTAP: SVM NFSv4.1 stat NFSv4.1 Read Throughput ONTAP: SVM NFSv4.1 timeseries Top $TopResources NFSv4.1 SVMs by Throughput ONTAP: SVM NFSv4.2 stat NFSv4.2 Read Throughput ONTAP: SVM NFSv4.2 timeseries Top $TopResources NFSv4.2 SVMs by Throughput"},{"location":"ontap-metrics/#svm_nfs_read_total","title":"svm_nfs_read_total","text":"<p>Total number Read of procedure requests. It is the total number of read success and read error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>read.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>read.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>read.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>read.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>read_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>read_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>read_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>read_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml <p>The <code>svm_nfs_read_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv4 stat NFSv4 Read IOPs ONTAP: SVM NFSv4 timeseries Top $TopResources NFSv4 SVMs by IOPs ONTAP: SVM NFSv4.1 stat NFSv4.1 Read IOPs ONTAP: SVM NFSv4.1 timeseries Top $TopResources NFSv4.1 SVMs by IOPs ONTAP: SVM NFSv4.2 stat NFSv4.2 Read IOPs ONTAP: SVM NFSv4.2 timeseries Top $TopResources NFSv4.2 SVMs by IOPs"},{"location":"ontap-metrics/#svm_nfs_readdir_avg_latency","title":"svm_nfs_readdir_avg_latency","text":"<p>Average latency in microseconds of ReadDir procedure requests. The counter keeps track of the average response time of ReadDir requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>readdir.average_latency</code>Unit: microsecType: averageBase: readdir.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>readdir.average_latency</code>Unit: microsecType: averageBase: readdir.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>readdir.average_latency</code>Unit: microsecType: averageBase: readdir.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>readdir.average_latency</code>Unit: microsecType: averageBase: readdir.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>readdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdir_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>readdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdir_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>readdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdir_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>readdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdir_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_readdir_total","title":"svm_nfs_readdir_total","text":"<p>Total number ReadDir of procedure requests. It is the total number of ReadDir success and ReadDir error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>readdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>readdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>readdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>readdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>readdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>readdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>readdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>readdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_readdirplus_avg_latency","title":"svm_nfs_readdirplus_avg_latency","text":"<p>Average latency in microseconds of ReadDirPlus procedure requests. The counter keeps track of the average response time of ReadDirPlus requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>readdirplus.average_latency</code>Unit: microsecType: averageBase: readdirplus.total conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>readdirplus_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readdirplus_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_readdirplus_total","title":"svm_nfs_readdirplus_total","text":"<p>Total number ReadDirPlus of procedure requests. It is the total number of ReadDirPlus success and ReadDirPlus error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>readdirplus.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>readdirplus_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_readlink_avg_latency","title":"svm_nfs_readlink_avg_latency","text":"<p>Average latency in microseconds of READLINK procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>readlink.average_latency</code>Unit: microsecType: averageBase: readlink.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>readlink.average_latency</code>Unit: microsecType: averageBase: readlink.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>readlink.average_latency</code>Unit: microsecType: averageBase: readlink.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>readlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readlink_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>readlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readlink_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>readlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: readlink_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_readlink_total","title":"svm_nfs_readlink_total","text":"<p>Total number of READLINK procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>readlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>readlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>readlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>readlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>readlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>readlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_reclaim_complete_avg_latency","title":"svm_nfs_reclaim_complete_avg_latency","text":"<p>Average latency in microseconds of RECLAIM_COMPLETE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>reclaim_complete.average_latency</code>Unit: microsecType: averageBase: reclaim_complete.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>reclaim_complete.average_latency</code>Unit: microsecType: averageBase: reclaim_complete.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>reclaim_complete_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: reclaim_complete_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>reclaim_complete_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: reclaim_complete_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_reclaim_complete_total","title":"svm_nfs_reclaim_complete_total","text":"<p>Total number of RECLAIM_COMPLETE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>reclaim_complete.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>reclaim_complete.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>reclaim_complete_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>reclaim_complete_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_release_lock_owner_avg_latency","title":"svm_nfs_release_lock_owner_avg_latency","text":"<p>Average Latency of RELEASE_LOCKOWNER procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>release_lock_owner.average_latency</code>Unit: microsecType: averageBase: release_lock_owner.total conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>release_lock_owner_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: release_lock_owner_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_release_lock_owner_total","title":"svm_nfs_release_lock_owner_total","text":"<p>Total number of RELEASE_LOCKOWNER procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>release_lock_owner.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>release_lock_owner_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_remove_avg_latency","title":"svm_nfs_remove_avg_latency","text":"<p>Average latency in microseconds of Remove procedure requests. The counter keeps track of the average response time of Remove requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>remove.average_latency</code>Unit: microsecType: averageBase: remove.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>remove.average_latency</code>Unit: microsecType: averageBase: remove.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>remove.average_latency</code>Unit: microsecType: averageBase: remove.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>remove.average_latency</code>Unit: microsecType: averageBase: remove.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>remove_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: remove_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>remove_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: remove_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>remove_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: remove_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>remove_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: remove_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_remove_total","title":"svm_nfs_remove_total","text":"<p>Total number Remove of procedure requests. It is the total number of Remove success and Remove error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>remove.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>remove.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>remove.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>remove.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>remove_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>remove_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>remove_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>remove_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_rename_avg_latency","title":"svm_nfs_rename_avg_latency","text":"<p>Average latency in microseconds of Rename procedure requests. The counter keeps track of the average response time of Rename requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>rename.average_latency</code>Unit: microsecType: averageBase: rename.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>rename.average_latency</code>Unit: microsecType: averageBase: rename.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>rename.average_latency</code>Unit: microsecType: averageBase: rename.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>rename.average_latency</code>Unit: microsecType: averageBase: rename.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>rename_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rename_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>rename_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rename_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>rename_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rename_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>rename_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rename_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_rename_total","title":"svm_nfs_rename_total","text":"<p>Total number Rename of procedure requests. It is the total number of Rename success and Rename error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>rename.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>rename.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>rename.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>rename.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>rename_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>rename_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>rename_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>rename_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_renew_avg_latency","title":"svm_nfs_renew_avg_latency","text":"<p>Average latency in microseconds of RENEW procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>renew.average_latency</code>Unit: microsecType: averageBase: renew.total conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>renew_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: renew_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_renew_total","title":"svm_nfs_renew_total","text":"<p>Total number of RENEW procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>renew.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>renew_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_restorefh_avg_latency","title":"svm_nfs_restorefh_avg_latency","text":"<p>Average latency in microseconds of RESTOREFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>restorefh.average_latency</code>Unit: microsecType: averageBase: restorefh.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>restorefh.average_latency</code>Unit: microsecType: averageBase: restorefh.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>restorefh.average_latency</code>Unit: microsecType: averageBase: restorefh.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>restorefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: restorefh_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>restorefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: restorefh_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>restorefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: restorefh_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_restorefh_total","title":"svm_nfs_restorefh_total","text":"<p>Total number of RESTOREFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>restorefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>restorefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>restorefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>restorefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>restorefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>restorefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_rmdir_avg_latency","title":"svm_nfs_rmdir_avg_latency","text":"<p>Average latency in microseconds of RmDir procedure requests. The counter keeps track of the average response time of RmDir requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>rmdir.average_latency</code>Unit: microsecType: averageBase: rmdir.total conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>rmdir_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: rmdir_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_rmdir_total","title":"svm_nfs_rmdir_total","text":"<p>Total number RmDir of procedure requests. It is the total number of RmDir success and RmDir error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>rmdir.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>rmdir_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_savefh_avg_latency","title":"svm_nfs_savefh_avg_latency","text":"<p>Average latency in microseconds of SAVEFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>savefh.average_latency</code>Unit: microsecType: averageBase: savefh.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>savefh.average_latency</code>Unit: microsecType: averageBase: savefh.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>savefh.average_latency</code>Unit: microsecType: averageBase: savefh.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>savefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: savefh_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>savefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: savefh_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>savefh_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: savefh_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_savefh_total","title":"svm_nfs_savefh_total","text":"<p>Total number of SAVEFH procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>savefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>savefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>savefh.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>savefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>savefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>savefh_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_secinfo_avg_latency","title":"svm_nfs_secinfo_avg_latency","text":"<p>Average latency in microseconds of SECINFO procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>secinfo.average_latency</code>Unit: microsecType: averageBase: secinfo.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>secinfo.average_latency</code>Unit: microsecType: averageBase: secinfo.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>secinfo.average_latency</code>Unit: microsecType: averageBase: secinfo.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>secinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>secinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>secinfo_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_secinfo_no_name_avg_latency","title":"svm_nfs_secinfo_no_name_avg_latency","text":"<p>Average latency in microseconds of SECINFO_NO_NAME operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>secinfo_no_name.average_latency</code>Unit: microsecType: averageBase: secinfo_no_name.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>secinfo_no_name.average_latency</code>Unit: microsecType: averageBase: secinfo_no_name.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>secinfo_no_name_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_no_name_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>secinfo_no_name_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: secinfo_no_name_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_secinfo_no_name_total","title":"svm_nfs_secinfo_no_name_total","text":"<p>Total number of SECINFO_NO_NAME operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>secinfo_no_name.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>secinfo_no_name.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>secinfo_no_name_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>secinfo_no_name_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_secinfo_total","title":"svm_nfs_secinfo_total","text":"<p>Total number of SECINFO procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>secinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>secinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>secinfo.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>secinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>secinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>secinfo_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_sequence_avg_latency","title":"svm_nfs_sequence_avg_latency","text":"<p>Average latency in microseconds of SEQUENCE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>sequence.average_latency</code>Unit: microsecType: averageBase: sequence.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>sequence.average_latency</code>Unit: microsecType: averageBase: sequence.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>sequence_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: sequence_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>sequence_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: sequence_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_sequence_total","title":"svm_nfs_sequence_total","text":"<p>Total number of SEQUENCE operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>sequence.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>sequence.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>sequence_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>sequence_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_set_ssv_avg_latency","title":"svm_nfs_set_ssv_avg_latency","text":"<p>Average latency in microseconds of SET_SSV operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>set_ssv.average_latency</code>Unit: microsecType: averageBase: set_ssv.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>set_ssv.average_latency</code>Unit: microsecType: averageBase: set_ssv.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>set_ssv_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: set_ssv_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>set_ssv_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: set_ssv_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_set_ssv_total","title":"svm_nfs_set_ssv_total","text":"<p>Total number of SET_SSV operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>set_ssv.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>set_ssv.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>set_ssv_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>set_ssv_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_setattr_avg_latency","title":"svm_nfs_setattr_avg_latency","text":"<p>Average latency in microseconds of SetAttr procedure requests. The counter keeps track of the average response time of SetAttr requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>setattr.average_latency</code>Unit: microsecType: averageBase: setattr.total conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>setattr.average_latency</code>Unit: microsecType: averageBase: setattr.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>setattr.average_latency</code>Unit: microsecType: averageBase: setattr.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>setattr.average_latency</code>Unit: microsecType: averageBase: setattr.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>setattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setattr_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>setattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setattr_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>setattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setattr_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>setattr_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setattr_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_setattr_total","title":"svm_nfs_setattr_total","text":"<p>Total number of Setattr procedure requests. It is the total number of Setattr success and setattr error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>setattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>setattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>setattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>setattr.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>setattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>setattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>setattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>setattr_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_setclientid_avg_latency","title":"svm_nfs_setclientid_avg_latency","text":"<p>Average latency in microseconds of SETCLIENTID procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>setclientid.average_latency</code>Unit: microsecType: averageBase: setclientid.total conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>setclientid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setclientid_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_setclientid_confirm_avg_latency","title":"svm_nfs_setclientid_confirm_avg_latency","text":"<p>Average latency in microseconds of SETCLIENTID_CONFIRM procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>setclientid_confirm.average_latency</code>Unit: microsecType: averageBase: setclientid_confirm.total conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>setclientid_confirm_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: setclientid_confirm_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_setclientid_confirm_total","title":"svm_nfs_setclientid_confirm_total","text":"<p>Total number of SETCLIENTID_CONFIRM procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>setclientid_confirm.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>setclientid_confirm_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_setclientid_total","title":"svm_nfs_setclientid_total","text":"<p>Total number of SETCLIENTID procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>setclientid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>setclientid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml"},{"location":"ontap-metrics/#svm_nfs_symlink_avg_latency","title":"svm_nfs_symlink_avg_latency","text":"<p>Average latency in microseconds of SymLink procedure requests. The counter keeps track of the average response time of SymLink requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>symlink.average_latency</code>Unit: microsecType: averageBase: symlink.total conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>symlink_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: symlink_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_symlink_total","title":"svm_nfs_symlink_total","text":"<p>Total number SymLink of procedure requests. It is the total number of SymLink success and create SymLink requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>symlink.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>symlink_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml"},{"location":"ontap-metrics/#svm_nfs_test_stateid_avg_latency","title":"svm_nfs_test_stateid_avg_latency","text":"<p>Average latency in microseconds of TEST_STATEID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>test_stateid.average_latency</code>Unit: microsecType: averageBase: test_stateid.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>test_stateid.average_latency</code>Unit: microsecType: averageBase: test_stateid.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>test_stateid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: test_stateid_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>test_stateid_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: test_stateid_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_test_stateid_total","title":"svm_nfs_test_stateid_total","text":"<p>Total number of TEST_STATEID operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>test_stateid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>test_stateid.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>test_stateid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>test_stateid_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_throughput","title":"svm_nfs_throughput","text":"<p>Rate of NFSv3 data transfers in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>total.throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>total.write_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>total.write_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>nfsv3_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>nfs4_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>nfs41_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>nfs42_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml <p>The <code>svm_nfs_throughput</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv3 stat NFSv3 Throughput ONTAP: SVM NFSv3 timeseries Top $TopResources NFSv3 SVMs by Throughput ONTAP: SVM NFSv4 stat NFSv4 Throughput ONTAP: SVM NFSv4.1 stat NFSv4.1 Throughput ONTAP: SVM NFSv4.2 stat NFSv4.2 Throughput"},{"location":"ontap-metrics/#svm_nfs_total_throughput","title":"svm_nfs_total_throughput","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/nfsv4_1.yaml"},{"location":"ontap-metrics/#svm_nfs_verify_avg_latency","title":"svm_nfs_verify_avg_latency","text":"<p>Average latency in microseconds of VERIFY procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>verify.average_latency</code>Unit: microsecType: averageBase: verify.total conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>verify.average_latency</code>Unit: microsecType: averageBase: verify.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>verify.average_latency</code>Unit: microsecType: averageBase: verify.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>verify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: verify_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>verify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: verify_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>verify_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: verify_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_verify_total","title":"svm_nfs_verify_total","text":"<p>Total number of VERIFY procedures</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>verify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>verify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>verify.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>verify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>verify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>verify_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_want_delegation_avg_latency","title":"svm_nfs_want_delegation_avg_latency","text":"<p>Average latency in microseconds of WANT_DELEGATION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>want_delegation.average_latency</code>Unit: microsecType: averageBase: want_delegation.total conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>want_delegation.average_latency</code>Unit: microsecType: averageBase: want_delegation.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>want_delegation_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: want_delegation_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>want_delegation_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: want_delegation_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_want_delegation_total","title":"svm_nfs_want_delegation_total","text":"<p>Total number of WANT_DELEGATION operations.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>want_delegation.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>want_delegation.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>want_delegation_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>want_delegation_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml"},{"location":"ontap-metrics/#svm_nfs_write_avg_latency","title":"svm_nfs_write_avg_latency","text":"<p>Average latency in microseconds of Write procedure requests. The counter keeps track of the average response time of Write requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>write.average_latency</code>Unit: microsecType: averageBase: write.total conf/restperf/9.12.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.latency_raw.write</code>Unit: microsecType: averageBase: svm_nfs_statistics.v3.iops_raw.write conf/keyperf/9.15.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>write.average_latency</code>Unit: microsecType: averageBase: write.total conf/restperf/9.12.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.latency_raw.write</code>Unit: microsecType: averageBase: svm_nfs_statistics.v4.iops_raw.write conf/keyperf/9.15.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>write.average_latency</code>Unit: microsecType: averageBase: write.total conf/restperf/9.12.0/nfsv4_1.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.latency_raw.write</code>Unit: microsecType: averageBase: svm_nfs_statistics.v41.iops_raw.write conf/keyperf/9.15.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>write.average_latency</code>Unit: microsecType: averageBase: write.total conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>write_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: write_total conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>write_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: write_total conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>write_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: write_total conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>write_avg_latency</code>Unit: microsecType: average,no-zero-valuesBase: write_total conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml <p>The <code>svm_nfs_write_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv3 stat NFSv3 Write Latency ONTAP: SVM NFSv3 timeseries Top $TopResources NFSv3 SVMs by Read and Write Latency ONTAP: SVM NFSv4 stat NFSv4 Write Latency ONTAP: SVM NFSv4 timeseries Top $TopResources NFSv4 SVMs by Read and Write Latency ONTAP: SVM NFSv4.1 stat NFSv4.1 Write Latency ONTAP: SVM NFSv4.1 timeseries Top $TopResources NFSv4.1 SVMs by Read and Write Latency ONTAP: SVM NFSv4.2 stat NFSv4.2 Write Latency ONTAP: SVM NFSv4.2 timeseries Top $TopResources NFSv4.2 SVMs by Read and Write Latency"},{"location":"ontap-metrics/#svm_nfs_write_ops","title":"svm_nfs_write_ops","text":"<p>Total observed NFSv3 write operations per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>nfsv3_write_ops</code>Unit: per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml <p>The <code>svm_nfs_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv3 stat NFSv3 Write IOPs ONTAP: SVM NFSv3 timeseries Top $TopResources NFSv3 SVMs by IOPs"},{"location":"ontap-metrics/#svm_nfs_write_throughput","title":"svm_nfs_write_throughput","text":"<p>Rate of NFSv3 write data transfers in bytes per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>write_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv3.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v3.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>total.write_throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v4.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>total.throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml KeyPerf <code>api/protocols/nfs/services</code> <code>statistics.v41.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>total.throughput</code>Unit: b_per_secType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>nfsv3_write_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>nfs4_write_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>nfs41_write_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>nfs42_write_throughput</code>Unit: b_per_secType: rate,no-zero-valuesBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml <p>The <code>svm_nfs_write_throughput</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv3 stat NFSv3 Write Throughput ONTAP: SVM NFSv3 timeseries Top $TopResources NFSv3 SVMs by Throughput ONTAP: SVM NFSv4 stat NFSv4 Write Throughput ONTAP: SVM NFSv4 timeseries Top $TopResources NFSv4 SVMs by Throughput ONTAP: SVM NFSv4.1 stat NFSv4.1 Write Throughput ONTAP: SVM NFSv4.1 timeseries Top $TopResources NFSv4.1 SVMs by Throughput ONTAP: SVM NFSv4.2 stat NFSv4.2 Write Throughput ONTAP: SVM NFSv4.2 timeseries Top $TopResources NFSv4.2 SVMs by Throughput"},{"location":"ontap-metrics/#svm_nfs_write_total","title":"svm_nfs_write_total","text":"<p>Total number of Write procedure requests. It is the total number of write success and write error requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_nfs_v3</code> <code>write.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv3.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v4</code> <code>write.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v41</code> <code>write.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_1.yaml RestPerf <code>api/cluster/counter/tables/svm_nfs_v42</code> <code>write.total</code>Unit: noneType: rateBase: conf/restperf/9.12.0/nfsv4_2.yaml ZapiPerf <code>perf-object-get-instances nfsv3</code> <code>write_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv3.yaml ZapiPerf <code>perf-object-get-instances nfsv4</code> <code>write_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4.yaml ZapiPerf <code>perf-object-get-instances nfsv4_1</code> <code>write_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml ZapiPerf <code>perf-object-get-instances nfsv4_2</code> <code>write_total</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml <p>The <code>svm_nfs_write_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM NFSv4 stat NFSv4 Write IOPs ONTAP: SVM NFSv4 timeseries Top $TopResources NFSv4 SVMs by IOPs ONTAP: SVM NFSv4.1 stat NFSv4.1 Write IOPs ONTAP: SVM NFSv4.1 timeseries Top $TopResources NFSv4.1 SVMs by IOPs ONTAP: SVM NFSv4.2 stat NFSv4.2 Write IOPs ONTAP: SVM NFSv4.2 timeseries Top $TopResources NFSv4.2 SVMs by IOPs"},{"location":"ontap-metrics/#svm_vol_avg_latency","title":"svm_vol_avg_latency","text":"<p>Performance metric aggregated over all types of I/O operations. svm_vol_avg_latency in microseconds is volume_avg_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.total conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>avg_latency</code>Unit: microsecType: averageBase: total_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT SVM Metrics timeseries Top $TopResources Average Latency by SVMs ONTAP: Cluster SVM Performance timeseries Top $TopResources Latency ONTAP: SVM Highlights stat SVM Average Latency"},{"location":"ontap-metrics/#svm_vol_nfs_access_latency","title":"svm_vol_nfs_access_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_access_latency is volume_nfs_access_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.access.total_time</code>Unit: statistics.nfs_ops_raw.access.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_access_latency</code>Unit: microsecType: averageBase: nfs_access_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_access_ops","title":"svm_vol_nfs_access_ops","text":"<p>Number of operations of the given type performed on this volume. svm_vol_nfs_access_ops is volume_nfs_access_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.access.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_access_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_getattr_latency","title":"svm_vol_nfs_getattr_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_getattr_latency is volume_nfs_getattr_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.getattr.total_time</code>Unit: statistics.nfs_ops_raw.getattr.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_getattr_latency</code>Unit: microsecType: averageBase: nfs_getattr_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_getattr_ops","title":"svm_vol_nfs_getattr_ops","text":"<p>Number of operations of the given type performed on this volume. svm_vol_nfs_getattr_ops is volume_nfs_getattr_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.getattr.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_getattr_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_lookup_latency","title":"svm_vol_nfs_lookup_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_lookup_latency is volume_nfs_lookup_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.lookup.total_time</code>Unit: statistics.nfs_ops_raw.lookup.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_lookup_latency</code>Unit: microsecType: averageBase: nfs_lookup_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_lookup_ops","title":"svm_vol_nfs_lookup_ops","text":"<p>Number of operations of the given type performed on this volume. svm_vol_nfs_lookup_ops is volume_nfs_lookup_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.lookup.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_lookup_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_other_latency","title":"svm_vol_nfs_other_latency","text":"<p>Average time for the WAFL filesystem to process other NFS operations to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency in microseconds (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_other_latency is volume_nfs_other_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_other_latency</code>Unit: microsecType: averageBase: nfs_other_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_other_ops","title":"svm_vol_nfs_other_ops","text":"<p>Number of other NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_other_ops is volume_nfs_other_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_punch_hole_latency","title":"svm_vol_nfs_punch_hole_latency","text":"<p>Average time for the WAFL filesystem to process NFS protocol hole-punch requests to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_punch_hole_latency in microseconds is volume_nfs_punch_hole_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_punch_hole_latency</code>Unit: microsecType: averageBase: nfs_punch_hole_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_punch_hole_ops","title":"svm_vol_nfs_punch_hole_ops","text":"<p>Number of NFS hole-punch requests per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_punch_hole_ops is volume_nfs_punch_hole_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_punch_hole_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_read_latency","title":"svm_vol_nfs_read_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_read_latency is volume_nfs_read_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.read.total_time</code>Unit: statistics.nfs_ops_raw.read.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_read_latency</code>Unit: microsecType: averageBase: nfs_read_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_read_ops","title":"svm_vol_nfs_read_ops","text":"<p>Number of operations of the given type performed on this volume. svm_vol_nfs_read_ops is volume_nfs_read_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.read.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_setattr_latency","title":"svm_vol_nfs_setattr_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_setattr_latency is volume_nfs_setattr_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.setattr.total_time</code>Unit: statistics.nfs_ops_raw.setattr.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_setattr_latency</code>Unit: microsecType: averageBase: nfs_setattr_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_setattr_ops","title":"svm_vol_nfs_setattr_ops","text":"<p>Number of operations of the given type performed on this volume. svm_vol_nfs_setattr_ops is volume_nfs_setattr_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.setattr.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_setattr_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_total_ops","title":"svm_vol_nfs_total_ops","text":"<p>Number of total NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_total_ops is volume_nfs_total_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_write_latency","title":"svm_vol_nfs_write_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_write_latency is volume_nfs_write_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.write.total_time</code>Unit: statistics.nfs_ops_raw.write.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_write_latency</code>Unit: microsecType: averageBase: nfs_write_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_nfs_write_ops","title":"svm_vol_nfs_write_ops","text":"<p>Number of operations of the given type performed on this volume. svm_vol_nfs_write_ops is volume_nfs_write_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.write.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_other_data","title":"svm_vol_other_data","text":"<p>Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on. svm_vol_other_data is volume_other_data aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.other</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_other_latency","title":"svm_vol_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on. svm_vol_other_latency in microseconds is volume_other_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.other conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_other_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Highlights timeseries SVM Average Latency"},{"location":"ontap-metrics/#svm_vol_other_ops","title":"svm_vol_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on. svm_vol_other_ops is volume_other_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_other_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFS Troubleshooting Highlights table SVM Performance Table ONTAP: SVM Highlights timeseries SVM IOPs"},{"location":"ontap-metrics/#svm_vol_read_data","title":"svm_vol_read_data","text":"<p>Performance metric for read I/O operations in bytes per seconds. svm_vol_read_data is volume_read_data aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT SVM Metrics timeseries Top $TopResources Average Throughput by SVMs ONTAP: Cluster SVM Performance timeseries Top $TopResources Throughput ONTAP: NFS Troubleshooting Highlights table SVM Performance Table ONTAP: SVM Highlights stat SVM Read Throughput ONTAP: SVM Highlights timeseries SVM Throughput"},{"location":"ontap-metrics/#svm_vol_read_latency","title":"svm_vol_read_latency","text":"<p>Performance metric for read I/O operations. svm_vol_read_latency in microseconds is volume_read_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.read conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Highlights stat SVM Average Read Latency ONTAP: SVM Highlights timeseries SVM Average Latency"},{"location":"ontap-metrics/#svm_vol_read_ops","title":"svm_vol_read_ops","text":"<p>Performance metric for read I/O operations. svm_vol_read_ops is volume_read_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFS Troubleshooting Highlights table SVM Performance Table ONTAP: SVM Highlights stat SVM Read IOPs ONTAP: SVM Highlights timeseries SVM IOPs"},{"location":"ontap-metrics/#svm_vol_total_data","title":"svm_vol_total_data","text":"<p>Performance metric aggregated over all types of I/O operations in bytes per seconds. svm_vol_total_data is volume_total_data aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>NA</code> <code>Harvest generated</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#svm_vol_total_ops","title":"svm_vol_total_ops","text":"<p>Performance metric aggregated over all types of I/O operations. svm_vol_total_ops is volume_total_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT SVM Metrics timeseries Top $TopResources IOPs by SVMs ONTAP: Cluster SVM Performance timeseries Top $TopResources IOPs ONTAP: NFS Troubleshooting Highlights table SVM Performance Table ONTAP: SVM Highlights stat SVM IOPs"},{"location":"ontap-metrics/#svm_vol_write_data","title":"svm_vol_write_data","text":"<p>Performance metric for write I/O operations in bytes per seconds. svm_vol_write_data is volume_write_data aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT SVM Metrics timeseries Top $TopResources Average Throughput by SVMs ONTAP: Cluster SVM Performance timeseries Top $TopResources Throughput ONTAP: NFS Troubleshooting Highlights table SVM Performance Table ONTAP: SVM Highlights stat SVM Throughput ONTAP: SVM Highlights stat SVM Write Throughput ONTAP: SVM Highlights timeseries SVM Throughput"},{"location":"ontap-metrics/#svm_vol_write_latency","title":"svm_vol_write_latency","text":"<p>Performance metric for write I/O operations. svm_vol_write_latency in microseconds is volume_write_latency aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.write</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.write conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Highlights stat SVM Average Write Latency ONTAP: SVM Highlights timeseries SVM Average Latency"},{"location":"ontap-metrics/#svm_vol_write_ops","title":"svm_vol_write_ops","text":"<p>Performance metric for write I/O operations. svm_vol_write_ops is volume_write_ops aggregated by <code>svm</code>.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>svm_vol_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: NFS Troubleshooting Highlights table SVM Performance Table ONTAP: SVM Highlights stat SVM Write IOPs ONTAP: SVM Highlights timeseries SVM IOPs"},{"location":"ontap-metrics/#svm_vscan_connections_active","title":"svm_vscan_connections_active","text":"<p>Total number of current active connections</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_vscan</code> <code>connections_active</code>Unit: noneType: rawBase: conf/restperf/9.13.0/vscan_svm.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan</code> <code>connections_active</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/vscan_svm.yaml <p>The <code>svm_vscan_connections_active</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS timeseries Virus Scan Connections Active ONTAP: Vscan Highlights timeseries Active Connections"},{"location":"ontap-metrics/#svm_vscan_dispatch_latency","title":"svm_vscan_dispatch_latency","text":"<p>Average dispatch latency in microseconds</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_vscan</code> <code>dispatch.latency</code>Unit: microsecType: averageBase: dispatch.requests conf/restperf/9.13.0/vscan_svm.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan</code> <code>dispatch_latency</code>Unit: microsecType: averageBase: dispatch_latency_base conf/zapiperf/cdot/9.8.0/vscan_svm.yaml <p>The <code>svm_vscan_dispatch_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS timeseries Virus Scan Latency ONTAP: Vscan Highlights timeseries Top $TopResources SVM by Dispatch Latency"},{"location":"ontap-metrics/#svm_vscan_scan_latency","title":"svm_vscan_scan_latency","text":"<p>Average scan latency in microseconds</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_vscan</code> <code>scan.latency</code>Unit: microsecType: averageBase: scan.requests conf/restperf/9.13.0/vscan_svm.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan</code> <code>scan_latency</code>Unit: microsecType: averageBase: scan_latency_base conf/zapiperf/cdot/9.8.0/vscan_svm.yaml <p>The <code>svm_vscan_scan_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS timeseries Virus Scan Latency ONTAP: Vscan Highlights timeseries Top $TopResources SVMs by Scan Latency"},{"location":"ontap-metrics/#svm_vscan_scan_noti_received_rate","title":"svm_vscan_scan_noti_received_rate","text":"<p>Total number of scan notifications received by the dispatcher per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_vscan</code> <code>scan.notification_received_rate</code>Unit: per_secType: rateBase: conf/restperf/9.13.0/vscan_svm.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan</code> <code>scan_noti_received_rate</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/vscan_svm.yaml <p>The <code>svm_vscan_scan_noti_received_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS timeseries Virus Scan Requests ONTAP: Vscan Highlights timeseries Top $TopResources SVMs by Scan Notifications Received Throughput"},{"location":"ontap-metrics/#svm_vscan_scan_request_dispatched_rate","title":"svm_vscan_scan_request_dispatched_rate","text":"<p>Total number of scan requests sent to the Vscanner per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/svm_vscan</code> <code>scan.request_dispatched_rate</code>Unit: per_secType: rateBase: conf/restperf/9.13.0/vscan_svm.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan</code> <code>scan_request_dispatched_rate</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/vscan_svm.yaml <p>The <code>svm_vscan_scan_request_dispatched_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM CIFS timeseries Virus Scan Requests ONTAP: Vscan Highlights timeseries Top $TopResources SVMs by Scan Requests Sent to Vscanner Throughput"},{"location":"ontap-metrics/#token_copy_bytes","title":"token_copy_bytes","text":"<p>Total number of bytes copied.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/token_manager</code> <code>token_copy.bytes</code>Unit: noneType: rateBase: conf/restperf/9.12.0/token_manager.yaml ZapiPerf <code>perf-object-get-instances token_manager</code> <code>token_copy_bytes</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/token_manager.yaml"},{"location":"ontap-metrics/#token_copy_failure","title":"token_copy_failure","text":"<p>Number of failed token copy requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/token_manager</code> <code>token_copy.failures</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/token_manager.yaml ZapiPerf <code>perf-object-get-instances token_manager</code> <code>token_copy_failure</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/token_manager.yaml"},{"location":"ontap-metrics/#token_copy_success","title":"token_copy_success","text":"<p>Number of successful token copy requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/token_manager</code> <code>token_copy.successes</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/token_manager.yaml ZapiPerf <code>perf-object-get-instances token_manager</code> <code>token_copy_success</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/token_manager.yaml"},{"location":"ontap-metrics/#token_create_bytes","title":"token_create_bytes","text":"<p>Total number of bytes for which tokens are created.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/token_manager</code> <code>token_create.bytes</code>Unit: noneType: rateBase: conf/restperf/9.12.0/token_manager.yaml ZapiPerf <code>perf-object-get-instances token_manager</code> <code>token_create_bytes</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/token_manager.yaml"},{"location":"ontap-metrics/#token_create_failure","title":"token_create_failure","text":"<p>Number of failed token create requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/token_manager</code> <code>token_create.failures</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/token_manager.yaml ZapiPerf <code>perf-object-get-instances token_manager</code> <code>token_create_failure</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/token_manager.yaml"},{"location":"ontap-metrics/#token_create_success","title":"token_create_success","text":"<p>Number of successful token create requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/token_manager</code> <code>token_create.successes</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/token_manager.yaml ZapiPerf <code>perf-object-get-instances token_manager</code> <code>token_create_success</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/token_manager.yaml"},{"location":"ontap-metrics/#token_zero_bytes","title":"token_zero_bytes","text":"<p>Total number of bytes zeroed.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/token_manager</code> <code>token_zero.bytes</code>Unit: noneType: rateBase: conf/restperf/9.12.0/token_manager.yaml ZapiPerf <code>perf-object-get-instances token_manager</code> <code>token_zero_bytes</code>Unit: noneType: rateBase: conf/zapiperf/cdot/9.8.0/token_manager.yaml"},{"location":"ontap-metrics/#token_zero_failure","title":"token_zero_failure","text":"<p>Number of failed token zero requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/token_manager</code> <code>token_zero.failures</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/token_manager.yaml ZapiPerf <code>perf-object-get-instances token_manager</code> <code>token_zero_failure</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/token_manager.yaml"},{"location":"ontap-metrics/#token_zero_success","title":"token_zero_success","text":"<p>Number of successful token zero requests.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/token_manager</code> <code>token_zero.successes</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/token_manager.yaml ZapiPerf <code>perf-object-get-instances token_manager</code> <code>token_zero_success</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/token_manager.yaml"},{"location":"ontap-metrics/#volume_analytics_bytes_used_by_accessed_time","title":"volume_analytics_bytes_used_by_accessed_time","text":"<p>Number of bytes used on-disk, broken down by date of last access.</p> API Endpoint Metric Template REST <code>api/storage/volumes/{volume.uuid}/files</code> <code>analytics.by_accessed_time.bytes_used.values</code> conf/rest/9.12.0/volume_analytics.yaml <p>The <code>volume_analytics_bytes_used_by_accessed_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Access ($Activity) History"},{"location":"ontap-metrics/#volume_analytics_bytes_used_by_modified_time","title":"volume_analytics_bytes_used_by_modified_time","text":"<p>Number of bytes used on-disk, broken down by date of last modification.</p> API Endpoint Metric Template REST <code>api/storage/volumes/{volume.uuid}/files</code> <code>analytics.by_modified_time.bytes_used.values</code> conf/rest/9.12.0/volume_analytics.yaml <p>The <code>volume_analytics_bytes_used_by_modified_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Modify ($Activity) History"},{"location":"ontap-metrics/#volume_analytics_bytes_used_percent_by_accessed_time","title":"volume_analytics_bytes_used_percent_by_accessed_time","text":"<p>Percent used on-disk, broken down by date of last access.</p> API Endpoint Metric Template REST <code>api/storage/volumes/{volume.uuid}/files</code> <code>analytics.by_accessed_time.bytes_used.percentages</code> conf/rest/9.12.0/volume_analytics.yaml <p>The <code>volume_analytics_bytes_used_percent_by_accessed_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Access ($Activity) History By Percent"},{"location":"ontap-metrics/#volume_analytics_bytes_used_percent_by_modified_time","title":"volume_analytics_bytes_used_percent_by_modified_time","text":"<p>Percent used on-disk, broken down by date of last modification.</p> API Endpoint Metric Template REST <code>api/storage/volumes/{volume.uuid}/files</code> <code>analytics.by_modified_time.bytes_used.percentages</code> conf/rest/9.12.0/volume_analytics.yaml <p>The <code>volume_analytics_bytes_used_percent_by_modified_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Modify ($Activity) History By Percent"},{"location":"ontap-metrics/#volume_analytics_dir_bytes_used","title":"volume_analytics_dir_bytes_used","text":"<p>The actual number of bytes used on disk by this file.</p> API Endpoint Metric Template REST <code>api/storage/volumes/{volume.uuid}/files</code> <code>analytics.bytes_used</code> conf/rest/9.12.0/volume_analytics.yaml <p>The <code>volume_analytics_dir_bytes_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: File System Analytics (FSA) Highlights timeseries Top $TopResources Volumes by Directory Growth ONTAP: File System Analytics (FSA) Highlights table Top $TopResources Volumes by Directory Growth"},{"location":"ontap-metrics/#volume_analytics_dir_file_count","title":"volume_analytics_dir_file_count","text":"<p>Number of files in a directory.</p> API Endpoint Metric Template REST <code>api/storage/volumes/{volume.uuid}/files</code> <code>analytics.file_count</code> conf/rest/9.12.0/volume_analytics.yaml <p>The <code>volume_analytics_dir_file_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: File System Analytics (FSA) Highlights stat Files ONTAP: File System Analytics (FSA) Highlights table Top $TopResources Volumes by Directory Growth"},{"location":"ontap-metrics/#volume_analytics_dir_subdir_count","title":"volume_analytics_dir_subdir_count","text":"<p>Number of sub directories in a directory.</p> API Endpoint Metric Template REST <code>api/storage/volumes/{volume.uuid}/files</code> <code>analytics.subdir_count</code> conf/rest/9.12.0/volume_analytics.yaml <p>The <code>volume_analytics_dir_subdir_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: File System Analytics (FSA) Highlights stat Directories ONTAP: File System Analytics (FSA) Highlights table Top $TopResources Volumes by Directory Growth"},{"location":"ontap-metrics/#volume_arw_status","title":"volume_arw_status","text":"<p>Represent the cluster level ARW status</p> API Endpoint Metric Template Rest <code>NA</code> <code>Harvest generated</code> conf/rest/9.14.0/volume.yaml <p>The <code>volume_arw_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Security Cluster Compliance table Cluster Compliance"},{"location":"ontap-metrics/#volume_autosize_grow_threshold_percent","title":"volume_autosize_grow_threshold_percent","text":"<p>Used space threshold which triggers autogrow. When the size-used is greater than this percent of size-total, the volume will be grown. The computed value is rounded down. The default value of this element varies from 85% to 98%, depending on the volume size. It is an error for the grow threshold to be less than or equal to the shrink threshold.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>autosize_grow_threshold_percent</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-autosize-attributes.grow-threshold-percent</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_autosize_grow_threshold_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Volume Autosize Table table Volumes Autogrow"},{"location":"ontap-metrics/#volume_autosize_maximum_size","title":"volume_autosize_maximum_size","text":"<p>The maximum size (in bytes) to which the volume would be grown automatically. The default value is 20% greater than the volume size. It is an error for the maximum volume size to be less than the current volume size. It is also an error for the maximum size to be less than or equal to the minimum size.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>max_autosize</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-autosize-attributes.maximum-size</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_autosize_maximum_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Volume Autosize Table table Volumes Autogrow"},{"location":"ontap-metrics/#volume_autosize_minimum_size","title":"volume_autosize_minimum_size","text":"<p>Minimum size in bytes up to which the volume shrinks automatically. This size cannot be greater than or equal to the maximum size of volume.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>min_autosize</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-autosize-attributes.minimum-size</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_autosize_minimum_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Volume Autosize Table table Volumes Autogrow"},{"location":"ontap-metrics/#volume_autosize_shrink_threshold_percent","title":"volume_autosize_shrink_threshold_percent","text":"<p>Used space threshold size, in percentage, for the automatic shrinkage of the volume. When the amount of used space in the volume drops below this threshold, the volume automatically shrinks unless it has reached the minimum size. The volume shrinks when the 'space.used' is less than the 'shrink_threshold' percent of 'space.size'. The 'shrink_threshold' size cannot be greater than or equal to the 'grow_threshold' size.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>autosize_shrink_threshold_percent</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-autosize-attributes.shrink-threshold-percent</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_autosize_shrink_threshold_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Volume Autosize Table table Volumes Autogrow"},{"location":"ontap-metrics/#volume_avg_latency","title":"volume_avg_latency","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.total</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.total conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>avg_latency</code>Unit: microsecType: averageBase: total_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_avg_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by Average Latency ONTAP: cDOT Volume Metrics timeseries Top $TopResources Volumes by Average Latency ONTAP: Cluster Throughput timeseries Max Latency ONTAP: Datacenter Performance timeseries Top $TopResources Latency by Cluster ONTAP: FlexGroup Highlights timeseries Top $TopResources Constituents by Average Latency ONTAP: MetroCluster Highlights stat Volume Average Latency ONTAP: Node Volume Performance timeseries Top $TopResources Volumes by Average Latency ONTAP: Volume Highlights stat Volume Average Latency ONTAP: Volume Highlights timeseries Top $TopResources Volumes by Average Latency ONTAP: Volume Deep Dive Highlights stat Avg Latency ONTAP: Volume Deep Dive Highlights stat Max Latency"},{"location":"ontap-metrics/#volume_capacity_tier_footprint","title":"volume_capacity_tier_footprint","text":"<p>This field represents the footprint of blocks written to the volume in bytes for bin 1.</p> API Endpoint Metric Template REST <code>api/private/cli/volume/footprint</code> <code>volume_blocks_footprint_bin1</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-footprint-get-iter</code> <code>volume-blocks-footprint-bin1</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_capacity_tier_footprint</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Capacity timeseries Top $TopResources Volumes by Capacity Tier Footprint ONTAP: FlexGroup Top Volume FabricPool timeseries Top $TopResources Volumes by Capacity Tier Footprint ONTAP: Volume Volume Hot-Cold Data table Volumes by Cold data ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Cold Data ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Cold Data % ONTAP: Volume FabricPool table Volumes Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Capacity Tier Footprint"},{"location":"ontap-metrics/#volume_capacity_tier_footprint_percent","title":"volume_capacity_tier_footprint_percent","text":"<p>This field represents the footprint of blocks written to the volume in bin 1 as a percentage of aggregate size.</p> API Endpoint Metric Template REST <code>api/private/cli/volume/footprint</code> <code>volume_blocks_footprint_bin1_percent</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-footprint-get-iter</code> <code>volume-blocks-footprint-bin1-percent</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_capacity_tier_footprint_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Capacity timeseries Top $TopResources Volumes by Capacity Tier Footprint % ONTAP: FlexGroup Top Volume FabricPool timeseries Top $TopResources Volumes by Capacity Tier Footprint % ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Capacity Tier Footprint %"},{"location":"ontap-metrics/#volume_clone_split_estimate","title":"volume_clone_split_estimate","text":"<p>Display an estimate of additional storage required in the underlying aggregate to perform a volume clone split operation.</p> API Endpoint Metric Template REST <code>clone.split_estimate</code> <code>api/storage/volumes</code> conf/rest/9.12.0/volume.yaml ZAPI <code>volume-clone-get-iter</code> <code>split-estimate</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_clone_split_estimate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Volume Table table Volumes in Cluster ONTAP: Volume Deep Dive Volume Capacity: $Volume table Volumes in Cluster"},{"location":"ontap-metrics/#volume_delayed_free_footprint","title":"volume_delayed_free_footprint","text":"<p>This field represents the delayed free blocks footprint in bytes. This system is used to improve delete performance by batching delete requests.</p> API Endpoint Metric Template REST <code>api/private/cli/volume/footprint</code> <code>delayed_free_footprint</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-footprint-get-iter</code> <code>delayed-free-footprint</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_delayed_free_footprint</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume FabricPool table Volumes Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Delayed Free Footprint"},{"location":"ontap-metrics/#volume_filesystem_size","title":"volume_filesystem_size","text":"<p>Filesystem size (in bytes) of the volume.  This is the total usable size of the volume, not including WAFL reserve.  This value is the same as Size except for certain SnapMirror destination volumes.  It is possible for destination volumes to have a different filesystem-size because the filesystem-size is sent across from the source volume.  This field is valid only when the volume is online.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>filesystem_size</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.filesystem-size</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_guarantee_footprint","title":"volume_guarantee_footprint","text":"<p>This field represents the volume guarantee footprint in bytes. Alternatively, it is the space reserved for future writes in the volume.</p> API Endpoint Metric Template REST <code>api/private/cli/volume/footprint</code> <code>volume_guarantee_footprint</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-footprint-get-iter</code> <code>volume-guarantee-footprint</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_guarantee_footprint</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume FabricPool table Volumes Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Capacity Tier Footprint"},{"location":"ontap-metrics/#volume_hot_data","title":"volume_hot_data","text":"<p>Hot data size that is physically used in the volume.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.14.0/volume.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_hot_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Volume Hot-Cold Data table Volumes by Cold data ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Hot Data ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Hot Data %"},{"location":"ontap-metrics/#volume_inode_files_total","title":"volume_inode_files_total","text":"<p>Total user-visible file (inode) count, i.e., current maximum number of user-visible files (inodes) that this volume can currently hold.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>files</code> conf/rest/9.12.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-inode-attributes.files-total</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_inode_files_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Inode timeseries Top $TopResources Volumes by Inode Files Total ONTAP: Volume Deep Dive Inodes timeseries Inode Files Total"},{"location":"ontap-metrics/#volume_inode_files_used","title":"volume_inode_files_used","text":"<p>Number of user-visible files (inodes) used. This field is valid only when the volume is online.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>files_used</code> conf/rest/9.12.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-inode-attributes.files-used</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_inode_files_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Inode timeseries Top $TopResources Volumes by Inode Files Used ONTAP: Volume Deep Dive Inodes timeseries Inode Files Used"},{"location":"ontap-metrics/#volume_inode_used_percent","title":"volume_inode_used_percent","text":"<p>volume_inode_files_used / volume_inode_total</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>inode_files_used, inode_files_total</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>inode_files_used, inode_files_total</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_inode_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Inode timeseries Top $TopResources Volumes by Inode Files Used Percentage ONTAP: Volume Deep Dive Inodes timeseries Inode Files Used Percentage"},{"location":"ontap-metrics/#volume_labels","title":"volume_labels","text":"<p>This metric provides information about Volume</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>Harvest generated</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT Capacity Metrics table Top $TopResources Volumes by Capacity Used % ONTAP: cDOT Capacity Metrics timeseries Top $TopResources Volumes by Capacity Used % ONTAP: Data Protection Highlights piechart Snapshot copies (local) ONTAP: Data Protection Highlights piechart SnapMirrors (local or remote) ONTAP: Data Protection Highlights piechart Back up to cloud ONTAP: Data Protection Highlights table Volumes ONTAP: Data Protection Snapshot Copies stat Volumes not protected ONTAP: Data Protection Snapshot Copies stat Volumes protected ONTAP: Data Protection Snapshot Copies stat Volumes breached ONTAP: Data Protection Snapshot Copies stat Volumes not breached ONTAP: Data Protection Snapshot Copies table Volumes Protected With Snapshot Copies (local) ONTAP: Data Protection Snapshot Copies table Volumes Breaching Snapshot Copy Reserve Space ONTAP: Data Protection Snapshot Copies stat &lt;10 Copies  ONTAP: Data Protection Snapshot Copies stat 10-100 Copies ONTAP: Data Protection Snapshot Copies stat 101-500 Copies ONTAP: Data Protection Snapshot Copies stat &gt;500 Copies ONTAP: Data Protection Snapshot Copies table Volume count by the number of Snapshot copies ONTAP: Datacenter Highlights table Object Count ONTAP: Datacenter Snapshots piechart Protected Status ONTAP: Datacenter Snapshots piechart Breached Status ONTAP: Datacenter Snapshots piechart Snapshot Copies ONTAP: FlexGroup Volume Table table FlexGroup Constituents in Cluster ONTAP: File System Analytics (FSA) Highlights stat Used ONTAP: File System Analytics (FSA) Highlights stat Available ONTAP: File System Analytics (FSA) Highlights stat Size ONTAP: File System Analytics (FSA) Highlights bargauge Used Percentage ONTAP: File System Analytics (FSA) Highlights stat Directories ONTAP: File System Analytics (FSA) Highlights timeseries Top $TopResources Volumes by Directory Growth ONTAP: File System Analytics (FSA) Highlights stat Files ONTAP: File System Analytics (FSA) Highlights table Top $TopResources Volumes by Directory Growth ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Access ($Activity) History ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Access ($Activity) History By Percent ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Modify ($Activity) History ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Modify ($Activity) History By Percent ONTAP: Health Volume table Volumes with Ransomware Issues (9.10+ Only) ONTAP: Health Volume table Volumes Move Issues ONTAP: NFS Troubleshooting Highlights table SVM Performance Table ONTAP: Quota Highlights table Reports ONTAP: Quota Space Usage timeseries Top $TopResources Quotas by Space Used ONTAP: Quota Space Usage timeseries Top $TopResources Quotas by Space Used % ONTAP: Quota Space Usage timeseries Top $TopResources Quotas by Files Used ONTAP: Quota Space Usage timeseries Top $TopResources Quotas by Files Used % ONTAP: S3 Object Storage Highlights table Bucket Overview ONTAP: Security Highlights stat Volume Encryption % ONTAP: Security Highlights stat Volume Autonomous Ransomware Protection % ONTAP: Security Highlights piechart Volume Encryption ONTAP: Security Highlights piechart Volume Autonomous Ransomware Protection ONTAP: Security Volume Encryption &amp; Autonomous Ransomware Protection table Volume Encryption &amp; Autonomous Ransomware Protection ONTAP: Security Cluster Compliance table Cluster Compliance ONTAP: SnapMirror Sources Highlights stat Unprotected Volumes ONTAP: SnapMirror Sources Highlights stat Protected Volumes ONTAP: Volume Highlights stat Volume Average Latency ONTAP: Volume Highlights stat Top $TopResources Volumes Total Throughput ONTAP: Volume Highlights stat Top $TopResources Volumes by Total IOPs ONTAP: Volume Highlights timeseries Top $TopResources Volumes by Average Latency ONTAP: Volume Highlights timeseries Top $TopResources Volumes by Average Throughput ONTAP: Volume Highlights timeseries Top $TopResources Volumes by Total IOPs ONTAP: Volume Volume Table table Volumes in Cluster ONTAP: Volume Volume Table table Top $TopResources Volumes by Read Latency ONTAP: Volume Volume Table table Top $TopResources Volumes by Read Throughput ONTAP: Volume Volume Table table Top $TopResources Volumes by Read IOPS ONTAP: Volume Volume Table table Top $TopResources Volumes by Write Latency ONTAP: Volume Volume Table table Top $TopResources Volumes by Write Throughput ONTAP: Volume Volume Table table Top $TopResources Volumes by Write IOPS ONTAP: Volume Snaplock table Volume Snaplock ONTAP: Volume Volume Hot-Cold Data table Volumes by Cold data ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Cold Data ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Hot Data ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Cold Data % ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Hot Data % ONTAP: Volume Volume Autosize Table table Volumes Autogrow ONTAP: Volume Performance timeseries Top $TopResources Volumes by Read Latency ONTAP: Volume Performance timeseries Top $TopResources Volumes by Write Latency ONTAP: Volume Performance timeseries Top $TopResources Volumes by Other Latency ONTAP: Volume Performance timeseries Top $TopResources Volumes by Read IOPs ONTAP: Volume Performance timeseries Top $TopResources Volumes by Write IOPs ONTAP: Volume Performance timeseries Top $TopResources Volumes by Other IOPs ONTAP: Volume Performance timeseries Top $TopResources Volumes by Read Throughput ONTAP: Volume Performance timeseries Top $TopResources Volumes by Write Throughput ONTAP: Volume QoS stat Top $TopResources QoS Volumes Latency ONTAP: Volume QoS stat Top $TopResources Qos Volumes Total Throughput ONTAP: Volume QoS stat Top $TopResources QoS Volumes by Total IOPs ONTAP: Volume QoS timeseries Top $TopResources QoS Volumes by Latency ONTAP: Volume QoS timeseries Top $TopResources QoS Volumes by Average Throughput ONTAP: Volume QoS timeseries Top $TopResources QoS Volumes by Total IOPs ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Read Latency ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Write Latency ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Read IOPS ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Write IOPS ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Other IOPS ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Read Throughput ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Write Throughput ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Sequential Reads ONTAP: Volume QoS timeseries Top $TopResources Volumes by QoS Volume Sequential Writes ONTAP: Volume I/O Density timeseries Top $TopResources Volumes by IO Density (IOPs/TiB) ONTAP: Volume I/O Density timeseries Bottom $TopResources Volumes by IO Density (IOPs/TiB) ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Physical Space Used ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Logical Space Used ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Volume Size Used ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Volume Total Size ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Size Available ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Reserve Size ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Reserve Available ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Size Used ONTAP: Volume Capacity timeseries Top $TopResources Volumes by Inactive Data ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Physical Space Used ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Logical Space Used ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Volume Size Used ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Snapshot Reserve ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Snapshot Reserve Used ONTAP: Volume Capacity % timeseries Top $TopResources Volumes by Inactive Data ONTAP: Volume Clients timeseries Top $TopResources Volumes Clients by Read IOPs ONTAP: Volume Clients timeseries Top $TopResources Volumes Clients by Write IOPs ONTAP: Volume Clients timeseries Top $TopResources Volumes Clients by Read Throughput ONTAP: Volume Clients timeseries Top $TopResources Volumes Clients by Write Throughput ONTAP: Volume Files timeseries Top $TopResources Volumes Files by Read IOPs ONTAP: Volume Files timeseries Top $TopResources Volumes Files by Write IOPs ONTAP: Volume Files timeseries Top $TopResources Volumes Files by Read Throughput ONTAP: Volume Files timeseries Top $TopResources Volumes Files by Write Throughput ONTAP: Volume Users timeseries Top $TopResources Volumes Users by Read IOPs ONTAP: Volume Users timeseries Top $TopResources Volumes Users by Write IOPs ONTAP: Volume Users timeseries Top $TopResources Volumes Users by Read Throughput ONTAP: Volume Users timeseries Top $TopResources Volumes Users by Write Throughput ONTAP: Volume Object Storage timeseries Top $TopResources Volumes by Object Storage GET Latency ONTAP: Volume Object Storage timeseries Top $TopResources Volumes by Object Storage GET Request Count ONTAP: Volume Object Storage timeseries Top $TopResources Volumes by Object Storage PUT Latency ONTAP: Volume Object Storage timeseries Top $TopResources Volumes by Object Storage PUT Request Count ONTAP: Volume Object Storage table Top $TopResources Volumes by Object Storage Requests ONTAP: Volume FabricPool table Volumes Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Performance Tier Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Performance Tier Footprint % ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Capacity Tier Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Capacity Tier Footprint % ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Delayed Free Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Metadata Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Total Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Total Metadata Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Capacity Tier Footprint ONTAP: Volume Inode timeseries Top $TopResources Volumes by Inode Files Used ONTAP: Volume Inode timeseries Top $TopResources Volumes by Inode Files Total ONTAP: Volume Inode timeseries Top $TopResources Volumes by Inode Files Used Percentage ONTAP: Volume Sis Stat timeseries Top $TopResources Volumes by Number of Compress Fail % ONTAP: Volume Sis Stat timeseries Top $TopResources Volumes by Number of Compress Attempts ONTAP: Volume Sis Stat timeseries Top $TopResources Volumes by Number of Compress Fail ONTAP: Volume Growth Rate timeseries Top $TopResources Volumes Per Growth Rate of Physical Used ONTAP: Volume Growth Rate timeseries Top $TopResources Volumes Per Growth Rate of Logical Used ONTAP: Volume Growth Rate table Top $TopResources Volumes by Physical Usage: Delta ONTAP: Volume Growth Rate table Top $TopResources Volumes by Logical Usage: Delta ONTAP: Volume Forecast Volume Capacity table Top $TopResources Volumes Per Size Used Percentage Trend ONTAP: Volume by SVM Highlights table Volume Performance for $SVM (Click volume for detailed drill-down) ONTAP: Volume Deep Dive Highlights table Volume Performance ONTAP: Volume Deep Dive Volume Capacity: $Volume table Volumes in Cluster"},{"location":"ontap-metrics/#volume_metadata_footprint","title":"volume_metadata_footprint","text":"<p>This field represents flexible volume or flexgroup metadata in bytes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume/footprint</code> <code>flexvol_metadata_footprint</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-footprint-get-iter</code> <code>flexvol-metadata-footprint</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_metadata_footprint</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume FabricPool table Volumes Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Metadata Footprint"},{"location":"ontap-metrics/#volume_new_status","title":"volume_new_status","text":"<p>This metric indicates a value of 1 if the volume state is online (indicating the volume is operational) and a value of 0 for any other state.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/volume.yaml ZAPI <code>NA</code> <code>Harvest generated</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_new_status</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexGroup Volume Table table FlexGroup Constituents in Cluster ONTAP: Health Volume table Volumes with Ransomware Issues (9.10+ Only) ONTAP: Health Volume table Volumes Move Issues ONTAP: Volume Volume Table table Volumes in Cluster ONTAP: Volume Deep Dive Volume Capacity: $Volume table Volumes in Cluster"},{"location":"ontap-metrics/#volume_nfs_access_latency","title":"volume_nfs_access_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.access.total_time</code>Unit: statistics.nfs_ops_raw.access.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_access_latency</code>Unit: microsecType: averageBase: nfs_access_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_access_ops","title":"volume_nfs_access_ops","text":"<p>Number of operations of the given type performed on this volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.access.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_access_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_getattr_latency","title":"volume_nfs_getattr_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.getattr.total_time</code>Unit: statistics.nfs_ops_raw.getattr.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_getattr_latency</code>Unit: microsecType: averageBase: nfs_getattr_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_getattr_ops","title":"volume_nfs_getattr_ops","text":"<p>Number of operations of the given type performed on this volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.getattr.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_getattr_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_lookup_latency","title":"volume_nfs_lookup_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.lookup.total_time</code>Unit: statistics.nfs_ops_raw.lookup.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_lookup_latency</code>Unit: microsecType: averageBase: nfs_lookup_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_lookup_ops","title":"volume_nfs_lookup_ops","text":"<p>Number of operations of the given type performed on this volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.lookup.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_lookup_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_other_latency","title":"volume_nfs_other_latency","text":"<p>Average time for the WAFL filesystem to process other NFS operations to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency in microseconds (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_other_latency</code>Unit: microsecType: averageBase: nfs_other_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_other_ops","title":"volume_nfs_other_ops","text":"<p>Number of other NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_punch_hole_latency","title":"volume_nfs_punch_hole_latency","text":"<p>Average time for the WAFL filesystem to process NFS protocol hole-punch requests to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_punch_hole_latency</code>Unit: microsecType: averageBase: nfs_punch_hole_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_punch_hole_ops","title":"volume_nfs_punch_hole_ops","text":"<p>Number of NFS hole-punch requests per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_punch_hole_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_read_latency","title":"volume_nfs_read_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.read.total_time</code>Unit: statistics.nfs_ops_raw.read.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_read_latency</code>Unit: microsecType: averageBase: nfs_read_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_read_ops","title":"volume_nfs_read_ops","text":"<p>Number of operations of the given type performed on this volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.read.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_setattr_latency","title":"volume_nfs_setattr_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.setattr.total_time</code>Unit: statistics.nfs_ops_raw.setattr.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_setattr_latency</code>Unit: microsecType: averageBase: nfs_setattr_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_setattr_ops","title":"volume_nfs_setattr_ops","text":"<p>Number of operations of the given type performed on this volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.setattr.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_setattr_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_total_ops","title":"volume_nfs_total_ops","text":"<p>Number of total NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_write_latency","title":"volume_nfs_write_latency","text":"<p>The raw data component latency in microseconds measured within ONTAP for all operations of the given type.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.write.total_time</code>Unit: statistics.nfs_ops_raw.write.countType: averageBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_write_latency</code>Unit: microsecType: averageBase: nfs_write_ops conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_nfs_write_ops","title":"volume_nfs_write_ops","text":"<p>Number of operations of the given type performed on this volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.nfs_ops_raw.write.count</code>Unit: Type: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>nfs_write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_num_compress_attempts","title":"volume_num_compress_attempts","text":"API Endpoint Metric Template REST <code>api/private/cli/volume/efficiency/stat</code> <code>num_compress_attempts</code> conf/rest/9.14.0/volume.yaml <p>The <code>volume_num_compress_attempts</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Sis Stat timeseries Top $TopResources Volumes by Number of Compress Fail % ONTAP: Volume Sis Stat timeseries Top $TopResources Volumes by Number of Compress Attempts"},{"location":"ontap-metrics/#volume_num_compress_fail","title":"volume_num_compress_fail","text":"API Endpoint Metric Template REST <code>api/private/cli/volume/efficiency/stat</code> <code>num_compress_fail</code> conf/rest/9.14.0/volume.yaml <p>The <code>volume_num_compress_fail</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Sis Stat timeseries Top $TopResources Volumes by Number of Compress Fail % ONTAP: Volume Sis Stat timeseries Top $TopResources Volumes by Number of Compress Fail"},{"location":"ontap-metrics/#volume_other_data","title":"volume_other_data","text":"<p>Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.other</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml"},{"location":"ontap-metrics/#volume_other_latency","title":"volume_other_latency","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.other</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.other conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>other_latency</code>Unit: microsecType: averageBase: other_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_other_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexGroup Volume WAFL Layer timeseries Top $TopResources Volumes by Other Latency ONTAP: Volume Performance timeseries Top $TopResources Volumes by Other Latency"},{"location":"ontap-metrics/#volume_other_ops","title":"volume_other_ops","text":"<p>Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.other</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>other_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_other_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexGroup Volume WAFL Layer timeseries Top $TopResources Volumes by Other IOPs ONTAP: Volume Performance timeseries Top $TopResources Volumes by Other IOPs ONTAP: Volume by SVM Highlights table Volume Performance for $SVM (Click volume for detailed drill-down) ONTAP: Volume Deep Dive Highlights table Volume Performance ONTAP: Volume Deep Dive Highlights timeseries Other IOPs"},{"location":"ontap-metrics/#volume_overwrite_reserve_available","title":"volume_overwrite_reserve_available","text":"<p>amount of storage space that is currently available for overwrites, calculated by subtracting the total amount of overwrite reserve space from the amount that has already been used.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>overwrite_reserve_total, overwrite_reserve_used</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>overwrite_reserve_total, overwrite_reserve_used</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_overwrite_reserve_total","title":"volume_overwrite_reserve_total","text":"<p>The size (in bytes) that is reserved for overwriting snapshotted data in an otherwise full volume. This space is usable only by space-reserved LUNs and files, and then only when the volume is full.This field is valid only when the volume is online.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>overwrite_reserve</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.overwrite-reserve</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_overwrite_reserve_used","title":"volume_overwrite_reserve_used","text":"<p>The reserved size (in bytes) that is not available for new overwrites. The number includes both the reserved size which has actually been used for overwrites as well as the size which was never allocated in the first place. This field is valid only when the volume is online.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>overwrite_reserve_used</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.overwrite-reserve-used</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_performance_tier_footprint","title":"volume_performance_tier_footprint","text":"<p>This field represents the footprint of blocks written to the volume in bytes for the performance tier (bin 0).</p> API Endpoint Metric Template REST <code>api/private/cli/volume/footprint</code> <code>volume_blocks_footprint_bin0</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-footprint-get-iter</code> <code>volume-blocks-footprint-bin0</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_performance_tier_footprint</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Capacity timeseries Top $TopResources Volumes by Performance Tier Footprint ONTAP: FlexGroup Top Volume FabricPool timeseries Top $TopResources Volumes by Performance Tier Footprint ONTAP: Volume FabricPool table Volumes Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Performance Tier Footprint"},{"location":"ontap-metrics/#volume_performance_tier_footprint_percent","title":"volume_performance_tier_footprint_percent","text":"<p>This field represents the footprint of blocks written to the volume in bin 0 as a percentage of aggregate size.</p> API Endpoint Metric Template REST <code>api/private/cli/volume/footprint</code> <code>volume_blocks_footprint_bin0_percent</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-footprint-get-iter</code> <code>volume-blocks-footprint-bin0-percent</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_performance_tier_footprint_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Capacity timeseries Top $TopResources Volumes by Performance Tier Footprint % ONTAP: FlexGroup Top Volume FabricPool timeseries Top $TopResources Volumes by Performance Tier Footprint % ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Performance Tier Footprint %"},{"location":"ontap-metrics/#volume_read_data","title":"volume_read_data","text":"<p>Performance metric for read I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.read</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>read_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by Average Throughput ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by Read Throughput ONTAP: cDOT Volume Metrics timeseries Top $TopResources Volumes by Average Throughput ONTAP: FlexGroup Highlights timeseries Top $TopResources Constituents by Average Throughput ONTAP: FlexGroup Volume Table table Top $TopResources Volumes by Read Throughput ONTAP: FlexGroup Volume WAFL Layer timeseries Top $TopResources Volumes by Read Throughput ONTAP: Node Volume Performance timeseries Top $TopResources Volumes by Average Throughput ONTAP: SVM Volume Performance timeseries Top $TopResources Volumes by Read Throughput ONTAP: Volume Highlights stat Top $TopResources Volumes Total Throughput ONTAP: Volume Highlights timeseries Top $TopResources Volumes by Average Throughput ONTAP: Volume Volume Table table Top $TopResources Volumes by Read Throughput ONTAP: Volume Performance timeseries Top $TopResources Volumes by Read Throughput ONTAP: Volume by SVM Highlights table Volume Performance for $SVM (Click volume for detailed drill-down) ONTAP: Volume Deep Dive Highlights table Volume Performance ONTAP: Volume Deep Dive Highlights stat Max Read Op Size ONTAP: Volume Deep Dive Highlights timeseries Read Throughput"},{"location":"ontap-metrics/#volume_read_latency","title":"volume_read_latency","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.read</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.read conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>read_latency</code>Unit: microsecType: averageBase: read_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_read_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by Read Latency ONTAP: FlexGroup Volume Table table Top $TopResources Volumes by Read Latency ONTAP: FlexGroup Volume WAFL Layer timeseries Top $TopResources Volumes by Read Latency ONTAP: SVM Volume Performance timeseries Top $TopResources Volumes by Read Latency ONTAP: Volume Volume Table table Top $TopResources Volumes by Read Latency ONTAP: Volume Performance timeseries Top $TopResources Volumes by Read Latency ONTAP: Volume Deep Dive Highlights timeseries Read Latency"},{"location":"ontap-metrics/#volume_read_ops","title":"volume_read_ops","text":"<p>Performance metric for read I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.read</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>read_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by Read IOPs ONTAP: FlexGroup Volume Table table Top $TopResources Volumes by Read IOPS ONTAP: FlexGroup Volume WAFL Layer timeseries Top $TopResources Volumes by Read IOPs ONTAP: SVM Volume Performance timeseries Top $TopResources Volumes by Read IOPs ONTAP: Volume Volume Table table Top $TopResources Volumes by Read IOPS ONTAP: Volume Performance timeseries Top $TopResources Volumes by Read IOPs ONTAP: Volume by SVM Highlights table Volume Performance for $SVM (Click volume for detailed drill-down) ONTAP: Volume Deep Dive Highlights table Volume Performance ONTAP: Volume Deep Dive Highlights stat Max Read Op Size ONTAP: Volume Deep Dive Highlights timeseries Read IOPs"},{"location":"ontap-metrics/#volume_sis_compress_saved","title":"volume_sis_compress_saved","text":"<p>The total disk space (in bytes) that is saved by compressing blocks on the referenced file system.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>compression_space_saved</code> conf/rest/9.12.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-sis-attributes.compression-space-saved</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_sis_compress_saved</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexGroup Volume Table table FlexGroup Constituents in Cluster ONTAP: Health Volume table Volumes with Ransomware Issues (9.10+ Only) ONTAP: Health Volume table Volumes Move Issues ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes by Compression Savings ONTAP: Volume Volume Table table Volumes in Cluster ONTAP: Volume Deep Dive Volume Capacity: $Volume table Volumes in Cluster"},{"location":"ontap-metrics/#volume_sis_compress_saved_percent","title":"volume_sis_compress_saved_percent","text":"<p>Percentage of the total disk space that is saved by compressing blocks on the referenced file system</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>compression_space_saved_percent</code> conf/rest/9.12.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-sis-attributes.percentage-compression-space-saved</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_sis_compress_saved_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Top Volume and LUN Capacity timeseries Top $TopResources Volumes by Compression Percent Saved ONTAP: SVM Volume Capacity % timeseries Top $TopResources Volumes by Compression Saved %"},{"location":"ontap-metrics/#volume_sis_dedup_saved","title":"volume_sis_dedup_saved","text":"<p>The total disk space (in bytes) that is saved by deduplication and file cloning.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>dedupe_space_saved</code> conf/rest/9.12.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-sis-attributes.deduplication-space-saved</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_sis_dedup_saved</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexGroup Volume Table table FlexGroup Constituents in Cluster ONTAP: Health Volume table Volumes with Ransomware Issues (9.10+ Only) ONTAP: Health Volume table Volumes Move Issues ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes by Deduplication Savings ONTAP: Volume Volume Table table Volumes in Cluster ONTAP: Volume Deep Dive Volume Capacity: $Volume table Volumes in Cluster"},{"location":"ontap-metrics/#volume_sis_dedup_saved_percent","title":"volume_sis_dedup_saved_percent","text":"<p>Percentage of the total disk space that is saved by deduplication and file cloning.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>dedupe_space_saved_percent</code> conf/rest/9.12.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-sis-attributes.percentage-deduplication-space-saved</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_sis_dedup_saved_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: LUN Top Volume and LUN Capacity timeseries Top $TopResources Volumes by Deduplication Percent Saved ONTAP: SVM Volume Capacity % timeseries Top $TopResources Volumes by Deduplication Saved %"},{"location":"ontap-metrics/#volume_sis_total_saved","title":"volume_sis_total_saved","text":"<p>Total space saved (in bytes) in the volume due to deduplication, compression, and file cloning.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>sis_space_saved</code> conf/rest/9.12.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-sis-attributes.total-space-saved</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_sis_total_saved</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes by Total Efficiency Savings"},{"location":"ontap-metrics/#volume_sis_total_saved_percent","title":"volume_sis_total_saved_percent","text":"<p>Percentage of total disk space that is saved by compressing blocks, deduplication and file cloning.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>sis_space_saved_percent</code> conf/rest/9.12.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-sis-attributes.percentage-total-space-saved</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_size","title":"volume_size","text":"<p>Physical size of the volume, in bytes. The minimum size for a FlexVol volume is 20MB and the minimum size for a FlexGroup volume is 200MB per constituent. The recommended size for a FlexGroup volume is a minimum of 100GB per constituent. For all volumes, the default size is equal to the minimum size.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>size</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.size</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Access ($Activity) History By Percent ONTAP: File System Analytics (FSA) Volume Activity barchart Volume Modify ($Activity) History By Percent ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes Per Volume Total Size ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Volume Total Size ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Space Used"},{"location":"ontap-metrics/#volume_size_available","title":"volume_size_available","text":"<p>The size (in bytes) that is still available in the volume. This field is valid only when the volume is online.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>available</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.size-available</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_size_available</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: File System Analytics (FSA) Highlights stat Available"},{"location":"ontap-metrics/#volume_size_total","title":"volume_size_total","text":"<p>Total usable size (in bytes) of the volume, not including WAFL reserve or volume snapshot reserve.  If the volume is restricted or offline, a value of 0 is returned.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>total</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.size-total</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_size_total</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: cDOT Capacity Metrics table Top $TopResources SVMs by Capacity Used % ONTAP: cDOT Capacity Metrics table Top $TopResources Volumes by Capacity Used % ONTAP: cDOT Capacity Metrics timeseries Top $TopResources SVMs by Capacity Used % ONTAP: cDOT Capacity Metrics timeseries Top $TopResources Volumes by Capacity Used % ONTAP: FlexGroup Volume Table table FlexGroup Constituents in Cluster ONTAP: File System Analytics (FSA) Highlights stat Size ONTAP: File System Analytics (FSA) Highlights bargauge Used Percentage ONTAP: Health Volume table Volumes with Ransomware Issues (9.10+ Only) ONTAP: Health Volume table Volumes Move Issues ONTAP: Volume Volume Table table Volumes in Cluster ONTAP: Volume Volume Autosize Table table Volumes Autogrow ONTAP: Volume Deep Dive Volume Capacity: $Volume table Volumes in Cluster"},{"location":"ontap-metrics/#volume_size_used","title":"volume_size_used","text":"<p>Number of bytes used in the volume.  If the volume is restricted or offline, a value of 0 is returned.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>used</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.size-used</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_size_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Capacity timeseries Top $TopResources Volumes by Space Used by Aggregate ONTAP: cDOT Capacity Metrics table Top $TopResources SVMs by Capacity Used % ONTAP: cDOT Capacity Metrics table Top $TopResources Volumes by Capacity Used % ONTAP: cDOT Capacity Metrics timeseries Top $TopResources SVMs by Capacity Used % ONTAP: cDOT Capacity Metrics timeseries Top $TopResources Volumes by Capacity Used % ONTAP: File System Analytics (FSA) Highlights stat Used ONTAP: File System Analytics (FSA) Highlights bargauge Used Percentage ONTAP: SVM Capacity timeseries Top $TopResources SVMs by Volume Space Usage ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes Per Volume Size Used ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Volume Size Used ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Space Used"},{"location":"ontap-metrics/#volume_size_used_percent","title":"volume_size_used_percent","text":"<p>percentage of utilized storage space in a volume relative to its total capacity</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>percent_used</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.percentage-size-used</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_size_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Capacity timeseries Top $TopResources Volumes by Space Used % ONTAP: FlexGroup Volume Table table FlexGroup Constituents in Cluster ONTAP: Health Volume table Volumes with Ransomware Issues (9.10+ Only) ONTAP: Health Volume table Volumes Move Issues ONTAP: LUN Top Volume and LUN Capacity timeseries Top $TopResources Volumes by Used % ONTAP: SVM Volume Capacity % timeseries Top $TopResources Volumes Per Volume Size Used ONTAP: Volume Volume Table table Volumes in Cluster ONTAP: Volume Volume Autosize Table table Volumes Autogrow ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Volume Size Used ONTAP: Volume Forecast Volume Capacity table Top $TopResources Volumes Per Size Used Percentage Trend ONTAP: Volume Deep Dive Volume Capacity: $Volume table Volumes in Cluster ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Space Used Percent ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Snapshot Space Used Percent"},{"location":"ontap-metrics/#volume_snaplock_labels","title":"volume_snaplock_labels","text":"<p>This metric provides information about VolumeSnaplock</p> API Endpoint Metric Template REST <code>api/private/cli/volume/snaplock</code> <code>Harvest generated</code> conf/rest/9.12.0/volume_snaplock.yaml <p>The <code>volume_snaplock_labels</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Snaplock table Volume Snaplock"},{"location":"ontap-metrics/#volume_snapshot_count","title":"volume_snapshot_count","text":"<p>Number of snapshots in the volume.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>snapshot_count</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-snapshot-attributes.snapshot-count</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_snapshot_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Snapshot Copies stat &lt;10 Copies  ONTAP: Data Protection Snapshot Copies stat 10-100 Copies ONTAP: Data Protection Snapshot Copies stat 101-500 Copies ONTAP: Data Protection Snapshot Copies stat &gt;500 Copies ONTAP: Data Protection Snapshot Copies table Volume count by the number of Snapshot copies ONTAP: Datacenter Snapshots piechart Snapshot Copies"},{"location":"ontap-metrics/#volume_snapshot_reserve_available","title":"volume_snapshot_reserve_available","text":"<p>The size (in bytes) that is available for Snapshot copies inside the Snapshot reserve. This value is zero if Snapshot spill is present. For 'none' guaranteed volumes, this may get reduced due to less available space in the aggregate. This parameter is not supported on Infinite Volumes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>snapshot_reserve_available</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.snapshot-reserve-available</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_snapshot_reserve_available</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Reserve Available ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Reserve Available ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Snapshot Space Used"},{"location":"ontap-metrics/#volume_snapshot_reserve_percent","title":"volume_snapshot_reserve_percent","text":"<p>The percentage of volume disk space that has been set aside as reserve for snapshot usage.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>percent_snapshot_space</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.percentage-snapshot-reserve</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_snapshot_reserve_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Volume Capacity % timeseries Top $TopResources Volumes Per Snapshot Reserve ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Snapshot Reserve ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Snapshot Space Used Percent"},{"location":"ontap-metrics/#volume_snapshot_reserve_size","title":"volume_snapshot_reserve_size","text":"<p>The size (in bytes) in the volume that has been set aside as reserve for snapshot usage.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>snapshot_reserve_size</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.snapshot-reserve-size</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_snapshot_reserve_size</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Data Protection Snapshot Copies stat Volumes breached ONTAP: Data Protection Snapshot Copies stat Volumes not breached ONTAP: Data Protection Snapshot Copies table Volumes Breaching Snapshot Copy Reserve Space ONTAP: Datacenter Snapshots piechart Breached Status ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Reserve Size ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Reserve Size ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Snapshot Space Used"},{"location":"ontap-metrics/#volume_snapshot_reserve_used","title":"volume_snapshot_reserve_used","text":"<p>amount of storage space currently used by a volume's snapshot reserve, which is calculated by subtracting the snapshot reserve available space from the snapshot reserve size.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>snapshot_reserve_size, snapshot_reserve_available</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>snapshot_reserve_size, snapshot_reserve_available</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_snapshot_reserve_used_percent","title":"volume_snapshot_reserve_used_percent","text":"<p>Percentage of the volume reserved for snapshots that has been used. Note that in some scenarios, it is possible to pass 100% of the space allocated.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>snapshot_space_used</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.percentage-snapshot-reserve-used</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_snapshot_reserve_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Capacity timeseries Top $TopResources Volumes by Snapshot Space Used % ONTAP: LUN Top Volume and LUN Capacity timeseries Top $TopResources Volumes by Snapshot Used % ONTAP: SVM Volume Capacity % timeseries Top $TopResources Volumes Per Snapshot Reserve Used ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Snapshot Reserve Used ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Snapshot Space Used Percent"},{"location":"ontap-metrics/#volume_snapshots_size_available","title":"volume_snapshots_size_available","text":"<p>Total free space (in bytes) available in the volume and the snapshot reserve. If this value is 0 or negative, a new snapshot cannot be created.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>size_available_for_snapshots</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.size-available-for-snapshots</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_snapshots_size_available</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Size Available ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Size Available ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Snapshot Space Used"},{"location":"ontap-metrics/#volume_snapshots_size_used","title":"volume_snapshots_size_used","text":"<p>The size (in bytes) that is used by snapshots in the volume.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>size_used_by_snapshots</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.size-used-by-snapshots</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_snapshots_size_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Capacity timeseries Top $TopResources Volumes by Snapshot Space Used ONTAP: Data Protection Snapshot Copies stat Volumes breached ONTAP: Data Protection Snapshot Copies stat Volumes not breached ONTAP: Data Protection Snapshot Copies table Volumes Breaching Snapshot Copy Reserve Space ONTAP: Datacenter Snapshots piechart Breached Status ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Size Used ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Snapshot Size Used ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Snapshot Space Used"},{"location":"ontap-metrics/#volume_space_expected_available","title":"volume_space_expected_available","text":"<p>The size (in bytes) that should be available for the volume irrespective of available size in the aggregate.This is same as size-available for 'volume' guaranteed volumes.For 'none' guaranteed volumes this value is calculated as if the aggregate has enough backing disk space to fully support the volume's size.Similar to the size-available property, this does not include Snapshot reserve.This count gets reduced if snapshots consume space above Snapshot reserve threshold.This parameter is not supported on Infinite Volumes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>expected_available</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.expected-available</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_space_logical_available","title":"volume_space_logical_available","text":"<p>The size (in bytes) that is logically available in the volume.This is the amount of free space available considering space saved by the storage efficiency features as being used.This does not include Snapshot reserve.This parameter is not supported on FlexGroups or Infinite Volumes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>logical_available</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.logical-available</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_space_logical_used","title":"volume_space_logical_used","text":"<p>The size (in bytes) that is logically used in the volume.This value includes all the space saved by the storage efficiency features along with the physically used space.This does not include Snapshot reserve but does consider Snapshot spill.This parameter is not supported on FlexGroups or Infinite Volumes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>logical_used</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.logical-used</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_space_logical_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexGroup Volume Table table FlexGroup Constituents in Cluster ONTAP: Health Volume table Volumes with Ransomware Issues (9.10+ Only) ONTAP: Health Volume table Volumes Move Issues ONTAP: SVM Capacity timeseries Top $TopResources SVMs by Logical Space Usage Across Volumes ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes Per Logical Space Used ONTAP: Volume Volume Table table Volumes in Cluster ONTAP: Volume I/O Density timeseries Top $TopResources Volumes by IO Density (IOPs/TiB) ONTAP: Volume I/O Density timeseries Bottom $TopResources Volumes by IO Density (IOPs/TiB) ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Logical Space Used ONTAP: Volume Growth Rate timeseries Top $TopResources Volumes Per Growth Rate of Logical Used ONTAP: Volume Growth Rate table Top $TopResources Volumes by Logical Usage: Delta ONTAP: Volume Deep Dive Volume Capacity: $Volume table Volumes in Cluster ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Space Used"},{"location":"ontap-metrics/#volume_space_logical_used_by_afs","title":"volume_space_logical_used_by_afs","text":"<p>The size (in bytes) that is logically used by the active filesystem of the volume.This value differs from 'logical-used' by the amount of Snapshot spill that exceeds Snapshot reserve.This parameter is not supported on FlexGroups or Infinite Volumes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>logical_used_by_afs</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.logical-used-by-afs</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_space_logical_used_by_snapshots","title":"volume_space_logical_used_by_snapshots","text":"<p>The size (in bytes) that is logically used across all Snapshot copies in the volume. This value differs from 'size-used-by-snapshots' by the space saved by the storage efficiency features across the Snapshot copies.This parameter is not supported on FlexGroups or Infinite Volumes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>logical_used_by_snapshots</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.logical-used-by-snapshots</code> conf/zapi/cdot/9.8.0/volume.yaml"},{"location":"ontap-metrics/#volume_space_logical_used_percent","title":"volume_space_logical_used_percent","text":"<p>Percentage of the logical used size of the volume.This parameter is not supported on FlexGroups or Infinite Volumes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>logical_used_percent</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.logical-used-percent</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_space_logical_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Volume Capacity % timeseries Top $TopResources Volumes Per Logical Space Used ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Logical Space Used ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Space Used Percent"},{"location":"ontap-metrics/#volume_space_performance_tier_inactive_user_data","title":"volume_space_performance_tier_inactive_user_data","text":"<p>The size that is physically used in the performance tier of the volume and has a cold temperature. This parameter is only supported if the volume is in an aggregate that is either attached to object store or could be attached to an object store.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>performance_tier_inactive_user_data</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.performance-tier-inactive-user-data</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_space_performance_tier_inactive_user_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Capacity timeseries Top $TopResources Volumes by Inactive Data"},{"location":"ontap-metrics/#volume_space_performance_tier_inactive_user_data_percent","title":"volume_space_performance_tier_inactive_user_data_percent","text":"<p>The size (in percent) that is physically used in the performance tier of the volume and has a cold temperature. This parameter is only supported if the volume is in an aggregate that is either attached to object store or could be attached to an object store.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>performance_tier_inactive_user_data_percent</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.performance-tier-inactive-user-data-percent</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_space_performance_tier_inactive_user_data_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Capacity % timeseries Top $TopResources Volumes by Inactive Data"},{"location":"ontap-metrics/#volume_space_physical_used","title":"volume_space_physical_used","text":"<p>The size (in bytes) that is physically used in the volume.This differs from 'total-used' space by the space that is reserved for future writes.The value includes blocks in use by Snapshot copies.This field is valid only if the volume is online.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>virtual_used</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.physical-used</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_space_physical_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: FlexGroup Volume Table table FlexGroup Constituents in Cluster ONTAP: Health Volume table Volumes with Ransomware Issues (9.10+ Only) ONTAP: Health Volume table Volumes Move Issues ONTAP: SVM Volume Capacity timeseries Top $TopResources Volumes Per Physical Space Used ONTAP: Volume Volume Table table Volumes in Cluster ONTAP: Volume Capacity timeseries Top $TopResources Volumes Per Physical Space Used ONTAP: Volume Growth Rate timeseries Top $TopResources Volumes Per Growth Rate of Physical Used ONTAP: Volume Growth Rate table Top $TopResources Volumes by Physical Usage: Delta ONTAP: Volume Deep Dive Volume Capacity: $Volume table Volumes in Cluster ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Space Used"},{"location":"ontap-metrics/#volume_space_physical_used_percent","title":"volume_space_physical_used_percent","text":"<p>The size (in percent) that is physically used in the volume.The percentage is based on volume size including the space reserved for Snapshot copies.This field is valid only if the volume is online.</p> API Endpoint Metric Template REST <code>api/private/cli/volume</code> <code>virtual_used_percent</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-get-iter</code> <code>volume-attributes.volume-space-attributes.physical-used-percent</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_space_physical_used_percent</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: SVM Volume Capacity % timeseries Top $TopResources Volumes Per Physical Space Used ONTAP: Volume Capacity % timeseries Top $TopResources Volumes Per Physical Space Used ONTAP: Volume Deep Dive Per Volume Statistics timeseries Per Volume Space Used Percent"},{"location":"ontap-metrics/#volume_tags","title":"volume_tags","text":"<p>Displays tags at the volume level.</p> API Endpoint Metric Template REST <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/volume.yaml"},{"location":"ontap-metrics/#volume_top_clients_read_data","title":"volume_top_clients_read_data","text":"<p>This metric measures the amount of data read by the top clients to a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/clients</code> <code>throughput.read</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_clients_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Clients timeseries Top $TopResources Volumes Clients by Read Throughput"},{"location":"ontap-metrics/#volume_top_clients_read_ops","title":"volume_top_clients_read_ops","text":"<p>This metric tracks the number of read operations performed by the top clients on a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/clients</code> <code>iops.read</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_clients_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Clients timeseries Top $TopResources Volumes Clients by Read IOPs"},{"location":"ontap-metrics/#volume_top_clients_write_data","title":"volume_top_clients_write_data","text":"<p>This metric measures the amount of data written by the top clients to a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/clients</code> <code>throughput.write</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_clients_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Clients timeseries Top $TopResources Volumes Clients by Write Throughput"},{"location":"ontap-metrics/#volume_top_clients_write_ops","title":"volume_top_clients_write_ops","text":"<p>This metric tracks the number of write operations performed by the top clients on a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/clients</code> <code>iops.write</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_clients_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Clients timeseries Top $TopResources Volumes Clients by Write IOPs"},{"location":"ontap-metrics/#volume_top_files_read_data","title":"volume_top_files_read_data","text":"<p>This metric measures the amount of data read from the files of a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/files</code> <code>throughput.read</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_files_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Files timeseries Top $TopResources Volumes Files by Read Throughput"},{"location":"ontap-metrics/#volume_top_files_read_ops","title":"volume_top_files_read_ops","text":"<p>This metric tracks the number of read operations performed on the files of a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/files</code> <code>iops.read</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_files_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Files timeseries Top $TopResources Volumes Files by Read IOPs"},{"location":"ontap-metrics/#volume_top_files_write_data","title":"volume_top_files_write_data","text":"<p>This metric measures the amount of data written to the top files of a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/files</code> <code>throughput.write</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_files_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Files timeseries Top $TopResources Volumes Files by Write Throughput"},{"location":"ontap-metrics/#volume_top_files_write_ops","title":"volume_top_files_write_ops","text":"<p>This metric tracks the number of write operations performed on the files of a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/files</code> <code>iops.write</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_files_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Files timeseries Top $TopResources Volumes Files by Write IOPs"},{"location":"ontap-metrics/#volume_top_users_read_data","title":"volume_top_users_read_data","text":"<p>This metric measures the amount of data read by users from a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/users</code> <code>throughput.read</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_users_read_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Users timeseries Top $TopResources Volumes Users by Read Throughput"},{"location":"ontap-metrics/#volume_top_users_read_ops","title":"volume_top_users_read_ops","text":"<p>This metric tracks the number of read operations performed by users on a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/users</code> <code>iops.read</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_users_read_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Users timeseries Top $TopResources Volumes Users by Read IOPs"},{"location":"ontap-metrics/#volume_top_users_write_data","title":"volume_top_users_write_data","text":"<p>This metric measures the amount of data written by users to a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/users</code> <code>throughput.write</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_users_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Users timeseries Top $TopResources Volumes Users by Write Throughput"},{"location":"ontap-metrics/#volume_top_users_write_ops","title":"volume_top_users_write_ops","text":"<p>This metric tracks the number of write operations performed by users on a specific volume.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes/*/top-metrics/users</code> <code>iops.write</code>Unit: Type: Base: conf/keyperf/9.15.0/volume.yaml <p>The <code>volume_top_users_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Users timeseries Top $TopResources Volumes Users by Write IOPs"},{"location":"ontap-metrics/#volume_total_data","title":"volume_total_data","text":"<p>This metric represents the total amount of data that has been read from and written to a specific volume in bytes per second.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.total</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>volume</code> <code>read_data, write_data</code>Unit: Type: Base: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_total_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Cluster Throughput timeseries Data ONTAP: Datacenter Performance timeseries Top $TopResources Throughput by Cluster ONTAP: SnapMirror Destinations Highlights timeseries Top $TopResources Destination Volumes by Average Throughput"},{"location":"ontap-metrics/#volume_total_footprint","title":"volume_total_footprint","text":"<p>This field represents the total footprint in bytes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume/footprint</code> <code>total_footprint</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-footprint-get-iter</code> <code>total-footprint</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_total_footprint</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume Volume Hot-Cold Data table Volumes by Cold data ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Cold Data % ONTAP: Volume Volume Hot-Cold Data timeseries Top $TopResources Volumes by Hot Data % ONTAP: Volume FabricPool table Volumes Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Total Footprint"},{"location":"ontap-metrics/#volume_total_metadata_footprint","title":"volume_total_metadata_footprint","text":"<p>This field represents the total metadata footprint in bytes.</p> API Endpoint Metric Template REST <code>api/private/cli/volume/footprint</code> <code>total_metadata_footprint</code> conf/rest/9.14.0/volume.yaml ZAPI <code>volume-footprint-get-iter</code> <code>volume_total_metadata_footprint</code> conf/zapi/cdot/9.8.0/volume.yaml <p>The <code>volume_total_metadata_footprint</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Volume FabricPool table Volumes Footprint ONTAP: Volume FabricPool timeseries Top $TopResources Volumes by Total Metadata Footprint"},{"location":"ontap-metrics/#volume_total_ops","title":"volume_total_ops","text":"<p>Performance metric aggregated over all types of I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.total</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>total_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_total_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by IOPs ONTAP: cDOT Cluster Metrics timeseries Top $TopResources Total IOPs by Cluster ONTAP: cDOT Volume Metrics timeseries Top $TopResources Volumes by IOPs ONTAP: Cluster Throughput timeseries IOPs ONTAP: Datacenter Performance timeseries Top $TopResources  IOPs by Cluster ONTAP: FlexGroup Highlights timeseries Top $TopResources Constituents by Total IOPs ONTAP: Node Volume Performance timeseries Top $TopResources Volumes by IOPs ONTAP: Volume Highlights stat Top $TopResources Volumes by Total IOPs ONTAP: Volume Highlights timeseries Top $TopResources Volumes by Total IOPs ONTAP: Volume I/O Density timeseries Top $TopResources Volumes by IO Density (IOPs/TiB) ONTAP: Volume I/O Density timeseries Bottom $TopResources Volumes by IO Density (IOPs/TiB) ONTAP: Volume by SVM Highlights table Volume Performance for $SVM (Click volume for detailed drill-down) ONTAP: Volume Deep Dive Highlights table Volume Performance"},{"location":"ontap-metrics/#volume_write_data","title":"volume_write_data","text":"<p>Performance metric for write I/O operations in bytes per seconds.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.throughput_raw.write</code>Unit: b_per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>write_data</code>Unit: b_per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_write_data</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by Average Throughput ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by Write Throughput ONTAP: cDOT Volume Metrics timeseries Top $TopResources Volumes by Average Throughput ONTAP: FlexGroup Highlights timeseries Top $TopResources Constituents by Average Throughput ONTAP: FlexGroup Volume Table table Top $TopResources Volumes by Write Throughput ONTAP: FlexGroup Volume WAFL Layer timeseries Top $TopResources Volumes by Write Throughput ONTAP: Node Volume Performance timeseries Top $TopResources Volumes by Average Throughput ONTAP: SVM Volume Performance timeseries Top $TopResources Volumes by Write Throughput ONTAP: Volume Highlights stat Top $TopResources Volumes Total Throughput ONTAP: Volume Highlights timeseries Top $TopResources Volumes by Average Throughput ONTAP: Volume Volume Table table Top $TopResources Volumes by Write Throughput ONTAP: Volume Performance timeseries Top $TopResources Volumes by Write Throughput ONTAP: Volume by SVM Highlights table Volume Performance for $SVM (Click volume for detailed drill-down) ONTAP: Volume Deep Dive Highlights table Volume Performance ONTAP: Volume Deep Dive Highlights stat Max Write Op Size ONTAP: Volume Deep Dive Highlights timeseries Write Throughput"},{"location":"ontap-metrics/#volume_write_latency","title":"volume_write_latency","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.latency_raw.write</code>Unit: microsecType: averageBase: volume_statistics.iops_raw.write conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>write_latency</code>Unit: microsecType: averageBase: write_ops conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_write_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by Write Latency ONTAP: FlexGroup Volume Table table Top $TopResources Volumes by Write Latency ONTAP: FlexGroup Volume WAFL Layer timeseries Top $TopResources Volumes by Write Latency ONTAP: SVM Volume Performance timeseries Top $TopResources Volumes by Write Latency ONTAP: Volume Volume Table table Top $TopResources Volumes by Write Latency ONTAP: Volume Performance timeseries Top $TopResources Volumes by Write Latency ONTAP: Volume Deep Dive Highlights timeseries Write Latency"},{"location":"ontap-metrics/#volume_write_ops","title":"volume_write_ops","text":"<p>Performance metric for write I/O operations.</p> API Endpoint Metric Template KeyPerf <code>api/storage/volumes</code> <code>statistics.iops_raw.write</code>Unit: per_secType: rateBase: conf/keyperf/9.15.0/volume.yaml ZapiPerf <code>perf-object-get-instances volume</code> <code>write_ops</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/volume.yaml <p>The <code>volume_write_ops</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Aggregate Volume Performance timeseries Top $TopResources Volumes by Write IOPs ONTAP: FlexGroup Volume Table table Top $TopResources Volumes by Write IOPS ONTAP: FlexGroup Volume WAFL Layer timeseries Top $TopResources Volumes by Write IOPs ONTAP: SVM Volume Performance timeseries Top $TopResources Volumes by Write IOPs ONTAP: Volume Volume Table table Top $TopResources Volumes by Write IOPS ONTAP: Volume Performance timeseries Top $TopResources Volumes by Write IOPs ONTAP: Volume by SVM Highlights table Volume Performance for $SVM (Click volume for detailed drill-down) ONTAP: Volume Deep Dive Highlights table Volume Performance ONTAP: Volume Deep Dive Highlights stat Max Write Op Size ONTAP: Volume Deep Dive Highlights timeseries Write IOPs"},{"location":"ontap-metrics/#vscan_labels","title":"vscan_labels","text":"<p>This metric provides information about Vscan</p> API Endpoint Metric Template REST <code>api/protocols/vscan</code> <code>Harvest generated</code> conf/rest/9.12.0/vscan.yaml"},{"location":"ontap-metrics/#vscan_scan_latency","title":"vscan_scan_latency","text":"<p>Average scan latency in microseconds</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/vscan</code> <code>scan.latency</code>Unit: microsecType: averageBase: scan.requests conf/restperf/9.13.0/vscan.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan_server</code> <code>scan_latency</code>Unit: microsecType: averageBase: scan_latency_base conf/zapiperf/cdot/9.8.0/vscan.yaml <p>The <code>vscan_scan_latency</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Vscan Connection Status Counters timeseries Top $TopResources Scanners by Scanner Latency"},{"location":"ontap-metrics/#vscan_scan_request_dispatched_rate","title":"vscan_scan_request_dispatched_rate","text":"<p>Total number of scan requests sent to the scanner per second</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/vscan</code> <code>scan.request_dispatched_rate</code>Unit: per_secType: rateBase: conf/restperf/9.13.0/vscan.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan_server</code> <code>scan_request_dispatched_rate</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/vscan.yaml <p>The <code>vscan_scan_request_dispatched_rate</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Vscan Connection Status Counters timeseries Top $TopResources Scanners by Scanner Requests Throughput"},{"location":"ontap-metrics/#vscan_scanner_stats_pct_cpu_used","title":"vscan_scanner_stats_pct_cpu_used","text":"<p>Percentage CPU utilization on scanner calculated over the last 15 seconds.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/vscan</code> <code>scanner.stats_percent_cpu_used</code>Unit: noneType: rawBase: conf/restperf/9.13.0/vscan.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan_server</code> <code>scanner_stats_pct_cpu_used</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/vscan.yaml <p>The <code>vscan_scanner_stats_pct_cpu_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Vscan Scanner utilization timeseries Scanner CPU Utilization"},{"location":"ontap-metrics/#vscan_scanner_stats_pct_mem_used","title":"vscan_scanner_stats_pct_mem_used","text":"<p>Percentage RAM utilization on scanner calculated over the last 15 seconds.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/vscan</code> <code>scanner.stats_percent_mem_used</code>Unit: noneType: rawBase: conf/restperf/9.13.0/vscan.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan_server</code> <code>scanner_stats_pct_mem_used</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/vscan.yaml <p>The <code>vscan_scanner_stats_pct_mem_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Vscan Scanner utilization timeseries Scanner Mem Utilization"},{"location":"ontap-metrics/#vscan_scanner_stats_pct_network_used","title":"vscan_scanner_stats_pct_network_used","text":"<p>Percentage network utilization on scanner calculated for the last 15 seconds.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/vscan</code> <code>scanner.stats_percent_network_used</code>Unit: noneType: rawBase: conf/restperf/9.13.0/vscan.yaml ZapiPerf <code>perf-object-get-instances offbox_vscan_server</code> <code>scanner_stats_pct_network_used</code>Unit: noneType: rawBase: conf/zapiperf/cdot/9.8.0/vscan.yaml <p>The <code>vscan_scanner_stats_pct_network_used</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Vscan Scanner utilization timeseries Scanner Network Utilization"},{"location":"ontap-metrics/#vscan_server_disconnected","title":"vscan_server_disconnected","text":"<p>Represent the disconnected vscan servers to the vscan pool</p> API Endpoint Metric Template Rest <code>NA</code> <code>Harvest generated</code> conf/rest/9.12.0/vscan.yaml <p>The <code>vscan_server_disconnected</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Vscan Vscan Server table Disconnected Vscan Servers in Cluster"},{"location":"ontap-metrics/#wafl_avg_msg_latency","title":"wafl_avg_msg_latency","text":"<p>Average turnaround time for WAFL messages in milliseconds.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>average_msg_latency</code>Unit: millisecType: averageBase: msg_total conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>avg_wafl_msg_latency</code>Unit: millisecType: averageBase: wafl_msg_total conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_avg_non_wafl_msg_latency","title":"wafl_avg_non_wafl_msg_latency","text":"<p>Average turnaround time for non-WAFL messages in milliseconds.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>average_non_wafl_msg_latency</code>Unit: millisecType: averageBase: non_wafl_msg_total conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>avg_non_wafl_msg_latency</code>Unit: millisecType: averageBase: non_wafl_msg_total conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_avg_repl_msg_latency","title":"wafl_avg_repl_msg_latency","text":"<p>Average turnaround time for replication WAFL messages in milliseconds.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>average_replication_msg_latency</code>Unit: millisecType: averageBase: replication_msg_total conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>avg_wafl_repl_msg_latency</code>Unit: millisecType: averageBase: wafl_repl_msg_total conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_cp_count","title":"wafl_cp_count","text":"<p>Array of counts of different types of Consistency Points (CP).</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>cp_count</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>cp_count</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml <p>The <code>wafl_cp_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Disk Disk Utilization timeseries CP (Consistency Points) Counts"},{"location":"ontap-metrics/#wafl_cp_phase_times","title":"wafl_cp_phase_times","text":"<p>Array of percentage time spent in different phases of Consistency Point (CP).</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>cp_phase_times</code>Unit: percentType: percentBase: total_cp_msecs conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>cp_phase_times</code>Unit: percentType: percentBase: total_cp_msecs conf/zapiperf/cdot/9.8.0/wafl.yaml <p>The <code>wafl_cp_phase_times</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Backend timeseries System Utilization"},{"location":"ontap-metrics/#wafl_memory_free","title":"wafl_memory_free","text":"<p>The current WAFL memory available in the system.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>memory_free</code>Unit: mbType: rawBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_memory_free</code>Unit: mbType: rawBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_memory_used","title":"wafl_memory_used","text":"<p>The current WAFL memory used in the system.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>memory_used</code>Unit: mbType: rawBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_memory_used</code>Unit: mbType: rawBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_msg_total","title":"wafl_msg_total","text":"<p>Total number of WAFL messages per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>msg_total</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_msg_total</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_non_wafl_msg_total","title":"wafl_non_wafl_msg_total","text":"<p>Total number of non-WAFL messages per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>non_wafl_msg_total</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>non_wafl_msg_total</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_read_io_type","title":"wafl_read_io_type","text":"<p>Percentage of reads served from buffer cache, external cache, or disk.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>read_io_type</code>Unit: percentType: percentBase: read_io_type_base conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>read_io_type</code>Unit: percentType: percentBase: read_io_type_base conf/zapiperf/cdot/9.8.0/wafl.yaml <p>The <code>wafl_read_io_type</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: Node Backend timeseries Reads From"},{"location":"ontap-metrics/#wafl_reads_from_cache","title":"wafl_reads_from_cache","text":"<p>WAFL reads from cache.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>reads_from_cache</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_reads_from_cache</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_reads_from_cloud","title":"wafl_reads_from_cloud","text":"<p>WAFL reads from cloud storage.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>reads_from_cloud</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_reads_from_cloud</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_reads_from_cloud_s2c_bin","title":"wafl_reads_from_cloud_s2c_bin","text":"<p>WAFL reads from cloud storage via s2c bin.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>reads_from_cloud_s2c_bin</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_reads_from_cloud_s2c_bin</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_reads_from_disk","title":"wafl_reads_from_disk","text":"<p>WAFL reads from disk.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>reads_from_disk</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_reads_from_disk</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_reads_from_ext_cache","title":"wafl_reads_from_ext_cache","text":"<p>WAFL reads from external cache.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>reads_from_external_cache</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_reads_from_ext_cache</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_reads_from_fc_miss","title":"wafl_reads_from_fc_miss","text":"<p>WAFL reads from remote volume for fc_miss.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>reads_from_fc_miss</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_reads_from_fc_miss</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_reads_from_pmem","title":"wafl_reads_from_pmem","text":"<p>Wafl reads from persistent mmeory.</p> API Endpoint Metric Template ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_reads_from_pmem</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_reads_from_ssd","title":"wafl_reads_from_ssd","text":"<p>WAFL reads from SSD.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>reads_from_ssd</code>Unit: noneType: deltaBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_reads_from_ssd</code>Unit: noneType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_repl_msg_total","title":"wafl_repl_msg_total","text":"<p>Total number of replication WAFL messages per second.</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>replication_msg_total</code>Unit: per_secType: rateBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>wafl_repl_msg_total</code>Unit: per_secType: rateBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_total_cp_msecs","title":"wafl_total_cp_msecs","text":"<p>Milliseconds spent in Consistency Point (CP).</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>total_cp_msecs</code>Unit: millisecType: deltaBase: conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>total_cp_msecs</code>Unit: millisecType: deltaBase: conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"ontap-metrics/#wafl_total_cp_util","title":"wafl_total_cp_util","text":"<p>Percentage of time spent in a Consistency Point (CP).</p> API Endpoint Metric Template RestPerf <code>api/cluster/counter/tables/wafl</code> <code>total_cp_util</code>Unit: percentType: percentBase: cpu_elapsed_time conf/restperf/9.12.0/wafl.yaml ZapiPerf <code>perf-object-get-instances wafl</code> <code>total_cp_util</code>Unit: percentType: percentBase: cpu_elapsed_time conf/zapiperf/cdot/9.8.0/wafl.yaml"},{"location":"partial-aggregation/","title":"Partial aggregation","text":"<p>ONTAP may report partial aggregate results for certain objects during events such as node outages or cluster disruptions. When this occurs, Harvest's performance collectors (ZapiPerf, RestPerf, KeyPerf, and StatPerf) will skip reporting performance counters for the affected objects to ensure data accuracy.</p>"},{"location":"partial-aggregation/#identifying-partial-aggregation","title":"Identifying Partial Aggregation","text":"<p>To determine whether partial aggregation affects an object, check the <code>numPartials</code> entry in the Harvest logs. If <code>numPartials</code> is greater than zero, it indicates that partial aggregations have occurred for that object.</p> <p>For example, consider the log entry: <code>Collected Poller=aff-251 collector=ZapiPerf:NFSv4 apiMs=870 bytesRx=3640 calcMs=0 exportMs=0 instances=56 instancesExported=41 metrics=400 metricsExported=340 numCalls=1 numPartials=15</code></p> <p>In this example, 15 out of 56 NFSv4 instances experienced partial aggregation.</p>"},{"location":"partial-aggregation/#node-scoped-objects-exception","title":"Node-Scoped Objects Exception","text":"<p>Node-scoped objects are not affected by issues related to partial aggregation. For these objects, Harvest emits metrics even when ONTAP reports partial aggregation. This behavior is controlled by the <code>allow_partial_aggregation</code> flag in the object's template configuration.</p> <p>When <code>allow_partial_aggregation: true</code> is set in a template, Harvest will continue to collect and emit metrics for that object regardless of the partial aggregation status.</p>"},{"location":"plugins/","title":"Plugins","text":""},{"location":"plugins/#built-in-plugins","title":"Built-in Plugins","text":"<p>The <code>plugin</code> feature allows users to manipulate and customize data collected by collectors without changing the collectors. Plugins have the same capabilities as collectors and therefore can collect data on their own as well. Furthermore, multiple plugins can be put in a pipeline to perform more complex operations.</p> <p>Harvest architecture defines two types of plugins:</p> <p>Built-in generic - Statically compiled, generic plugins. \"Generic\" means the plugin is collector-agnostic. These plugins are provided in this package and listed in the right sidebar.</p> <p>Built-in custom - These plugins are statically compiled, collector-specific plugins. Their source code should reside inside the <code>plugins/</code>  subdirectory of the collector package (e.g. (<code>cmd/collectors/rest/plugins/svm/svm.go</code>)[https://github.com/NetApp/harvest/blob/main/cmd/collectors/rest/plugins/svm/svm.go]). Custom plugins have access to all the parameters of their parent collector and should therefore be treated with great care.</p> <p>This documentation gives an overview of builtin plugins. For other plugins, see their respective documentation. For writing your own plugin, see Developer's documentation.</p> <p>Note: the rules are executed in the same order as you've added them.</p>"},{"location":"plugins/#aggregator","title":"Aggregator","text":"<p>Aggregator creates a new collection of metrics (Matrix) by summarizing and/or averaging metric values from an existing Matrix for a given label. For example, if the collected metrics are for volumes, you can create an aggregation for nodes or svms.</p>"},{"location":"plugins/#rule-syntax","title":"Rule syntax","text":"<p>simplest case:</p> <pre><code>plugins:\n  Aggregator:\n    - LABEL\n# will aggregate a new Matrix based on target label LABEL\n</code></pre> <p>If you want to specify which labels should be included in the new instances, you can add those space-seperated after <code>LABEL</code>:</p> <pre><code>    - LABEL LABEL1,LABEL2\n    # same, but LABEL1 and LABEL2 will be copied into the new instances\n    # (default is to only copy LABEL and any global labels (such as cluster and datacenter)\n</code></pre> <p>Or include all labels:</p> <pre><code>    - LABEL ...\n    # copy all labels of the original instance\n</code></pre> <p>By default, aggregated metrics will be prefixed with <code>LABEL</code>. For example if the object of the original Matrix is <code>volume</code> (meaning metrics are prefixed with <code>volume_</code>) and <code>LABEL</code> is <code>aggr</code>, then the metric <code>volume_read_ops</code> will become <code>aggr_volume_read_ops</code>, etc. You can override this by providing the <code>&lt;&gt;OBJ</code> using the following syntax:</p> <pre><code>    - LABEL&lt;&gt;OBJ\n    # use OBJ as the object of the new matrix, e.g. if the original object is \"volume\" and you\n    # want to leave metric names unchanged, use \"volume\"\n</code></pre> <p>Finally, sometimes you only want to aggregate instances with a specific label value. You can use <code>&lt;VALUE&gt;</code> for that ( optionally follow by <code>OBJ</code>):</p> <pre><code>    - LABEL&lt;VALUE&gt;\n    # aggregate all instances if LABEL has value VALUE\n    - LABEL&lt;`VALUE`&gt;\n    # same, but VALUE is regular expression\n    - LABEL&lt;LABELX=`VALUE`&gt;\n    # same, but check against \"LABELX\" (instead of \"LABEL\")\n</code></pre> <p>Examples:</p> <pre><code>plugins:\n  Aggregator:\n    # will aggregate metrics of the aggregate. The labels \"node\" and \"type\" are included in the new instances\n    - aggr node type\n    # aggregate instances if label \"type\" has value \"flexgroup\"\n    # include all original labels\n    - type&lt;flexgroup&gt; ...\n    # aggregate all instances if value of \"volume\" ends with underscore and 4 digits\n    - volume&lt;`_\\d{4}$`&gt;\n</code></pre>"},{"location":"plugins/#aggregation-rules","title":"Aggregation rules","text":"<p>The plugin tries to intelligently aggregate metrics based on a few rules:</p> <ul> <li>Sum - the default rule, if no other rules apply</li> <li>Average - if any of the following is true:<ul> <li>metric name has suffix <code>_percent</code> or <code>_percentage</code></li> <li>metric name has prefix <code>average_</code> or <code>avg_</code></li> <li>metric has property (<code>metric.GetProperty()</code>) <code>percent</code> or <code>average</code></li> </ul> </li> <li>Weighted Average - applied if metric has property <code>average</code> and suffix <code>_latency</code> and if there is a   matching <code>_ops</code> metric. (This is currently only matching to ZapiPerf metrics, which use the Property field of   metrics.)</li> <li>Ignore - metrics created by some plugins, such as value_to_num by LabelAgent</li> </ul>"},{"location":"plugins/#max","title":"Max","text":"<p>Max creates a new collection of metrics (Matrix) by calculating max of metric values from an existing Matrix for a given label. For example, if the collected metrics are for disks, you can create max at the node or aggregate level. Refer Max Examples for more details.</p>"},{"location":"plugins/#max-rule-syntax","title":"Max Rule syntax","text":"<p>simplest case:</p> <pre><code>plugins:\n  Max:\n    - LABEL\n# create a new Matrix of max values on target label LABEL\n</code></pre> <p>If you want to specify which labels should be included in the new instances, you can add those space-seperated after <code>LABEL</code>:</p> <pre><code>    - LABEL LABEL1,LABEL2\n    # similar to the above example, but LABEL1 and LABEL2 will be copied into the new instances\n    # (default is to only copy LABEL and all global labels (such as cluster and datacenter)\n</code></pre> <p>Or include all labels:</p> <pre><code>    - LABEL ...\n    # copy all labels of the original instance\n</code></pre> <p>By default, metrics will be prefixed with <code>LABEL</code>. For example if the object of the original Matrix is <code>volume</code> (meaning metrics are prefixed with <code>volume_</code>) and <code>LABEL</code> is <code>aggr</code>, then the metric <code>volume_read_ops</code> will become <code>aggr_volume_read_ops</code>. You can override this using the <code>&lt;&gt;OBJ</code> pattern shown below:</p> <pre><code>    - LABEL&lt;&gt;OBJ\n    # use OBJ as the object of the new matrix, e.g. if the original object is \"volume\" and you\n    # want to leave metric names unchanged, use \"volume\"\n</code></pre> <p>Finally, sometimes you only want to generate instances with a specific label value. You can use <code>&lt;VALUE&gt;</code> for that ( optionally followed by <code>OBJ</code>):</p> <pre><code>    - LABEL&lt;VALUE&gt;\n    # aggregate all instances if LABEL has value VALUE\n    - LABEL&lt;`VALUE`&gt;\n    # same, but VALUE is regular expression\n    - LABEL&lt;LABELX=`VALUE`&gt;\n    # same, but check against \"LABELX\" (instead of \"LABEL\")\n</code></pre>"},{"location":"plugins/#max-examples","title":"Max Examples","text":"<pre><code>plugins:\n  Max:\n    # will create max of each aggregate metric. All metrics will be prefixed with aggr_disk_max. All labels are included in the new instances\n    - aggr&lt;&gt;aggr_disk_max ...\n    # calculate max instances if label \"disk\" has value \"1.1.0\". Prefix with disk_max\n    # include all original labels\n    - disk&lt;1.1.0&gt;disk_max ...\n    # max of all instances if value of \"volume\" ends with underscore and 4 digits\n    - volume&lt;`_\\d{4}$`&gt;\n</code></pre>"},{"location":"plugins/#labelagent","title":"LabelAgent","text":"<p>LabelAgent are used to manipulate instance labels based on rules. You can define multiple rules, here is an example of what you could add to the yaml file of a collector:</p> <pre><code>plugins:\n  LabelAgent:\n    # our rules:\n    split: node `/` ,aggr,plex,disk\n    replace_regex: node node `^(node)_(\\d+)_.*$` `Node-$2`\n</code></pre> <p>Note: Labels for creating new label should use name defined in right side of =&gt;. If not present then left side of =&gt; is used.</p>"},{"location":"plugins/#split","title":"split","text":"<p>Rule syntax:</p> <pre><code>split:\n  - LABEL `SEP` LABEL1,LABEL2,LABEL3\n# source label - separator - comma-seperated target labels\n</code></pre> <p>Splits the value of a given label by separator <code>SEP</code> and creates new labels if their number matches to the number of target labels defined in rule. To discard a subvalue, just add a redundant <code>,</code> in the names of the target labels.</p> <p>Example:</p> <pre><code>split:\n  - node `/` ,aggr,plex,disk\n# will split the value of \"node\" using separator \"/\"\n# will expect 4 values: first will be discarded, remaining\n# three will be stored as labels \"aggr\", \"plex\" and \"disk\"\n</code></pre>"},{"location":"plugins/#split_regex","title":"split_regex","text":"<p>Does the same as <code>split</code> but uses a regular expression instead of a separator.</p> <p>Rule syntax:</p> <pre><code>split_regex:\n  - LABEL `REGEX` LABEL1,LABEL2,LABEL3\n</code></pre> <p>Example:</p> <pre><code>split_regex:\n  - node `.*_(ag\\d+)_(p\\d+)_(d\\d+)` aggr,plex,disk\n# will look for \"_ag\", \"_p\", \"_d\", each followed by one\n# or more numbers, if there is a match, the submatches\n# will be stored as \"aggr\", \"plex\" and \"disk\"\n</code></pre>"},{"location":"plugins/#split_pairs","title":"split_pairs","text":"<p>Rule syntax:</p> <pre><code>split_pairs:\n  - LABEL `SEP1` `SEP2`\n# source label - pair separator - key-value separator\n</code></pre> <p>Extracts key-value pairs from the value of source label <code>LABEL</code>. Note that you need to add these keys in the export options, otherwise they will not be exported.</p> <p>Example:</p> <pre><code>split_pairs:\n  - comment ` ` `:`\n# will split pairs using a single space and split key-values using colon\n# e.g. if comment=\"owner:jack contact:some@email\", the result will be\n# two new labels: owner=\"jack\" and contact=\"some@email\"\n</code></pre>"},{"location":"plugins/#join","title":"join","text":"<p>Join multiple label values using separator <code>SEP</code> and create a new label.</p> <p>Rule syntax:</p> <pre><code>join:\n  - LABEL `SEP` LABEL1,LABEL2,LABEL3\n# target label - separator - comma-seperated source labels\n</code></pre> <p>Example:</p> <pre><code>join:\n  - plex_long `_` aggr,plex\n# will look for the values of labels \"aggr\" and \"plex\",\n# if they are set, a new \"plex_long\" label will be added\n# by joining their values with \"_\"\n</code></pre> <p>You can also use <code>join</code> to add a new label with a different name than the one collected, while keeping the original label intact. Use an empty separator and a single source label:</p> <pre><code>join:\n  - aggregate `` aggr\n# will create a new label \"aggregate\" using the value of the existing \"aggr\" label\n# the original \"aggr\" label is preserved; this is useful when you want to expose\n# a collected label under a different name without losing the original\n</code></pre>"},{"location":"plugins/#replace","title":"replace","text":"<p>Substitute substring <code>OLD</code> with <code>NEW</code> in label <code>SOURCE</code> and store in <code>TARGET</code>. Note that target and source labels can be the same.</p> <p>Rule syntax:</p> <pre><code>replace:\n  - SOURCE TARGET `OLD` `NEW`\n# source label - target label - substring to replace - replace with\n</code></pre> <p>Example:</p> <pre><code>replace:\n  - node node_short `node_` ``\n# this rule will just remove \"node_\" from all values of label\n# \"node\". E.g. if label is \"node_jamaica1\", it will rewrite it \n# as \"jamaica1\"\n</code></pre>"},{"location":"plugins/#replace_regex","title":"replace_regex","text":"<p>Same as <code>replace</code>, but will use a regular expression instead of <code>OLD</code>. Note you can use <code>$n</code> to specify <code>n</code>th submatch in <code>NEW</code>.</p> <p>Rule syntax:</p> <pre><code>replace_regex:\n  - SOURCE TARGET `REGEX` `NEW`\n# source label - target label - substring to replace - replace with\n</code></pre> <p>Example:</p> <pre><code>replace_regex:\n  - node node `^(node)_(\\d+)_.*$` `Node-$2`\n# if there is a match, will capitalize \"Node\" and remove suffixes.\n# E.g. if label is \"node_10_dc2\", it will rewrite it as\n# will rewrite it as \"Node-10\"\n</code></pre>"},{"location":"plugins/#exclude_equals","title":"exclude_equals","text":"<p>Exclude each instance, if the value of <code>LABEL</code> is exactly <code>VALUE</code>. Exclude means that metrics for this instance will not be exported.</p> <p>Rule syntax:</p> <pre><code>exclude_equals:\n  - LABEL `VALUE`\n# label name - label value\n</code></pre> <p>Example:</p> <pre><code>exclude_equals:\n  - vol_type `flexgroup_constituent`\n# all instances, which have label \"vol_type\" with value\n# \"flexgroup_constituent\" will not be exported\n</code></pre>"},{"location":"plugins/#exclude_contains","title":"exclude_contains","text":"<p>Same as <code>exclude_equals</code>, but all labels that contain <code>VALUE</code> will be excluded</p> <p>Rule syntax:</p> <pre><code>exclude_contains:\n  - LABEL `VALUE`\n# label name - label value\n</code></pre> <p>Example:</p> <pre><code>exclude_contains:\n  - vol_type `flexgroup_`\n# all instances, which have label \"vol_type\" which contain\n# \"flexgroup_\" will not be exported\n</code></pre>"},{"location":"plugins/#exclude_regex","title":"exclude_regex","text":"<p>Same as <code>exclude_equals</code>, but will use a regular expression and all matching instances will be excluded.</p> <p>Rule syntax:</p> <pre><code>exclude_regex:\n  - LABEL `REGEX`\n# label name - regular expression\n</code></pre> <p>Example:</p> <pre><code>exclude_regex:\n  - vol_type `^flex`\n# all instances, which have label \"vol_type\" which starts with\n# \"flex\" will not be exported\n</code></pre>"},{"location":"plugins/#include_equals","title":"include_equals","text":"<p>Include each instance, if the value of <code>LABEL</code> is exactly <code>VALUE</code>. Include means that metrics for this instance will be exported and instances that do not match will not be exported.</p> <p>Rule syntax:</p> <pre><code>include_equals:\n  - LABEL `VALUE`\n# label name - label value\n</code></pre> <p>Example:</p> <pre><code>include_equals:\n  - vol_type `flexgroup_constituent`\n# all instances, which have label \"vol_type\" with value\n# \"flexgroup_constituent\" will be exported\n</code></pre>"},{"location":"plugins/#include_contains","title":"include_contains","text":"<p>Same as <code>include_equals</code>, but all labels that contain <code>VALUE</code> will be included</p> <p>Rule syntax:</p> <pre><code>include_contains:\n  - LABEL `VALUE`\n# label name - label value\n</code></pre> <p>Example:</p> <pre><code>include_contains:\n  - vol_type `flexgroup_`\n# all instances, which have label \"vol_type\" which contain\n# \"flexgroup_\" will be exported\n</code></pre>"},{"location":"plugins/#include_regex","title":"include_regex","text":"<p>Same as <code>include_equals</code>, but a regular expression will be used for inclusion. Similar to the other includes, all matching instances will be included and all non-matching will not be exported.</p> <p>Rule syntax:</p> <pre><code>include_regex:\n  - LABEL `REGEX`\n# label name - regular expression\n</code></pre> <p>Example:</p> <pre><code>include_regex:\n  - vol_type `^flex`\n# all instances, which have label \"vol_type\" which starts with\n# \"flex\" will be exported\n</code></pre>"},{"location":"plugins/#value_mapping","title":"value_mapping","text":"<p>value_mapping was deprecated in 21.11 and removed in 22.02. Use value_to_num mapping instead.</p>"},{"location":"plugins/#value_to_num","title":"value_to_num","text":"<p>Map values of a given label to a numeric metric (of type <code>uint8</code>). This rule maps values of a given label to a numeric metric (of type <code>unit8</code>). Healthy is mapped to 1 and all non-healthy values are mapped to 0.</p> <p>This is handy to manipulate the data in the DB or Grafana (e.g. change color based on status or create alert).</p> <p>Note that you don't define the numeric values yourself, instead, you only provide the possible (expected) values, the plugin will map each value to its index in the rule.</p> <p>Rule syntax:</p> <pre><code>value_to_num:\n  - METRIC LABEL ZAPI_VALUE REST_VALUE `N`\n# map values of LABEL to 1 if it is ZAPI_VALUE or REST_VALUE\n# otherwise, value of METRIC is set to N\n</code></pre> <p>The default value <code>N</code> is optional, if no default value is given and the label value does not match any of the given values, the metric value will not be set.</p> <p>Examples:</p> <pre><code>value_to_num:\n  - status state up online `0`\n# a new metric will be created with the name \"status\"\n# if an instance has label \"state\" with value \"up\", the metric value will be 1,\n# if it's \"online\", the value will be set to 1,\n# if it's any other value, it will be set to the specified default, 0\n</code></pre> <pre><code>value_to_num:\n  - status state up online `4`\n# metric value will be set to 1 if \"state\" is \"up\", otherwise to **4**\n</code></pre> <pre><code>value_to_num:\n  - status outage - - `0` #ok_value is empty value. \n# metric value will be set to 1 if \"outage\" is empty, if it's any other value, it will be set to the default, 0\n# '-' is a special symbol in this mapping, and it will be converted to blank while processing.\n</code></pre>"},{"location":"plugins/#value_to_num_regex","title":"value_to_num_regex","text":"<p>Same as value_to_num, but will use a regular expression. All matches are mapped to 1 and non-matches are mapped to 0.</p> <p>This is handy to manipulate the data in the DB or Grafana (e.g. change color based on status or create alert).</p> <p>Note that you don't define the numeric values, instead, you provide the expected values and the plugin will map each value to its index in the rule.</p> <p>Rule syntax:</p> <pre><code>value_to_num_regex:\n  - METRIC LABEL ZAPI_REGEX REST_REGEX `N`\n# map values of LABEL to 1 if it matches ZAPI_REGEX or REST_REGEX\n# otherwise, value of METRIC is set to N\n</code></pre> <p>The default value <code>N</code> is optional, if no default value is given and the label value does not match any of the given values, the metric value will not be set.</p> <p>Examples:</p> <pre><code>value_to_num_regex:\n  - certificateuser methods .*cert.*$ .*certificate.*$ `0`\n# a new metric will be created with the name \"certificateuser\"\n# if an instance has label \"methods\" with value contains \"cert\", the metric value will be 1,\n# if value contains \"certificate\", the value will be set to 1,\n# if value doesn't contain \"cert\" and \"certificate\", it will be set to the specified default, 0\n</code></pre> <pre><code>value_to_num_regex:\n  - status state ^up$ ^ok$ `4`\n# metric value will be set to 1 if label \"state\" matches regex, otherwise set to **4**\n</code></pre>"},{"location":"plugins/#metricagent","title":"MetricAgent","text":"<p>MetricAgent are used to manipulate metrics based on rules. You can define multiple rules, here is an example of what you could add to the yaml file of a collector:</p> <pre><code>plugins:\n  MetricAgent:\n    compute_metric:\n      - snapshot_maxfiles_possible ADD snapshot.max_files_available snapshot.max_files_used\n      - raid_disk_count ADD block_storage.primary.disk_count block_storage.hybrid_cache.disk_count\n</code></pre> <p>Note: Metric names used to create new metrics can come from the left or right side of the rename operator (<code>=&gt;</code>) Note: The metric agent currently does not work for histogram or array metrics.</p>"},{"location":"plugins/#compute_metric","title":"compute_metric","text":"<p>This rule creates a new metric (of type float64) using the provided scalar or an existing metric value combined with a mathematical operation.</p> <p>You can provide a numeric value or a metric name with an operation. The plugin will use the provided number or fetch the value of a given metric, perform the requested mathematical operation, and store the result in new custom metric.</p> <p>Currently, we support these operations: ADD SUBTRACT MULTIPLY DIVIDE PERCENT</p> <p>Rule syntax:</p> <pre><code>compute_metric:\n  - METRIC OPERATION METRIC1 METRIC2 METRIC3\n# target new metric - mathematical operation - input metric names \n# apply OPERATION on metric values of METRIC1, METRIC2 and METRIC3 and set result in METRIC\n# METRIC1, METRIC2, METRIC3 can be a scalar or an existing metric name.\n</code></pre> <p>Examples:</p> <pre><code>compute_metric:\n  - space_total ADD space_available space_used\n# a new metric will be created with the name \"space_total\"\n# if an instance has metric \"space_available\" with value \"1000\", and \"space_used\" with value \"400\",\n# the result value will be \"1400\" and set to metric \"space_total\".\n</code></pre> <pre><code>compute_metric:\n  - disk_count ADD primary.disk_count secondary.disk_count hybrid.disk_count\n# value of metric \"disk_count\" would be addition of all the given disk_counts metric values.\n# disk_count = primary.disk_count + secondary.disk_count + hybrid.disk_count\n</code></pre> <pre><code>compute_metric:\n  - files_available SUBTRACT files files_used\n# value of metric \"files_available\" would be subtraction of the metric value of files_used from metric value of files.\n# files_available = files - files_used\n</code></pre> <pre><code>compute_metric:\n  - total_bytes MULTIPLY bytes_per_sector sector_count\n# value of metric \"total_bytes\" would be multiplication of metric value of bytes_per_sector and metric value of sector_count.\n# total_bytes = bytes_per_sector * sector_count\n</code></pre> <pre><code>compute_metric:\n  - uptime MULTIPLY stats.power_on_hours 60 60\n# value of metric \"uptime\" would be multiplication of metric value of stats.power_on_hours and scalar value of 60 * 60.\n# total_bytes = bytes_per_sector * sector_count\n</code></pre> <pre><code>compute_metric:\n  - transmission_rate DIVIDE transfer.bytes_transferred transfer.total_duration\n# value of metric \"transmission_rate\" would be division of metric value of transfer.bytes_transferred by metric value of transfer.total_duration.\n# transmission_rate = transfer.bytes_transferred / transfer.total_duration\n</code></pre> <pre><code>compute_metric:\n  - inode_used_percent PERCENT inode_files_used inode_files_total\n# a new metric named \"inode_used_percent\" will be created by dividing the metric \"inode_files_used\" by \n#  \"inode_files_total\" and multiplying the result by 100.\n# inode_used_percent = inode_files_used / inode_files_total * 100\n</code></pre>"},{"location":"plugins/#changelog","title":"ChangeLog","text":"<p>The ChangeLog plugin is a feature of Harvest, designed to detect and track changes related to the creation, modification, and deletion of an object. By default, it supports volume, svm, and node objects. Its functionality can be extended to track changes in other objects by making relevant changes in the template.</p> <p>Please note that the ChangeLog plugin requires the <code>uuid</code> label, which is unique, to be collected by the template. Without the <code>uuid</code> label, the plugin will not function.</p> <p>The ChangeLog feature only detects changes when Harvest is up and running. It does not detect changes that occur when Harvest is down. Additionally, the plugin does not detect changes in metric values by default, but it can be configured to do so.</p>"},{"location":"plugins/#enabling-the-plugin","title":"Enabling the Plugin","text":"<p>The plugin can be enabled in the templates under the plugins section. Steps are documented here.</p> <p>For volume, svm, and node objects, you can enable the plugin with the following configuration:</p> <pre><code>plugins:\n  - ChangeLog\n</code></pre> <p>For other objects, you need to specify the labels to track in the plugin configuration. These labels should be relevant to the object you want to track. If these labels are not specified in the template, the plugin will not be able to track changes for the object.</p> <p>Here's an example of how to enable the plugin for an aggregate object:</p> <pre><code>plugins:\n  - ChangeLog:\n      track:\n        - aggr\n        - node\n        - state\n</code></pre> <p>In the above configuration, the plugin will track changes in the <code>aggr</code>, <code>node</code>, and <code>state</code> labels for the aggregate object.</p>"},{"location":"plugins/#default-tracking-for-svm-node-volume","title":"Default Tracking for svm, node, volume","text":"<p>By default, the plugin tracks changes in the following labels for svm, node, and volume objects:</p> <ul> <li>svm: svm, state, type, anti_ransomware_state</li> <li>node: node, location, healthy</li> <li>volume: node, volume, svm, style, type, aggr, state, status</li> </ul> <p>Other objects are not tracked by default.</p> <p>These default settings can be overwritten as needed in the relevant templates. For instance, if you want to track <code>junction_path</code> label and <code>size_total</code> metric for Volume, you can overwrite this in the volume template.</p> <pre><code>plugins:\n  - ChangeLog:\n      - track:\n        - node\n        - volume\n        - svm\n        - style\n        - type\n        - aggr\n        - state\n        - status\n        - junction_path\n        - size_total\n</code></pre>"},{"location":"plugins/#change-types-and-metrics","title":"Change Types and Metrics","text":"<p>The ChangeLog plugin publishes a metric with various labels providing detailed information about the change when an object is created, modified, or deleted.</p>"},{"location":"plugins/#object-creation","title":"Object Creation","text":"<p>When a new object is created, the ChangeLog plugin will publish a metric with the following labels:</p> Label Description object name of the ONTAP object that was changed op type of change that was made metric value timestamp when Harvest captured the change. 1698735558 in the example below <p>Example of metric shape for object creation:</p> <pre><code>change_log{aggr=\"umeng_aff300_aggr2\", cluster=\"umeng-aff300-01-02\", datacenter=\"u2\", index=\"0\", instance=\"localhost:12993\", job=\"prometheus\", node=\"umeng-aff300-01\", object=\"volume\", op=\"create\", style=\"flexvol\", svm=\"harvest\", volume=\"harvest_demo\"} 1698735558\n</code></pre>"},{"location":"plugins/#object-modification","title":"Object Modification","text":"<p>When an existing object is modified, the ChangeLog plugin will publish a metric with the following labels:</p> Label Description <code>object</code> Name of the ONTAP object that was changed <code>op</code> Type of change that was made <code>track</code> Property of the object which was modified <code>new_value</code> New value of the object after the change (only available for label changes and not for metric changes) <code>old_value</code> Previous value of the object before the change (only available for label changes and not for metric changes) <code>metric value</code> Timestamp when Harvest captured the change. <code>1698735677</code> in the example below <code>category</code> Type of the change, indicating whether it is a <code>metric</code> or a <code>label</code> change <p>Example of metric shape for object modification for label:</p> <pre><code>change_log{aggr=\"umeng_aff300_aggr2\", category=\"label\", cluster=\"umeng-aff300-01-02\", datacenter=\"u2\", index=\"1\", instance=\"localhost:12993\", job=\"prometheus\", new_value=\"offline\", node=\"umeng-aff300-01\", object=\"volume\", old_value=\"online\", op=\"update\", style=\"flexvol\", svm=\"harvest\", track=\"state\", volume=\"harvest_demo\"} 1698735677\n</code></pre> <p>Example of metric shape for metric value change:</p> <pre><code>change_log{aggr=\"umeng_aff300_aggr2\", category=\"metric\", cluster=\"umeng-aff300-01-02\", datacenter=\"u2\", index=\"3\", instance=\"localhost:12993\", job=\"prometheus\", node=\"umeng-aff300-01\", object=\"volume\", op=\"metric_change\", track=\"volume_size_total\", svm=\"harvest\", volume=\"harvest_demo\"} 1698735800\n</code></pre>"},{"location":"plugins/#object-deletion","title":"Object Deletion","text":"<p>When an object is deleted, the ChangeLog plugin will publish a metric with the following labels:</p> Label Description object name of the ONTAP object that was changed op type of change that was made metric value timestamp when Harvest captured the change. 1698735708 in the example below <p>Example of metric shape for object deletion:</p> <pre><code>change_log{aggr=\"umeng_aff300_aggr2\", cluster=\"umeng-aff300-01-02\", datacenter=\"u2\", index=\"2\", instance=\"localhost:12993\", job=\"prometheus\", node=\"umeng-aff300-01\", object=\"volume\", op=\"delete\", style=\"flexvol\", svm=\"harvest\", volume=\"harvest_demo\"} 1698735708\n</code></pre>"},{"location":"plugins/#viewing-the-metrics","title":"Viewing the Metrics","text":"<p>You can view the metrics published by the ChangeLog plugin in the <code>ChangeLog Monitor</code> dashboard in <code>Grafana</code>. This dashboard provides a visual representation of the changes tracked by the plugin for volume, svm, and node objects.</p>"},{"location":"plugins/#volumetopclients","title":"VolumeTopClients","text":"<p>The <code>VolumeTopClients</code> plugin is used to track a volume's top clients and top files in terms of read and write IOPS, as well as read and write throughput. This plugin is available only through the RestPerf Collector in ONTAP version 9.13.1 and later.</p>"},{"location":"plugins/#enabling-the-plugin_1","title":"Enabling the Plugin","text":"<p>Top Clients and Files collection is disabled by default. To enable Top Clients and Files tracking in Harvest, follow these steps:</p> <ol> <li>Ensure you are using ONTAP version 9.12 or later.</li> <li>Enable the Top Clients collection in the RestPerf Collector Volume template via the <code>VolumeTopClients</code> plugin.</li> </ol> <p>For detailed steps on how to enable the plugin, refer to the discussion here.</p>"},{"location":"plugins/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"plugins/#max_volumes","title":"<code>max_volumes</code>","text":"<p>The <code>max_volumes</code> parameter specifies the maximum number of top volumes to track. By default, this value is set to 5, but it can be configured up to a maximum of 50.</p> <p>The plugin will select the top volumes based on the descending order of read IOPS, write IOPS, read throughput, and write throughput in each performance poll. This means that during each performance poll, the plugin will:</p> <ol> <li>Collect the read IOPS, write IOPS, read throughput, and write throughput for all volumes.</li> <li>Sort the volumes in descending order based on their metric values.</li> <li>Select the top volumes as specified by <code>max_volumes</code>.</li> <li>Collect top clients and top files metrics for these volumes.</li> </ol>"},{"location":"plugins/#objects","title":"<code>objects</code>","text":"<p>The <code>objects</code> parameter allows you to specify which metrics to collect. By default, both client and file data will be collected. You can customize this by including or excluding specific objects:</p> <pre><code>- objects:\n    - client  # collect read/write operations and throughput metrics for the top clients.\n    - file    # collect read/write operations and throughput metrics for the top files\n</code></pre> <p>If the <code>objects</code> parameter is not defined, both client and file data will be collected by default.</p>"},{"location":"plugins/#viewing-the-metrics_1","title":"Viewing the Metrics","text":"<p>You can view the metrics published by the <code>VolumeTopClients</code> plugin in the <code>Volume</code> dashboard under the <code>Clients</code> and <code>Files</code> row in Grafana.</p>"},{"location":"prepare-7mode-clusters/","title":"ONTAP 7mode","text":"<p>NetApp Harvest requires login credentials to access monitored hosts. Although, a generic admin account can be used, it is best practice to create a dedicated monitoring account with the least privilege access.</p> <p>ONTAP 7-mode supports only username / password based authentication with NetApp Harvest. Harvest communicates with monitored systems exclusively via HTTPS, which is not enabled by default in Data ONTAP 7-mode. Login as a user with full administrative privileges and execute the following steps.</p>"},{"location":"prepare-7mode-clusters/#enabling-https-and-tls-ontap-7-mode-only","title":"Enabling HTTPS and TLS (ONTAP 7-mode only)","text":"<p>Verify SSL is configured</p> <pre><code>secureadmin status ssl\n</code></pre> <p>If ssl is \u2018active\u2019 continue. If not, setup SSL and be sure to choose a Key length (bits) of 2048:</p> <pre><code>secureadmin setup ssl\n</code></pre> <pre><code>SSL Setup has already been done before. Do you want to proceed? [no] yes\nCountry Name (2 letter code) [US]: NL\nState or Province Name (full name) [California]: Noord-Holland\nLocality Name (city, town, etc.) [Santa Clara]: Schiphol\nOrganization Name (company) [Your Company]: NetApp\nOrganization Unit Name (division): SalesEngineering\nCommon Name (fully qualified domain name) [sdt-7dot1a.nltestlab.hq.netapp.com]:\nAdministrator email: noreply@netapp.com\nDays until expires [5475] :5475 Key length (bits) [512] :2048\n</code></pre> <p>Enable management via SSL and enable TLS</p> <pre><code>options httpd.admin.ssl.enable on\noptions tls.enable on  \n</code></pre>"},{"location":"prepare-7mode-clusters/#creating-ontap-user","title":"Creating ONTAP user","text":""},{"location":"prepare-7mode-clusters/#create-the-role-with-required-capabilities","title":"Create the role with required capabilities","text":"<pre><code>role add netapp-harvest-role -c \"Role for performance monitoring by NetApp Harvest\" -a login-http-admin,api-system-get-version,api-system-get-info,api-perf-object-*,api-emsautosupport-log \n</code></pre>"},{"location":"prepare-7mode-clusters/#create-a-group-for-this-role","title":"Create a group for this role","text":"<pre><code>useradmin group add netapp-harvest-group -c \"Group for performance monitoring by NetApp Harvest\" -r netapp-harvest-role \n</code></pre>"},{"location":"prepare-7mode-clusters/#create-a-user-for-the-role-and-enter-the-password-when-prompted","title":"Create a user for the role and enter the password when prompted","text":"<pre><code>useradmin user add netapp-harvest -c \"User account for performance monitoring by NetApp Harvest\" -n \"NetApp Harvest\" -g netapp-harvest-group\n</code></pre> <p>The user is now created and can be configured for use by NetApp Harvest.</p>"},{"location":"prepare-cdot-clusters/","title":"ONTAP cDOT","text":""},{"location":"prepare-cdot-clusters/#prepare-ontap-cdot-cluster","title":"Prepare ONTAP cDOT cluster","text":"<p>NetApp Harvest requires login credentials to access monitored hosts. Although a generic admin account can be used, it is better to create a dedicated read-only monitoring account.</p> <p>In the examples below, the user, group, roles, etc., use a naming convention of <code>netapp-harvest</code>. These can be modified as needed to match your organizational needs.</p> <p>There are a few steps required to prepare each system for monitoring. Harvest supports two authentication styles (<code>auth_style</code>) to connect to ONTAP clusters: <code>basic_auth</code> and <code>certificate_auth</code>. Both work well, but if you're starting fresh, the recommendation is to create a read-only harvest user on your ONTAP server and use certificate-based TLS authentication.</p> <p>Here's a summary of what we're going to do</p> <ol> <li>Create a read-only ONTAP role with the necessary capabilities that Harvest will use to auth and collect data</li> <li>Create a user account using the role created in step #1</li> <li>Update the <code>harvest.yml</code> file to use the user account and password created in step #2 and start Harvest.</li> </ol> <p>There are two ways to create a read-only ONTAP role. Pick the one that best fits your needs.</p> <ul> <li>Create a role with read-only access to all API objects via System Manager.</li> <li>Create a role with read-only access to the limited set of APIs Harvest collects via ONTAP's command line interface (CLI).</li> </ul>"},{"location":"prepare-cdot-clusters/#ontap-cli","title":"ONTAP CLI","text":"<p>We are going to:</p> <ol> <li>create a Harvest role with read-only access to a limited set of objects</li> <li>create a Harvest user and assign it to that role</li> <li>Depending on which collectors you want to use, add permissions for one or all of the Rest, Zapi, StatPerf collectors.</li> </ol> <p>Login to the CLI of your cDOT ONTAP system using SSH.</p>"},{"location":"prepare-cdot-clusters/#rest-least-privilege-role","title":"REST least-privilege role","text":"<p>Verify there are no errors when you copy/paste these. Warnings are fine.</p> <pre><code>security login rest-role create -role harvest-rest-role -access readonly -api /api/cloud/targets\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/cluster\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/cluster/counter/tables\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/cluster/mediators\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/cluster/metrocluster/diagnostics\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/cluster/nodes\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/cluster/ntp/servers\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/cluster/peers\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/cluster/sensors\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/name-services/ldap\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/name-services/nis\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/network/ethernet/ports\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/network/ethernet/switch/ports\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/network/fc/ports\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/network/ip/interfaces\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/network/ip/routes\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/protocols/cifs/services\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/protocols/cifs/sessions\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/protocols/cifs/shares\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/protocols/locks\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/protocols/ndmp/sessions\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/protocols/nfs/connected-clients\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/protocols/nfs/export-policies\n# s3 is buggy in 9.15, use protocols endpoint instead. See https://mysupport.netapp.com/site/bugs-online/product/ONTAP/JiraNgage/CONTAP-210232\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/protocols\n# security login rest-role create -role harvest-rest-role -access readonly -api /api/protocols/s3/buckets\n# security login rest-role create -role harvest-rest-role -access readonly -api /api/protocols/s3/services\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/security\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/security/accounts\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/security/audit/destinations\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/security/certificates\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/security/login/messages\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/security/ssh\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/snapmirror/relationships\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/snapmirror/policies\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/aggregates\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/disks\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/flexcache/flexcaches\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/luns\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/namespaces\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/qtrees\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/qos/policies\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/qos/workloads\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/quota/reports\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/shelves\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/snapshot-policies\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/storage/volumes\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/support/auto-update\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/support/autosupport\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/support/ems/destinations\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/support/ems/events\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/support/ems/messages\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/svm/peers\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/svm/svms\n\n# Private CLI endpoints\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/support/alerts\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/aggr\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/cluster/date\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/disk\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/vserver/export-policy/rule\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/security/certificate\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/security/ssl\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/network/connections/active\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/network/interface\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/network/port\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/network/port/ifgrp\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/node\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/qos/adaptive-policy-group\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/qos/policy-group\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/qos/workload\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/qtree\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/snapmirror\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/storage/failover\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/storage/shelf\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/system/chassis/fru\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/system/controller/fru\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/system/health/subsystem\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/system/node/environment/sensors\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/volume\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/vserver\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/vserver/cifs/share\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/private/cli/vserver/object-store-server/bucket/policy\n</code></pre> <p>See #2991 for more information.</p> <p>Associate the REST role with the harvest user</p> <p>Using password authentication</p> <pre><code>security login create -user-or-group-name harvest2 -application http -role harvest-rest-role -authentication-method password\n</code></pre> If you get an error <code>command failed: duplicate entry</code> when running the previous command <p>Remove the previous entry and recreate like so:</p> <pre><code>security login delete -user-or-group-name harvest2 -application http -authentication-method *\nsecurity login create -user-or-group-name harvest2 -application http -role harvest-rest-role -authentication-method password\n</code></pre> <p>Using certificate authentication</p> <pre><code>security login create -user-or-group-name harvest2 -application http -role harvest-rest-role -authentication-method cert\n</code></pre> If you get an error <code>command failed: duplicate entry</code> when running the previous command <p>Remove the previous entry and recreate like so:</p> <pre><code>security login delete -user-or-group-name harvest2 -application http -authentication-method *\nsecurity login create -user-or-group-name harvest2 -application http -role harvest-rest-role -authentication-method cert\n</code></pre>"},{"location":"prepare-cdot-clusters/#zapi-least-privilege-role","title":"Zapi least-privilege role","text":"<p>Verify there are no errors when you copy/paste these. Warnings are fine.</p> <pre><code>security login role create -role harvest2-role -access readonly -cmddirname \"cluster\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"event notification destination show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"event notification destination\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"event log\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"event catalog show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"lun\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"metrocluster configuration-settings mediator add\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"metrocluster\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"network connections active show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"network fcp adapter show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"network interface\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"network port show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"network port ifgrp show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"network route show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"qos adaptive-policy-group\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"qos policy-group\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"qos workload show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"security\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"snapmirror\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"statistics\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"storage aggregate\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"storage disk\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"storage encryption disk\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"storage failover show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"storage iscsi-initiator show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"storage shelf\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"system chassis fru show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"system controller fru show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"system health alert show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"system health status show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"system health subsystem show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"system license show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"system node\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"system node environment sensors show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"system service-processor show\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"version\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"volume\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"vserver\"\n</code></pre>"},{"location":"prepare-cdot-clusters/#associate-the-zapi-role-with-the-harvest-user","title":"Associate the ZAPI role with the harvest user","text":"<p>Use this for password authentication</p> <pre><code># If the harvest2 user does not exist, you will be prompted to enter a password\nsecurity login create -user-or-group-name harvest2 -application ontapi -role harvest2-role -authentication-method password\n</code></pre> <p>Or this for certificate authentication</p> <pre><code>security login create -user-or-group-name harvest2 -application ontapi -role harvest2-role -authentication-method cert\n</code></pre> <p>Verify that the harvest2-role role has web access by running the following commands.</p> <pre><code>vserver services web access show -role harvest2-role -name ontapi\n</code></pre> <p>If there are no matching entries, enable access by running the following.</p> <pre><code>vserver services web access create -role harvest2-role -name ontapi \n</code></pre>"},{"location":"prepare-cdot-clusters/#ontap-910-volume-performance-metrics","title":"ONTAP 9.10+ volume performance metrics","text":"<p>Starting with ONTAP 9.10 the Zapi based collection needs a REST read permission on the volumes endpoint so Harvest can look up volume data required to collect performance metrics. If you already created the full REST least-privilege role, you already have this (<code>/api/storage/volumes</code>) and can skip the commands below. Otherwise add just this minimal permission and associate the HTTP application with the REST role:</p> <pre><code>security login rest-role create -role harvest-rest-role -access readonly -api /api/storage/volumes\nsecurity login create -user-or-group-name harvest2 -application http -role harvest-rest-role -authentication-method password\n</code></pre> <p>Use <code>-authentication-method cert</code> instead of <code>password</code> if using certificate auth.</p>"},{"location":"prepare-cdot-clusters/#statperf-least-privilege-role","title":"StatPerf least-privilege role","text":"<p>Verify there are no errors when you copy/paste these. Warnings are fine.</p> <pre><code>security login role create -role harvest2-role -access all -cmddirname \"set\"\nsecurity login role create -role harvest2-role -access readonly -cmddirname \"statistics\"\n\nsecurity login rest-role create -role harvest-rest-role -access readonly -api /api/cluster\nsecurity login rest-role create -role harvest-rest-role -access read_create -api /api/private/cli\nsecurity login create -user-or-group-name harvest2 -application ssh -authentication-method password -role harvest2-role\nsecurity login create -user-or-group-name harvest2 -application http -authentication-method password -role harvest-rest-role\n</code></pre> <p>Note: StatPerf Collector is not supported for FSx systems.</p>"},{"location":"prepare-cdot-clusters/#7-mode-cli","title":"7-Mode CLI","text":"<p>Login to the CLI of your 7-Mode ONTAP system (e.g., using SSH). First, we create a user role. If you want to give the user readonly access to all API objects, type in the following command:</p> <pre><code>useradmin role modify harvest2-role -a login-http-admin,api-system-get-version, \\\napi-system-get-info,api-perf-object-*,api-ems-autosupport-log,api-diagnosis-status-get, \\\napi-lun-list-info,api-diagnosis-subsystem-config-get-iter,api-disk-list-info, \\\napi-diagnosis-config-get-iter,api-aggr-list-info,api-volume-list-info, \\\napi-storage-shelf-environment-list-info,api-qtree-list,api-quota-report\n</code></pre>"},{"location":"prepare-cdot-clusters/#system-manager","title":"System Manager","text":"<p>Open System Manager. Click on CLUSTER in the left menu bar, Settings and Users and Roles.</p> <p></p> <p>In the right column, under Roles, click on Add to add a new role.</p> <p></p> <p>Choose a role name (e.g. harvest2-role). In the REST API PATH field, type /api and select Read-Only for ACCESS. Click on Save.</p> <p></p> <p>In the left column, under Users, click on Add to create a new user. Choose a username. Under Role, select the role that we just created. Under User Login Methods select ONTAPI, and one of the two authentication methods. Press the <code>Add</code> button and select HTTP and one of the authentication methods. Type in a password if you chose Password. Click on Save</p> <p></p> <p>If you chose Password, you can add the username and password to the Harvest configuration file and start Harvest. If you chose Certificate jump to Using Certificate Authentication to generate certificates files.</p> System Manager Classic interface <p>Open System Manager. Click on the Settings icon in the top-right corner of the window.</p> <p></p> <p>Click on Roles in the left menu bar and click Add. Choose a role name (e.g. harvest2-role).</p> <p></p> <p>Under Role Attributes click on Add, under Command type DEFAULT, leave Query empty, select readonly under Access Level, click on OK and Add.</p> <p>After you click on Add, this is what you should see:</p> <p></p> <p>Now we need to create a user. Click on Users in the left menu bar and Add. Choose a username and password. Under User Login Methods click on Add, select ontapi as Application and select the role that we just created as Role. Repeat by clicking on Add, select http as Application and select the role that we just created as Role. Click on Add in the pop-up window to save.</p> <p></p>"},{"location":"prepare-cdot-clusters/#using-certificate-authentication","title":"Using Certificate Authentication","text":"<p>See comments here for troubleshooting client certificate authentication.</p> <p>Client certificate authentication allows you to authenticate with your ONTAP cluster without including username/passwords in your <code>harvest.yml</code> file. The process to set up client certificates is straightforward, although self-signed certificates introduce more work as does Go's strict treatment of common names.</p> <p>Unless you've installed production certificates on your ONTAP cluster, you'll need to replace your cluster's common-name-based self-signed certificates with a subject alternative name-based certificate. After that step is completed, we'll create client certificates and add those for passwordless login.</p> <p>If you can't or don't want to replace your ONTAP cluster certificates, there are some workarounds. You can</p> <ul> <li>Use <code>use_insecure_tls: true</code> in your <code>harvest.yml</code> to disable certificate verification</li> <li>Change your <code>harvest.yml</code> to connect via hostname instead of IP address</li> </ul>"},{"location":"prepare-cdot-clusters/#create-self-signed-subject-alternate-name-certificates-for-ontap","title":"Create Self-Signed Subject Alternate Name Certificates for ONTAP","text":"<p>Subject alternate name (SAN) certificates allow multiple hostnames in a single certificate. Starting with Go 1.3, when connecting to a cluster via its IP address, the CN field in the server certificate is ignored. This often causes errors like this: <code>x509: cannot validate certificate for 127.0.0.1 because it doesn't contain any IP SANs</code></p>"},{"location":"prepare-cdot-clusters/#overview-of-steps-to-create-a-self-signed-san-certificate-and-make-ontap-use-it","title":"Overview of steps to create a self-signed SAN certificate and make ONTAP use it","text":"<ol> <li>Create a root key</li> <li>Create a root certificate authority certificate</li> <li>Create a SAN certificate for your ONTAP cluster, using #2 to create it</li> <li>Install root ca certificate created in step #2 on cluster</li> <li>Install SAN certificate created in step #3 on your cluster</li> <li>Modify your cluster/SVM to use the new certificate installed at step #5</li> </ol>"},{"location":"prepare-cdot-clusters/#setup","title":"Setup","text":"<pre><code># create a place to store the certificate authority files, adjust as needed\nmkdir -p ca/{private,certs}\n</code></pre>"},{"location":"prepare-cdot-clusters/#create-a-root-key","title":"Create a root key","text":"<pre><code>cd ca\n# generate a private key that we will use to create our self-signed certificate authority\nopenssl genrsa -out private/ca.key.pem 4096\nchmod 400 private/ca.key.pem\n</code></pre>"},{"location":"prepare-cdot-clusters/#create-a-root-certificate-authority-certificate","title":"Create a root certificate authority certificate","text":"<p>Download the sample openssl.cnf file and put it in the directory we created in setup. Edit line 9, changing <code>dir</code> to point to your <code>ca</code> directory created in setup.</p> <pre><code>openssl req -config openssl.cnf -key private/ca.key.pem -new -x509 -days 7300 -sha256 -extensions v3_ca -out certs/ca.cert.pem\n\n# Verify\nopenssl x509 -noout -text -in certs/ca.cert.pem\n\n# Make sure these are present\n    Signature Algorithm: sha256WithRSAEncryption               &lt;======== Signature Algorithm can not be sha-1\n        X509v3 extensions:\n            X509v3 Subject Key Identifier: \n                --removed\n            X509v3 Authority Key Identifier: \n                --removed\n\n            X509v3 Basic Constraints: critical\n                CA:TRUE                                        &lt;======== CA must be true\n            X509v3 Key Usage: critical\n                Digital Signature, Certificate Sign, CRL Sign  &lt;======== Digital and certificate signature\n</code></pre>"},{"location":"prepare-cdot-clusters/#create-a-san-certificate-for-your-ontap-cluster","title":"Create a SAN certificate for your ONTAP cluster","text":"<p>First, we'll create the certificate signing request and then the certificate. In this example, the ONTAP cluster is named <code>umeng-aff300-05-06</code>, update accordingly.</p> <p>Download the sample server_cert.cnf file and put it in the directory we created in setup. Edit lines 18-21 to include your ONTAP cluster hostnames and IP addresses. Edit lines 6-11 with new names as needed.</p> <pre><code>openssl req -new -newkey rsa:4096 -nodes -sha256 -subj \"/\" -config server_cert.cnf -outform pem -out umeng-aff300-05-06.csr -keyout umeng-aff300-05-06.key\n\n# Verify\nopenssl req -text -noout -in umeng-aff300-05-06.csr\n\n# Make sure these are present\n        Attributes:\n        Requested Extensions:\n            X509v3 Subject Alternative Name:         &lt;======== Section that lists alternate DNS and IP names\n                DNS:umeng-aff300-05-06-cm.rtp.openenglab.netapp.com, DNS:umeng-aff300-05-06, IP Address:10.193.48.11, IP Address:10.193.48.11\n    Signature Algorithm: sha256WithRSAEncryption     &lt;======== Signature Algorithm can not be sha-1\n</code></pre> <p>We'll now use the certificate signing request and the recently created certificate authority to create a new SAN certificate for our cluster.</p> <pre><code>openssl x509 -req -sha256 -days 30 -in umeng-aff300-05-06.csr -CA certs/ca.cert.pem -CAkey private/ca.key.pem -CAcreateserial -out umeng-aff300-05-06.crt -extensions req_ext -extfile server_cert.cnf\n\n# Verify\nopenssl x509 -text -noout -in umeng-aff300-05-06.crt\n\n# Make sure these are present\nX509v3 extensions:\n            X509v3 Subject Alternative Name:       &lt;======== Section that lists alternate DNS and IP names\n                DNS:umeng-aff300-05-06-cm.rtp.openenglab.netapp.com, DNS:umeng-aff300-05-06, IP Address:10.193.48.11, IP Address:10.193.48.11\n    Signature Algorithm: sha256WithRSAEncryption   &lt;======== Signature Algorithm can not be sha-1\n</code></pre>"},{"location":"prepare-cdot-clusters/#install-root-ca-certificate-on-cluster","title":"Install Root CA Certificate On Cluster","text":"<p>Login to your cluster with admin credentials and install the server certificate authority. Copy from <code>ca/certs/ca.cert.pem</code></p> <pre><code>ssh admin@IP\numeng-aff300-05-06::*&gt; security certificate install -type server-ca\n\nPlease enter Certificate: Press &lt;Enter&gt; when done\n-----BEGIN CERTIFICATE-----\n...\n-----END CERTIFICATE-----\n\nYou should keep a copy of the CA-signed digital certificate for future reference.\n\nThe installed certificate's CA and serial number for reference:\nCA: ntap\nSerial: 46AFFC7A3A9999999E8FB2FEB0\n\nThe certificate's generated name for reference: ntap\n</code></pre> <p>Now install the server certificate we created above with SAN. Copy certificate from <code>ca/umeng-aff300-05-06.crt</code> and private key from <code>ca/umeng-aff300-05-06.key</code></p> <pre><code>umeng-aff300-05-06::*&gt; security certificate install -type server\n\nPlease enter Certificate: Press &lt;Enter&gt; when done\n-----BEGIN CERTIFICATE-----\n..\n-----END CERTIFICATE-----\n\nPlease enter Private Key: Press &lt;Enter&gt; when done\n-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n\nPlease enter certificates of Certification Authorities (CA) which form the certificate chain of the server certificate. This starts with the issuing CA certificate of the server certificate and can range up to the root CA certificate.\n\nDo you want to continue entering root and/or intermediate certificates {y|n}: n\n</code></pre> <p>If ONTAP tells you the provided certificate does not have a common name in the subject field, type the hostname of the cluster like this:</p> <pre><code>The provided certificate does not have a common name in the subject field.\n\nEnter a valid common name to continue installation of the certificate:\n\nEnter a valid common name to continue installation of the certificate: umeng-aff300-05-06-cm.rtp.openenglab.netapp.com\n\nYou should keep a copy of the private key and the CA-signed digital certificate for future reference.\n\nThe installed certificate's CA and serial number for reference:\nCA: ntap\nSerial: 67A94AA25B229A68AC5BABACA8939A835AA998A58\n\nThe certificate's generated name for reference: umeng-aff300-05-06-cm.rtp.openenglab.netapp.com\n</code></pre>"},{"location":"prepare-cdot-clusters/#modify-the-admin-svm-to-use-the-new-certificate","title":"Modify the admin SVM to use the new certificate","text":"<p>We'll modify the cluster's admin SVM to use the just installed server certificate and certificate authority.</p> <pre><code>vserver show -type admin -fields vserver,type\nvserver            type\n------------------ -----\numeng-aff300-05-06 admin\n\numeng-aff300-05-06::*&gt; ssl modify -vserver umeng-aff300-05-06 -server-enabled true -serial 67A94AA25B229A68AC5BABACA8939A835AA998A58 -ca ntap\n  (security ssl modify)\n</code></pre> <p>You can verify the certificate(s) are installed and working by using <code>openssl</code> like so:</p> <pre><code>openssl s_client -CAfile certs/ca.cert.pem -showcerts -servername server -connect umeng-aff300-05-06-cm.rtp.openenglab.netapp.com:443\n\nCONNECTED(00000005)\ndepth=1 C = US, ST = NC, L = RTP, O = ntap, OU = ntap\nverify return:1\ndepth=0 \nverify return:1\n...\n</code></pre> <p>without the <code>-CAfile</code>, <code>openssl</code> will report</p> <pre><code>CONNECTED(00000005)\ndepth=0 \nverify error:num=20:unable to get local issuer certificate\nverify return:1\ndepth=0 \nverify error:num=21:unable to verify the first certificate\nverify return:1\n---\n</code></pre>"},{"location":"prepare-cdot-clusters/#create-client-certificates-for-password-less-login","title":"Create Client Certificates for Password-less Login","text":"<p>Copy the server certificate we created above into the Harvest install directory.</p> <pre><code>cp ca/umeng-aff300-05-06.crt /opt/harvest\ncd /opt/harvest\n</code></pre> <p>Create a self-signed client key and certificate with the same name as the hostname where Harvest is running. It's not required to name the key/cert pair after the hostname, but if you do, Harvest will load them automatically when you specify <code>auth_style: certificate_auth</code> otherwise you can point to them directly. See Pollers for details.</p> <p>Change the common name to the ONTAP user you set up with the harvest role above. e.g <code>harvest2</code></p> <pre><code>cd /opt/harvest\nmkdir cert\nopenssl req -x509 -nodes -days 1095 -newkey rsa:2048 -keyout cert/$(hostname).key -out cert/$(hostname).pem -subj \"/CN=harvest2\"\n</code></pre>"},{"location":"prepare-cdot-clusters/#install-client-certificates-on-cluster","title":"Install Client Certificates on Cluster","text":"<p>Login to your cluster with admin credentials and install the client certificate. Copy from <code>cert/$(hostname).pem</code></p> <pre><code>ssh admin@IP\numeng-aff300-05-06::*&gt;  security certificate install -type client-ca -vserver umeng-aff300-05-06\n\nPlease enter Certificate: Press &lt;Enter&gt; when done\n-----BEGIN CERTIFICATE-----\n...\n-----END CERTIFICATE-----\n\nYou should keep a copy of the CA-signed digital certificate for future reference.\n\nThe installed certificate's CA and serial number for reference:\nCA: cbg\nSerial: B77B59444444CCCC\n\nThe certificate's generated name for reference: cbg_B77B59444444CCCC\n</code></pre> <p>Now that the client certificate is installed, let's enable it.</p> <pre><code>umeng-aff300-05-06::*&gt; ssl modify -vserver umeng-aff300-05-06 -client-enabled true\n  (security ssl modify)\n</code></pre> <p>Verify with a recent version of <code>curl</code>. If you are running on a Mac see below.</p> <pre><code>curl --cacert umeng-aff300-05-06.crt --key cert/$(hostname).key --cert cert/$(hostname).pem https://umeng-aff300-05-06-cm.rtp.openenglab.netapp.com/api/storage/disks\n</code></pre>"},{"location":"prepare-cdot-clusters/#update-harvestyml-to-use-client-certificates","title":"Update Harvest.yml to use client certificates","text":"<p>Update the poller section with <code>auth_style: certificate_auth</code> like this:</p> <pre><code>  u2-cert: \n    auth_style: certificate_auth\n    addr: umeng-aff300-05-06-cm.rtp.openenglab.netapp.com\n</code></pre> <p>Restart your poller and enjoy your password-less life-style.</p>"},{"location":"prepare-cdot-clusters/#macos","title":"macOS","text":"<p>The version of <code>curl</code> installed on macOS up through Monterey is not recent enough to work with self-signed SAN certs. You will need to install a newer version of <code>curl</code> via Homebrew, MacPorts, source, etc.</p> <p>Example of failure when running with an older version of <code>curl</code> - you will see this in client auth test step above.</p> <pre><code>curl --version\ncurl 7.64.1 (x86_64-apple-darwin20.0) libcurl/7.64.1 (SecureTransport) LibreSSL/2.8.3 zlib/1.2.11 nghttp2/1.41.0\n\ncurl --cacert umeng-aff300-05-06.crt --key cert/cgrindst-mac-0.key --cert cert/cgrindst-mac-0.pem https://umeng-aff300-05-06-cm.rtp.openenglab.netapp.com/api/storage/disks\n\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\n</code></pre> <p>Let's install <code>curl</code> via Homebrew. Make sure you don't miss the message that Homebrew prints about your path.</p> <pre><code>If you need to have curl first in your PATH, run:\n  echo 'export PATH=\"/usr/local/opt/curl/bin:$PATH\"' &gt;&gt; /Users/cgrindst/.bash_profile\n</code></pre> <p>Now when we make a client auth request with our self-signed certificate, it works! <code>\\o/</code></p> <pre><code>brew install curl\n\ncurl --version\ncurl 7.80.0 (x86_64-apple-darwin20.6.0) libcurl/7.80.0 (SecureTransport) OpenSSL/1.1.1l zlib/1.2.11 brotli/1.0.9 zstd/1.5.0 libidn2/2.3.2 libssh2/1.10.0 nghttp2/1.46.0 librtmp/2.3 OpenLDAP/2.6.0\nRelease-Date: 2021-11-10\nProtocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtmp rtsp scp sftp smb smbs smtp smtps telnet tftp \nFeatures: alt-svc AsynchDNS brotli GSS-API HSTS HTTP2 HTTPS-proxy IDN IPv6 Kerberos Largefile libz MultiSSL NTLM NTLM_WB SPNEGO SSL TLS-SRP UnixSockets zstd\n\ncurl --cacert umeng-aff300-05-06.crt --key cert/cgrindst-mac-0.key --cert cert/cgrindst-mac-0.pem https://umeng-aff300-05-06-cm.rtp.openenglab.netapp.com/api/storage/disks\n\n{\n  \"records\": [\n    {\n      \"name\": \"1.1.22\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/storage/disks/1.1.22\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>Change directory to your Harvest home directory (replace <code>/opt/harvest/</code> if this is not the default):</p> <pre><code>$ cd /opt/harvest/\n</code></pre> <p>Generate an SSL cert and key pair with the following command. Note that it's preferred to generate these files using the hostname of the local machine. The command below assumes <code>debian8</code> as our hostname name and <code>harvest2</code> as the user we created in the previous step:</p> <pre><code>openssl req -x509 -nodes -days 1095 -newkey rsa:2048 -keyout cert/debian8.key \\\n -out cert/debian8.pem  -subj \"/CN=harvest2\"\n</code></pre> <p>Next, open the public key (<code>debian8.pem</code> in our example) and copy all of its content. Login into your ONTAP CLI and run this command by replacing CLUSTER with the name of your cluster.</p> <pre><code>security certificate install -type client-ca -vserver CLUSTER\n</code></pre> <p>Paste the public key content and hit enter. Output should be similar to this:</p> <pre><code>jamaica::&gt; security certificate install -type client-ca -vserver jamaica \n\nPlease enter Certificate: Press &lt;Enter&gt; when done\n-----BEGIN CERTIFICATE-----                       \nMIIDETCCAfmgAwIBAgIUP9EUXyl2BDSUOkNEcDU0yqbJ29IwDQYJKoZIhvcNAQEL\nBQAwGDEWMBQGA1UEAwwNaGFydmVzdDItY2xpMzAeFw0yMDEwMDkxMjA0MDhaFw0y\nMzEwMDktcGFueSBMdGQxFzAVBgNVBAMlc3QyLWNsaTMwggEiMA0tcGFueSBGCSqG\nSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCVVy25BeCRoGCJWFOlyUL7Ddkze4Hl2/6u\nqye/3mk5vBNsGuXUrtad5XfBB70Ez9hWl5sraLiY68ro6MyX1icjiUTeaYDvS/76\nIw7HeXJ5Pyb/fWth1nePunytoLyG/vaTCySINkIV5nlxC+k0X3wWFJdfJzhloPtt\n1Vdm7aCF2q6a2oZRnUEBGQb6t5KyF0/Xh65mvfgB0pl/AS2HY5Gz+~L54Xyvs+BY\nV7UmTop7WBYl0L3QXLieERpHXnyOXmtwlm1vG5g4n/0DVBNTBXjEdvc6oRh8sxBN\nZlQWRApE7pa/I1bLD7G2AiS4UcPmR4cEpPRVEsOFOaAN3Z3YskvnAgMBAAGjUzBR\nMB0GA1UdDgQWBBQr4syV6TCcgO/5EcU/F8L2YYF15jAfBgNVHSMEGDAWgBQr4syV\n6TCcgO/5EcU/F8L2YYF15jAPBgNVHRMdfdfwerH/MA0GCSqGSIb^ECd3DQEBCwUA\nA4IBAQBjP1BVhClRKkO/M3zlWa2L9Ztce6SuGwSnm6Ebmbs+iMc7o2N9p3RmV6Xl\nh6NcdXRzzPAVrUoK8ewhnBzdghgIPoCI6inAf1CUhcCX2xcnE/osO+CfvKuFnPYE\nWQ7UNLsdfka0a9kTK13r3GMs09z/VsDs0gD8UhPjoeO7LQhdU9tJ/qOaSP3s48pv\nsYzZurHUgKmVOaOE4t9DAdevSECEWCETRETA$Vbn%@@@%%rcdrctru65ryFaByb+\nhTtGhDnoHwzt/cAGvLGV/RyWdGFAbu7Fb1rV94ceggE7nh1FqbdLH9siot6LlnQN\nMhEWp5PYgndOW49dDYUxoauCCkiA\n-----END CERTIFICATE-----\n\n\nYou should keep a copy of the CA-signed digital certificate for future reference.\n\nThe installed certificate's CA and serial number for reference:\nCA: harvest2\nSerial: 3FD1145F2976043012213d3009095534CCRDBD2\n\nThe certificate's generated name for reference: harvest2\n</code></pre> <p>Finally, we need to enable SSL authentication with the following command (replace CLUSTER with the name of your cluster):</p> <pre><code>security ssl modify -client-enabled true -vserver CLUSTER\n</code></pre>"},{"location":"prepare-cisco-switch/","title":"Cisco Switches","text":""},{"location":"prepare-cisco-switch/#prepare-cisco-switch","title":"Prepare Cisco switch","text":"<p>NetApp Harvest requires login credentials to access Cisco switches. Although, a generic admin account can be used, it is better to create a dedicated monitoring user with read-only permissions.</p> <p>If you want to create a dedicated monitoring user for Harvest, follow the steps below.</p> <ol> <li>ssh into the switch with a user than can create new users. e.g. <code>ssh admin@switch-ip</code></li> <li>Create a new user with read-only permissions by running the following commands. Replace password with a strong password.</li> </ol> <pre><code>configure terminal\nusername ro_user role network-operator password Netapp123\nexit\n</code></pre>"},{"location":"prepare-cisco-switch/#enable-nx-api-on-cisco-switch","title":"Enable NX-API on Cisco switch","text":"<p>NetApp Harvest uses NX-API to collect metrics from Cisco switches. You need to enable NX-API on the switch, follow the steps below.</p> <ol> <li>ssh into the switch with a user than can enable NX-API. e.g. <code>ssh admin@switch-ip</code></li> <li>Enable NX-API by running the following commands:</li> </ol> <pre><code>configure terminal\nfeature nxapi\nexit\n</code></pre>"},{"location":"prepare-cisco-switch/#reference","title":"Reference","text":"<p>See Configuring User Accounts and RBAC for more information on Cisco NX-OS user accounts and RBAC.</p> <p>See NX-OS Programmability Guide for more information on the Cisco NX-API.</p>"},{"location":"prepare-eseries/","title":"Prepare E-Series Systems","text":"<p>Beta Feature</p> <p>E-Series monitoring support is new in Harvest and should be considered beta. Feedback and bug reports are welcome on GitHub Discussions.</p> <p>NetApp E-Series storage systems with REST API support can be monitored directly by Harvest. This guide covers the setup required to prepare your E-Series array.</p>"},{"location":"prepare-eseries/#monitor-role-requirements","title":"Monitor Role Requirements","text":"<p>For Harvest monitoring, use a user with the Monitor role. Official Documentation: View Local User Roles</p> <p>For E-Series configuration in Harvest, see Configure E-Series.</p>"},{"location":"prepare-fsx-clusters/","title":"Amazon FSx for ONTAP","text":""},{"location":"prepare-fsx-clusters/#prepare-amazon-fsx-for-ontap","title":"Prepare Amazon FSx for ONTAP","text":"<p>To set up Harvest and FSx make sure you read through  Monitoring FSx for ONTAP file systems using Harvest and Grafana</p>"},{"location":"prepare-fsx-clusters/#supported-harvest-dashboards","title":"Supported Harvest Dashboards","text":"<p>Amazon FSx for ONTAP exposes a different set of metrics than ONTAP cDOT. That means a limited set of out-of-the-box dashboards are supported and some panels may be missing information.</p> <p>You can also enable the KeyPerf collector for FSx systems to collect performance metrics that are not available via the <code>ZapiPerf/RestPerf</code> collector.</p> <p>The dashboards that work with FSx are tagged with <code>fsx</code> and listed below:</p> <ul> <li>ONTAP: cDOT</li> <li>ONTAP: Cluster</li> <li>ONTAP: Data Protection</li> <li>ONTAP: Datacenter</li> <li>ONTAP: FlexCache</li> <li>ONTAP: FlexGroup</li> <li>ONTAP: FPolicy</li> <li>ONTAP: LUN</li> <li>ONTAP: NFS Troubleshooting</li> <li>ONTAP: Quota</li> <li>ONTAP: Security</li> <li>ONTAP: SVM</li> <li>ONTAP: Volume</li> <li>ONTAP: Volume by SVM</li> <li>ONTAP: Volume Deep Dive</li> </ul>"},{"location":"prepare-storagegrid-clusters/","title":"StorageGRID","text":""},{"location":"prepare-storagegrid-clusters/#prepare-storagegrid-cluster","title":"Prepare StorageGRID cluster","text":"<p>NetApp Harvest requires login credentials to access StorageGRID hosts. Although, a generic admin account can be used, it is better to create a dedicated monitoring user with the fewest permissions.</p> <p>Here's a summary of what we're going to do</p> <ol> <li>Create a StorageGRID group with the necessary capabilities that Harvest will use to auth and collect data</li> <li>Create a user assigned to the group created in step #1.</li> </ol>"},{"location":"prepare-storagegrid-clusters/#create-storagegrid-group-permissions","title":"Create StorageGRID group permissions","text":"<p>These steps are documented here.</p> <p>You will need a root or admin account to create a new group permission.</p> <ol> <li>Select CONFIGURATION &gt; Access control &gt; Admin groups</li> <li>Select <code>Create group</code></li> <li>Select <code>Local group</code></li> <li>Enter a display name for the group, which you can update later as required. For example, <code>Harvest</code> or <code>monitoring</code>.</li> <li>Enter a unique name for the group, which you cannot update later.</li> <li>Select <code>Continue</code></li> <li>On the <code>Manage group permissions</code> screen, select the permissions you want. At a minimum, Harvest requires    the <code>Tenant accounts</code> and <code>Metrics query</code> permissions.</li> <li>Select <code>Save changes</code></li> </ol> <p></p>"},{"location":"prepare-storagegrid-clusters/#create-a-storagegrid-user","title":"Create a StorageGRID user","text":"<p>These steps are documented here.</p> <p>You will need a root or admin account to create a new user.</p> <ol> <li>Select CONFIGURATION &gt; Access control &gt; Admin users</li> <li>Select <code>Create user</code></li> <li>Enter the user\u2019s full name, a unique username, and a password.</li> <li>Select <code>Continue</code>.</li> <li>Assign the user to the previously created <code>harvest</code> group.</li> <li>Select Create user and select Finish.</li> </ol> <p></p>"},{"location":"prepare-storagegrid-clusters/#reference","title":"Reference","text":"<p>See group permissions for more information on StorageGRID permissions.</p>"},{"location":"prometheus-exporter/","title":"Prometheus Exporter","text":"Prometheus Install <p>The information below describes how to setup Harvest's Prometheus exporter.  If you need help installing or setting up Prometheus, check  out their documentation.</p>"},{"location":"prometheus-exporter/#overview","title":"Overview","text":"<p>The Prometheus exporter is responsible for:</p> <ul> <li>formatting metrics into the Prometheus line protocol</li> <li>creating a web-endpoint on <code>http://&lt;ADDR&gt;:&lt;PORT&gt;/metrics</code> (or <code>https:</code> if TLS is enabled) for Prometheus to scrape</li> </ul> <p>A web end-point is required because Prometheus scrapes Harvest by polling that end-point.</p> <p>In addition to the <code>/metrics</code> end-point, the Prometheus exporter also serves an overview of all metrics and collectors available on its root address <code>scheme://&lt;ADDR&gt;:&lt;PORT&gt;/</code>.</p> <p>Because Prometheus polls Harvest, remember to update your Prometheus configuration and tell Prometheus how to scrape each poller.</p>"},{"location":"prometheus-exporter/#how-should-i-configure-the-prometheus-exporter","title":"How should I configure the Prometheus exporter?","text":"<p>There are several ways to configure the Prometheus exporter with various trade-offs outlined below:</p> <ul> <li>a per-poller prom_port in the <code>Pollers</code> section</li> <li>a port range in the <code>Exporters</code> section</li> <li>a single port in the <code>Exporters</code> section</li> <li>embedded exporters in the <code>Pollers</code> section</li> </ul> <p>We recommend the first two options, using a per-poller <code>prom_port</code> or <code>port_range</code>, since they work for the majority of  use cases and are the easiest to manage.</p> <p>Use <code>prom_port</code> when you want the most control over the Prometheus port with the least amount of management. Use <code>port_range</code> when you want Harvest to manage the port numbers for you, but be aware that the port numbers depend on the order of pollers in your <code>harvest.yml</code>. You need to keep that order consistent otherwise it will appear that you have lost data.  </p> Name Pros Cons Notes prom_port Precisely control each Poller's Prometheus exporter port. The port is defined in one place, beside each poller. You have to manage which port each poller should use. Start with this until you outgrow it. Many folks never do. port_range Less to manage since Harvest assigns the port numbers for you based on the order of the pollers in your <code>harvest.yml</code> You need to be mindful of the order of pollers in your <code>harvest.yml</code> file and be careful about changing that order when adding/removing pollers. Reordering will cause the Prometheus port to change. Since Prometheus includes the port in the <code>instance</code> label, changing the port causes Prometheus to treat metrics with different ports as different instances. That means it will appear that you have lost data because the metrics with the new port are distinct from the metrics with the older port. See #2782 for details. Less to manage, but makes sure you understand how to control the order of your pollers. port in Exporters Precisely control each Poller's Prometheus exporter port.Can define multiple Prometheus exporters, each with custom configuration. Similar to <code>prom_port</code> but with an unnecessary level of indirection that makes you repeat yourself. Exporter that Harvest always shipped with. Most folks should use <code>prom_port</code> unless they need to configure many instances of the Prometheus exporter, which is rare. embedded exporter All the pros of <code>port in Exporters</code> but without the unnecessary indirection Removes the level of indirection and allows you to define the exporter in one place, but more verbose than per-poller <code>prom_port</code>. Most folks should use <code>prom_port</code> unless they need to configure many instances of the Prometheus exporter, which is rare."},{"location":"prometheus-exporter/#parameters","title":"Parameters","text":"<p>All parameters of the exporter are defined in the <code>Exporters</code> section of <code>harvest.yml</code>.</p> <p>An overview of all parameters:</p> parameter type description default <code>add_meta_tags</code> bool, optional add <code>HELP</code> and <code>TYPE</code> metatags to metrics (currently no useful information, but required by some tools) <code>false</code> <code>allow_addrs</code> list of strings, optional allow access only if host matches any of the provided addresses <code>allow_addrs_regex</code> list of strings, optional allow access only if host address matches at least one of the regular expressions <code>cache_max_keep</code> string (Go duration format), optional maximum amount of time metrics are cached (in case Prometheus does not timely collect the metrics) <code>5m</code> <code>disk_cache</code> object, optional disk-based cache configuration <code>global_prefix</code> string, optional add a prefix to all metrics (e.g. <code>netapp_</code>) <code>local_http_addr</code> string, optional address of the HTTP server Harvest starts for Prometheus to scrape:use <code>localhost</code> to serve only on the local machineuse <code>0.0.0.0</code> (default) if Prometheus is scrapping from another machine <code>0.0.0.0</code> <code>port_range</code> int-int (range), overrides <code>port</code> if specified lower port to upper port (inclusive) of the HTTP end-point to create when a poller specifies this exporter. Starting at lower port, each free port will be tried sequentially up to the upper port. <code>port</code> int, required if port_range is not specified port of the HTTP end-point <code>sort_labels</code> bool, optional sort metric labels before exporting. VictoriaMetrics requires this otherwise stale metrics are reported. <code>false</code> <code>tls</code> <code>tls</code> optional If present, enables TLS transport. If running in a container, see note tls <code>cert_file</code>, <code>key_file</code> required child of <code>tls</code> Relative or absolute path to TLS certificate and key file. TLS 1.3 certificates required.FIPS complaint P-256 TLS 1.3 certificates can be created with <code>bin/harvest admin tls create server</code>, <code>openssl</code>, <code>mkcert</code>, etc."},{"location":"prometheus-exporter/#per-poller-prom_port","title":"Per-poller prom_port","text":"<p>Define a Prometheus exporter in the <code>Exporters</code> section of your <code>harvest.yml</code> file, use that exporter in <code>Defaults</code>,  and then each poller lists its <code>prom_port</code> in the <code>Pollers</code> section.</p> <pre><code>Exporters:\n  my_prom:\n    exporter: Prometheus\n    add_meta_tags: true\n    sort_labels: true\n\nDefaults:\n  auth_style: basic_auth\n  username: harvest\n  password: pass\n  exporters:\n    - my_prom\n</code></pre> <p>Then update your pollers in the <code>Pollers</code> section of your <code>harvest.yml</code> file.</p> <pre><code>Pollers:\n  cluster-01:\n    addr: 10.0.1.1\n    prom_port: 12990\n  cluster-02:\n    addr: 10.0.1.2\n    prom_port: 12991\n</code></pre>"},{"location":"prometheus-exporter/#port-range","title":"Port Range","text":"<p>Port range works by defining a range of ports in the <code>Exporters</code> section of your <code>harvest.yml</code> file. Harvest will assign the first available port in the range to each poller that uses the exporter. That means you need to be careful about the order of your pollers in the <code>harvest.yml</code> file.</p> <p>If you add or remove pollers, the order of the pollers may change, and the port assigned to each poller may change. To mitigate this:</p> <ul> <li>when you add new pollers to the <code>harvest.yml</code> file, add them to the end of the <code>Pollers</code> section of your <code>harvest.yml</code> file.</li> <li>when you want to remove pollers, instead of deleting them, add the <code>disabled: true</code> parameter to the poller. The poller will not be started, but the port will be reserved. That way, the order of later pollers won't change. </li> </ul> <pre><code>Exporters:\n  prom-prod:\n    exporter: Prometheus\n    port_range: 2000-2030\nDefaults:\n  exporters:\n    - prom-prod    \nPollers:\n  cluster-01:\n    addr: 10.0.1.1\n    disabled: true  # This poller will not be used\n  cluster-02:\n    addr: 10.0.1.2\n  cluster-03:\n    addr: 10.0.1.3\n  # ... more\n  cluster-16:\n    addr: 10.0.1.16\n</code></pre> <p>In the example above, fifteen pollers will collect metrics from 15 clusters and make those metrics available to a single instance of Prometheus named <code>prom-prod</code>. Fifteen web end-points will be created on the available free ports between 2000 and 2030 (inclusive). Port 2000 will be assigned to <code>cluster-01</code>, port 2001 to <code>cluster-02</code>, and so on.</p> <p>After starting the pollers in the example above, running <code>bin/harvest status</code> shows the following. Since <code>cluster-01</code> is disabled, it won't be started and its port will be skipped. If no free port can be found, an error will be logged.</p> <pre><code>  Datacenter  |    Poller    | PID  | PromPort |  Status     \n--------------+--------------+------+----------+----------\n  dc-01       | cluster-01   | 2339 |          | disabled  \n  dc-01       | cluster-02   | 2343 |  2001    | running  \n  dc-01       | cluster-03   | 2351 |  2002    | running  \n...\n  dc-01       | cluster-14   | 2405 |  2013    | running  \n  dc-01       | cluster-15   | 2502 |  2014    | running  \n  dc-01       | cluster-16   | 2514 |  2015    | running  \n</code></pre>"},{"location":"prometheus-exporter/#single-port-exporter","title":"Single Port Exporter","text":"<p>Define a Prometheus exporter in the <code>Exporters</code> section of your <code>harvest.yml</code> file. Give that exporter a <code>port</code> and update a single poller to use this exporter. Each poller requires a different Prometheus exporter.</p> <pre><code>Exporters:\n  my_prom:\n    exporter: Prometheus\n    add_meta_tags: true\n    sort_labels: true\n    port: 12990\n</code></pre> <p>Then update a single poller in the <code>Pollers</code> section of your <code>harvest.yml</code> file to reference the Prometheus exporter.</p> <pre><code>Pollers:\n  cluster-01:\n    addr: 10.0.1.1\n    exporters:\n      - my_prom\n</code></pre>"},{"location":"prometheus-exporter/#embedded-exporter","title":"Embedded Exporter","text":"<p>This example is similar to the single port exporter example, but the exporter is defined in the <code>Pollers</code> section. No need to define the exporter in the <code>Exporters</code> section.</p> <pre><code>Pollers:\n  cluster-01:\n    addr: 10.0.1.1\n    exporters:\n      - exporter: Prometheus\n        add_meta_tags: true\n        sort_labels: true\n        port: 12990\n</code></pre>"},{"location":"prometheus-exporter/#allow_addrs","title":"allow_addrs","text":"<pre><code>Exporters:\n  my_prom:\n    allow_addrs:\n      - 192.168.0.102\n      - 192.168.0.103\n</code></pre> <p>Access will only be allowed from these two addresses.</p>"},{"location":"prometheus-exporter/#allow_addrs_regex","title":"allow_addrs_regex","text":"<pre><code>Exporters:\n  my_prom:\n    allow_addrs_regex:\n      - `^192.168.0.\\d+$`\n</code></pre> <p>Access will only be allowed from the IP4 range <code>192.168.0.0</code>-<code>192.168.0.255</code>.</p>"},{"location":"prometheus-exporter/#disk_cache","title":"disk_cache","text":"<p>The <code>disk_cache</code> parameter enables disk-based staging of metrics before they are served to Prometheus. Instead of storing formatted metrics in memory, Harvest flushes them to disk files. When Prometheus scrapes the <code>/metrics</code> endpoint, Harvest reads these cached files from disk and streams them directly to Prometheus. This approach reduces memory overhead, making it ideal for large deployments with many metrics.</p> <p>Configuration:</p> <p>The <code>disk_cache</code> parameter requires a <code>path</code> field that specifies the directory where cache files will be stored. The path is mandatory when using disk cache.</p> <p>Notes:</p> <ul> <li>The <code>path</code> is required when using <code>disk_cache</code></li> <li>Harvest will automatically create a subdirectory for each poller to avoid conflicts between multiple pollers</li> <li>The cache directory is cleared on startup</li> <li>Ensure the specified directory is writable by the Harvest process</li> </ul> <p>Example:</p> <pre><code>Exporters:\n  prom_disk:\n    exporter: Prometheus\n    port_range: 13000-13100\n    disk_cache:\n      path: /var/lib/harvest/cache\n\nPollers:\n  cluster-01:\n    addr: 10.0.1.1\n    exporters:\n      - prom_disk\n  cluster-02:\n    addr: 10.0.1.2\n    exporters:\n      - prom_disk\n</code></pre> <p>In this example, cache files will be created in: - <code>/var/lib/harvest/cache/cluster-01/</code> - <code>/var/lib/harvest/cache/cluster-02/</code></p>"},{"location":"prometheus-exporter/#configure-prometheus-to-scrape-harvest-pollers","title":"Configure Prometheus to scrape Harvest pollers","text":"<p>There are two ways to tell Prometheus how to scrape Harvest: using HTTP service discovery (SD) or listing each poller individually.</p> <p>HTTP service discovery is the more flexible of the two. It is also less error-prone, and easier to manage. Combined with the port_range configuration described above, SD is the least effort to configure Prometheus and the easiest way to keep both Harvest and Prometheus in sync.</p> <p>NOTE HTTP service discovery does not work with Docker yet. With Docker, you will need to list each poller individually or if possible, use the Docker Compose workflow that uses file service discovery to achieve a similar ease-of-use as HTTP service discovery.</p> <p>See the example below for how to use HTTP SD and port_range together.</p>"},{"location":"prometheus-exporter/#prometheus-http-service-discovery","title":"Prometheus HTTP Service Discovery","text":"<p>HTTP service discovery was introduced in Prometheus version 2.28.0. Make sure you're using that version or later.</p> <p>The way service discovery works is:</p> <ul> <li>shortly after a poller starts up, it registers with the SD node (if one exists)</li> <li>the poller sends a heartbeat to the SD node, by default every 45s.</li> <li>if a poller fails to send a heartbeat, the SD node removes the poller from the list of active targets after a minute</li> <li>the SD end-point is reachable via SCHEMA:///api/v1/sd <p>To use HTTP service discovery you need to:</p> <ol> <li>tell Harvest to start the HTTP service discovery process</li> <li>tell Prometheus to use the HTTP service discovery endpoint</li> </ol>"},{"location":"prometheus-exporter/#enable-http-service-discovery-in-harvest","title":"Enable HTTP service discovery in Harvest","text":"<p>Add the following to your <code>harvest.yml</code></p> <pre><code>Admin:\n  httpsd:\n    listen: :8887\n</code></pre> <p>This tells Harvest to create an HTTP service discovery end-point on interface <code>0.0.0.0:8887</code>. If you want to only listen on localhost, use <code>127.0.0.1:&lt;port&gt;</code> instead. See net.Dial for details on the supported listen formats.</p> <p>Start the SD process by running <code>bin/harvest admin start</code>. Once it is started, you can curl the end-point for the list of running Harvest pollers.</p> <pre><code>curl -s 'http://localhost:8887/api/v1/sd' | jq .\n[\n  {\n    \"targets\": [\n      \"10.0.1.55:12990\",\n      \"10.0.1.55:15037\",\n      \"127.0.0.1:15511\",\n      \"127.0.0.1:15008\",\n      \"127.0.0.1:15191\",\n      \"10.0.1.55:15343\"\n    ]\n  }\n]\n</code></pre>"},{"location":"prometheus-exporter/#harvest-http-service-discovery-options","title":"Harvest HTTP Service Discovery options","text":"<p>HTTP service discovery (SD) is configured in the <code>Admin &gt; httpsd</code> section of your <code>harvest.yml</code>.</p> parameter type description default <code>listen</code> required Interface and port to listen on, use localhost:PORT or :PORT for all interfaces <code>auth_basic</code> optional If present, enables basic authentication on <code>/api/v1/sd</code> end-point auth_basic <code>username</code>, <code>password</code> required child of <code>auth_basic</code> <code>tls</code> optional If present, enables TLS transport. If running in a container, see note tls <code>cert_file</code>, <code>key_file</code> required child of <code>tls</code> Relative or absolute path to TLS certificate and key file. TLS 1.3 certificates required.FIPS complaint P-256 TLS 1.3 certificates can be created with <code>bin/harvest admin tls create server</code> <code>ssl_cert</code>, <code>ssl_key</code> optional if <code>auth_style</code> is <code>certificate_auth</code> Absolute paths to SSL (client) certificate and key used to authenticate with the target system.If not provided, the poller will look for <code>&lt;hostname&gt;.key</code> and <code>&lt;hostname&gt;.pem</code> in <code>$HARVEST_HOME/cert/</code>.To create certificates for ONTAP systems, see using certificate authentication <code>heart_beat</code> optional, Go Duration format How frequently each poller sends a heartbeat message to the SD node 45s <code>expire_after</code> optional, Go Duration format If a poller fails to send a heartbeat, the SD node removes the poller after this duration 1m"},{"location":"prometheus-exporter/#enable-http-service-discovery-in-prometheus","title":"Enable HTTP service discovery in Prometheus","text":"<p>Edit your <code>prometheus.yml</code> and add the following section</p> <p><code>$ vim /etc/prometheus/prometheus.yml</code></p> <pre><code>scrape_configs:\n  - job_name: harvest\n    http_sd_configs:\n      - url: http://localhost:8887/api/v1/sd\n</code></pre> <p>Harvest and Prometheus both support basic authentication for HTTP SD end-points. To enable basic auth, add the following to your Harvest config.</p> <pre><code>Admin:\n  httpsd:\n    listen: :8887\n    # Basic auth protects GETs and publishes\n    auth_basic:\n      username: admin\n      password: admin\n</code></pre> <p>Don't forget to also update your Prometheus config with the matching basic_auth credentials.</p>"},{"location":"prometheus-exporter/#prometheus-http-service-discovery-and-port-range","title":"Prometheus HTTP Service Discovery and Port Range","text":"<p>HTTP SD combined with Harvest's <code>port_range</code> feature leads to significantly less configuration in your <code>harvest.yml</code>. For example, if your clusters all export to the same Prometheus instance, you can refactor the per-poller exporter into a single exporter shared by all clusters in <code>Defaults</code> as shown below:</p> <p>Notice that none of the pollers specify an exporter. Instead, all the pollers share the single exporter named <code>prometheus-r</code> listed in <code>Defaults</code>. <code>prometheus-r</code> is the only exporter defined and as specified will manage up to 1,000 Harvest Prometheus exporters.</p> <p>If you add or remove more clusters in the <code>Pollers</code> section, you do not have to change Prometheus since it dynamically pulls the targets from the Harvest admin node.</p> <pre><code>Admin:\n  httpsd:\n    listen: :8887\n\nExporters:\n  prometheus-r:\n    exporter: Prometheus\n    port_range: 13000-13999\n\nDefaults:\n  collectors:\n    - Zapi\n    - ZapiPerf\n  use_insecure_tls: false\n  auth_style: password\n  username: admin\n  password: pass\n  exporters:\n    - prometheus-r\n\nPollers:\n  umeng_aff300:\n    datacenter: meg\n    addr: 10.193.48.11\n\n  F2240-127-26:\n    datacenter: meg\n    addr: 10.193.6.61\n\n  # ... add more clusters\n</code></pre>"},{"location":"prometheus-exporter/#static-scrape-targets","title":"Static Scrape Targets","text":"<p>If we define two Prometheus exporters at ports: 12990 and 14567 in the <code>harvest.yml</code> file like so, you need to add two targets to your <code>prometheus.yml</code> too.</p> <pre><code>$ vim harvest.yml\n</code></pre> <pre><code>Exporters:\n  prometheus1:\n    exporter: Prometheus\n    port: 12990\n  prometheus2:\n    exporter: Prometheus\n    port: 14567\n\nPollers:\n  cluster1:\n    addr: 10.0.1.1\n    username: user\n    password: pass\n    exporters:\n      - prometheus1\n  cluster2:\n      addr: 10.0.1.1\n      username: user\n      password: pass\n      exporters:\n        - prometheus2\n</code></pre> <pre><code>$ vim /etc/prometheus/prometheus.yml\n</code></pre> <p>Scroll down to near the end of the file and add the following lines:</p> <pre><code>  - job_name: 'harvest'\n    scrape_interval: 60s\n    static_configs:\n      - targets:\n          - 'localhost:12990'\n          - 'localhost:14567'\n</code></pre> <p>NOTE If Prometheus is not on the same machine as Harvest, then replace <code>localhost</code> with the IP address of your Harvest machine. Also note the scrape interval above is set to 1m. That matches the polling frequency of the default Harvest collectors. If you change the polling frequency of a Harvest collector to a lower value, you should also change the scrape interval.</p>"},{"location":"prometheus-exporter/#prometheus-exporter-and-tls","title":"Prometheus Exporter and TLS","text":"<p>The Harvest Prometheus exporter can be configured to serve its metrics via <code>HTTPS</code> by configuring the <code>tls</code> section in the <code>Exporters</code> section of <code>harvest.yml</code>.</p> <p>Let's walk through an example of how to set up Harvest's Prometheus exporter and how to configure Prometheus to use TLS.</p>"},{"location":"prometheus-exporter/#generate-tls-certificates","title":"Generate TLS Certificates","text":"<p>We'll use Harvest's admin command line tool to create a self-signed TLS certificate key/pair for the exporter and Prometheus. Note: If running in a container, see note.</p> <pre><code>cd $Harvest_Install_Directory\nbin/harvest admin tls create server\n2023/06/23 09:39:48 wrote cert/admin-cert.pem\n2023/06/23 09:39:48 wrote cert/admin-key.pem\n</code></pre> <p>Two files are created. Since we want to use these certificates for our Prometheus exporter, let's rename them to make that clearer.</p> <pre><code>mv cert/admin-cert.pem cert/prom-cert.pem\nmv cert/admin-key.pem cert/prom-key.pem\n</code></pre>"},{"location":"prometheus-exporter/#configure-harvest-prometheus-exporter-to-use-tls","title":"Configure Harvest Prometheus Exporter to use TLS","text":"<p>Edit your <code>harvest.yml</code> and add a TLS section to your exporter block like this:</p> <pre><code>Exporters:\n  my-exporter:\n    local_http_addr: localhost\n    exporter: Prometheus\n    port: 16001\n    tls:\n      cert_file: cert/prom-cert.pem\n      key_file: cert/prom-key.pem\n</code></pre> <p>Update one of your Pollers to use this exporter and start the poller.</p> <pre><code>Pollers:\n  my-cluster:\n    datacenter: dc-1\n    addr: 10.193.48.11\n    exporters:\n      - my-exporter     # Use TLS exporter we created above\n</code></pre> <p>When the poller is started, it will log whether <code>https</code> or <code>http</code> is being used as part of the <code>url</code> like so:</p> <pre><code>bin/harvest start -f my-cluster\n2023-06-23T10:02:03-04:00 INF prometheus/httpd.go:40 &gt; server listen Poller=my-cluster exporter=my-exporter url=https://localhost:16001/metrics\n</code></pre> <p>If the <code>url</code> schema is <code>https</code>, TLS is being used.</p> <p>You can use curl to scrape the Prometheus exporter and verify that TLS is being used like so:</p> <pre><code>curl --cacert cert/prom-cert.pem https://localhost:16001/metrics\n\n# or use --insecure to tell curl to skip certificate validation\n# curl --insecure cert/prom-cert.pem https://localhost:16001/metrics  \n</code></pre>"},{"location":"prometheus-exporter/#configure-prometheus-to-use-tls","title":"Configure Prometheus to use TLS","text":"<p>Let's configure Prometheus to use <code>HTTPs</code> to communicate with the exporter setup above.</p> <p>Edit your <code>prometheus.yml</code> and add or adapt your <code>scrape_configs</code> job. You need to add <code>scheme: https</code> and setup a <code>tls_config</code> block to point to the earlier created <code>prom-cert.pem</code> like so:</p> <pre><code>scrape_configs:\n  - job_name: 'harvest-https'\n    scheme: https\n    tls_config:\n      ca_file: /path/to/prom-cert.pem\n    static_configs:\n    - targets:\n        - 'localhost:16001'\n</code></pre> <p>Start Prometheus and visit http://localhost:9090/targets with your browser.  You should see https://localhost:16001/metrics in the list of targets. </p> <p></p>"},{"location":"prometheus-exporter/#prometheus-alerts","title":"Prometheus Alerts","text":"<p>Prometheus includes out-of-the-box support for simple alerting. Alert rules are configured in your <code>prometheus.yml</code> file. Setup and details can be found in the Prometheus guide on alerting.</p> <p>Harvest also includes EMS and sample alerts for reference.  Refer to the EMS Collector for more details about EMS events. Refer to the EMS alert runbook for descriptions and remediation steps.</p>"},{"location":"prometheus-exporter/#alertmanager","title":"Alertmanager","text":"<p>Prometheus's builtin alerts are good for simple workflows. They do a nice job telling you what's happening at the moment. If you need a richer solution that includes summarization, notification, advanced delivery, deduplication, etc. checkout Alertmanager.</p>"},{"location":"prometheus-exporter/#reference","title":"Reference","text":"<ul> <li>Prometheus Alerting</li> <li>Alertmanager</li> <li>Alertmanager's notification metrics</li> <li>Prometheus Linter</li> <li>Collection of example Prometheus Alerts</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Welcome to the NetApp Harvest Getting Started Guide. This tutorial will guide you through the steps required to deploy an instance of NetApp Harvest, Prometheus, and Grafana on a Linux platform to monitor an ONTAP cluster.</p> <p>If you prefer to use the container version of NetApp Harvest, please refer to the container installation guide.</p> <p>This tutorial uses <code>systemd</code> to manage Harvest, Prometheus, and Grafana. If you would rather run the processes directly, feel free to ignore the sections of the tutorial that set up <code>systemd</code> service files.</p>"},{"location":"quickstart/#1-set-installation-path","title":"1. Set Installation Path","text":"<p>First, set the installation path as an environment variable. For example, we'll use <code>/opt/netapp/harvest</code>.</p> <pre><code>HARVEST_INSTALL_PATH=/opt/netapp/harvest\nmkdir -p ${HARVEST_INSTALL_PATH}\n</code></pre>"},{"location":"quickstart/#2-create-harvest-user-and-group","title":"2. Create Harvest User and Group","text":"<p>Create a dedicated user and group for running Harvest services.</p> <pre><code>sudo groupadd harvest\nsudo useradd -r -g harvest harvest\n</code></pre>"},{"location":"quickstart/#3-install-harvest","title":"3. Install Harvest","text":"<p>Harvest is distributed as a container, native tarball, and RPM and Debs. Pick the one that works best for you. More details can be found in the installation documentation.</p> <p>For this guide, we'll use the tarball package as an example.</p> <p>Visit the releases page and take note of the latest release. Update the <code>HARVEST_VERSION</code> environment variable with the latest release in the script below. For example, to download the <code>24.11.0</code> release you would use <code>HARVEST_VERSION=24.11.0</code>.</p> <p>After updating the <code>HARVEST_VERSION</code> environment variable, run the bash script to download Harvest and untar it into your <code>HARVEST_INSTALL_PATH</code> directory.</p> <pre><code>HARVEST_VERSION=24.11.0\ncd ${HARVEST_INSTALL_PATH}\nwget https://github.com/NetApp/harvest/releases/download/v${HARVEST_VERSION}/harvest-${HARVEST_VERSION}-1_linux_amd64.tar.gz\ntar -xvf harvest-${HARVEST_VERSION}-1_linux_amd64.tar.gz\n</code></pre> <p>Change ownership of the files to the <code>harvest</code> user and group:</p> <pre><code>sudo chown -R harvest:harvest ${HARVEST_INSTALL_PATH}\n</code></pre>"},{"location":"quickstart/#4-install-prometheus","title":"4. Install Prometheus","text":"<p>To install Prometheus, follow these steps. For more details see Prometheus installation.</p> <pre><code>PROMETHEUS_VERSION=2.49.1\ncd ${HARVEST_INSTALL_PATH}\nwget https://github.com/prometheus/prometheus/releases/download/v${PROMETHEUS_VERSION}/prometheus-${PROMETHEUS_VERSION}.linux-amd64.tar.gz\ntar -xvf prometheus-${PROMETHEUS_VERSION}.linux-amd64.tar.gz\nmv prometheus-${PROMETHEUS_VERSION}.linux-amd64 prometheus-${PROMETHEUS_VERSION}\n</code></pre> <p>If you want to manage Prometheus with <code>systemd</code>, you can create a service file for Prometheus like so. This step is optional. A service file will attempt to restart Prometheus automatically when the machine is restarted.</p> <p>Create a service file for Prometheus:</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/prometheus.service\n[Unit]\nDescription=Prometheus Server\nDocumentation=https://prometheus.io/docs/introduction/overview/\nAfter=network-online.target\n\n[Service]\nUser=root\nRestart=on-failure\nExecStart=${HARVEST_INSTALL_PATH}/prometheus-${PROMETHEUS_VERSION}/prometheus --config.file=${HARVEST_INSTALL_PATH}/prometheus-${PROMETHEUS_VERSION}/prometheus.yml\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <p>Reload the systemd configuration and start Prometheus:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable prometheus\nsudo systemctl start prometheus\n</code></pre> <p>Check if Prometheus is up and running:</p> <pre><code>sudo systemctl status prometheus\n</code></pre> <p>You should see output indicating that the Prometheus service is active and running.</p> Alternative: Start Prometheus Directly    If you would rather start Prometheus directly and kick the tires before creating a service file, you can run the following command to start Prometheus in the background:    <pre><code>nohup ${HARVEST_INSTALL_PATH}/prometheus-${PROMETHEUS_VERSION}/prometheus --config.file=${HARVEST_INSTALL_PATH}/prometheus-${PROMETHEUS_VERSION}/prometheus.yml &gt; prometheus.log 2&gt;&amp;1 &amp;\n</code></pre>    This command uses <code>nohup</code> to run Prometheus in the background and redirects the output to <code>prometheus.log</code>."},{"location":"quickstart/#5-install-grafana","title":"5. Install Grafana","text":"<p>To install Grafana, follow these steps:</p> <pre><code>GRAFANA_VERSION=10.4.5\ncd ${HARVEST_INSTALL_PATH}\nwget https://dl.grafana.com/oss/release/grafana-${GRAFANA_VERSION}.linux-amd64.tar.gz\ntar -xvf grafana-${GRAFANA_VERSION}.linux-amd64.tar.gz\n</code></pre> <p>If you want to manage Grafana with <code>systemd</code>, you can create a service file for Grafana like so. This step is optional. A service file will attempt to restart Grafana automatically when the machine is restarted.</p> <p>Create a service file for Grafana:</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/grafana.service\n[Unit]\nDescription=Grafana Server\nDocumentation=https://grafana.com/docs/grafana/latest/setup-grafana/installation/\nAfter=network-online.target\n\n[Service]\nUser=root\nRestart=on-failure\nExecStart=${HARVEST_INSTALL_PATH}/grafana-v${GRAFANA_VERSION}/bin/grafana-server --config=${HARVEST_INSTALL_PATH}/grafana-v${GRAFANA_VERSION}/conf/defaults.ini --homepath=${HARVEST_INSTALL_PATH}/grafana-v${GRAFANA_VERSION}\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <p>Reload the systemd configuration and start Grafana:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable grafana\nsudo systemctl start grafana\n</code></pre> <p>Check if Grafana is up and running:</p> <pre><code>sudo systemctl status grafana\n</code></pre> <p>You should see output indicating that the Grafana service is active and running.</p> Alternative: Start Grafana Directly    If you would rather start Grafana directly and kick the tires before creating a service file, you can run the following command to start Grafana in the background:    <pre><code>nohup ${HARVEST_INSTALL_PATH}/grafana-v${GRAFANA_VERSION}/bin/grafana-server --config=${HARVEST_INSTALL_PATH}/grafana-v${GRAFANA_VERSION}/conf/defaults.ini --homepath=${HARVEST_INSTALL_PATH}/grafana-v${GRAFANA_VERSION} &gt; grafana.log 2&gt;&amp;1 &amp;\n</code></pre>    This command uses <code>nohup</code> to run Grafana in the background and redirects the output to <code>grafana.log</code>."},{"location":"quickstart/#6-configuration-file","title":"6. Configuration File","text":"<p>By default, Harvest loads its configuration information from the <code>./harvest.yml</code> file. If you would rather use a different file, use the <code>--config</code> command line argument flag to specify the path to your config file.</p> <p>To start collecting metrics, you need to define at least one <code>poller</code> and one <code>exporter</code> in your configuration file. This is useful if you want to monitor resource usage by Harvest and serves as a good example. Feel free to delete it if you want.</p> <p>The next step is to add pollers for your ONTAP clusters in the Pollers section of the Harvest configuration file, <code>harvest.yml</code>.</p> <p>Edit the Harvest configuration file:</p> <pre><code>cd ${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64\nsudo -u harvest vi harvest.yml\n</code></pre> <p>Copy and paste the following YAML configuration into your editor and update the <code>$cluster-management-ip</code>, <code>$username</code>, and <code>$password</code> sections to match your ONTAP system.</p> <pre><code>Exporters:\n  prometheus1:\n    exporter: Prometheus\n    port_range: 13000-13100\n\nDefaults:\n  collectors:\n    - Zapi\n    - ZapiPerf\n    - Ems\n    - Rest\n    - RestPerf\n  use_insecure_tls: true\n\nPollers:\n  jamaica:\n    datacenter: DC-01\n    addr: $cluster-management-ip\n    auth_style: basic_auth\n    username: $username\n    password: $password\n    exporters:\n      - prometheus1\n</code></pre> <p>Note: The ONTAP user specified in this configuration must have the appropriate permissions as outlined in the Prepare cDot Clusters documentation.</p>"},{"location":"quickstart/#7-edit-prometheus-config-file","title":"7. Edit Prometheus Config File","text":"<p>Edit the Prometheus configuration file:</p> <pre><code>cd ${HARVEST_INSTALL_PATH}/prometheus-${PROMETHEUS_VERSION}\nvi prometheus.yml\n</code></pre> <p>Add the following under the <code>scrape_configs</code> section. The targets you are adding should match the range of ports you specified in your <code>harvest.yml</code> file (in the example above, we use the port_range <code>13000-13100</code>).</p> <pre><code>  - job_name: 'harvest'\n    static_configs:\n      - targets: ['localhost:13000', 'localhost:13001', 'localhost:13002']  # Add ports as defined in the port range\n</code></pre> <p>For example, if your port range in the Harvest configuration is <code>13000-13100</code>, you should add the ports within this range that you plan to use.</p> <p>Restart Prometheus to apply the changes:</p> <pre><code>sudo systemctl restart prometheus\n</code></pre> <p>Check if Prometheus is up and running:</p> <pre><code>sudo systemctl status prometheus\n</code></pre>"},{"location":"quickstart/#8-start-harvest","title":"8. Start Harvest","text":"<p>To start the Harvest pollers, follow these steps. For more details see Harvest service.</p> <p>Create a systemd service file for Harvest pollers:</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/poller@.service\n[Unit]\nDescription=\"NetApp Harvest Poller instance %I\"\nPartOf=harvest.target\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nUser=harvest\nGroup=harvest\nType=simple\nRestart=on-failure\nWorkingDirectory=${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64\nExecStart=${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64/bin/harvest --config ${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64/harvest.yml start -f %i\n\n[Install]\nWantedBy=harvest.target\nEOF\n</code></pre> <p>Create a target file for Harvest:</p> <pre><code>cd ${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64\nbin/harvest generate systemd | sudo tee /etc/systemd/system/harvest.target\n</code></pre> <p>Reload the systemd configuration and start Harvest:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable harvest.target\nsudo systemctl start harvest.target\n</code></pre> <p>Verify that the pollers have started successfully by checking their status:</p> <pre><code>systemctl status \"poller*\"\n</code></pre> Optional: Log to File Instead of journald    By default, the systemd service logs to stdout, which means log messages are captured by journald (and on some systems like RHEL8, also in `/var/log/messages`).    If you want Harvest to log directly to files instead, you can modify the service file to use the `--logtofile` flag and set the `HARVEST_LOGS` environment variable to specify the log directory.    Modify the poller service file like below:    <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/poller@.service\n[Unit]\nDescription=\"NetApp Harvest Poller instance %I\"\nPartOf=harvest.target\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nUser=harvest\nGroup=harvest\nType=simple\nRestart=on-failure\nEnvironment=\"HARVEST_LOGS=/opt/harvest/logs\"\nWorkingDirectory=${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64\nExecStart=${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64/bin/harvest --config ${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64/harvest.yml start --logtofile -f %i\n\n[Install]\nWantedBy=harvest.target\nEOF\n</code></pre>    **Create the log directory**:    <pre><code>sudo mkdir -p /opt/harvest/logs\nsudo chown harvest:harvest /opt/harvest/logs\n</code></pre>    **Reload and restart**:    <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart harvest.target\n</code></pre>    Logs will be written to `/opt/harvest/logs/poller_.log` (e.g., `/opt/harvest/logs/poller_jamaica.log`).   Alternative: Start Harvest Directly    If you would rather start Harvest directly and kick the tires before creating a service file, you can run the following command to start Harvest:    <pre><code>cd ${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64\nbin/harvest start\n</code></pre>    Verify that the pollers have started successfully by checking their status:    <pre><code>bin/harvest status\n</code></pre>    The output should look similar to this:    <pre><code>Datacenter | Poller  |   PID   | PromPort | Status\n-----------+---------+---------+----------+----------\nDC-01      | jamaica | 1280145 |    13000 | running\n</code></pre> <p>The logs of each poller can be found in <code>/var/log/harvest/</code>.</p>"},{"location":"quickstart/#9-add-prometheus-datasource-in-grafana","title":"9. Add Prometheus Datasource in Grafana","text":"<p>To add a Prometheus datasource in Grafana, follow these steps:</p> <ol> <li>Open your web browser and navigate to Grafana (http://localhost:3000). When prompted for credentials, use Grafana defaults admin/admin. You should change the default credentials once you log in.</li> <li>Navigate to the data sources section by visiting http://localhost:3000/connections/datasources or by clicking the hamburger menu (three horizontal lines) at the top-left of the page and navigate to Connections and then Data Sources.</li> <li>Click on Add data source.</li> <li>Select Prometheus from the list of available data sources.</li> <li>In the Prometheus server URL field, enter (http://localhost:9090).</li> <li>Click on Save and test.</li> <li>At the bottom of the page, you should see the message 'Successfully queried the Prometheus API.' For detailed instructions, please refer to the configure Prometheus Data Source documentation.</li> </ol>"},{"location":"quickstart/#10-generate-grafana-api-token","title":"10. Generate Grafana API Token","text":"<p>To import Grafana dashboards using the <code>bin/harvest grafana import</code> command, you need a Grafana API token. Follow these steps to generate it:</p> <ol> <li>Open your web browser and navigate to Grafana (http://localhost:3000). Enter your Grafana credentials to log in. The default username and password are <code>admin</code>.</li> <li>Click the hamburger menu (three horizontal lines) at the top-left of the page and Navigate to Administration -&gt; Users and access and then select Service Account.</li> <li>Click on Add Service Account.</li> <li>Enter the display name Harvest.</li> <li>Set the role to Editor.</li> <li>Click on Create. The service account will appear in the dashboard.</li> <li>Navigate back to Service Account.</li> <li>Click on Add service account token for the Harvest service account.</li> <li>Click on Generate Token.</li> <li>Click on Copy to clipboard and close.</li> </ol> <p>IMPORTANT: This is the only opportunity to save the token. Immediately paste it into a text file and save it. The token will be needed by Harvest later on.</p> <p>For detailed instructions, please refer to the Grafana API Keys documentation.</p>"},{"location":"quickstart/#11-import-grafana-dashboards","title":"11. Import Grafana Dashboards","text":"<p>To import Grafana dashboards, use the following command:</p> <pre><code>cd ${HARVEST_INSTALL_PATH}/harvest-${HARVEST_VERSION}-1_linux_amd64\nbin/harvest grafana import --token YOUR_TOKEN_HERE\n</code></pre> <p>Replace <code>YOUR_TOKEN_HERE</code> with the token obtained in step 10.</p> <p>You will be prompted to save your API key (token) for later use. Press <code>n</code> to not save the token in your harvest.yml file.</p> <p>After a few seconds, all the dashboards will be imported into Grafana.</p>"},{"location":"quickstart/#12-verify-dashboards-in-grafana","title":"12. Verify Dashboards in Grafana","text":"<p>After adding the Prometheus datasource, you can verify that your dashboards are correctly displaying data. Follow these steps:</p> <ol> <li>Open your web browser and navigate to Grafana (http://localhost:3000). Enter your Grafana credentials to log in. The default username and password are <code>admin</code>.</li> <li>Click on the \"three lines\" button (also known as the hamburger menu) in the top left corner of the Grafana interface. From the menu, select Dashboards.</li> <li>Open the Volume dashboard. Once the dashboard opens, you should see volume data displayed.</li> </ol>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues, check the logs in <code>/var/log/harvest</code> and refer to the troubleshooting section on the wiki. You can also reach out for help on Discord or via email at ng-harvest-files@netapp.com.</p>"},{"location":"quickstart/#conclusion","title":"Conclusion","text":"<p>\ud83c\udf8a Congratulations! You have successfully set up NetApp Harvest, Prometheus, and Grafana. Enjoy monitoring your systems and feel free to reach out on Discord, GitHub, or email.</p>"},{"location":"release-notes/","title":"Release Notes","text":"<ul> <li>Changelog</li> <li>Releases</li> </ul>"},{"location":"storagegrid-metrics/","title":"StorageGrid Metrics","text":"<p>This document describes which StorageGRID metrics are collected and what those metrics are named in Harvest, including:</p> <ul> <li>Details about which Harvest metrics each dashboard uses. These can be generated on demand by running <code>bin/harvest grafana metrics</code>. See #1577 for details.</li> </ul> <pre><code>Creation Date : 2026-Feb-20\nStorageGrid Version: 11.6.0\n</code></pre> Navigate to Grafana dashboards <p>Add your Grafana instance to the following form and save it. When you click on dashboard links on this page, a link to your dashboard will be opened. NAbox hosts Grafana on a subdomain like so: https://localhost/grafana/</p> <p> Grafana Host Save </p>"},{"location":"storagegrid-metrics/#understanding-the-structure","title":"Understanding the structure","text":"<p>Below is an annotated example of how to interpret the structure of each of the metrics.</p> <p>storagegrid_tenant_usage_data_bytes Name of the metric exported by Harvest</p> <p>The logical size of all objects for the tenant. Description of the StorageGrid metric</p> <ul> <li>API will be REST depending on which protocol is used to collect the metric</li> <li>Endpoint name of the REST api used to collect this metric</li> <li>Metric name of the StorageGrid metric</li> <li>Template path of the template that collects the metric</li> </ul> API Endpoint Metric Template REST <code>api/grid/accounts-cache</code> dataBytes conf/storagegrid/11.6.0/tenant.yaml"},{"location":"storagegrid-metrics/#metrics","title":"Metrics","text":""},{"location":"storagegrid-metrics/#storagegrid_content_buckets_and_containers","title":"storagegrid_content_buckets_and_containers","text":"<p>Total number of S3 buckets and Swift containers</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_content_buckets_and_containers</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_content_buckets_and_containers</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: S3 Content timeseries Top $TopResources Nodes by S3 Buckets and Swift Containers"},{"location":"storagegrid-metrics/#storagegrid_content_objects","title":"storagegrid_content_objects","text":"<p>Total number of S3 and Swift objects (excluding empty objects)</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_content_objects</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_content_objects</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: S3 Content timeseries Top $TopResources Nodes by S3 and Swift Objects (excluding empty objects)"},{"location":"storagegrid-metrics/#storagegrid_ilm_awaiting_client_objects","title":"storagegrid_ilm_awaiting_client_objects","text":"<p>Total number of objects on this node awaiting ILM evaluation because of client operation (for example, ingest)</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_ilm_awaiting_client_objects</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_ilm_awaiting_client_objects</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Information Lifecycle Management (ILM) timeseries ILM queue (Objects) StorageGrid: S3 ILM timeseries Top $TopResources Nodes by ILM Awaiting Object Evaluation (incoming from clients)"},{"location":"storagegrid-metrics/#storagegrid_ilm_awaiting_total_objects","title":"storagegrid_ilm_awaiting_total_objects","text":"<p>Total number of objects on this node awaiting ILM evaluation</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_ilm_awaiting_total_objects</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_ilm_awaiting_total_objects</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: S3 ILM timeseries Top $TopResources Nodes by ILM Awaiting Object Evaluation"},{"location":"storagegrid-metrics/#storagegrid_ilm_objects_processed","title":"storagegrid_ilm_objects_processed","text":"<p>Objects processed by ILM that actually had work done</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_ilm_objects_processed</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_ilm_objects_processed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Information Lifecycle Management (ILM) timeseries ILM evaluation rate (objects/second)"},{"location":"storagegrid-metrics/#storagegrid_ilm_scan_objects_per_second","title":"storagegrid_ilm_scan_objects_per_second","text":"<p>ILM scan rate (objects per second)</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_ilm_scan_objects_per_second</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_ilm_scan_objects_per_second</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: S3 ILM timeseries Top $TopResources Nodes by ILM Scan Rate"},{"location":"storagegrid-metrics/#storagegrid_metadata_queries_average_latency_milliseconds","title":"storagegrid_metadata_queries_average_latency_milliseconds","text":"<p>Average metadata query latency in milliseconds</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_metadata_queries_average_latency_milliseconds</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_metadata_queries_average_latency_milliseconds</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: S3 Content timeseries Top $TopResources Nodes by Metadata Query Latency"},{"location":"storagegrid-metrics/#storagegrid_network_received_bytes","title":"storagegrid_network_received_bytes","text":"<p>Total amount of data received</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_network_received_bytes</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_network_received_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Performance timeseries Network traffic"},{"location":"storagegrid-metrics/#storagegrid_network_transmitted_bytes","title":"storagegrid_network_transmitted_bytes","text":"<p>Total amount of data sent</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_network_transmitted_bytes</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_network_transmitted_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Performance timeseries Network traffic"},{"location":"storagegrid-metrics/#storagegrid_node_cpu_utilization_percentage","title":"storagegrid_node_cpu_utilization_percentage","text":"API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_node_cpu_utilization_percentage</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_node_cpu_utilization_percentage</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Nodes timeseries Top $TopResources nodes by CPU usage"},{"location":"storagegrid-metrics/#storagegrid_private_ilm_awaiting_delete_objects","title":"storagegrid_private_ilm_awaiting_delete_objects","text":"<p>The total number of objects on this node awaiting deletion</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_private_ilm_awaiting_delete_objects</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_private_ilm_awaiting_delete_objects</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Information Lifecycle Management (ILM) timeseries ILM queue (Objects)"},{"location":"storagegrid-metrics/#storagegrid_private_load_balancer_storage_request_body_bytes_bucket","title":"storagegrid_private_load_balancer_storage_request_body_bytes_bucket","text":"API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_private_load_balancer_storage_request_body_bytes_bucket</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_private_load_balancer_storage_request_body_bytes_bucket</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Traffic Classification heatmap Write Request Rate by Object Size and Policy StorageGrid: Overview Traffic Classification heatmap Read Request Rate by Object Size and Policy"},{"location":"storagegrid-metrics/#storagegrid_private_load_balancer_storage_request_count","title":"storagegrid_private_load_balancer_storage_request_count","text":"API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_private_load_balancer_storage_request_count</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_private_load_balancer_storage_request_count</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: StorageGrid FabricPool Highlights timeseries Top $TopResources Load Balancer Request Completion Rates by Policy StorageGrid: Overview Performance timeseries Error rate (requests/second) StorageGrid: Overview Performance timeseries Average request duration (non-error) StorageGrid: Overview Traffic Classification timeseries Top $TopResources Load Balancer Request Completion Rates by Policy StorageGrid: Overview Traffic Classification timeseries Top $TopResources Error Response Rates by Policy StorageGrid: Overview Traffic Classification timeseries Top $TopResources Average Request Durations (Non-Error) By Policy"},{"location":"storagegrid-metrics/#storagegrid_private_load_balancer_storage_request_time","title":"storagegrid_private_load_balancer_storage_request_time","text":"API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_private_load_balancer_storage_request_time</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_private_load_balancer_storage_request_time</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Performance timeseries Average request duration (non-error) StorageGrid: Overview Traffic Classification timeseries Top $TopResources Average Request Durations (Non-Error) By Policy"},{"location":"storagegrid-metrics/#storagegrid_private_load_balancer_storage_rx_bytes","title":"storagegrid_private_load_balancer_storage_rx_bytes","text":"API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_private_load_balancer_storage_rx_bytes</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_private_load_balancer_storage_rx_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: StorageGrid FabricPool Highlights timeseries Top $TopResources Load Balancer Request Traffic by Policy StorageGrid: Overview Traffic Classification timeseries Top $TopResources Load Balancer Request Traffic by Policy"},{"location":"storagegrid-metrics/#storagegrid_private_load_balancer_storage_tx_bytes","title":"storagegrid_private_load_balancer_storage_tx_bytes","text":"API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_private_load_balancer_storage_tx_bytes</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_private_load_balancer_storage_tx_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel ONTAP: StorageGrid FabricPool Highlights timeseries Top $TopResources Load Balancer Request Traffic by Policy StorageGrid: Overview Traffic Classification timeseries Top $TopResources Load Balancer Request Traffic by Policy"},{"location":"storagegrid-metrics/#storagegrid_private_s3_total_requests","title":"storagegrid_private_s3_total_requests","text":"<p>Number of S3 requests.</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_private_s3_total_requests</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_private_s3_total_requests</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Performance timeseries S3 operations StorageGrid: S3 Highlights timeseries Top $TopResources Nodes by GET Operations StorageGrid: S3 Highlights timeseries Top $TopResources Nodes by DELETE Operations StorageGrid: S3 Highlights timeseries Top $TopResources Nodes by PUT Operations StorageGrid: S3 Highlights timeseries Top $TopResources Nodes by HEAD Operations"},{"location":"storagegrid-metrics/#storagegrid_s3_data_transfers_bytes_ingested","title":"storagegrid_s3_data_transfers_bytes_ingested","text":"<p>S3 data upload rate (ingestion) in bytes</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_s3_data_transfers_bytes_ingested</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_s3_data_transfers_bytes_ingested</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: S3 Highlights timeseries Top $TopResources Nodes by S3 Upload Rate"},{"location":"storagegrid-metrics/#storagegrid_s3_data_transfers_bytes_retrieved","title":"storagegrid_s3_data_transfers_bytes_retrieved","text":"<p>S3 data download rate (retrieval) in bytes</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_s3_data_transfers_bytes_retrieved</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_s3_data_transfers_bytes_retrieved</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: S3 Highlights timeseries Top $TopResources Nodes by S3 Download Rate"},{"location":"storagegrid-metrics/#storagegrid_s3_operations_failed","title":"storagegrid_s3_operations_failed","text":"<p>The total number of failed S3 operations (HTTP status codes 4xx and 5xx), excluding those caused by S3 authorization failure</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_s3_operations_failed</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_s3_operations_failed</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Performance timeseries S3 API requests StorageGrid: S3 Highlights timeseries Top $TopResources Nodes by S3 Operations per Second (failed)"},{"location":"storagegrid-metrics/#storagegrid_s3_operations_successful","title":"storagegrid_s3_operations_successful","text":"<p>The total number of successful S3 operations (HTTP status code 2xx)</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_s3_operations_successful</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_s3_operations_successful</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Performance timeseries S3 API requests StorageGrid: S3 Highlights timeseries Top $TopResources Nodes by S3 Successful Operations"},{"location":"storagegrid-metrics/#storagegrid_s3_operations_unauthorized","title":"storagegrid_s3_operations_unauthorized","text":"<p>The total number of failed S3 operations that are the result of an authorization failure (HTTP status codes 4xx)</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_s3_operations_unauthorized</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_s3_operations_unauthorized</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Performance timeseries S3 API requests StorageGrid: S3 Highlights timeseries Top $TopResources Nodes by S3 Operations per Second (unauthorized)"},{"location":"storagegrid-metrics/#storagegrid_storage_utilization_data_bytes","title":"storagegrid_storage_utilization_data_bytes","text":"<p>An estimate of the total size of replicated and erasure coded object data on the Storage Node</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_storage_utilization_data_bytes</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_storage_utilization_data_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Highlights table Data space usage breakdown StorageGrid: Overview Highlights timeseries Data storage over time StorageGrid: Overview Nodes timeseries Top $TopResources nodes by data usage StorageGrid: S3 Capacity timeseries Top $TopResources Nodes by Used Storage for Data"},{"location":"storagegrid-metrics/#storagegrid_storage_utilization_metadata_allowed_bytes","title":"storagegrid_storage_utilization_metadata_allowed_bytes","text":"<p>The total space available on storage volume 0 for object metadata</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_storage_utilization_metadata_allowed_bytes</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_storage_utilization_metadata_allowed_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Highlights table Metadata allowed space usage breakdown"},{"location":"storagegrid-metrics/#storagegrid_storage_utilization_metadata_bytes","title":"storagegrid_storage_utilization_metadata_bytes","text":"<p>The amount of object metadata on storage volume 0, in bytes</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_storage_utilization_metadata_bytes</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_storage_utilization_metadata_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Highlights table Metadata allowed space usage breakdown"},{"location":"storagegrid-metrics/#storagegrid_storage_utilization_total_space_bytes","title":"storagegrid_storage_utilization_total_space_bytes","text":"<p>Total storage space available in bytes</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_storage_utilization_total_space_bytes</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_storage_utilization_total_space_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: S3 Capacity timeseries Top $TopResources Nodes by Percent Usable Space"},{"location":"storagegrid-metrics/#storagegrid_storage_utilization_usable_space_bytes","title":"storagegrid_storage_utilization_usable_space_bytes","text":"<p>The total amount of object storage space remaining</p> API Endpoint Metric Template REST <code>prometheus</code> <code>storagegrid_storage_utilization_usable_space_bytes</code> conf/storagegrid/11.6.0/storagegrid_metrics.yaml <p>The <code>storagegrid_storage_utilization_usable_space_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Highlights table Data space usage breakdown StorageGrid: Overview Nodes timeseries Top $TopResources nodes by data usage StorageGrid: S3 Capacity timeseries Top $TopResources Nodes by Percent Usable Space"},{"location":"storagegrid-metrics/#storagegrid_tenant_usage_data_bytes","title":"storagegrid_tenant_usage_data_bytes","text":"<p>The logical size of all objects for the tenant</p> API Endpoint Metric Template REST <code>grid/accounts-cache</code> <code>dataBytes</code> conf/storagegrid/11.6.0/tenant.yaml <p>The <code>storagegrid_tenant_usage_data_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Storage timeseries Top $TopResources tenants by logical space used StorageGrid: Overview Storage timeseries Top $TopResources tenants by quota usage"},{"location":"storagegrid-metrics/#storagegrid_tenant_usage_quota_bytes","title":"storagegrid_tenant_usage_quota_bytes","text":"<p>The maximum amount of logical space available for the tenant's object. If a quota metric is not provided, an unlimited amount of space is available</p> API Endpoint Metric Template REST <code>grid/accounts-cache</code> <code>policy.quotaObjectBytes</code> conf/storagegrid/11.6.0/tenant.yaml <p>The <code>storagegrid_tenant_usage_quota_bytes</code> metric is visualized in the following Grafana dashboards:</p> Dashboard Row Type Panel StorageGrid: Overview Storage timeseries Top $TopResources tenants by quota usage"},{"location":"system-requirements/","title":"System Requirements","text":"<p>Harvest is written in Go, which means it runs on recent Linux systems. It also runs on Macs for development.</p> <p>Hardware requirements depend on how many clusters you monitor and the number of metrics you chose to collect. With the default configuration, when monitoring 10 clusters, we recommend:</p> <ul> <li>CPU: 2 cores</li> <li>Memory: 1 GB</li> <li>Disk: 500 MB (mostly used by log files)</li> </ul> <p>Note: These CPU, memory, and disk requirements are just for Harvest and do not include Prometheus, InfluxDB, Grafana.</p> <p>Harvest is compatible with:</p> <ul> <li>Prometheus: <code>2.33</code> or higher</li> <li>InfluxDB: <code>v2</code></li> <li>Grafana: <code>8.1.X</code> or higher</li> <li>Docker: <code>20.10.0</code> or higher and compatible Docker Compose</li> </ul>"},{"location":"victoriametrics-exporter/","title":"VictoriaMetrics Exporter","text":"VictoriaMetrics Install <p>The information below describes how to setup Harvest's VictoriaMetrics exporter. If you need help installing or setting up VictoriaMetrics, check out their documentation.</p>"},{"location":"victoriametrics-exporter/#overview","title":"Overview","text":"<p>The VictoriaMetrics Exporter will format metrics into Prometheus exposition format. The Exporter is compatible with VictoriaMetrics v1.129.1.</p>"},{"location":"victoriametrics-exporter/#parameters","title":"Parameters","text":"<p>Overview of all parameters is provided below. Only one of <code>url</code> or <code>addr</code> should be provided and at least one of them is required. If <code>addr</code> is specified, it should be a valid TCP address or hostname of the VictoriaMetrics server and should not include the scheme and port.</p> <p><code>addr</code> only works with HTTP. If you need to use HTTPS, you should use <code>url</code> instead.</p> <p>If <code>url</code> is specified, you must add all arguments to the url. Harvest will do no additional processing and use exactly what you specify. ( e.g. <code>url: http://localhost:8428/api/v1/import/prometheus</code>. When using <code>url</code>, the <code>addr</code> and <code>port</code> field will be ignored.</p> parameter type description default <code>url</code> string URL of the database, format: <code>SCHEME://HOST[:PORT]</code> <code>addr</code> string address of the database, format: <code>HOST</code> (HTTP only) <code>port</code> int, optional port of the database <code>8086</code> <code>client_timeout</code> int, optional client timeout in seconds <code>5</code>"},{"location":"victoriametrics-exporter/#example","title":"Example","text":"<p>snippet from <code>harvest.yml</code> using <code>addr</code>: (supports HTTP only))</p> <pre><code>Exporters:\n  my_victoriametrics:\n    exporter: VictoriaMetrics\n    addr: localhost\n</code></pre> <p>snippet from <code>harvest.yml</code> using <code>url</code>: (supports both HTTP/HTTPS))</p> <pre><code>Exporters:\n  victoriametrics2:\n    exporter: VictoriaMetrics\n    url: http://localhost:8428/api/v1/import/prometheus\n</code></pre>"},{"location":"architecture/rest-collector/","title":"REST collector","text":""},{"location":"architecture/rest-collector/#status","title":"Status","text":"<p>~~Accepted~~ Superseded by REST strategy</p> <p> The exact version of ONTAP that has full ZAPI parity is subject to change.  Everywhere you see version 9.12, may become 9.13 or later.</p>"},{"location":"architecture/rest-collector/#context","title":"Context","text":"<p>We need to document and communicate to customers: - when they should switch from the ZAPI collectors to the REST ones - what versions of ONTAP are supported by Harvest's REST collectors - how to fill ONTAP gaps between the ZAPI and REST APIs</p> <p>The ONTAP version information is important because gaps are addressed in later versions of cDOT.</p>"},{"location":"architecture/rest-collector/#considered-options","title":"Considered Options","text":"<ol> <li> <p>Only REST A clean cut-over, stop using ZAPI, and switch completely to REST.</p> </li> <li> <p>Both Support both ZAPI and REST collectors running at the same time, collecting the same objects. Flexible, but has the downside of last-write wins. Not recommended unless you selectively pick non-overlapping sets of objects.</p> </li> <li> <p>Template change that supports both Change the template to break ties, priority, etc. Rejected because additional complexity not worth the benefits.</p> </li> <li> <p>private-cli When there are REST gaps that have not been filled yet or will never be filled (WONTFIX), the Harvest REST collector will provide infrastructure and documentation on how to use private-cli pass-through to address gaps.</p> </li> </ol>"},{"location":"architecture/rest-collector/#chosen-decision","title":"Chosen Decision","text":"<p>For clusters with ONTAP versions &lt; 9.12, we recommend customers use the ZAPI collectors. (#2) (#4)</p> <p>Once ONTAP 9.12+ is released and customers have upgraded to it, they should make a clean cut-over to the REST collectors (#1).  ONTAP 9.12 is the version of ONTAP that has the best parity with what Harvest collects in terms of config and performance counters.  Harvest REST collectors, templates, and dashboards are validated against ONTAP 9.12+.  Most of the REST config templates will work before 9.12, but unless you have specific needs, we recommend sticking with the ZAPI collectors until you upgrade to 9.12.</p> <p>There is little value in running both the ZAPI and REST collectors for an overlapping set of objects.  It's unlikely you want to collect the same object via REST and ZAPI at the same time. Harvest doesn't support this use-case, but does nothing to detect or prevent it.</p> <p>If you want to collect a non-overlapping set of objects with REST and ZAPI, you can.  If you do, we recommend you disable the ZAPI object collector.  For example, if you enable the REST <code>disk</code> template, you should disable the ZAPI <code>disk</code> template.  We do NOT recommend collecting an overlapping set of objects with both collectors since the last one to run will overwrite previously collected data.</p> <p>Harvest will document how to use the REST private cli pass-through to collect custom and non-public counters.</p> <p>The Harvest team recommends that customers open ONTAP issues for REST public API gaps that need filled.</p>"},{"location":"architecture/rest-collector/#consequences","title":"Consequences","text":"<p>The Harvest REST collectors will work with limitations on earlier versions of ONTAP.  ONTAP 9.12+ is the minimally validated version.  We only validate the full set of templates, dashboards, counters, etc. on versions of ONTAP 9.12+</p> <p>Harvest does not prevent you from collecting the same resource with ZAPI and REST.</p>"},{"location":"architecture/rest-strategy/","title":"REST Strategy","text":""},{"location":"architecture/rest-strategy/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/rest-strategy/#context","title":"Context","text":"<p>ONTAP has published a customer product communiqu\u00e9 (CPC-00410) (June 2024) announcing that the previously planned end of availability (EOA) for ZAPI has been deferred indefinitely.</p> <p>This document describes how Harvest handles the ONTAP transition from ZAPI to REST.  In most cases, no action is required on your part.</p>"},{"location":"architecture/rest-strategy/#harvest-api-transition","title":"Harvest API Transition","text":"<p>Harvest tries to use the protocol you specify in your <code>harvest.yml</code> config file.</p> <p>When specifying the ZAPI collector, Harvest will use the ZAPI protocol unless the cluster no longer speaks Zapi, in which case, Harvest will switch to REST.</p> <p>If you specify the REST collector, Harvest will use the REST protocol.</p> <p>Harvest includes a full set of REST templates that export identical metrics as the included ZAPI templates. No changes to dashboards or downstream metric-consumers should be required.  See below if you have  added metrics to the Harvest out-of-the-box templates.</p> <p>Read on if you want to know how you can use REST sooner, or you want to take advantage of REST-only features in ONTAP.</p>"},{"location":"architecture/rest-strategy/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"architecture/rest-strategy/#how-does-harvest-decide-whether-to-use-rest-or-zapi-apis","title":"How does Harvest decide whether to use REST or ZAPI APIs?","text":"<p>Harvest attempts to use the collector defined in your <code>harvest.yml</code> config file.</p> <ul> <li> <p>If you specify the ZAPI collector, Harvest will use the ZAPI protocol as long as the cluster still speaks Zapi.    If the cluster no longer understands Zapi, Harvest will switch to Rest.</p> </li> <li> <p>If you specify the REST collector, Harvest will use REST.</p> </li> </ul> <p>Earlier versions of Harvest included a <code>prefer_zapi</code> poller option and a <code>HARVEST_NO_COLLECTOR_UPGRADE</code> environment variable. Both of these options are ignored in Harvest versions <code>23.08</code> onwards.</p>"},{"location":"architecture/rest-strategy/#why-would-i-switch-to-rest-before-9131","title":"Why would I switch to REST before <code>9.13.1</code>?","text":"<ul> <li>You have advanced use cases to validate before ONTAP removes ZAPIs</li> <li>You want to take advantage of new ONTAP features that are only available via REST (e.g., cloud features, event remediation, name services, cluster peers, etc.)</li> <li>You want to collect a metric that is not available via ZAPI</li> <li>You want to collect a metric from the ONTAP CLI. The REST API includes a private CLI pass-through to access any ONTAP CLI command</li> </ul>"},{"location":"architecture/rest-strategy/#can-i-start-using-rest-before-9131","title":"Can I start using REST before <code>9.13.1</code>?","text":"<p>Yes. Many customers do. Be aware of the following limitations:</p> <ol> <li>ONTAP includes a subset of performance counters via REST beginning in ONTAP 9.11.1.</li> <li>There may be performance metrics missing from versions of ONTAP earlier than <code>9.11.1</code>.</li> </ol> <p>Where performance metrics are concerned, because of point #2, our recommendation is to wait until at least ONTAP <code>9.12.1</code> before switching to the <code>RestPerf</code> collector. You can continue using the <code>ZapiPerf</code> collector until you switch.</p>"},{"location":"architecture/rest-strategy/#a-counter-is-missing-from-rest-what-do-i-do","title":"A counter is missing from REST. What do I do?","text":"<p>The Harvest team has ensured that all the out-of-the-box ZAPI templates have matching REST templates with identical metrics as of Harvest <code>22.11</code> and ONTAP <code>9.12.1</code>. Any additional ZAPI Perf counters you have added may be missing from ONTAP REST Perf. </p> <p>Join the Harvest discord channel and ask us about the counter. Sometimes we may know which release the missing counter is coming in, otherwise we can point you to the ONTAP process to request new counters.</p>"},{"location":"architecture/rest-strategy/#can-i-use-the-rest-and-zapi-collectors-at-the-same-time","title":"Can I use the REST and ZAPI collectors at the same time?","text":"<p>Yes. Harvest ensures that duplicate resources are not collected from both collectors.</p> <p>When there is potential duplication, Harvest first resolves the conflict in the order collectors are defined in your poller and then negotiates with the cluster on  the most appropriate API to use per above.</p> <p>Let's take a look at a few examples using the following poller definition:</p> <pre><code>cluster-1:\n    datacenter: dc-1\n    addr: 10.1.1.1\n    collectors:\n        - Zapi\n        - Rest\n</code></pre> <ul> <li> <p>When <code>cluster-1</code> is running ONTAP <code>9.9.X</code> (ONTAP still supports ZAPIs), the Zapi collector will be used since it is   listed first in the list of <code>collectors</code>. When collecting a REST-only resource like, <code>nfs_client</code>, the Rest collector will be used   since <code>nfs_client</code> objects are only available via REST.</p> </li> <li> <p>When <code>cluster-1</code> is running ONTAP <code>9.18.1</code> (ONTAP no longer supports ZAPIs),   the Rest collector will be used since ONTAP can no longer speak the ZAPI protocol.</p> </li> </ul> <p>If you want the REST collector to be used in all cases, change the order in the <code>collectors</code> section so <code>Rest</code> comes before <code>Zapi</code>.</p> <p>If the resource does not exist for the first collector, the next collector will be tried. Using the example above, when collecting <code>VolumeAnalytics</code> resources, the Zapi collector will not run for <code>VolumeAnalytics</code> objects since that resource is only available via REST. The Rest collector will run and collect the <code>VolumeAnalytics</code> objects.</p>"},{"location":"architecture/rest-strategy/#ive-added-counters-to-existing-zapi-templates-will-those-counters-work-in-rest","title":"I've added counters to existing ZAPI templates. Will those counters work in REST?","text":"<p><code>ZAPI</code> config metrics often have a REST equivalent that can be found in ONTAP's ONTAPI to REST mapping document.</p> <p>ZAPI performance metrics may be missing in REST. If you have added new metrics or templates to the <code>ZapiPerf</code> collector, those metrics likely aren't available via REST.  You can check if the performance counter is available, ask the Harvest team on Discord, or ask ONTAP to add the counter you need.</p>"},{"location":"architecture/rest-strategy/#reference","title":"Reference","text":"<p>Table of ONTAP versions, dates and API notes.</p> ONTAPversion ReleaseDate ONTAPNotes <code>9.11.1</code> Q2 2022 First version of ONTAP with REST performance metrics <code>9.12.1</code> Q4 2022 ZAPIs still supported - REST performance metrics have parity with Harvest <code>22.11</code> collected ZAPI performance metrics <code>9.13.1</code> Q2 2023 ZAPIs still supported. <code>9.14.1</code>+ Q2 2024+ ZAPI end of availability (EOA) deferred: See ONTAP communique for details. ZAPIs remain active for 30 days following an upgrade. If unused during this period, they are automatically disabled but can be re-enabled using the command-line interface (CLI). NetApp recommends transitioning to REST APIs for all new automation projects."},{"location":"help/config-collection/","title":"Harvest Config Collection Guide","text":"<p>This guide is designed to help you validate your Harvest configuration (<code>harvest.yml</code>) on various platforms. The commands in this guide will generate redacted output that personally identifiable information (PII) removed. This makes it safe for you to share the output. Follow the instructions specific to your platform. If you wish to share it with the Harvest team, please email them at ng-harvest-files@netapp.com.</p>"},{"location":"help/config-collection/#rpm-deb-and-native-installations","title":"RPM, DEB, and Native Installations","text":"<p>To print a redacted version of your Harvest configuration to the console, use the following command:</p> <pre><code>cd /opt/harvest\nexport CONFIG_FILE_NAME=harvest.yml\nbin/harvest doctor --print --config $CONFIG_FILE_NAME\n</code></pre>"},{"location":"help/config-collection/#docker-container","title":"Docker Container","text":"<p>For Docker containers, use the following command to print a redacted version of your Harvest configuration to the console:</p> <pre><code>cd to/where/your/harvest.yml/is\nexport CONFIG_FILE_NAME=harvest.yml\ndocker run --rm --entrypoint \"bin/harvest\" --volume \"$(pwd)/$CONFIG_FILE_NAME:/opt/harvest/harvest.yml\" ghcr.io/netapp/harvest doctor --print\n</code></pre>"},{"location":"help/config-collection/#nabox3","title":"NABox3","text":"<p>If you're using NABox3, you'll need to ssh into your NABox instance. Then, use the following command to print a redacted version of your Harvest configuration to the console:</p> <pre><code>dc exec -w /conf nabox-harvest2 /netapp-harvest/bin/harvest doctor --print\n</code></pre>"},{"location":"help/config-collection/#nabox4","title":"NABox4","text":"<p>If you're using NABox4, you'll need to ssh into your NABox instance. Then, use the following command to print a redacted version of your Harvest configuration to the console:</p> <pre><code>dc exec -w /harvest -e HARVEST_CONF=/harvest-conf harvest /harvest/bin/harvest doctor --print\n</code></pre> <p>If your configuration file name is different from the default <code>harvest.yml</code>, remember to change the <code>CONFIG_FILE_NAME</code> environment variable to match your file name.</p>"},{"location":"help/faq/","title":"FAQ","text":""},{"location":"help/faq/#how-do-i-migrate-from-harvest-16-to-20","title":"How do I migrate from Harvest 1.6 to 2.0?","text":"<p>There currently is not a tool to migrate data from Harvest 1.6 to 2.0. The most common workaround is to run both, 1.6 and 2.0, in parallel. Run both, until the 1.6 data expires due to normal retention policy, and then fully cut over to 2.0.</p> <p>Technically, it\u2019s possible to take a Graphite DB, extract the data, and send it to a Prometheus db, but it\u2019s not an area we\u2019ve invested in. If you want to explore that option, check out the promtool which supports importing, but probably not worth the effort.</p>"},{"location":"help/faq/#how-do-i-share-sensitive-log-files-with-netapp","title":"How do I share sensitive log files with NetApp?","text":"<p>Email them to ng-harvest-files@netapp.com This mail address is accessible to NetApp Harvest employees only.</p>"},{"location":"help/faq/#multi-tenancy","title":"Multi-tenancy","text":""},{"location":"help/faq/#question","title":"Question","text":"<p>Is there a way to allow per SVM level user views?  I need to offer 1 tenant per SVM. Can I limit visibility to specific SVMs? Is there an SVM dashboard available?</p>"},{"location":"help/faq/#answer","title":"Answer","text":"<p>You can do this with Grafana. Harvest can provide the labels for SVMs. The pieces are there but need to be put together.</p> <p>Grafana templates support the $__user variable to make pre-selections and decisions. You can use that + metadata mapping the user &lt;-&gt; SVM. With both of those you can build SVM specific dashboards.</p> <p>There is a German service provider who is doing this. They have service managers responsible for a set of customers \u2013 and only want to see the data/dashboards of their corresponding customers.</p>"},{"location":"help/faq/#harvest-authentication-and-permissions","title":"Harvest Authentication and Permissions","text":""},{"location":"help/faq/#question_1","title":"Question","text":"<p>What permissions does Harvest need to talk to ONTAP?</p>"},{"location":"help/faq/#answer_1","title":"Answer","text":"<p>Permissions, authentication, role based security, and creating a Harvest user are covered here.</p>"},{"location":"help/faq/#ontap-counters-are-missing","title":"ONTAP counters are missing","text":""},{"location":"help/faq/#question_2","title":"Question","text":"<p>How do I make Harvest collect additional ONTAP counters?</p>"},{"location":"help/faq/#answer_2","title":"Answer","text":"<p>Instead of modifying the out-of-the-box templates in the <code>conf/</code> directory, it is better to create your own custom templates following these instructions.</p>"},{"location":"help/faq/#capacity-metrics","title":"Capacity Metrics","text":""},{"location":"help/faq/#question_3","title":"Question","text":"<p>How are capacity and other metrics calculated by Harvest?</p>"},{"location":"help/faq/#answer_3","title":"Answer","text":"<p>Each collector has its own way of collecting and post-processing metrics. Check the documentation of each individual collector (usually under section #Metrics). Capacity and hardware-related metrics are collected by the Zapi collector which emits metrics as they are without any additional calculation. Performance metrics are collected by the ZapiPerf collector and the final values are calculated from the delta of two consequent polls.</p>"},{"location":"help/faq/#tagging-volumes","title":"Tagging Volumes","text":""},{"location":"help/faq/#question_4","title":"Question","text":"<p>How do I tag ONTAP volumes with metadata and surface that data in Harvest?</p>"},{"location":"help/faq/#answer_4","title":"Answer","text":"<p>See volume tagging issue and volume tagging via sub-templates</p>"},{"location":"help/faq/#rest-and-zapi-documentation","title":"REST and Zapi Documentation","text":""},{"location":"help/faq/#question_5","title":"Question","text":"<p>How do I relate ONTAP REST endpoints to ZAPI APIs and attributes?</p>"},{"location":"help/faq/#answer_5","title":"Answer","text":"<p>Please refer to the ONTAPI to REST API mapping document.</p>"},{"location":"help/faq/#sizing","title":"Sizing","text":"<p>How much disk space is required by Prometheus?</p> <p>This depends on the collectors you've added, # of nodes monitored, cardinality of labels, # instances, retention, ingest rate, etc. A good approximation is to curl your Harvest exporter and count the number of samples that it publishes and then feed that information into a Prometheus sizing formula.</p> <p>Prometheus stores an average of 1-2 bytes per sample. To plan the capacity of a Prometheus server, you can use the rough formula: needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample</p> <p>A rough approximation is outlined https://devops.stackexchange.com/questions/9298/how-to-calculate-disk-space-required-by-prometheus-v2-2</p>"},{"location":"help/faq/#topk-usage-in-grafana","title":"Topk usage in Grafana","text":""},{"location":"help/faq/#question_6","title":"Question","text":"<p>In Grafana, why do I see more results from topk than I asked for?</p>"},{"location":"help/faq/#answer_6","title":"Answer","text":"<p>Topk is one of Prometheus's out-of-the-box aggregation operators, and is used to calculate the largest k elements by sample value.</p> <p>Depending on the time range you select, Prometheus will often return more results than you asked for. That's because Prometheus is picking the topk for each time in the graph. In other words, different time series are the topk at different times in the graph. When you use a large duration, there are often many time series.</p> <p>This is a limitation of Prometheus and can be mitigated by:</p> <ul> <li>reducing the time range to a smaller duration that includes fewer topk results - something like a five to ten minute range works well for most of Harvest's charts</li> <li>the panel's table shows the current topk rows and that data can be used to supplement the additional series shown in the charts</li> </ul> <p>Additional details: here, here, and here</p>"},{"location":"help/faq/#where-are-harvest-container-images-published","title":"Where are Harvest container images published?","text":"<p>Harvest container images are published to both GitHub's image registry (ghcr.io) and Docker's image registry (hub.docker.com). By default, <code>ghcr.io</code> is used for pulling images.</p> <p>Please note that <code>cr.netapp.io</code> is no longer maintained. If you have been using <code>cr.netapp.io</code> to pull Harvest images, please switch to <code>ghcr.io</code> or Docker Hub as your container image registry. Starting in 2024, we will cease publishing Harvest container images to <code>cr.netapp.io</code>.</p>"},{"location":"help/faq/#how-do-i-switch-between-image-registries","title":"How do I switch between image registries?","text":""},{"location":"help/faq/#answer_7","title":"Answer","text":"<p>Replace all instances of <code>rahulguptajss/harvest:latest</code> with <code>ghcr.io/netapp/harvest:latest</code>:</p> <ul> <li> <p>Edit your docker-compose file and make those replacements or regenerate the compose file.</p> </li> <li> <p>Update any shell or Ansible scripts you have that are also using those images</p> </li> <li> <p>After making these changes, you should stop your containers, pull new images, and restart.</p> </li> </ul> <p>You can verify that you're using the GitHub Container Registry images like so:</p> <p>Before</p> <pre><code>docker image ls -a\nREPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\nrahulguptajss/harvest       latest    80061bbe1c2c   10 days ago    56.4MB &lt;=== Docker Hub\nprom/prometheus             v2.33.1   e528f02c45a6   3 weeks ago    204MB\ngrafana/grafana             8.3.4     4a34578e4374   5 weeks ago    274MB\n</code></pre> <p>Pull image from GitHub Container Registry</p> <pre><code>docker pull ghcr.io/netapp/harvest:latest\nUsing default tag: latest\nlatest: Pulling from ghcr.io/netapp/harvest\nDigest: sha256:6ff88153812ebb61e9dd176182bf8a792cde847748c5654d65f4630e61b1f3ae\nStatus: Image is up to date for ghcr.io/netapp/harvest:latest\nghcr.io/netapp/harvest:latest\n</code></pre> <p>Notice that the <code>IMAGE ID</code> for both images are identical since the images are the same.</p> <pre><code>docker image ls -a\nREPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\nghcr.io/netapp/harvest      latest    80061bbe1c2c   10 days ago    56.4MB  &lt;== Harvest image from GitHub Container Registry\nrahulguptajss/harvest       latest    80061bbe1c2c   10 days ago    56.4MB\nprom/prometheus             v2.33.1   e528f02c45a6   3 weeks ago    204MB\ngrafana/grafana             8.3.4     4a34578e4374   5 weeks ago    274MB\n</code></pre> <p>We can now remove the Docker Hub pulled image</p> <pre><code>docker image rm rahulguptajss/harvest:latest\nUntagged: rahulguptajss/harvest:latest\nUntagged: rahulguptajss/harvest@sha256:6ff88153812ebb61e9dd176182bf8a792cde847748c5654d65f4630e61b1f3ae\n\ndocker image ls -a\nREPOSITORY              TAG       IMAGE ID       CREATED        SIZE\nghcr.io/netapp/harvest   latest    80061bbe1c2c   10 days ago    56.4MB\nprom/prometheus         v2.33.1   e528f02c45a6   3 weeks ago    204MB\ngrafana/grafana         8.3.4     4a34578e4374   5 weeks ago    274MB\n</code></pre>"},{"location":"help/faq/#ports","title":"Ports","text":""},{"location":"help/faq/#what-ports-does-harvest-use","title":"What ports does Harvest use?","text":""},{"location":"help/faq/#answer_8","title":"Answer","text":"<p>The default ports are shown in the following diagram.</p> <p></p> <ul> <li>Harvest's pollers use ZAPI or REST to communicate with ONTAP on port <code>443</code></li> <li>Each poller exposes the Prometheus port defined in your <code>harvest.yml</code> file</li> <li>Prometheus scrapes each poller-exposed Prometheus port (<code>promPort1</code>, <code>promPort2</code>, <code>promPort3</code>)</li> <li>Prometheus's default port is <code>9090</code></li> <li>Grafana's default port is <code>3000</code></li> </ul>"},{"location":"help/faq/#snapmirror_labels","title":"Snapmirror_labels","text":""},{"location":"help/faq/#why-do-my-snapmirror_labels-have-an-empty-source_node","title":"Why do my snapmirror_labels have an empty source_node?","text":""},{"location":"help/faq/#answer_9","title":"Answer","text":"<p>Snapmirror relationships have a source and destination node. ONTAP however does not expose the source side of that relationship, only the destination side is returned via ZAPI/REST APIs. Because of that, the Prometheus metric named, <code>snapmirror_labels</code>, will have an empty <code>source_node</code> label.</p> <p>The dashboards show the correct value for <code>source_node</code> since we join multiple metrics in the Grafana panels to synthesize that information.</p> <p>In short: don't rely on the <code>snapmirror_labels</code> for <code>source_node</code> labels. If you need <code>source_node</code> you will need to do a similar join as the Snapmirror dashboard does.</p> <p>See https://github.com/NetApp/harvest/issues/1192 for more information and linked pull requests for REST and ZAPI.</p>"},{"location":"help/faq/#nfs-clients-dashboard","title":"NFS Clients Dashboard","text":""},{"location":"help/faq/#why-do-my-nfs-clients-dashboard-have-no-data","title":"Why do my NFS Clients Dashboard have no data?","text":""},{"location":"help/faq/#answer_10","title":"Answer","text":"<p>NFS Clients dashboard is only available through Rest Collector. This information is not available through Zapi. You must enable the Rest collector in your harvest.yml config and uncomment the nfs_clients.yaml section in your default.yaml file.</p> <p>Note: Enabling nfs_clients.yaml may slow down data collection.</p>"},{"location":"help/faq/#file-analytics-dashboard","title":"File Analytics Dashboard","text":""},{"location":"help/faq/#why-do-my-file-analytics-dashboard-have-no-data","title":"Why do my File Analytics Dashboard have no data?","text":""},{"location":"help/faq/#answer_11","title":"Answer","text":"<p>This dashboard requires ONTAP 9.8+ and the APIs are only available via REST. Please enable the REST collector in your harvest config. To collect and display usage data such as capacity analytics, you need to enable File System Analytics on a volume. Please see https://docs.netapp.com/us-en/ontap/task_nas_file_system_analytics_enable.html for more details.</p>"},{"location":"help/faq/#why-do-i-have-volume-sis-stat-panel-empty-in-volume-dashboard","title":"Why do I have Volume Sis Stat panel empty in Volume dashboard?","text":""},{"location":"help/faq/#answer_12","title":"Answer","text":"<p>This panel requires ONTAP 9.12+ and the APIs are only available via REST. Enable the REST collector in your <code>harvest.yml</code> config.</p>"},{"location":"help/log-collection/","title":"Harvest Logs Collection Guide","text":"<p>This guide will help you collect Harvest logs on various platforms. Follow the instructions specific to your platform. If you would like to share the collected logs with the Harvest team, please email them to ng-harvest-files@netapp.com.</p> <p>If the files are too large to email, let us know at the address above or on Discord,  and we'll send you a file sharing link to upload your files.</p>"},{"location":"help/log-collection/#rpm-deb-and-native-installations","title":"RPM, DEB, and Native Installations","text":"<p>For RPM, DEB, and native installations, use the following command to create a compressed tar file containing the logs:</p> <pre><code>tar -czvf harvest_logs.tar.gz -C /var/log harvest\n</code></pre> <p>This command will create a file named <code>harvest_logs.tar.gz</code> with the contents of the <code>/var/log/harvest</code> directory.</p>"},{"location":"help/log-collection/#docker-container","title":"Docker Container","text":"<p>For Docker containers, first, identify the container ID for your Harvest instance. Then, replace <code>&lt;container_id&gt;</code> with the actual container ID in the following command:</p> <pre><code>docker logs &lt;container_id&gt; &amp;&gt; harvest_logs.txt &amp;&amp; tar -czvf harvest_logs.tar.gz harvest_logs.txt\n</code></pre> <p>This command will create a file named <code>harvest_logs.tar.gz</code> containing the logs from the specified container.</p>"},{"location":"help/log-collection/#nabox-4","title":"NABox 4","text":"<p>Collect a support bundle from the NABox web interface by clicking the <code>About</code> button in the left gutter and then clicking the <code>Download Support Bundle</code> button. </p>"},{"location":"help/log-collection/#nabox-3","title":"NABox 3","text":"<p>For NABox installations, ssh into your nabox instance, and use the following command to create a compressed tar file containing the logs:</p> <pre><code>dc logs nabox-api &gt; nabox-api.log; dc logs nabox-harvest2 &gt; nabox-harvest2.log;\\\n  tar -czf nabox-logs-`date +%Y-%m-%d_%H:%M:%S`.tgz *\n</code></pre> <p>This command will create a file named <code>nabox-logs-$date.tgz</code> containing the nabox-api and Harvest poller logs.</p> <p>For more information, see the NABox documentation on collecting logs</p>"},{"location":"help/troubleshooting/","title":"Checklists for Harvest","text":"<p>A set of steps to go through when something goes wrong.</p>"},{"location":"help/troubleshooting/#what-version-of-ontap-do-you-have","title":"What version of ONTAP do you have?","text":"<p>Run the following, replacing <code>&lt;poller&gt;</code> with the poller from your <code>harvest.yaml</code></p> <pre><code>./bin/harvest zapi -p &lt;poller&gt; show system\n</code></pre> <p>Copy and paste the output into your issue. Here's an example: <pre><code>./bin/harvest -p infinity show system\nconnected to infinity (NetApp Release 9.8P2: Tue Feb 16 03:49:46 UTC 2021)\n[results]                             -                                   *\n  [build-timestamp]                   -                          1613447386\n  [is-clustered]                      -                                true\n  [version]                           - NetApp Release 9.8P2: Tue Feb 16 03:49:46 UTC 2021\n  [version-tuple]                     -                                   *\n    [system-version-tuple]            -                                   *\n      [generation]                    -                                   9\n      [major]                         -                                   8\n      [minor]                         -                                   0\n</code></pre></p>"},{"location":"help/troubleshooting/#install-fails","title":"Install fails","text":"<p>I tried to install and ...</p>"},{"location":"help/troubleshooting/#how-do-i-tell-if-harvest-is-doing-anything","title":"How do I tell if Harvest is doing anything?","text":"<p>You believe Harvest is installed fine, but it's not working.</p> <ul> <li>Post the contents of your <code>harvest.yml</code></li> </ul> <p>Try validating your <code>harvest.yml</code> with <code>yamllint</code> like so: <code>yamllint -d relaxed harvest.yml</code> If you do not have yamllint installed, look here.</p> <p>There should be no errors - warnings like the following are fine: <pre><code>harvest.yml\n  64:1      warning  too many blank lines (3 &gt; 0)  (empty-lines)\n</code></pre></p> <ul> <li> <p>How did you start Harvest?</p> </li> <li> <p>What do you see in <code>/var/log/harvest/*</code></p> </li> <li> <p>What does <code>ps aux | grep poller</code> show?</p> </li> <li> <p>If you are using Prometheus, try hitting Harvest's Prometheus endpoint like so:</p> </li> </ul> <p><code>curl http://machine-this-is-running-harvest:prometheus-port-in-harvest-yaml/metrics</code></p> <ul> <li>Check file ownership (user/group) and file permissions of your templates, executable, etc in your Harvest home directory (<code>ls -la /opt/harvest/</code>) See also.</li> </ul>"},{"location":"help/troubleshooting/#how-do-i-start-harvest-in-debug-mode","title":"How do I start Harvest in debug mode?","text":"<p>Use the <code>--debug</code> flag when starting a poller to enable debug logging (<code>--debug</code> is shorthand for <code>--loglevel 1</code>). Another useful flag is <code>--foreground</code>, which causes all log messages to be written to the terminal.  Note that you can only start one poller in foreground mode.</p> <p>The amount of logged information can be controlled with the <code>--loglevel</code> flag followed by an integer value. The integer values are as follows: - 0: Trace - 1: Debug - 2: Info (default) - 3: Warning - 4: Error - 5: Critical</p> <p>Examples:</p> <pre><code>bin/harvest start $POLLER_NAME --foreground --debug\nor\nbin/harvest start $POLLER_NAME --loglevel=1 --collectors Zapi --objects Qtree\n</code></pre>"},{"location":"help/troubleshooting/#how-do-i-start-harvest-in-foreground-mode","title":"How do I start Harvest in foreground mode?","text":"<p>See How do I start Harvest in debug mode?</p>"},{"location":"help/troubleshooting/#how-do-i-start-my-poller-with-only-one-collector","title":"How do I start my poller with only one collector?","text":"<p>Since a poller will start a large number of collectors (each collector-object pair is treated as a collector), it is often hard to find the issue you are looking for in the abundance of log messages. It might be therefore useful to start one single collector-object pair when troubleshooting. You can use the <code>--collectors</code> and <code>--objects</code> flags for that. For example, start only the ZapiPerf collector with the SystemNode object:</p> <p><code>harvest start my_poller --collectors ZapiPerf --objects SystemNode</code></p> <p>(To find to correct object name, check <code>conf/COLLECTOR/default.yaml</code> file of the collector).</p>"},{"location":"help/troubleshooting/#errors-in-the-log-file","title":"Errors in the log file","text":""},{"location":"help/troubleshooting/#some-of-my-clusters-are-not-showing-up-in-grafana","title":"Some of my clusters are not showing up in Grafana","text":"<p>The logs show these errors: <pre><code>context deadline exceeded (Client.Timeout or context cancellation while reading body)\n\nand then for each volume\n\nskipped instance [9c90facd-3730-48f1-b55c-afacc35c6dbe]: not found in cache\n</code></pre></p>"},{"location":"help/troubleshooting/#workarounds","title":"Workarounds","text":"<p>context deadline exceeded (Client.Timeout or context cancellation while reading body)</p> <p>means Harvest is timing out when talking to your cluster. This sometimes happens when you have a large number of resources (e.g. volumes).</p> <p>There are a few parameters that you can change to avoid this from happening. You can do this by editing the subtemplate of the resource affected. E.g. you can add the parameters in <code>conf/zapiperf/cdot/9.8.0/volume.yaml</code> or <code>conf/zapi/cdot/9.8.0/volume.yaml</code>. If the errors happen for most of the resources, you can add them in the main template of the collector (<code>conf/zapi/default.yaml</code> or <code>conf/zapiperf/default.yaml</code>) to apply them on all objects.</p>"},{"location":"help/troubleshooting/#client_timeout","title":"<code>client_timeout</code>","text":"<p>Increase the <code>client_timeout</code> value by adding a <code>client_timeout</code> line at the beginning of the template, like so:</p> <pre><code># increase the timeout to 1 minute\nclient_timeout: 1m\n</code></pre>"},{"location":"help/troubleshooting/#batch_size","title":"<code>batch_size</code>","text":"<p>Decrease the <code>batch_size</code> value by adding a <code>batch_size</code> line at the beginning of the template. The default value of this parameter is <code>500</code>. By decreasing it, the collector will fetch less instances during each API request. Example:</p> <pre><code># decrease number of instances to 200 for each API request\nbatch_size: 200\n</code></pre>"},{"location":"help/troubleshooting/#schedule","title":"<code>schedule</code>","text":"<p>If nothing else helps, you can increase the data poll interval of the collector (default is <code>1m</code> for ZapiPerf and <code>3m</code> for Zapi). You can do this either by adding a <code>schedule</code> attribute to the template or, if it already exists, by changing the <code>- data</code> line.</p> <p>Example for ZapiPerf:</p> <p><pre><code># increase data poll frequency to 2 minutes\nschedule:\n  - counter: 20m\n  - instance: 10m\n  - data: 2m\n</code></pre> Example for Zapi:</p> <pre><code># increase data poll frequency to 5 minutes\nschedule:\n  - instance: 10m\n  - data: 5m\n</code></pre>"},{"location":"help/troubleshooting/#prometheus-http-service-discovery-doesnt-work","title":"Prometheus HTTP Service Discovery doesn't work","text":"<p>Some things to check:</p> <ul> <li>Make sure the Harvest admin node is started via <code>bin/harvest admin start</code> and there are no errors printed to the console</li> <li>Make sure your <code>harvest.yml</code> includes a valid <code>Admin:</code> section</li> <li>Ensure <code>bin/harvest doctor</code> runs without error. If it does, include the output of <code>bin/harvest doctor --print</code> in Slack or your GitHub issue</li> <li>Ensure your <code>/etc/prometheus/prometheus.yml</code> has a scrape config with <code>http_sd_configs</code> and it points to the admin node's <code>ip:port</code></li> <li>Ensure there are no errors in your poller logs (<code>/var/log/harvest</code>) related to the poller publishing its Prometheus port to the admin node. Something like this should help narrow it down: <code>grep -R -E \"error.*poller.go\" /var/log/harvest/</code><ul> <li>If you see errors like <code>dial udp 1.1.1.1:80: connect: network is unreachable</code>, make sure your machine has a default route setup for your main interface</li> </ul> </li> <li>If the admin node is running, your <code>harvest.yml</code> includes the <code>Admin:</code> section, and your pollers are using the Prometheus exporter you should be able to curl the admin node endpoint for a list of running Harvest pollers like this: <pre><code>curl -s -k https://localhost:8887/api/v1/sd | jq .\n[\n  {\n    \"targets\": [\n      \":12994\"\n    ],\n    \"labels\": {\n      \"__meta_poller\": \"F2240-127-26\"\n    }\n  },\n  {\n    \"targets\": [\n      \":39000\"\n    ],\n    \"labels\": {\n      \"__meta_poller\": \"simple1\"\n    }\n  }\n]\n</code></pre></li> </ul>"},{"location":"help/troubleshooting/#how-do-i-run-harvest-commands-in-nabox","title":"How do I run Harvest commands in NAbox?","text":"<p>NAbox is a vApp running Alpine Linux and Docker. NAbox runs Harvest as a set of Docker containers. That means to execute Harvest commands on NAbox, you need to <code>exec</code> into the container by following these commands.</p> <ol> <li> <p>ssh into your NAbox instance</p> </li> <li> <p>Start bash in the Harvest container</p> </li> </ol> <pre><code>dc exec nabox-harvest2 bash\n</code></pre> <p>You should see no errors and your prompt will change to something like <code>root@nabox-harvest2:/app#</code></p> <p>Below are examples of running Harvest commands against a cluster named <code>umeng-aff300-05-06</code>. Replace with your cluster name as appropriate.</p> <pre><code># inside container\n\n&gt; cat /etc/issue\nDebian GNU/Linux 10 \\n \\l\n\n&gt; cd /netapp-harvest\nbin/harvest version\nharvest version 22.08.0-1 (commit 93db10a) (build date 2022-08-19T09:10:05-0400) linux/amd64\nchecking GitHub for latest... you have the latest \u2713\n\n# harvest.yml is found at /conf/harvest.yml\n\n&gt; bin/zapi --poller umeng-aff300-05-06 show system\nconnected to umeng-aff300-05-06 (NetApp Release 9.9.1P9X3: Tue Apr 19 19:05:24 UTC 2022)\n[results]                                          -                                   *\n  [build-timestamp]                                -                          1650395124\n  [is-clustered]                                   -                                true\n  [version]                                        - NetApp Release 9.9.1P9X3: Tue Apr 19 19:05:24 UTC 2022\n  [version-tuple]                                  -                                   *\n    [system-version-tuple]                         -                                   *\n      [generation]                                 -                                   9\n      [major]                                      -                                   9\n      [minor]                                      -                                   1\n\nbin/zapi -p umeng-aff300-05-06 show data --api environment-sensors-get-iter --max 10000 &gt; env-sensor.xml\n</code></pre> <p>The <code>env-sensor.xml</code> file will be written to the <code>/opt/packages/harvest2</code> directory on the host.</p> <p>If needed, you can <code>scp</code> that file off NAbox and share it with the Harvest team.</p>"},{"location":"help/troubleshooting/#rest-collector-auth-errors","title":"Rest Collector Auth errors?","text":"<p>If you are seeing errors like <code>User is not authorized</code> or <code>not authorized for that command</code> while using <code>Rest</code> Collector. Follow below steps to make sure permissions are set correctly.</p> <ol> <li>Verify that user has permissions for relevant authentication method.</li> </ol> <p><code>security login show -vserver ROOT_VSERVER -user-or-group-name harvest2 -application http</code></p> <p></p> <ol> <li>Verify that user has read-only permissions to api.</li> </ol> <pre><code>security login role show -role harvest2-role\n</code></pre> <p></p> <ol> <li>Verify if an entry is present for following command.</li> </ol> <pre><code>vserver services web access show -role harvest2-role -name rest\n</code></pre> <p>If It is missing then add an entry with following commands</p> <pre><code>vserver services web access create -vserver umeng-aff300-01-02 -name rest -role harvest2-role\n</code></pre>"},{"location":"help/troubleshooting/#why-do-i-have-gaps-in-my-dashboards","title":"Why do I have gaps in my dashboards?","text":"<p>Here are possible reasons and things to check:</p> <ul> <li>Prometheus <code>scrape_interval</code> found via (http://$promIP:9090/config)</li> <li>Prometheus log files</li> <li>Harvest collector scrape interval check your:<ul> <li><code>conf/zapi/default.yaml</code> - default for config is 3m</li> <li><code>conf/zapiperf/default.yaml</code> - default of perf is 1m</li> </ul> </li> <li>Check you poller logs for any errors or lag messages</li> <li>When using VictoriaMetrics, make sure your Prometheus exporter config includes <code>sort_labels: true</code>, since VictoriaMetrics will mark series stale if the label order changes between polls.</li> </ul>"},{"location":"help/troubleshooting/#nabox","title":"NABox","text":"<p>For NABox installations, refer to the NABox documentation on troubleshooting:</p> <p>NABox Troubleshooting</p>"},{"location":"install/containerd/","title":"Containerized Harvest on Mac using containerd","text":"<p>Harvest runs natively on a Mac already. If you need that, git clone and use <code>GOOS=darwin make build</code>. </p> <p>This page describes how to run Harvest on your Mac in a containerized environment (Compose, K8, etc.) The documentation below uses Rancher Desktop, but lima works just as well. Keep in mind, both of them are considered alpha. They work, but are still undergoing a lot of change.</p>"},{"location":"install/containerd/#setup","title":"Setup","text":"<p>We're going to: - Install and Start Rancher Desktop - (Optional) Create Harvest Docker image by following Harvest's existing documentation - Generate a Compose file following Harvest existing documentation - Concatenate the Prometheus/Grafana compose file with the harvest compose file since Rancher doesn't support multiple compose files yet   - Fixup the concatenated file - Start containers</p> <p>Under the hood, Rancher is using lima. If you want to skip Rancher and use lima directly that works too.</p>"},{"location":"install/containerd/#install-and-start-rancher-desktop","title":"Install and Start Rancher Desktop","text":"<p>We'll use brew to install Rancher.</p> <pre><code>brew install rancher\n</code></pre> <p>After Rancher Desktop installs, start it <code>Cmd + Space</code> type: Rancher and wait for it to start a VM and download images. Once everything is started continue.</p>"},{"location":"install/containerd/#create-harvest-docker-image","title":"Create Harvest Docker image","text":"<p>You only need to create a new image if you've made changes to Harvest. If you just want to use the latest version of Harvest, skip this step.</p> <p>These are the same steps outline on Building Harvest Docker Image except we replace <code>docker build</code> with <code>nerdctl</code> like so:</p> <pre><code>source .harvest.env\nnerdctl build -f container/onePollerPerContainer/Dockerfile --build-arg GO_VERSION=${GO_VERSION} -t harvest:latest . --no-cache \n</code></pre>"},{"location":"install/containerd/#generate-a-harvest-compose-file","title":"Generate a Harvest compose file","text":"<p>Follow the existing documentation to set up your <code>harvest.yml</code> file</p> <p>Create your <code>harvest-compose.yml</code> file like this:</p> <pre><code>docker run --rm \\\n  --env UID=$(id -u) --env GID=$(id -g) \\\n  --entrypoint \"bin/harvest\" \\\n  --volume \"$(pwd):/opt/temp\" \\\n  --volume \"$(pwd)/harvest.yml:/opt/harvest/harvest.yml\" \\\n  ghcr.io/netapp/harvest \\\n  generate docker full \\\n  --output harvest-compose.yml # --image tag, if you built a new image above\n</code></pre>"},{"location":"install/containerd/#combine-prometheusgrafana-and-harvest-compose-file","title":"Combine Prometheus/Grafana and Harvest compose file","text":"<p>Currently <code>nerdctl compose</code> does not support running with multiple compose files, so we'll concat the <code>prom-stack.yml</code> and the <code>harvest-compose.yml</code> into one file and then fix it up.</p> <pre><code>cat prom-stack.yml harvest-compose.yml &gt; both.yml\n\n# jump to line 45 and remove redundant version and services lines (lines 45, 46, 47 should be removed)\n# fix indentation of remaining lines - in vim, starting at line 46\n# Shift V\n# Shift G\n# Shift .\n# Esc\n# Shift ZZ\n</code></pre>"},{"location":"install/containerd/#start-containers","title":"Start containers","text":"<pre><code>nerdctl compose -f both.yml up -d\n\nnerdctl ps -a\n\nCONTAINER ID    IMAGE                               COMMAND                   CREATED               STATUS    PORTS                       NAMES\nbd7131291960    docker.io/grafana/grafana:latest    \"/run.sh\"                 About a minute ago    Up        0.0.0.0:3000-&gt;3000/tcp      grafana\nf911553a14e2    docker.io/prom/prometheus:latest    \"/bin/prometheus --c\u2026\"    About a minute ago    Up        0.0.0.0:9090-&gt;9090/tcp      prometheus\n037a4785bfad    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    About a minute ago    Up        0.0.0.0:15007-&gt;15007/tcp    poller_simple7_v21.11.0513\n03fb951cfe26    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    59 seconds ago        Up        0.0.0.0:15025-&gt;15025/tcp    poller_simple25_v21.11.0513\n049d0d65b434    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    About a minute ago    Up        0.0.0.0:16050-&gt;16050/tcp    poller_simple49_v21.11.0513\n0b77dd1bc0ff    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    About a minute ago    Up        0.0.0.0:16067-&gt;16067/tcp    poller_u2_v21.11.0513\n1cabd1633c6f    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    About a minute ago    Up        0.0.0.0:15015-&gt;15015/tcp    poller_simple15_v21.11.0513\n1d78c1bf605f    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    About a minute ago    Up        0.0.0.0:15062-&gt;15062/tcp    poller_sandhya_v21.11.0513\n286271eabc1d    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    About a minute ago    Up        0.0.0.0:15010-&gt;15010/tcp    poller_simple10_v21.11.0513\n29710da013d4    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    About a minute ago    Up        0.0.0.0:12990-&gt;12990/tcp    poller_simple1_v21.11.0513\n321ae28637b6    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    About a minute ago    Up        0.0.0.0:15020-&gt;15020/tcp    poller_simple20_v21.11.0513\n39c91ae54d68    docker.io/library/cbg:latest        \"bin/poller --poller\u2026\"    About a minute ago    Up        0.0.0.0:15053-&gt;15053/tcp    poller_simple-53_v21.11.0513\n\nnerdctl logs poller_simple1_v21.11.0513\nnerdctl compose -f both.yml down\n\n# http://localhost:9090/targets   Prometheus\n# http://localhost:3000           Grafana\n# http://localhost:15062/metrics  Poller metrics\n</code></pre>"},{"location":"install/containers/","title":"Docker","text":""},{"location":"install/containers/#overview","title":"Overview","text":"<p>Harvest is container-ready and supports several deployment options:</p> <ul> <li> <p>Stand-up Prometheus, Grafana, and Harvest via Docker Compose. Choose this if   you want to hit the ground running. Install, volume and network mounts automatically handled.</p> </li> <li> <p>Stand-up Harvest via Docker Compose that offers   more flexibility in configuration. Choose this if you only want to run Harvest containers. Since you pick-and-choose what gets built and how it's deployed, stronger familiarity with containers is   recommended.</p> </li> <li> <p>If you prefer Ansible, David Blackwell created   an Ansible script that   stands up Harvest, Grafana, and Prometheus.</p> </li> <li> <p>Want to run Harvest on a Mac   via containerd and Rancher Desktop? We got you   covered.</p> </li> <li> <p>K8 Deployment via Kompose</p> </li> </ul>"},{"location":"install/containers/#docker-compose","title":"Docker Compose","text":"<p>This is a quick way to install and get started with Harvest. Follow the four steps below to:</p> <ul> <li>Setup Harvest, Grafana, and Prometheus via Docker Compose</li> <li>Harvest dashboards are automatically imported and setup in Grafana with a Prometheus data source</li> <li>A separate poller container is created for each monitored cluster</li> <li>All pollers are automatically added as Prometheus scrape targets</li> </ul>"},{"location":"install/containers/#setup-harvestyml","title":"Setup harvest.yml","text":"<ul> <li>Create a <code>harvest.yml</code> file with your cluster details, below is an example with annotated comments. Modify as needed    for your scenario.</li> </ul> <p>This config is using the Prometheus exporter port_range feature, so you don't have to manage the Prometheus exporter port mappings for each poller.</p> <pre><code>Exporters:\n  prometheus1:\n    exporter: Prometheus\n    addr: 0.0.0.0\n    port_range: 2000-2030  # &lt;====== adjust to be greater than equal to the number of monitored clusters\n\nDefaults:\n  collectors:\n    - Zapi\n    - ZapiPerf\n    - EMS\n  use_insecure_tls: true   # &lt;====== adjust as needed to enable/disable TLS checks \n  exporters:\n    - prometheus1\n\nPollers:\n  infinity:                # &lt;====== add your cluster(s) here, they use the exporter defined three lines above\n    datacenter: DC-01\n    addr: 10.0.1.2\n    auth_style: basic_auth\n    username: user\n    password: 123#abc\n  # next cluster ....  \n</code></pre>"},{"location":"install/containers/#generate-a-docker-compose-for-your-pollers","title":"Generate a Docker compose for your Pollers","text":"<ul> <li>Generate a Docker compose file from your <code>harvest.yml</code></li> </ul> <pre><code>docker run --rm \\\n  --env UID=$(id -u) --env GID=$(id -g) \\\n  --entrypoint \"bin/harvest\" \\\n  --volume \"$(pwd):/opt/temp\" \\\n  --volume \"$(pwd)/harvest.yml:/opt/harvest/harvest.yml\" \\\n  ghcr.io/netapp/harvest \\\n  generate docker full \\\n  --output harvest-compose.yml\n</code></pre> <p>By default, the above command uses the harvest configuration file(<code>harvest.yml</code>) located in the current directory. If you want to use a harvest config from a different location.</p> What if my harvest configuration file is somewhere else or not named harvest.yml <p>Use the following docker run command, updating the <code>HYML</code> variable with the absolute path to your <code>harvest.yml</code>.</p> <pre><code>HYML=\"/opt/custom_harvest.yml\"; \\\ndocker run --rm \\\n--env UID=$(id -u) --env GID=$(id -g) \\\n--entrypoint \"bin/harvest\" \\\n--volume \"$(pwd):/opt/temp\" \\\n--volume \"${HYML}:${HYML}\" \\\nghcr.io/netapp/harvest:latest \\\ngenerate docker full \\\n--output harvest-compose.yml \\\n--config \"${HYML}\"\n</code></pre> <p><code>generate docker full</code> does two things:</p> <ol> <li>Creates a Docker compose file with a container for each Harvest poller defined in your <code>harvest.yml</code></li> <li>Creates a matching Prometheus service discovery file for each Harvest poller (located    in <code>container/prometheus/harvest_targets.yml</code>). Prometheus uses this file to scrape the Harvest pollers.</li> </ol>"},{"location":"install/containers/#start-everything","title":"Start everything","text":"<p>Bring everything up </p> <pre><code>docker compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans\n</code></pre>"},{"location":"install/containers/#note-on-docker-logging-configuration","title":"Note on Docker Logging Configuration","text":"<p>By default, Docker uses the <code>json-file</code> logging driver which does not limit the size of the logs. This can cause your system to run out of disk space. Docker provides several options for logging configuration, including different logging drivers and options for log rotation.</p> <p>Docker recommends using the <code>local</code> driver to prevent disk-exhaustion. More details can be found in Docker logging documentation</p>"},{"location":"install/containers/#prometheus-and-grafana","title":"Prometheus and Grafana","text":"<p>The <code>prom-stack.yml</code> compose file creates a <code>frontend</code> and <code>backend</code> network. Prometheus and Grafana publish their admin ports on the front-end network and are routable to the local machine. By default, the Harvest pollers are part of the backend network and also expose their Prometheus web end-points.  If you do not want their end-points exposed, add the <code>--port=false</code> option to the <code>generate</code> sub-command in the previous step.</p>"},{"location":"install/containers/#prometheus","title":"Prometheus","text":"<p>After bringing up the <code>prom-stack.yml</code> compose file, you can check Prometheus's list of targets at <code>http://IP_OF_PROMETHEUS:9090/targets</code>.</p>"},{"location":"install/containers/#customize-prometheuss-retention-time","title":"Customize Prometheus's Retention Time","text":"<p>By default, <code>prom-stack.yml</code> is configured for a one year data retention period. To increase this, for example, to two years, you can create a specific configuration file and make your changes there. This prevents your custom settings from being overwritten if you regenerate the default <code>prom-stack.yml</code> file. Here's the process:</p> <ul> <li>Copy the original <code>prom-stack.yml</code> to a new file named <code>prom-stack-prod.yml</code>:</li> </ul> <pre><code>cp prom-stack.yml prom-stack-prod.yml\n</code></pre> <ul> <li>Edit <code>prom-stack-prod.yml</code> to include the extended data retention setting by updating the <code>--storage.tsdb.retention.time=2y</code> line under the Prometheus service's <code>command</code> section:</li> </ul> <pre><code>command:\n  - '--config.file=/etc/prometheus/prometheus.yml'\n  - '--storage.tsdb.path=/prometheus'\n  - '--storage.tsdb.retention.time=2y'       # Sets data retention to 2 years\n  - '--web.console.libraries=/usr/share/prometheus/console_libraries'\n  - '--web.console.templates=/usr/share/prometheus/consoles'\n</code></pre> <ul> <li>Save the changes to <code>prom-stack-prod.yml</code>.</li> </ul> <p>Now, you can start your Docker containers with the updated configuration that includes the 1-year data retention period by executing the command below:</p> <pre><code>docker compose -f prom-stack-prod.yml -f harvest-compose.yml up -d --remove-orphans\n</code></pre>"},{"location":"install/containers/#grafana","title":"Grafana","text":"<p>After bringing up the <code>prom-stack.yml</code> compose file, you can access Grafana at <code>http://IP_OF_GRAFANA:3000</code>.</p> <p>You will be prompted to create a new password the first time you log in. Grafana's default credentials are</p> <pre><code>username: admin\npassword: admin\n</code></pre>"},{"location":"install/containers/#manage-pollers","title":"Manage pollers","text":""},{"location":"install/containers/#how-do-i-add-a-new-poller","title":"How do I add a new poller?","text":"<p>Note: All of your template customizations should follow the custom.yaml approach mentioned in template customization, otherwise any changes you've made in the <code>conf</code> directory will be overwritten.</p> <ol> <li>Add poller to <code>harvest.yml</code></li> <li>Regenerate compose file by running harvest generate</li> <li>Run docker compose up, for example,</li> </ol> <pre><code>docker compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans\n</code></pre>"},{"location":"install/containers/#stop-all-containers","title":"Stop all containers","text":"<pre><code>docker compose -f prom-stack.yml -f harvest-compose.yml down\n</code></pre> <p>If you encounter the following error message while attempting to stop your Docker containers using <code>docker-compose down</code></p> <pre><code>Error response from daemon: Conflict. The container name \"/poller-u2\" is already in use by container\n</code></pre> <p>This error is likely due to running <code>docker-compose down</code> from a different directory than where you initially ran <code>docker-compose up</code>.</p> <p>To resolve this issue, make sure to run the <code>docker-compose down</code> command from the same directory where you ran <code>docker-compose up</code>. This will ensure that Docker can correctly match the container names and IDs with the directory you are working in.  Alternatively, you can stop the Harvest, Prometheus, and Grafana containers by using the following command:</p> <pre><code>docker ps -aq --filter \"name=prometheus\" --filter \"name=grafana\" --filter \"name=poller-\" | xargs docker stop | xargs docker rm\n</code></pre> <p>Note: Deleting or stopping Docker containers does not remove the data stored in Docker volumes.</p>"},{"location":"install/containers/#upgrade-harvest","title":"Upgrade Harvest","text":"<p>Note: If you want to keep your historical Prometheus data, and you set up your Docker Compose workflow before Harvest <code>22.11</code>, please read how to migrate your Prometheus volume before continuing with the upgrade steps below.</p> <p>If you need to customize your Prometheus configuration, such as changing the data retention period, please refer to the instructions on customizing the Prometheus configuration.</p> <p>To upgrade Harvest:</p> <ol> <li> <p>Retrieve the most recent version of the Harvest Docker image by executing the following command.This is needed since the new version may contain new templates, dashboards, or other files not included in the Docker    image.    <pre><code>docker pull ghcr.io/netapp/harvest\n</code></pre></p> </li> <li> <p>Stop all containers</p> </li> <li> <p>Regenerate your <code>harvest-compose.yml</code> file by    running harvest generate. Make sure you don't skip this step. It is essential as it updates local copies of templates and dashboards, which are then mounted to the containers. If this step is skipped, Harvest will run with older templates and dashboards which will likely cause problems.    By default, generate will use the <code>latest</code> tag. If you want to upgrade to a <code>nightly</code> build see the twisty.</p> I want to upgrade to a nightly build <p>Tell the <code>generate</code> cmd to use a different tag like so:</p> <pre><code>docker run --rm \\\n  --env UID=$(id -u) --env GID=$(id -g) \\\n  --entrypoint \"bin/harvest\" \\\n  --volume \"$(pwd):/opt/temp\" \\\n  --volume \"$(pwd)/harvest.yml:/opt/harvest/harvest.yml\" \\\n  ghcr.io/netapp/harvest:nightly \\\n  generate docker full \\\n  --image ghcr.io/netapp/harvest:nightly \\\n  --output harvest-compose.yml\n</code></pre> </li> <li> <p>Restart your containers using the following:</p> <pre><code>docker compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans\n</code></pre> Troubleshooting <p>If you encounter the following error:</p> <pre><code>network harvest_backend was found but has incorrect label com.docker.compose.network set to \"harvest_backend\"\n</code></pre> <p>Remove the conflicting networks:</p> <pre><code>docker network rm harvest_backend harvest_frontend\n</code></pre> <p>Then, restart your containers again using the command above.</p> </li> </ol>"},{"location":"install/containers/#building-harvest-docker-image","title":"Building Harvest Docker Image","text":"<p>Building a custom Harvest Docker image is only necessary if you require a tailored solution. If your intention is to run Harvest using Docker without any customizations, please refer to the Overview section above.</p> <pre><code>source .harvest.env\ndocker build -f container/onePollerPerContainer/Dockerfile --build-arg GO_VERSION=${GO_VERSION} -t harvest:latest . --no-cache\n</code></pre>"},{"location":"install/harvest-containers/","title":"Harvest containers","text":"<p>Follow this method if your goal is to establish a separate harvest container for each poller defined in <code>harvest.yml</code> file. Please note that these containers must be incorporated into your current infrastructure, which might include systems like Prometheus or Grafana.</p>"},{"location":"install/harvest-containers/#setup-harvestyml","title":"Setup harvest.yml","text":"<ul> <li>Create a <code>harvest.yml</code> file with your cluster details, below is an example with annotated comments. Modify as needed   for your scenario.</li> </ul> <p>This config is using the Prometheus exporter port_range feature, so you don't have to manage the Prometheus exporter port mappings for each poller.</p> <pre><code>Exporters:\n  prometheus1:\n    exporter: Prometheus\n    addr: 0.0.0.0\n    port_range: 2000-2030  # &lt;====== adjust to be greater than equal to the number of monitored clusters\n\nDefaults:\n  collectors:\n    - Zapi\n    - ZapiPerf\n    - EMS\n  use_insecure_tls: true   # &lt;====== adjust as needed to enable/disable TLS checks \n  exporters:\n    - prometheus1\n\nPollers:\n  infinity:                # &lt;====== add your cluster(s) here, they use the exporter defined three lines above\n    datacenter: DC-01\n    addr: 10.0.1.2\n    auth_style: basic_auth\n    username: user\n    password: 123#abc\n  # next cluster ....  \n</code></pre>"},{"location":"install/harvest-containers/#generate-a-docker-compose-for-your-pollers","title":"Generate a Docker compose for your Pollers","text":"<ul> <li>Generate a Docker compose file from your <code>harvest.yml</code></li> </ul> <pre><code>docker run --rm \\\n  --env UID=$(id -u) --env GID=$(id -g) \\\n  --entrypoint \"bin/harvest\" \\\n  --volume \"$(pwd):/opt/temp\" \\\n  --volume \"$(pwd)/harvest.yml:/opt/harvest/harvest.yml\" \\\n  ghcr.io/netapp/harvest \\\n  generate docker \\\n  --output harvest-compose.yml\n</code></pre>"},{"location":"install/harvest-containers/#start-everything","title":"Start everything","text":"<p>Bring everything up </p> <pre><code>docker compose -f harvest-compose.yml up -d --remove-orphans\n</code></pre>"},{"location":"install/harvest-containers/#manage-pollers","title":"Manage pollers","text":""},{"location":"install/harvest-containers/#how-do-i-add-a-new-poller","title":"How do I add a new poller?","text":"<p>Note: All of your template customizations should follow the custom.yaml approach mentioned in template customization, otherwise any changes you've made in the <code>conf</code> directory will be overwritten.</p> <ol> <li>Add poller to <code>harvest.yml</code></li> <li>Regenerate compose file by running harvest generate</li> <li>Run docker compose up, for example,</li> </ol> <pre><code>docker compose -f harvest-compose.yml up -d --remove-orphans\n</code></pre>"},{"location":"install/harvest-containers/#stop-all-containers","title":"Stop all containers","text":"<pre><code>docker compose-f harvest-compose.yml down\n</code></pre>"},{"location":"install/harvest-containers/#upgrade-harvest","title":"Upgrade Harvest","text":"<p>To upgrade Harvest:</p> <ol> <li> <p>Retrieve the most recent version of the Harvest Docker image by executing the following command.This is needed since the new version may contain new templates, dashboards, or other files not included in the Docker    image.    <pre><code>docker pull ghcr.io/netapp/harvest\n</code></pre></p> </li> <li> <p>Stop all containers</p> </li> <li> <p>Regenerate your <code>harvest-compose.yml</code> file by    running harvest generate    By default, generate will use the <code>latest</code> tag. If you want to upgrade to a <code>nightly</code> build see the twisty.</p> I want to upgrade to a nightly build <p>Tell the <code>generate</code> cmd to use a different tag like so:</p> <pre><code>docker run --rm \\\n  --env UID=$(id -u) --env GID=$(id -g) \\\n  --entrypoint \"bin/harvest\" \\\n  --volume \"$(pwd):/opt/temp\" \\\n  --volume \"$(pwd)/harvest.yml:/opt/harvest/harvest.yml\" \\\n  ghcr.io/netapp/harvest:nightly \\\n  generate docker \\\n  --image ghcr.io/netapp/harvest:nightly \\\n  --output harvest-compose.yml\n</code></pre> </li> <li> <p>Restart your containers using the following:</p> </li> </ol> <pre><code>docker compose -f harvest-compose.yml up -d --remove-orphans\n</code></pre>"},{"location":"install/k8/","title":"K8 Deployment","text":"<p>The following steps are provided for reference purposes only. Depending on the specifics of your k8 configuration, you may need to make modifications to the steps or files as necessary.</p>"},{"location":"install/k8/#requirements","title":"Requirements","text":"<ul> <li>Kompose: <code>v1.25</code> or higher</li> </ul>"},{"location":"install/k8/#deployment","title":"Deployment","text":"<ul> <li>Local k8 Deployment</li> <li>Cloud Deployment</li> </ul>"},{"location":"install/k8/#local-k8-deployment","title":"Local k8 Deployment","text":"<p>To run Harvest resources in Kubernetes, please execute the following commands:</p> <ol> <li>After adding your clusters to <code>harvest.yml</code>, generate <code>harvest-compose.yml</code> and <code>prom-stack.yml</code>.</li> </ol> <pre><code>docker run --rm \\\n  --env UID=$(id -u) --env GID=$(id -g) \\\n  --entrypoint \"bin/harvest\" \\\n  --volume \"$(pwd):/opt/temp\" \\\n  --volume \"$(pwd)/harvest.yml:/opt/harvest/harvest.yml\" \\\n  ghcr.io/netapp/harvest \\\n  generate docker full \\\n  --output harvest-compose.yml\n</code></pre> example harvest.yml <p> <pre><code>Tools:\nExporters:\n    prometheus1:\n        exporter: Prometheus\n        port_range: 12990-14000\nDefaults:\n    use_insecure_tls: true\n    collectors:\n      - Zapi\n      - ZapiPerf\n    exporters:\n      - prometheus1\nPollers:\n    u2:\n        datacenter: u2\n        addr: ADDRESS\n        username: USER\n        password: PASS\n</code></pre> </p> harvest-compose.yml <p> <pre><code>version: \"3.7\"\n\nservices:\n\n  u2:\n    image: ghcr.io/netapp/harvest:latest\n    container_name: poller-u2\n    restart: unless-stopped\n    ports:\n      - 12990:12990\n    command: '--poller u2 --promPort 12990 --config /opt/harvest.yml'\n    volumes:\n      - /Users/harvest/conf:/opt/harvest/conf\n      - /Users/harvest/cert:/opt/harvest/cert\n      - /Users/harvest/harvest.yml:/opt/harvest.yml\n    networks:\n      - backend\n</code></pre> </p> <ol> <li>Using <code>kompose</code>, convert <code>harvest-compose.yml</code> and <code>prom-stack.yml</code> into Kubernetes resources and save them as <code>kub.yaml</code>.</li> </ol> <pre><code>kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n</code></pre> kub.yaml <p> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n    kompose.service.type: nodeport\n    kompose.version: 1.28.0 (HEAD)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: grafana\n  name: grafana\nspec:\n  ports:\n    - name: \"3000\"\n      port: 3000\n      targetPort: 3000\n  selector:\n    io.kompose.service: grafana\n  type: NodePort\nstatus:\n  loadBalancer: {}\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n    kompose.service.type: nodeport\n    kompose.version: 1.28.0 (HEAD)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: prometheus\n  name: prometheus\nspec:\n  ports:\n    - name: \"9090\"\n      port: 9090\n      targetPort: 9090\n  selector:\n    io.kompose.service: prometheus\n  type: NodePort\nstatus:\n  loadBalancer: {}\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n    kompose.version: 1.28.0 (HEAD)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: u2\n  name: u2\nspec:\n  ports:\n    - name: \"12990\"\n      port: 12990\n      targetPort: 12990\n  selector:\n    io.kompose.service: u2\nstatus:\n  loadBalancer: {}\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n    kompose.service.type: nodeport\n    kompose.version: 1.28.0 (HEAD)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: grafana\n  name: grafana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: grafana\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n        kompose.service.type: nodeport\n        kompose.version: 1.28.0 (HEAD)\n      creationTimestamp: null\n      labels:\n        io.kompose.network/harvest-backend: \"true\"\n        io.kompose.network/harvest-frontend: \"true\"\n        io.kompose.service: grafana\n    spec:\n      containers:\n        - image: grafana/grafana:8.3.4\n          name: grafana\n          ports:\n            - containerPort: 3000\n          resources: {}\n          volumeMounts:\n            - mountPath: /var/lib/grafana\n              name: grafana-data\n            - mountPath: /etc/grafana/provisioning\n              name: grafana-hostpath1\n      restartPolicy: Always\n      volumes:\n        - hostPath:\n            path: /Users/harvest\n          name: grafana-data\n        - hostPath:\n            path: /Users/harvest/grafana\n          name: grafana-hostpath1\nstatus: {}\n\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  creationTimestamp: null\n  name: harvest-backend\nspec:\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              io.kompose.network/harvest-backend: \"true\"\n  podSelector:\n    matchLabels:\n      io.kompose.network/harvest-backend: \"true\"\n\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  creationTimestamp: null\n  name: harvest-frontend\nspec:\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              io.kompose.network/harvest-frontend: \"true\"\n  podSelector:\n    matchLabels:\n      io.kompose.network/harvest-frontend: \"true\"\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n    kompose.service.type: nodeport\n    kompose.version: 1.28.0 (HEAD)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: prometheus\n  name: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: prometheus\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n        kompose.service.type: nodeport\n        kompose.version: 1.28.0 (HEAD)\n      creationTimestamp: null\n      labels:\n        io.kompose.network/harvest-backend: \"true\"\n        io.kompose.service: prometheus\n    spec:\n      containers:\n        - args:\n            - --config.file=/etc/prometheus/prometheus.yml\n            - --storage.tsdb.path=/prometheus\n            - --web.console.libraries=/usr/share/prometheus/console_libraries\n            - --web.console.templates=/usr/share/prometheus/consoles\n          image: prom/prometheus:v2.33.1\n          name: prometheus\n          ports:\n            - containerPort: 9090\n          resources: {}\n          volumeMounts:\n            - mountPath: /etc/prometheus\n              name: prometheus-hostpath0\n            - mountPath: /prometheus\n              name: prometheus-data\n      restartPolicy: Always\n      volumes:\n        - hostPath:\n            path: /Users/harvest/container/prometheus\n          name: prometheus-hostpath0\n        - hostPath:\n            path: /Users/harvest\n          name: prometheus-data\nstatus: {}\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n    kompose.version: 1.28.0 (HEAD)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: u2\n  name: u2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: u2\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert --file harvest-compose.yml --file prom-stack.yml --out kub.yaml --volumes hostPath\n        kompose.version: 1.28.0 (HEAD)\n      creationTimestamp: null\n      labels:\n        io.kompose.network/harvest-backend: \"true\"\n        io.kompose.service: u2\n    spec:\n      containers:\n        - args:\n            - --poller\n            - u2\n            - --promPort\n            - \"12990\"\n            - --config\n            - /opt/harvest.yml\n          image: ghcr.io/netapp/harvest:latest\n          name: poller-u2\n          ports:\n            - containerPort: 12990\n          resources: {}\n          volumeMounts:\n            - mountPath: /opt/harvest/conf\n              name: u2-hostpath0\n            - mountPath: /opt/harvest/cert\n              name: u2-hostpath1\n            - mountPath: /opt/harvest.yml\n              name: u2-hostpath2\n      restartPolicy: Always\n      volumes:\n        - hostPath:\n            path: /Users/harvest/conf\n          name: u2-hostpath0\n        - hostPath:\n            path: /Users/harvest/cert\n          name: u2-hostpath1\n        - hostPath:\n            path: /Users/harvest/harvest.yml\n          name: u2-hostpath2\nstatus: {}\n</code></pre> </p> <ol> <li>Apply <code>kub.yaml</code> to k8.</li> </ol> <pre><code>kubectl apply --filename kub.yaml\n</code></pre> <ol> <li>List running pods.</li> </ol> <pre><code>kubectl get pods\n</code></pre> pods <p> <pre><code>NAME                          READY   STATUS    RESTARTS   AGE\nprometheus-666fc7b64d-xfkvk   1/1     Running   0          43m\ngrafana-7cd8bdc9c9-wmsxh      1/1     Running   0          43m\nu2-7dfb76b5f6-zbfm6           1/1     Running   0          43m\n</code></pre> </p>"},{"location":"install/k8/#remove-all-harvest-resources-from-k8","title":"Remove all Harvest resources from k8","text":"<p><code>kubectl delete --filename kub.yaml</code></p>"},{"location":"install/k8/#helm-chart","title":"Helm Chart","text":"<p>Generate helm charts</p> <pre><code>kompose convert --file harvest-compose.yml --file prom-stack.yml --chart --volumes hostPath --out harvestchart\n</code></pre>"},{"location":"install/k8/#cloud-deployment","title":"Cloud Deployment","text":"<p>We will use <code>configMap</code> to generate Kubernetes resources for deploying Harvest pollers in a cloud environment. Please note the following assumptions for the steps below:</p> <ul> <li>The steps provided are solely for the deployment of Harvest poller pods. Separate configurations are required to set up Prometheus and Grafana.</li> <li> <p>Networking between Harvest and Prometheus must be configured, and this can be accomplished by adding the network configuration in <code>harvest-compose.yaml</code>.</p> </li> <li> <p>After configuring the clusters in <code>harvest.yml</code>, generate <code>harvest-compose.yml</code>. We also want to remove the <code>conf</code> directory from the <code>harvest-compose.yml</code> file, otherwise <code>kompose</code> will create an empty configMap for it. We'll remove the <code>conf</code> directory by commenting out that line using <code>sed</code>.  </p> </li> </ul> <pre><code>docker run --rm \\\n  --env UID=$(id -u) --env GID=$(id -g) \\\n  --entrypoint \"bin/harvest\" \\\n  --volume \"$(pwd):/opt/temp\" \\\n  --volume \"$(pwd)/harvest.yml:/opt/harvest/harvest.yml\" \\\n  ghcr.io/netapp/harvest \\\n  generate docker full \\\n  --output harvest-compose.yml\n\nsed -i '/\\/conf/s/^/#/g' harvest-compose.yml\n</code></pre> harvest-compose.yml <p> <pre><code>version: \"3.7\"\n\nservices:\n\n  u2:\n    image: ghcr.io/netapp/harvest:latest\n    container_name: poller-u2\n    restart: unless-stopped\n    ports:\n      - 12990:12990\n    command: '--poller u2 --promPort 12990 --config /opt/harvest.yml'\n    volumes:\n      #      - /Users/harvest/conf:/opt/harvest/conf\n      - /Users/harvest/cert:/opt/harvest/cert\n      - /Users/harvest/harvest.yml:/opt/harvest.yml\n</code></pre> </p> <ol> <li>Using <code>kompose</code>, convert <code>harvest-compose.yml</code> into Kubernetes resources and save them as <code>kub.yaml</code>.</li> </ol> <pre><code>kompose convert --file harvest-compose.yml --volumes configMap -o kub.yaml\n</code></pre> kub.yaml <p> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert --file harvest-compose.yml --volumes configMap -o kub.yaml\n    kompose.version: 1.28.0 (HEAD)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: u2\n  name: u2\nspec:\n  ports:\n    - name: \"12990\"\n      port: 12990\n      targetPort: 12990\n  selector:\n    io.kompose.service: u2\nstatus:\n  loadBalancer: {}\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert --file harvest-compose.yml --volumes configMap -o kub.yaml\n    kompose.version: 1.28.0 (HEAD)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: u2\n  name: u2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: u2\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert --file harvest-compose.yml --volumes configMap -o kub.yaml\n        kompose.version: 1.28.0 (HEAD)\n      creationTimestamp: null\n      labels:\n        io.kompose.network/harvest-default: \"true\"\n        io.kompose.service: u2\n    spec:\n      containers:\n        - args:\n            - --poller\n            - u2\n            - --promPort\n            - \"12990\"\n            - --config\n            - /opt/harvest.yml\n          image: ghcr.io/netapp/harvest:latest\n          name: poller-u2\n          ports:\n            - containerPort: 12990\n          resources: {}\n          volumeMounts:\n            - mountPath: /opt/harvest/cert\n              name: u2-cm0\n            - mountPath: /opt/harvest.yml\n              name: u2-cm1\n              subPath: harvest.yml\n      restartPolicy: Always\n      volumes:\n        - configMap:\n            name: u2-cm0\n          name: u2-cm0\n        - configMap:\n            items:\n              - key: harvest.yml\n                path: harvest.yml\n            name: u2-cm1\n          name: u2-cm1\nstatus: {}\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  creationTimestamp: null\n  labels:\n    io.kompose.service: u2\n  name: u2-cm0\n\n---\napiVersion: v1\ndata:\n  harvest.yml: |+\n    Tools:\n    Exporters:\n        prometheus1:\n            exporter: Prometheus\n            port_range: 12990-14000\n            add_meta_tags: false\n    Defaults:\n        use_insecure_tls: true\n        prefer_zapi: true\n    Pollers:\n\n        u2:\n            datacenter: u2\n            addr: ADDRESS\n            username: USER\n            password: PASS\n            collectors:\n                - Rest\n            exporters:\n                - prometheus1\n\nkind: ConfigMap\nmetadata:\n  annotations:\n    use-subpath: \"true\"\n  creationTimestamp: null\n  labels:\n    io.kompose.service: u2\n  name: u2-cm1\n\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  creationTimestamp: null\n  name: harvest-default\nspec:\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              io.kompose.network/harvest-default: \"true\"\n  podSelector:\n    matchLabels:\n      io.kompose.network/harvest-default: \"true\"\n</code></pre> </p> <ol> <li>Apply <code>kub.yaml</code> to k8.</li> </ol> <pre><code>kubectl apply --filename kub.yaml\n</code></pre> <ol> <li>List running pods.</li> </ol> <pre><code>kubectl get pods\n</code></pre> pods <p> <pre><code>NAME                  READY   STATUS    RESTARTS   AGE\nu2-6864cc7dbc-v6444   1/1     Running   0          6m27s\n</code></pre> </p>"},{"location":"install/k8/#remove-all-harvest-resources-from-k8_1","title":"Remove all Harvest resources from k8","text":"<p><code>kubectl delete --filename kub.yaml</code></p>"},{"location":"install/k8/#helm-chart_1","title":"Helm Chart","text":"<p>Generate helm charts</p> <pre><code>kompose convert --file harvest-compose.yml --chart --volumes configMap --out harvestchart\n</code></pre>"},{"location":"install/native/","title":"Native","text":""},{"location":"install/native/#installation","title":"Installation","text":"<p>Visit the Releases page and copy the <code>tar.gz</code> link for the latest release. For example, to download the <code>23.08.0</code> release:</p> <pre><code>VERSION=23.08.0\nwget https://github.com/NetApp/harvest/releases/download/v${VERSION}/harvest-${VERSION}-1_linux_amd64.tar.gz\ntar -xvf harvest-${VERSION}-1_linux_amd64.tar.gz\ncd harvest-${VERSION}-1_linux_amd64\n\n# Run Harvest with the default unix localhost collector\nbin/harvest start\n</code></pre> With curl <p>If you don't have <code>wget</code> installed, you can use <code>curl</code> like so:</p> <pre><code>curl -L -O https://github.com/NetApp/harvest/releases/download/v22.08.0/harvest-22.08.0-1_linux_amd64.tar.gz\n</code></pre>"},{"location":"install/native/#upgrade","title":"Upgrade","text":"<p>Stop Harvest: <pre><code>cd &lt;existing harvest directory&gt;\nbin/harvest stop\n</code></pre></p> <p>Verify that all pollers have stopped: <pre><code>bin/harvest status\nor\npgrep --full '\\-\\-poller'  # should return nothing if all pollers are stopped\n</code></pre></p> <p>Download the latest release and extract it to a new directory. For example, to upgrade to the 23.11.0 release:</p> <pre><code>VERSION=23.11.0\nwget https://github.com/NetApp/harvest/releases/download/v${VERSION}/harvest-${VERSION}-1_linux_amd64.tar.gz\ntar -xvf harvest-${VERSION}-1_linux_amd64.tar.gz\ncd harvest-${VERSION}-1_linux_amd64\n</code></pre> <p>Copy your old <code>harvest.yml</code> into the new install directory: <pre><code>cp /path/to/old/harvest/harvest.yml /path/to/new/harvest/harvest.yml\n</code></pre></p> <p>After upgrade, re-import all dashboards (either <code>bin/harvest grafana import</code> cli or via the Grafana UI) to get any new enhancements in dashboards. For more details, see the dashboards documentation.</p> <p>It's best to run Harvest as a non-root user. Make sure the user running Harvest can write to <code>/var/log/harvest/</code> or tell Harvest to write the logs somewhere else with the <code>HARVEST_LOGS</code> environment variable.</p> <p>If something goes wrong, examine the logs files in <code>/var/log/harvest</code>, check out the troubleshooting section on the wiki and jump onto Discord and ask for help.</p>"},{"location":"install/overview/","title":"Overview","text":"<p>Get up and running with Harvest on your preferred platform. We provide pre-compiled binaries for Linux, RPMs, Debs, as well  as prebuilt container images for both nightly  and stable releases.</p> <ul> <li>Binaries for Linux</li> <li>RPM and Debs</li> <li>Containers</li> </ul>"},{"location":"install/overview/#nabox","title":"Nabox","text":"<p>Instructions on how to install Harvest via NAbox.</p>"},{"location":"install/overview/#source","title":"Source","text":"<p>To build Harvest from source code follow these steps.</p> <ol> <li><code>git clone https://github.com/NetApp/harvest.git</code></li> <li>cd <code>harvest</code></li> <li>check the version of go required in the <code>go.mod</code> file</li> <li>ensure you have a working Go environment at that version or newer. Go installs found here. </li> <li><code>make build</code> (if you want to run Harvest from a Mac use <code>GOOS=darwin make build</code>) </li> <li><code>bin/harvest version</code></li> </ol> <p>Checkout the <code>Makefile</code> for other targets of interest.</p>"},{"location":"install/package-managers/","title":"Package Managers","text":""},{"location":"install/package-managers/#redhat","title":"Redhat","text":"<p>Installation and upgrade of the Harvest package may require root or administrator privileges</p>"},{"location":"install/package-managers/#installation","title":"Installation","text":"<p>Download the latest rpm of Harvest from the releases tab and install with yum.</p> <pre><code>sudo yum install harvest.XXX.rpm\n</code></pre>"},{"location":"install/package-managers/#upgrade","title":"Upgrade","text":"<p>Download the latest rpm of Harvest from the releases tab and upgrade with yum.</p> <pre><code>sudo yum upgrade harvest.XXX.rpm\n</code></pre> <p>Once the installation or upgrade has finished, edit the harvest.yml configuration file located in <code>/opt/harvest/harvest.yml</code></p> <p>After editing <code>/opt/harvest/harvest.yml</code>, manage Harvest with <code>systemctl start|stop|restart harvest</code>.</p> <p>After upgrade, re-import all dashboards (either <code>bin/harvest grafana import</code> cli or via the Grafana UI) to get any new enhancements in dashboards. For more details, see the dashboards documentation.</p> <p>To ensure that you don't run into permission issues, make sure you manage Harvest using <code>systemctl</code> instead of running the harvest binary directly.</p> Changes install makes <ul> <li>Directories <code>/var/log/harvest/</code> and <code>/var/log/run/</code> are created</li> <li>A <code>harvest</code> user and group are created and the installed files are chowned to harvest</li> <li>Systemd <code>/etc/systemd/system/harvest.service</code> file is created and enabled</li> </ul>"},{"location":"install/package-managers/#debian","title":"Debian","text":"<p>Installation and upgrade of the Harvest package may require root or administrator privileges</p>"},{"location":"install/package-managers/#installation_1","title":"Installation","text":"<p>Download the latest deb of Harvest from the releases tab and install with apt.</p> <pre><code>sudo apt install ./harvest-&lt;RELEASE&gt;.amd64.deb\n</code></pre>"},{"location":"install/package-managers/#upgrade_1","title":"Upgrade","text":"<p>Download the latest deb of Harvest from the releases tab and upgrade with apt.</p> <pre><code>sudo apt install --only-upgrade ./harvest-&lt;RELEASE&gt;.amd64.deb\n</code></pre> <p>Once the installation or upgrade has finished, edit the harvest.yml configuration file located in <code>/opt/harvest/harvest.yml</code></p> <p>After editing <code>/opt/harvest/harvest.yml</code>, manage Harvest with <code>systemctl start|stop|restart harvest</code>.</p> <p>After upgrade, re-import all dashboards (either <code>bin/harvest grafana import</code> cli or via the Grafana UI) to get any new enhancements in dashboards. For more details, see the dashboards documentation.</p> <p>To ensure that you don't run into permission issues, make sure you manage Harvest using <code>systemctl</code> instead of running the harvest binary directly.</p> Changes install makes <ul> <li>Directories <code>/var/log/harvest/</code> and <code>/var/log/run/</code> are created</li> <li>A <code>harvest</code> user and group are created and the installed files are chowned to harvest</li> <li>Systemd <code>/etc/systemd/system/harvest.service</code> file is created and enabled</li> </ul>"},{"location":"install/podman/","title":"Containerized Harvest on Linux using Rootless Podman","text":"<p>RHEL 8 ships with Podman instead of Docker. There are two ways to run containers with Podman: rootless or with root. Both setups are outlined below. The Podman ecosystem is changing rapidly so the shelf life of these instructions may be short. Make sure you have at least the same versions of the tools listed below. </p> <p>If you don't want to bother with Podman, you can also install Docker on RHEL 8 and use it to run Harvest per normal.</p>"},{"location":"install/podman/#setup","title":"Setup","text":"<p>Make sure your OS is up-to-date with <code>yum update</code>. Podman's dependencies are updated frequently.</p> <pre><code>sudo yum remove docker-ce\nsudo yum module enable -y container-tools:rhel8\nsudo yum module install -y container-tools:rhel8\nsudo yum install podman podman-docker podman-plugins\n</code></pre> <p>We also need to install Docker Compose since Podman uses it for compose workflows. Install <code>docker-compose</code> like this: <pre><code>VERSION=1.29.2\nsudo curl -L \"https://github.com/docker/compose/releases/download/$VERSION/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\nsudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n</code></pre></p> <p>After all the packages are installed, start the Podman systemd socket-activated service:</p> <pre><code>sudo systemctl start podman.socket\n</code></pre>"},{"location":"install/podman/#containerized-harvest-on-linux-using-rootful-podman","title":"Containerized Harvest on Linux using Rootful Podman","text":"<p>Make sure you're able to curl the endpoint.</p> <pre><code>sudo curl -H \"Content-Type: application/json\" --unix-socket /var/run/docker.sock http://localhost/_ping\n</code></pre> <p>If the <code>sudo curl</code> does not print <code>OK\u23ce</code> troubleshoot before continuing.</p> <p>Proceed to Running Harvest</p>"},{"location":"install/podman/#containerized-harvest-on-linux-using-rootless-podman_1","title":"Containerized Harvest on Linux using Rootless Podman","text":"<p>To run Podman rootless, we'll create a non-root user named: <code>harvest</code> to run Harvest.</p> <pre><code># as root or sudo\nusermod --append --groups wheel harvest\n</code></pre> <p>Login with the harvest user, set up the podman.socket, and make sure the curl below works. <code>su</code> or <code>sudo</code> aren't sufficient, you need to <code>ssh</code> into the machine as the harvest user or use <code>machinectl login</code>. See sudo-rootless-podman for details.</p> <pre><code># these must be run as the harvest user\nsystemctl --user enable podman.socket\nsystemctl --user start podman.socket\nsystemctl --user status podman.socket\nexport DOCKER_HOST=unix:///run/user/$UID/podman/podman.sock\n\nsudo curl -H \"Content-Type: application/json\" --unix-socket /var/run/docker.sock http://localhost/_ping\n</code></pre> <p>If the <code>sudo curl</code> does not print <code>OK\u23ce</code> troubleshoot before continuing.</p> <p>Run podman info and make sure <code>runRoot</code> points to <code>/run/user/$UID/containers</code> (see below). If it doesn't, you'll probably run into problems when restarting the machine. See errors after rebooting.</p> <pre><code>podman info | grep runRoot\n  runRoot: /run/user/1001/containers\n</code></pre>"},{"location":"install/podman/#running-harvest","title":"Running Harvest","text":"<p>By default, Cockpit runs on port 9090, same as Prometheus. We'll change Prometheus's host port to 9091, so we can run both Cockpit and Prometheus. Line <code>2</code> below does that.</p> <p>With these changes, the standard Harvest compose instructions can be followed as normal now. In summary,</p> <ol> <li>Add the clusters, exporters, etc. to your <code>harvest.yml</code> file</li> <li> <p>Generate a compose file from your <code>harvest.yml</code> by running </p> <pre><code>docker run --rm \\\n  --env UID=$(id -u) --env GID=$(id -g) \\\n  --entrypoint \"bin/harvest\" \\\n  --volume \"$(pwd):/opt/temp\" \\\n  --volume \"$(pwd)/harvest.yml:/opt/harvest/harvest.yml\" \\\n  ghcr.io/netapp/harvest \\\n  generate docker full \\\n  --output harvest-compose.yml \\\n  --promPort 9091\n</code></pre> </li> <li> <p>Bring everything up </p> <pre><code>docker-compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans\n</code></pre> </li> </ol> <p>After starting the containers, you can view them with <code>podman ps -a</code> or using Cockpit <code>https://host-ip:9090/podman</code>.</p> <pre><code>podman ps -a\nCONTAINER ID  IMAGE                                   COMMAND               CREATED        STATUS            PORTS                     NAMES\n45fd00307d0a  ghcr.io/netapp/harvest:latest           --poller unix --p...  5 seconds ago  Up 5 seconds ago  0.0.0.0:12990-&gt;12990/tcp  poller_unix_v21.11.0\nd40585bb903c  localhost/prom/prometheus:latest        --config.file=/et...  5 seconds ago  Up 5 seconds ago  0.0.0.0:9091-&gt;9090/tcp    prometheus\n17a2784bc282  localhost/grafana/grafana:latest                              4 seconds ago  Up 5 seconds ago  0.0.0.0:3000-&gt;3000/tcp    grafana\n</code></pre>"},{"location":"install/podman/#troubleshooting","title":"Troubleshooting","text":"<p>Check Podman's troubleshooting docs</p>"},{"location":"install/podman/#nothing-works","title":"Nothing works","text":"<p>Make sure the <code>DOCKER_HOST</code> env variable is set and that this curl works. <pre><code>sudo curl -H \"Content-Type: application/json\" --unix-socket /var/run/docker.sock http://localhost/_ping\n</code></pre></p> <p>Make sure your containers can talk to each other.</p> <pre><code>ping prometheus\nPING prometheus (10.88.2.3): 56 data bytes\n64 bytes from 10.88.2.3: seq=0 ttl=42 time=0.059 ms\n64 bytes from 10.88.2.3: seq=1 ttl=42 time=0.065 ms\n</code></pre>"},{"location":"install/podman/#errors-after-rebooting","title":"Errors after rebooting","text":"<p>After restarting the machine, I see errors like these when running <code>podman ps</code>.</p> <pre><code>podman ps -a\nERRO[0000] error joining network namespace for container 424df6c: error retrieving network namespace at /run/user/1001/netns/cni-5fb97adc-b6ef-17e8-565b-0481b311ba09: failed to Statfs \"/run/user/1001/netns/cni-5fb97adc-b6ef-17e8-565b-0481b311ba09\": no such file or directory\n</code></pre> <p>Run <code>podman info</code> and make sure <code>runRoot</code> points to <code>/run/user/$UID/containers</code> (see below). If it instead points to <code>/tmp/podman-run-$UID</code> you will likely have problems when restarting the machine. Typically, this happens because you used su to become the harvest user or ran podman as root. You can fix this by logging in as the <code>harvest</code> user and running <code>podman system reset</code>.</p> <pre><code>podman info | grep runRoot\n  runRoot: /run/user/1001/containers\n</code></pre>"},{"location":"install/podman/#linger-errors","title":"Linger errors","text":"<p>When you logout, systemd may remove some temp files and tear down Podman's rootless network. Workaround is to run the following as the harvest user. Details here</p> <pre><code>loginctl enable-linger\n</code></pre>"},{"location":"install/podman/#versions","title":"Versions","text":"<p>The following versions were used to validate this workflow.</p> <pre><code>podman version\n\nVersion:      3.2.3\nAPI Version:  3.2.3\nGo Version:   go1.15.7\nBuilt:        Thu Jul 29 11:02:43 2021\nOS/Arch:      linux/amd64\n\ndocker-compose -v\ndocker-compose version 1.29.2, build 5becea4c\n\ncat /etc/redhat-release\nRed Hat Enterprise Linux release 8.4 (Ootpa)\n</code></pre>"},{"location":"install/podman/#references","title":"References","text":"<ul> <li>https://github.com/containers/podman</li> <li>https://www.redhat.com/sysadmin/sudo-rootless-podman</li> <li>https://www.redhat.com/sysadmin/podman-docker-compose</li> <li>https://fedoramagazine.org/use-docker-compose-with-podman-to-orchestrate-containers-on-fedora/</li> <li>https://podman.io/getting-started/network.html mentions the need for <code>podman-plugins</code>, otherwise rootless containers running in separate containers cannot see each other</li> <li>Troubleshoot Podman</li> </ul>"},{"location":"install/quadlet/","title":"Containerized Harvest on Linux using Podman Quadlets","text":"<p>This documentation describes how to run Harvest in a container using Podman Quadlets. Quadlets are a way to manage Podman containers with systemd. This is useful for running Harvest as a service on Linux.</p> <p>In this guide, we'll create a Podman Quadlet for each Harvest poller and a systemd service that starts the pollers when the system boots. </p>"},{"location":"install/quadlet/#summary-of-steps","title":"Summary of Steps","text":"<ol> <li>Install Harvest, set up your <code>harvest.yml</code>, and generate a <code>harvest-compose.yml</code> using the Harvest CLI per the Harvest documentation. details</li> <li>Use podlet to generate Quadlet files from the <code>harvest-compose.yml</code> file from step 1. details</li> <li>Make a few edits to the generated Quadlet files and move them to <code>/etc/containers/systemd/</code>. details</li> <li>Tell systemd to reload the services by running <code>systemctl daemon-reload</code>.</li> <li>Start the systemd managed pollers (e.g., <code>sudo systemctl start poller-sar</code>).</li> <li>Reboot the machine and verify that all pollers restart.</li> </ol>"},{"location":"install/quadlet/#generate-harvest-compose-files","title":"Generate Harvest Compose Files","text":"<p>Install Harvest and generate harvest-compose.yml.</p> <pre><code># cat example harvest-compose.yml\n\nservices:\n  u2:\n    image: ghcr.io/netapp/harvest:latest\n    container_name: poller-u2\n    ports:\n      - \"12990:12990\"\n    command: '--poller u2 --promPort 12990 --config /opt/harvest.yml'\n    volumes:\n      - ./cert:/opt/harvest/cert\n      - ./harvest.yml:/opt/harvest.yml\n      - ./conf:/opt/harvest/conf\n    networks:\n      - backend\n\n  sar:\n    image: ghcr.io/netapp/harvest:latest\n    container_name: poller-sar\n    ports:\n      - \"12991:12991\"\n    command: '--poller sar --promPort 12991 --config /opt/harvest.yml'\n    volumes:\n      - ./cert:/opt/harvest/cert\n      - ./harvest.yml:/opt/harvest.yml\n      - ./conf:/opt/harvest/conf\n    networks:\n      - backend\n</code></pre>"},{"location":"install/quadlet/#podlet","title":"Podlet","text":"<p>Install podlet or compare the example <code>harvest-compose.yml</code> above with the generated quadlets below and make similar edits for your <code>harvest-compose.yml</code>.</p> <pre><code>podlet compose harvest-compose.yml\n</code></pre> <pre><code># u2.container\n[Container]\nContainerName=poller-u2\nExec=--poller u2 --promPort 12990 --config /opt/harvest.yml\nImage=ghcr.io/netapp/harvest:latest\nNetwork=backend\nPublishPort=12990:12990\nVolume=./cert:/opt/harvest/cert\nVolume=./harvest.yml:/opt/harvest.yml\nVolume=./conf:/opt/harvest/conf\n\n---\n\n# sar.container\n[Container]\nContainerName=poller-sar\nExec=--poller sar --promPort 12991 --config /opt/harvest.yml\nImage=ghcr.io/netapp/harvest:latest\nNetwork=backend\nPublishPort=12991:12991\nVolume=./cert:/opt/harvest/cert\nVolume=./harvest.yml:/opt/harvest.yml\nVolume=./conf:/opt/harvest/conf\n</code></pre>"},{"location":"install/quadlet/#edit-quadlet-files","title":"Edit Quadlet Files","text":"<p>Podlet created two YAML documents as shown above, one for each poller. We are going to make the following adjustments to the podlet output and copy/paste the final output into <code>/etc/containers/systemd/poller-u2.container</code> and <code>/etc/containers/systemd/poller-sar.container</code>.</p> <p>The edits are:</p> <ol> <li>Remove the <code>Network=backend</code> line that was included for Prometheus.</li> <li>Change all Volume paths to fully qualified paths. (e.g., change <code>./cert</code> to <code>/opt/harvest/cert</code> if you installed Harvest at <code>/opt/harvest</code>)</li> <li>Because we want to restart the pollers when the machine reboots add the following section:</li> </ol> <pre><code>[Install]\n# Start by default on boot\nWantedBy=multi-user.target default.target\n\n[Service]\nRestart=always\n</code></pre> <p>Here is the final output for the sar poller service with all the edits applied: </p> <pre><code># sar.container\n[Container]\nContainerName=poller-sar\nExec=--poller sar --promPort 12991 --config /opt/harvest.yml\nImage=ghcr.io/netapp/harvest:latest\nPublishPort=12991:12991\nVolume=/opt/harvest/cert:/opt/harvest/cert\nVolume=/opt/harvest/harvest.yml:/opt/harvest.yml\nVolume=/opt/harvest/conf:/opt/harvest/conf\n\n[Install]\n# Start by default on boot\nWantedBy=multi-user.target default.target\n\n[Service]\nRestart=always\n</code></pre>"},{"location":"install/quadlet/#move-quadlet-files","title":"Move Quadlet Files","text":"<p>Move each service file to <code>/etc/containers/systemd/</code> e.g. <code>mv poller-sar.container /etc/containers/systemd/poller-sar.container</code></p>"},{"location":"install/quadlet/#references","title":"References","text":"<ul> <li>Make systemd better for Podman with Quadlet</li> <li>https://docs.podman.io/en/latest/markdown/podman-systemd.unit.5.html</li> <li>https://github.com/containers/podlet</li> </ul>"},{"location":"mcp/examples/","title":"Usage Examples","text":"<p>This section provides example queries you can ask your MCP client (GitHub Copilot, Claude Desktop, etc.) when using the Harvest MCP Server. For more examples and community discussions about MCP usage, see: Harvest MCP Discussion </p> <p>Higher-capability language models provide better analysis and insights. When possible use the latest model versions with large context windows. You will get better results when using flagship models like GPT-5.X, Sonnet 4.X, Gemini 3, etc. </p> <p>The following examples were run with Claude Sonnet 4 large language model. </p>"},{"location":"mcp/examples/#getting-the-best-results","title":"Getting the Best Results","text":""},{"location":"mcp/examples/#use-harvest-mcp-provided-prompt","title":"Use Harvest MCP Provided Prompt","text":"<p>For optimal analysis and insights, always start by setting the Analysis Expert prompt available in the Harvest MCP server. This prompt provides your MCP client with instructions for analyzing Harvest data.</p> <p>Access the prompt: Most MCP clients support prompts - look for a <code>/mcp</code> command or prompts menu to select the \"Analysis Expert\" prompt.</p>"},{"location":"mcp/examples/#reference-questions","title":"Reference Questions","text":"<p>Below are example questions that work well with the Harvest MCP Server:</p>"},{"location":"mcp/examples/#infrastructure-health","title":"Infrastructure Health","text":"<p>\"What's the overall health of my infrastructure?\"</p> <p>Expected response: Comprehensive health summary showing cluster status, active alerts, capacity issues, and immediate action items with priority levels.</p>"},{"location":"mcp/examples/#capacity-analysis","title":"Capacity Analysis","text":"<p>\"Which volumes are approaching capacity limits and need attention?\"</p> <p>Expected response: List of volumes above 90% utilization with details about clusters, SVMs, growth trends, and recommended actions including timeline for capacity expansion.</p>"},{"location":"mcp/examples/#performance-investigation","title":"Performance Investigation","text":"<p>\"Show me systems experiencing performance issues with high latency or IOPS\"</p> <p>Expected response: Analysis of volume and node performance metrics, identification of hotspots, correlation with capacity utilization, and specific recommendations for optimization.</p>"},{"location":"mcp/examples/#trend-analysis","title":"Trend Analysis","text":"<p>\"Analyze storage growth patterns across my clusters over the past 30 days and predict when I'll need to add capacity\"</p> <p>Expected response: Detailed growth analysis by cluster and aggregate, mathematical projections of space consumption, identification of fastest-growing workloads, and recommended expansion timeline with sizing guidance.</p>"},{"location":"mcp/examples/#integration-tips","title":"Integration Tips","text":"<ol> <li>Always set the prompt first before asking questions</li> <li>Use specific questions rather than vague requests</li> <li>Ask follow-up questions to dive deeper into specific areas</li> <li>Combine multiple areas (e.g., \"Show me capacity and performance issues together\")</li> <li>Request different perspectives (executive summary vs. technical details)</li> </ol>"},{"location":"mcp/examples/#mcp-clients","title":"MCP Clients","text":"<p>Common MCP clients that work with Harvest MCP Server:</p> <ul> <li>GitHub Copilot: Integrated in VS Code, supports MCP connections</li> <li>Claude Desktop: Anthropic's desktop application with MCP support</li> <li>Custom MCP Clients: Any application implementing the MCP standard</li> </ul>"},{"location":"mcp/installation/","title":"Installation","text":"<p>Harvest MCP Server is distributed as a Docker container image, or you can build it from source.</p>"},{"location":"mcp/installation/#container-images","title":"Container Images","text":"<p>Harvest MCP Server is available as pre-built container images:</p> Image Description <code>ghcr.io/netapp/harvest-mcp:latest</code> Stable release version <code>ghcr.io/netapp/harvest-mcp:nightly</code> Latest development builds"},{"location":"mcp/installation/#mcp-client-integration","title":"MCP Client Integration","text":""},{"location":"mcp/installation/#stdio-mode","title":"Stdio Mode","text":"<p>For MCP clients like GitHub Copilot, add to your mcp.json:</p> <pre><code>{\n  \"servers\": {\n    \"harvest-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"--env\", \"HARVEST_TSDB_URL=http://your-prometheus:9090\",\n        \"ghcr.io/netapp/harvest-mcp:latest\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/installation/#http-mode","title":"HTTP Mode","text":"<p>For HTTP-based MCP clients, first start the server:</p> <pre><code>docker run -d \\\n  --name harvest-mcp-server \\\n  -p 8082:8082 \\\n  --env HARVEST_TSDB_URL=http://your-prometheus:9090 \\\n  ghcr.io/netapp/harvest-mcp:latest \\\n  start --http --port 8082 --host 0.0.0.0\n</code></pre> <p>If you only want to bind to localhost, omit the <code>--host</code> option.</p> <p>Then configure your mcp.json:</p> <pre><code>{\n  \"servers\": {\n    \"harvest-mcp\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:8082\"\n    }\n  }\n}\n</code></pre> <p>For remote server access:</p> <pre><code>{\n  \"servers\": {\n    \"harvest-mcp\": {\n      \"type\": \"http\",\n      \"url\": \"http://your-server-ip:8082\"\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/installation/#building-from-source","title":"Building from Source","text":""},{"location":"mcp/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go(check <code>.harvest.env</code> in the repository root for the exact required version)</li> <li>Git</li> <li>Make</li> <li>Docker (optional, for building Docker images)</li> </ul>"},{"location":"mcp/installation/#clone-the-repository","title":"Clone the Repository","text":"<p>First, clone the Harvest repository:</p> <pre><code>git clone https://github.com/NetApp/harvest.git\ncd harvest\n</code></pre>"},{"location":"mcp/installation/#build-docker-image","title":"Build Docker Image","text":"<p>Build your own Docker image from source:</p> <pre><code># Navigate to the mcp directory\ncd mcp\n\n# Build the Docker image using make (creates harvest-mcp:latest by default)\nmake docker-build\n\n# Or specify a custom tag\nmake docker-build DOCKER_TAG=harvest-mcp:local\n</code></pre> <p>Alternatively, build directly with Docker:</p> <pre><code># From the harvest repository root\ndocker build -f mcp/Dockerfile -t harvest-mcp:local .\n</code></pre>"},{"location":"mcp/installation/#running-the-built-docker-image","title":"Running the Built Docker Image","text":"<p>After building, use your local image in your MCP client configuration. See MCP Client Integration above for configuration examples - just replace <code>ghcr.io/netapp/harvest-mcp:latest</code> with your local image tag (e.g., <code>harvest-mcp:local</code>).</p>"},{"location":"mcp/installation/#build-native-binary","title":"Build Native Binary","text":"<p>Create a standalone binary package:</p> <pre><code># Navigate to the mcp directory\ncd mcp\n\n# build for specific platforms:\nGOOS=linux GOARCH=amd64 make package    # Linux AMD64\nGOOS=darwin GOARCH=arm64 make package   # macOS ARM64 (Apple Silicon)\n\n# Creates: dist/harvest-mcp-&lt;version&gt;-&lt;release&gt;_&lt;os&gt;_&lt;arch&gt;.tar.gz\n</code></pre>"},{"location":"mcp/installation/#running-the-native-binary","title":"Running the Native Binary","text":"<p>After extracting the package, configure it in mcp.json for MCP clients.</p>"},{"location":"mcp/installation/#configure-in-mcpjson","title":"Configure in mcp.json","text":"<p>For MCP clients like GitHub Copilot, add to your mcp.json:</p> <pre><code>{\n  \"servers\": {\n    \"harvest-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"/path/to/harvest-mcp/bin/harvest-mcp\",\n      \"args\": [\"start\"],\n      \"env\": {\n        \"HARVEST_TSDB_URL\": \"http://your-prometheus:9090\"\n      }\n    }\n  }\n}\n</code></pre> <p>For HTTP mode, start the server first:</p> <pre><code># Start the server\nHARVEST_TSDB_URL=http://your-prometheus:9090 /path/to/harvest-mcp/bin/harvest-mcp start --http --port 8082\n</code></pre> <p>Then configure your mcp.json:</p> <pre><code>{\n  \"servers\": {\n    \"harvest-mcp\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:8082\"\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/installation/#logs","title":"Logs","text":"<p>To view the MCP server logs:</p> <pre><code>docker logs &lt;container-id&gt;\n</code></pre>"},{"location":"mcp/installation/#configuration","title":"Configuration","text":"<p>For complete configuration options and environment variables, run:</p> <pre><code>docker run --rm ghcr.io/netapp/harvest-mcp:latest start --help\n</code></pre> <p>This displays all available environment variables with descriptions, authentication options, and advanced settings.</p>"},{"location":"mcp/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Usage Examples</li> </ul>"},{"location":"mcp/overview/","title":"Harvest Model Context Protocol Server","text":"<p>The Harvest Model Context Protocol (MCP) server provides MCP clients like GitHub Copilot, Claude Desktop, and other large language models (LLMs) access to your infrastructure monitoring data collected by Harvest from ONTAP, StorageGRID, E-Series and Cisco Nexus Switches.</p>"},{"location":"mcp/overview/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol (MCP) is an open standard that enables interactions between MCP clients and external data sources. The Harvest MCP server provides APIs for large language models (LLMs) to query your Harvest-collected open-metrics data. These APIs enable intelligent data analysis and insights.</p>"},{"location":"mcp/overview/#what-you-can-ask","title":"What You Can Ask","text":"<p>The Harvest MCP server allows you to ask natural language questions about your infrastructure monitoring data. Here are some example queries:</p> <p>Simple Health Checks</p> <ul> <li>\"What's the overall health of my infrastructure?\"</li> <li>\"Are there any active alerts I should know about?\"</li> </ul> <p>Capacity Analysis</p> <ul> <li>\"Which volumes are running out of space?\"</li> <li>\"Show me the top 5 volumes by utilization\"</li> <li>\"Analyze storage growth trends over the past month\"</li> </ul> <p>Performance Investigation</p> <ul> <li>\"Which systems are experiencing high latency?\"</li> <li>\"Which volumes have performance issues?\"</li> <li>\"Show me performance bottlenecks across my clusters\"</li> </ul>"},{"location":"mcp/overview/#architecture","title":"Architecture","text":"<p>The Harvest MCP Server operates as a lightweight service that:</p> <ol> <li>Connects to your existing Prometheus/VictoriaMetrics instance containing Harvest data</li> <li>Provides a standardized MCP interface for MCP clients (GitHub Copilot, Claude Desktop, etc.)</li> <li>Enables natural language queries against your infrastructure data</li> </ol> <pre><code>graph LR\n    A[MCP Client&lt;br/&gt;GitHub Copilot&lt;br/&gt;Claude Desktop] --&gt; B[Harvest MCP Server]\n    B --&gt; C[Prometheus/VictoriaMetrics]\n    C --&gt; D[Harvest Pollers]\n    D --&gt; F[ONTAP Clusters]\n    D --&gt; G[StorageGRID]  \n    D --&gt; H[Cisco Nexus Switches]\n    D --&gt; I[E-Series Arrays]</code></pre>"},{"location":"mcp/overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Prometheus or VictoriaMetrics instance with Harvest data</li> <li>Docker environment for running the MCP server</li> <li>Network connectivity from MCP server to your time series database</li> </ul> <p>For information about Harvest deployment and configuration, see:</p> <ul> <li>Harvest Concepts</li> <li>Installation Overview</li> <li>System Requirements</li> </ul>"},{"location":"mcp/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Install the MCP Server</li> <li>Configure Environment Variables</li> <li>Try the Usage Examples</li> </ul>"},{"location":"resources/ems-alert-runbook/","title":"EMS Alert Runbook","text":"<p>This document describes each ONTAP event management system (EMS) event that Harvest collects and remediation steps.</p>"},{"location":"resources/ems-alert-runbook/#aws-credentials-not-initialized","title":"AWS Credentials Not Initialized","text":"<p>Impact: Availability</p> <p>EMS Event: <code>cloud.aws.iamNotInitialized</code></p> <p>This event occurs when a module attempts to access Amazon Web Services (AWS) Identity and Access Management (IAM) role-based credentials from the cloud credentials thread before they are initialized.</p> <p>Remediation</p> <p>Wait for the cloud credential thread, as well as the system, to complete initialization.</p>"},{"location":"resources/ems-alert-runbook/#antivirus-server-busy","title":"Antivirus Server Busy","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.vscanConnBackPressure</code></p> <p>The antivirus server is too busy to accept any new scan requests.</p> <p>Remediation</p> <p>If this message occurs frequently, ensure that there are enough antivirus servers to handle the virus scan load generated by the SVM.</p>"},{"location":"resources/ems-alert-runbook/#cloud-tier-unreachable","title":"Cloud Tier Unreachable","text":"<p>Impact: Availability</p> <p>EMS Event: <code>object.store.unavailable</code></p> <p>A storage node cannot connect to Cloud Tier object store API. Some data will be inaccessible.</p> <p>Remediation</p> <p>If you use on-premises products, perform the following corrective actions: </p> <ol> <li>Verify that your intercluster LIF is online and functional by using the \"network interface show\" command.</li> <li>Check the network connectivity to the object store server by using the \"ping\" command over the destination node intercluster LIF.</li> <li>Ensure the following:     a. The configuration of your object store has not changed.     b. The login and connectivity information is still valid. Contact NetApp technical support if the issue persists.</li> </ol> <p>If you use Cloud Volumes ONTAP, perform the following corrective actions: </p> <ol> <li>Ensure that the configuration of your object store has not changed.</li> <li>Ensure that the login and connectivity information is still valid. Contact NetApp technical support if the issue persists.</li> </ol>"},{"location":"resources/ems-alert-runbook/#directory-size-is-approaching-the-maximum-directory-size-maxdirsize-limit","title":"Directory size is approaching the maximum directory size (maxdirsize) limit","text":"<p>Impact: Availability</p> <p>EMS Event: <code>wafl.dir.size.warning</code></p> <p>This message occurs when the size of a directory surpasses a configured percentage (default: 90%) of its current maximum directory size (maxdirsize) limit.</p> <p>Remediation</p> <p>Use the \"volume file show-inode\" command with the file ID and volume name information to find the file path. Reduce the number of files in the directory. If not possible, use the (privilege:advanced) option \"volume modify -volume vol_name -maxdir-size new_value\" to increase the maximum number of files per directory. However, doing so could impact system performance. If you need to increase the maximum directory size, contact NetApp technical support.</p>"},{"location":"resources/ems-alert-runbook/#disk-out-of-service","title":"Disk Out of Service","text":"<p>Impact: Availability</p> <p>EMS Event: <code>disk.outOfService</code></p> <p>This event occurs when a disk is removed from service because it has been marked failed, is being sanitized, or has entered the Maintenance Center.</p>"},{"location":"resources/ems-alert-runbook/#disk-shelf-power-supply-discovered","title":"Disk Shelf Power Supply Discovered","text":"<p>Impact: Configuration</p> <p>EMS Event: <code>diskShelf.psu.added</code></p> <p>This message occurs when a power supply unit is added to the disk shelf.</p>"},{"location":"resources/ems-alert-runbook/#disk-shelves-power-supply-removed","title":"Disk Shelves Power Supply Removed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>diskShelf.psu.removed</code></p> <p>This message occurs when a power supply unit is removed from the disk shelf.</p>"},{"location":"resources/ems-alert-runbook/#fc-target-port-commands-exceeded","title":"FC Target Port Commands Exceeded","text":"<p>Impact: Availability</p> <p>EMS Event: <code>scsitarget.fct.port.full</code></p> <p>The number of outstanding commands on the physical FC target port exceeds the supported limit. The port does not have sufficient buffers for the outstanding commands. It is overrun or the fan-in is too steep because too many initiator I/Os are using it.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Evaluate the host fan-in on the port, and perform one of the following actions:       a. Reduce the number of hosts that log in to this port.       b. Reduce the number of LUNs accessed by the hosts that log in to this port.       c. Reduce the host command queue depth.</li> <li>Monitor the \"queue_full\" counter on the \"fcp_port\" CM object, and ensure that it does not increase. For example:  statistics show -object fcp_port -counter queue_full -instance port.portname -raw</li> <li>Monitor the threshold counter and ensure that it does not increase. For example: statistics show -object fcp_port -counter threshold_full -instance port.portname -raw</li> </ol>"},{"location":"resources/ems-alert-runbook/#fabricpool-mirror-replication-resync-completed","title":"FabricPool Mirror Replication Resync Completed","text":"<p>Impact: Capacity</p> <p>EMS Event: <code>wafl.ca.resync.complete</code></p> <p>This message occurs when Data ONTAP(R) completes the resync process from the primary object store to the mirror object store for a mirrored FabricPool aggregate.</p>"},{"location":"resources/ems-alert-runbook/#fabricpool-space-usage-limit-nearly-reached","title":"FabricPool Space Usage Limit Nearly Reached","text":"<p>Impact: Capacity</p> <p>EMS Event: <code>fabricpool.nearly.full</code></p> <p>The total cluster-wide FabricPool space usage of object stores from capacity-licensed providers has nearly reached the licensed limit.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Check the percentage of the licensed capacity used by each FabricPool storage tier by using the \"storage aggregate object-store show-space\" command.</li> <li>Delete Snapshot copies from volumes with the tiering policy \"snapshot\" or \"backup\" by using the \"volume snapshot delete\" command to clear up space.</li> <li>Install a new license on the cluster to increase the licensed capacity.</li> </ol>"},{"location":"resources/ems-alert-runbook/#fabricpool-space-usage-limit-reached","title":"FabricPool Space Usage Limit Reached","text":"<p>Impact: Capacity</p> <p>EMS Event: <code>fabricpool.full</code></p> <p>The total cluster-wide FabricPool space usage of object stores from capacity-licensed providers has reached  the license limit.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Check the percentage of the licensed capacity used by each FabricPool storage tier by using the \"storage aggregate object-store show-space\" command.</li> <li>Delete Snapshot copies from volumes with the tiering policy \"snapshot\" or \"backup\" by using the \"volume snapshot delete\" command to clear up space.</li> <li>Install a new license on the cluster to increase the licensed capacity.</li> </ol>"},{"location":"resources/ems-alert-runbook/#fanout-snapmirror-relationship-common-snapshot-deleted","title":"Fanout SnapMirror Relationship Common Snapshot Deleted","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sms.fanout.comm.snap.deleted</code></p> <p>This message occurs when an older Snapshot(tm) copy is deleted as part of a SnapMirror\u00ae Synchronous resynchronize or update (common Snapshot copy) operation, which could lead to a \"no common Snapshot scenario\" between the synchronous and asynchronous disaster recovery (DR) copies that share the same source volume. If there is no common Snapshot copy between the synchronous and asynchronous DR copies, then a re-baseline will need to be performed during a disaster recovery.</p> <p>Remediation</p> <p>You can ignore this message if there is no asynchronous relationship configured for the synchronous source volume. If there is an asynchronous relationship configured, then update the asynchronous relationship by using the \"snapmirror update\" command. The SnapMirror update operation will transfer the snapshots that will act as common snapshots between the synchronous and asynchronous destinations.</p>"},{"location":"resources/ems-alert-runbook/#giveback-of-storage-pool-failed","title":"Giveback of Storage Pool Failed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>gb.netra.ca.check.failed</code></p> <p>This event occurs during the migration of an storage pool (aggregate) as part of a storage failover (SFO) giveback, when the destination node cannot reach the object stores.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Verify that your intercluster LIF is online and functional by using the \"network interface show\" command.</li> <li>Check network connectivity to the object store server by using the\"'ping\" command over the destination node intercluster LIF. </li> <li>Verify that the configuration of your object store has not changed and that login and connectivity information is still accurate by using the \"aggregate object-store config show\" command.</li> </ol> <p>Alternatively, you can override the error by specifying false for the \"require-partner-waiting\" parameter of the giveback command.</p> <p>Contact NetApp technical support for more information or assistance.</p>"},{"location":"resources/ems-alert-runbook/#ha-interconnect-down","title":"HA Interconnect Down","text":"<p>Impact: Availability</p> <p>EMS Event: <code>callhome.hainterconnect.down</code></p> <p>The high-availability (HA) interconnect is down. Risk of service outage when failover is not available.</p> <p>Remediation</p> <p>Corrective actions depend on the number and type of HA interconnect links supported by the platform, as well as the reason why the interconnect is down. </p> <ul> <li>If the links are down:<ul> <li>Verify that both controllers in the HA pair are operational.</li> <li>For externally connected links, make sure that the interconnect cables are connected properly and that the small form-factor pluggables (SFPs), if applicable, are seated properly on both controllers.</li> <li>For internally connected links, disable and re-enable the links, one after the other, by using the \"ic link off\" and \"ic link on\" commands. </li> </ul> </li> <li>If links are disabled, enable the links by using the \"ic link on\" command. </li> <li>If a peer is not connected, disable and re-enable the links, one after the other, by using the \"ic link off\" and \"ic link on\" commands.</li> </ul> <p>Contact NetApp technical support if the issue persists.</p>"},{"location":"resources/ems-alert-runbook/#lun-destroyed","title":"LUN Destroyed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>LUN.destroy</code></p> <p>This event occurs when a LUN is destroyed.</p>"},{"location":"resources/ems-alert-runbook/#lun-offline","title":"LUN Offline","text":"<p>Impact: Availability</p> <p>EMS Event: <code>LUN.offline</code></p> <p>This message occurs when a LUN is brought offline manually.</p> <p>Remediation</p> <p>Bring the LUN back online.</p>"},{"location":"resources/ems-alert-runbook/#main-unit-fan-failed","title":"Main Unit Fan Failed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>monitor.fan.failed</code></p> <p>One or more main unit fans have failed. The system remains operational.</p> <p>However, if the condition persists for too long, the overtemperature might trigger an automatic shutdown.</p> <p>Remediation</p> <p>Reseat the failed fans. If the error persists, replace them.</p>"},{"location":"resources/ems-alert-runbook/#main-unit-fan-in-warning-state","title":"Main Unit Fan in Warning State","text":"<p>Impact: Availability</p> <p>EMS Event: <code>monitor.fan.warning</code></p> <p>This event occurs when one or more main unit fans are in a warning state.</p> <p>Remediation</p> <p>Replace the indicated fans to avoid overheating.</p>"},{"location":"resources/ems-alert-runbook/#max-sessions-per-user-exceeded","title":"Max Sessions Per User Exceeded","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.cifsMaxSessPerUsrConn</code></p> <p>You have exceeded the maximum number of sessions allowed per user over a TCP connection. Any request to establish a session will be denied until some sessions are released. </p> <p>Remediation</p> <p>Perform the following corrective actions: </p> <ol> <li>Inspect all the applications that run on the client, and terminate any that are not operating properly.</li> <li>Reboot the client.</li> <li>Check if the issue is caused by a new or existing application:     a. If the application is new, set a higher threshold for the client by using the \"cifs option modify -max-opens-same-file-per-tree\" command. In some cases, clients operate as expected, but require a higher threshold. You should have advanced privilege to set a higher threshold for the client.      b. If the issue is caused by an existing application, there might be an issue with the client. Contact NetApp technical support for more information or assistance.</li> </ol>"},{"location":"resources/ems-alert-runbook/#max-times-open-per-file-exceeded","title":"Max Times Open Per File Exceeded","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.cifsMaxOpenSameFile</code></p> <p>You have exceeded the maximum number of times that you can open the file over a TCP connection. Any request to open this file will be denied until you close some open instances of the file. This typically indicates abnormal application behavior.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Inspect the applications that run on the client using this TCP connection. The client might be operating incorrectly because of the application running on it.</li> <li>Reboot the client.</li> <li>Check if the issue is caused by a new or existing application:     a. If the application is new, set a higher threshold for the client by using the \"cifs option modify -max-opens-same-file-per-tree\" command. In some cases, clients operate as expected, but require a higher threshold. You should have advanced privilege to set a higher threshold for the client.      b. If the issue is caused by an existing application, there might be an issue with the client. Contact NetApp technical support for more information or assistance.</li> </ol>"},{"location":"resources/ems-alert-runbook/#metrocluster-automatic-unplanned-switchover-disabled","title":"MetroCluster Automatic Unplanned Switchover Disabled","text":"<p>Impact: Availability</p> <p>EMS Event: <code>mcc.config.auso.stDisabled</code></p> <p>This message occurs when automatic unplanned switchover capability is disabled.</p> <p>Remediation</p> <p>Run the \"metrocluster modify -node-name  -automatic-switchover-onfailure true\" command for each node in the cluster to enable automatic switchover."},{"location":"resources/ems-alert-runbook/#metrocluster-monitoring","title":"MetroCluster Monitoring","text":"<p>Impact: Availability</p> <p>EMS Event: <code>hm.alert.raised</code></p> <p>Aggregate was left behind during switchback.</p> <p>Remediation</p> <p>1) Check the aggregate state by using the command \"aggr show\". 2) If the aggregate is online, return it to its original owner by using the command \"metrocluster switchback\".</p>"},{"location":"resources/ems-alert-runbook/#nfsv4-store-pool-exhausted","title":"NFSv4 Store Pool Exhausted","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.nfsV4PoolExhaust</code></p> <p>A NFSv4 store pool has been exhausted.</p> <p>Remediation</p> <p>If the NFS server is unresponsive for more than 10 minutes after this event, contact NetApp technical support.</p>"},{"location":"resources/ems-alert-runbook/#nvme-namespace-destroyed","title":"NVMe Namespace Destroyed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>NVMeNS.destroy</code></p> <p>This event occurs when an NVMe namespace is destroyed.</p>"},{"location":"resources/ems-alert-runbook/#nvme-namespace-offline","title":"NVMe Namespace Offline","text":"<p>Impact: Availability</p> <p>EMS Event: <code>NVMeNS.offline</code></p> <p>This event occurs when an NVMe namespace is brought offline manually.</p>"},{"location":"resources/ems-alert-runbook/#nvme-namespace-online","title":"NVMe Namespace Online","text":"<p>Impact: Availability</p> <p>EMS Event: <code>NVMeNS.online</code></p> <p>This event occurs when an NVMe namespace is brought online manually.</p>"},{"location":"resources/ems-alert-runbook/#nvme-of-license-grace-period-active","title":"NVMe-oF License Grace Period Active","text":"<p>Impact: Availability</p> <p>EMS Event: <code>nvmf.graceperiod.active</code></p> <p>This event occurs on a daily basis when the NVMe over Fabrics (NVMe-oF) protocol is in use and the grace period of the license is active. The NVMe-oF functionality requires a license after the license grace period expires. NVMe-oF functionality is disabled when the license grace period is over.</p> <p>Remediation</p> <p>Contact your sales representative to obtain an NVMe-oF license, and add it to the cluster, or remove all instances of NVMe-oF configuration from the cluster.</p>"},{"location":"resources/ems-alert-runbook/#nvme-of-license-grace-period-expired","title":"NVMe-oF License Grace Period Expired","text":"<p>Impact: Availability</p> <p>EMS Event: <code>nvmf.graceperiod.expired</code></p> <p>The NVMe over Fabrics (NVMe-oF) license grace period is over and the NVMe-oF functionality is disabled.</p> <p>Remediation</p> <p>Contact your sales representative to obtain an NVMe-oF license, and add it to the cluster.</p>"},{"location":"resources/ems-alert-runbook/#nvme-of-license-grace-period-start","title":"NVMe-oF License Grace Period Start","text":"<p>Impact: Availability</p> <p>EMS Event: <code>nvmf.graceperiod.start</code></p> <p>The NVMe over Fabrics (NVMe-oF) configuration was detected during the upgrade to ONTAP 9.5 software. NVMe-oF functionality requires a license after the license grace period expires.</p> <p>Remediation</p> <p>Contact your sales representative to obtain an NVMe-oF license, and add it to the cluster.</p>"},{"location":"resources/ems-alert-runbook/#nvram-battery-low","title":"NVRAM Battery Low","text":"<p>Impact: Availability</p> <p>EMS Event: <code>callhome.battery.low</code></p> <p>The NVRAM battery capacity is critically low. There might be a potential data loss if the battery runs out of power.</p> <p>Your system generates and transmits an AutoSupport or \"call home\" message to NetApp technical support and the configured destinations if it is configured to do so. The successful delivery of an AutoSupport message significantly improves problem determination and resolution.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>View the battery's current status, capacity, and charging state by using the \"system node environment sensors show\" command.</li> <li>If the battery was replaced recently or the system was non-operational for an extended period of time, monitor the battery to verify that it is charging properly.</li> <li>Contact NetApp technical support if the battery runtime continues to decrease below critical levels, and the storage system shuts down automatically.</li> </ol>"},{"location":"resources/ems-alert-runbook/#netbios-name-conflict","title":"NetBIOS Name Conflict","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.cifsNbNameConflict</code></p> <p>The NetBIOS Name Service has received a negative response to a name registration request, from a remote machine. This is typically caused by a conflict in the NetBIOS name or an alias. As a result, clients might not be able to access data or connect to the right data-serving node in the cluster.</p> <p>Remediation</p> <p>Perform any one of the following corrective actions:</p> <ul> <li>If there is a conflict in the NetBIOS name or an alias, perform one of the following:<ul> <li>Delete the duplicate NetBIOS alias by using the \"vserver cifs delete -aliases alias -vserver vserver\" command.</li> <li>Rename a NetBIOS alias by deleting the duplicate name and adding an alias with a new name by using the \"vserver cifs create -aliases alias -vserver vserver\" command. </li> </ul> </li> <li>If there are no aliases configured and there is a conflict in the NetBIOS name, then rename the CIFS server by using the \"vserver cifs delete -vserver vserver\" and \"vserver cifs create -cifs-server netbiosname\" commands. NOTE: Deleting a CIFS server can make data inaccessible. </li> <li>Remove NetBIOS name or rename the NetBIOS on the remote machine.</li> </ul>"},{"location":"resources/ems-alert-runbook/#no-registered-scan-engine","title":"No Registered Scan Engine","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.vscanNoRegdScanner</code></p> <p>The antivirus connector notified ONTAP that it does not have a registered scan engine. This might cause data unavailability if the \"scan-mandatory\" option is enabled. </p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Ensure that the scan engine software installed on the antivirus server is compatible with ONTAP.</li> <li>Ensure that scan engine software is running and configured to connect to the antivirus connector over local loopback.</li> </ol>"},{"location":"resources/ems-alert-runbook/#no-vscan-connection","title":"No Vscan Connection","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.vscanNoScannerConn</code></p> <p>ONTAP has no Vscan connection to service virus scan requests. This might cause data unavailability if the \"scan-mandatory\" option is enabled.</p> <p>Remediation</p> <p>Ensure that the scanner pool is properly configured and the antivirus servers are active and connected to ONTAP.</p>"},{"location":"resources/ems-alert-runbook/#node-panic","title":"Node Panic","text":"<p>Impact: Performance</p> <p>EMS Event: <code>sk.panic</code></p> <p>This event is issued when a panic occurs.</p> <p>Remediation</p> <p>Contact NetApp customer support.</p>"},{"location":"resources/ems-alert-runbook/#node-root-volume-space-low","title":"Node Root Volume Space Low","text":"<p>Impact: Capacity</p> <p>EMS Event: <code>mgmtgwd.rootvolrec.low.space</code></p> <p>The system has detected that the root volume is dangerously low on space. The node is not fully operational. Data LIFs might have failed over within the cluster, because of which NFS and CIFS access is limited on the node. Administrative capability is limited to local recovery procedures for the node to clear up space on the root volume.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Clear up space on the root volume by deleting old Snapshot copies, deleting files you no longer need from the /mroot directory, or expanding the root volume capacity.</li> <li>Reboot the controller.</li> </ol> <p>Contact NetApp technical support for more information or assistance.</p>"},{"location":"resources/ems-alert-runbook/#non-responsive-antivirus-server","title":"Non-responsive AntiVirus Server","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.vscanConnInactive</code></p> <p>This event occurs when ONTAP(R) detects a non-responsive antivirus (AV) server and forcibly closes its Vscan connection.</p> <p>Remediation</p> <p>Ensure that the AV server installed on the AV connector can connect to the Storage Virtual Machine (SVM) and receive the scan requests.</p>"},{"location":"resources/ems-alert-runbook/#nonexistent-admin-share","title":"Nonexistent Admin Share","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.cifsNoPrivShare</code></p> <p>Vscan issue: a client has attempted to connect to a nonexistent ONTAP_ADMIN$ share.</p> <p>Remediation</p> <p>Ensure that Vscan is enabled for the mentioned SVM ID. Enabling Vscan on a SVM causes the ONTAP_ADMIN$ share to be created for the SVM automatically.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-added","title":"ONTAP Mediator Added","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.added</code></p> <p>This message occurs when ONTAP Mediator is added successfully on a cluster.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-ca-certificate-expired","title":"ONTAP Mediator CA Certificate Expired","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.cacert.expired</code></p> <p>This message occurs when the ONTAP Mediator certificate authority (CA) certificate has expired. As a result, all further communication to the ONTAP Mediator will not be possible.</p> <p>Remediation</p> <p>Remove the configuration of the current ONTAP Mediator by using the \"snapmirror mediator remove\" command. Update a new CA certificate on the ONTAP Mediator server. Reconfigure access to the ONTAP Mediator by using the \"snapmirror mediator add\" command.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-ca-certificate-expiring","title":"ONTAP Mediator CA Certificate Expiring","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.cacert.expiring</code></p> <p>This message occurs when the ONTAP Mediator certificate authority (CA) certificate is due to expire within the next 30 days.</p> <p>Remediation</p> <p>Before this certificate expires, remove the configuration of the current ONTAP Mediator by using the \"snapmirror mediator remove\" command. Update a new CA certificate on the ONTAP Mediator server. Reconfigure access to the ONTAP Mediator by using the \"snapmirror mediator add\" command.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-client-certificate-expired","title":"ONTAP Mediator Client Certificate Expired","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.clientc.expired</code></p> <p>This message occurs when the ONTAP Mediator client certificate has expired. As a result, all further communication to the ONTAP Mediator will not be possible.</p> <p>Remediation</p> <p>Remove the configuration of the current ONTAP Mediator by using the \"snapmirror mediator remove\" command. Reconfigure access to the ONTAP Mediator by using the \"snapmirror mediator add\" command.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-client-certificate-expiring","title":"ONTAP Mediator Client Certificate Expiring","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.clientc.expiring</code></p> <p>This message occurs when the ONTAP Mediator client certificate is due to expire within the next 30 days.</p> <p>Remediation</p> <p>Before this certificate expires, remove the configuration of the current ONTAP Mediator by using the \"snapmirror mediator remove\" command. Reconfigure access to the ONTAP Mediator by using the \"snapmirror mediator add\" command.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-not-accessible","title":"ONTAP Mediator Not Accessible","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.misconfigured</code></p> <p>This message occurs when either the ONTAP Mediator is repurposed or the Mediator package is no longer installed on the Mediator server. As a result, SnapMirror failover is not possible.</p> <p>Remediation</p> <p>Remove the configuration of the current ONTAP Mediator by using the \"snapmirror mediator remove\" command. Reconfigure access to the ONTAP Mediator by using the \"snapmirror mediator add\" command.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-removed","title":"ONTAP Mediator Removed","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.removed</code></p> <p>This message occurs when ONTAP Mediator is removed successfully from a cluster.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-server-certificate-expired","title":"ONTAP Mediator Server Certificate Expired","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.serverc.expired</code></p> <p>This message occurs when the ONTAP Mediator server certificate has expired. As a result, all further communication to the ONTAP Mediator will not be possible.</p> <p>Remediation</p> <p>Remove the configuration of the current ONTAP Mediator by using the \"snapmirror mediator remove\" command. Update a new server certificate on the ONTAP Mediator server. Reconfigure access to the ONTAP Mediator by using the \"snapmirror mediator add\" command.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-server-certificate-expiring","title":"ONTAP Mediator Server Certificate Expiring","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.serverc.expiring</code></p> <p>This message occurs when the ONTAP Mediator server certificate is due to expire within the next 30 days.</p> <p>Remediation</p> <p>Before this certificate expires, remove the configuration of the current ONTAP Mediator by using the \"snapmirror mediator remove\" command. Update a new server certificate on the ONTAP Mediator server. Reconfigure access to the ONTAP Mediator by using the \"snapmirror mediator add\" command.</p>"},{"location":"resources/ems-alert-runbook/#ontap-mediator-unreachable","title":"ONTAP Mediator Unreachable","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sm.mediator.unreachable</code></p> <p>This message occurs when the ONTAP Mediator is unreachable on a cluster. As a result, SnapMirror failover is not possible.</p> <p>Remediation</p> <p>Check the network connectivity to the ONTAP Mediator by using the \"network ping\" and \"network traceroute\" commands. If the issue persists, remove the configuration of the current ONTAP Mediator by using the \"snapmirror mediator remove\" command. Reconfigure access to the ONTAP Mediator by using the \"snapmirror mediator add\" command.</p>"},{"location":"resources/ems-alert-runbook/#object-store-host-unresolvable","title":"Object Store Host Unresolvable","text":"<p>Impact: Availability</p> <p>EMS Event: <code>objstore.host.unresolvable</code></p> <p>The object store server host name cannot be resolved to an IP address. The object store client cannot communicate with the object-store server without resolving to an IP address. As a result, data might be inaccessible.</p> <p>Remediation</p> <p>Check the DNS configuration to verify that the host name is configured correctly with an IP address.</p>"},{"location":"resources/ems-alert-runbook/#object-store-intercluster-lif-down","title":"Object Store Intercluster LIF Down","text":"<p>Impact: Availability</p> <p>EMS Event: <code>objstore.interclusterlifDown</code></p> <p>The object-store client cannot find an operational LIF to communicate with the object store server. The node will not allow object store client traffic until the intercluster LIF is operational. As a result, data might be inaccessible.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Check the intercluster LIF status by using the \"network interface show -role intercluster\" command.</li> <li>Verify that the intercluster LIF is configured correctly and operational.</li> <li>If an intercluster LIF is not configured, add it by using the \"network interface create -role intercluster\" command.</li> </ol>"},{"location":"resources/ems-alert-runbook/#object-store-signature-mismatch","title":"Object Store Signature Mismatch","text":"<p>Impact: Availability</p> <p>EMS Event: <code>osc.signatureMismatch</code></p> <p>The request signature sent to the object store server does not match the signature calculated by the client. As a result, data might be inaccessible.</p> <p>Remediation</p> <p>Verify that the secret access key is configured correctly. If it is configured correctly, contact NetApp technical support for assistance.</p>"},{"location":"resources/ems-alert-runbook/#qos-monitor-memory-maxed-out","title":"QoS Monitor Memory Maxed Out","text":"<p>Impact: Capacity</p> <p>EMS Event: <code>qos.monitor.memory.maxed</code></p> <p>This event occurs when a QoS subsystem's dynamic memory reaches its limit for the current platform hardware. As a result, some QoS features might operate in a limited capacity.</p> <p>Remediation</p> <p>Delete some active workloads or streams to free up memory. Use the \"statistics show -object workload -counter ops\" command to determine which workloads are active. Active workloads show non-zero ops. Then use the \"workload delete \" command multiple times to remove specific workloads. Alternatively, use the \"stream delete -workload  *\" command to delete the associated streams from the active workload."},{"location":"resources/ems-alert-runbook/#readdir-timeout","title":"READDIR Timeout","text":"<p>Impact: Availability</p> <p>EMS Event: <code>wafl.readdir.expired</code></p> <p>A READDIR file operation has exceeded the timeout that it is allowed to run in WAFL. This can be because of very large or sparse directories. Corrective action is recommended.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Find information specific to recent directories that have had READDIR file operations expire by using the following 'diag' privilege nodeshell CLI command: wafl readdir notice show.</li> <li>Check if directories are indicated as sparse or not:     a. If a directory is indicated as sparse, it is recommended that you copy the contents of the directory to a new directory to remove the sparseness of the directory file.      b. If a directory is not indicated as sparse and the directory is large, it is recommended that you reduce the size of the directory file by reducing the number of file entries in the directory.</li> </ol>"},{"location":"resources/ems-alert-runbook/#ransomware-activity-detected","title":"Ransomware Activity Detected","text":"<p>Impact: Security</p> <p>EMS Event: <code>callhome.arw.activity.seen</code></p> <p>To protect the data from the detected ransomware, a Snapshot copy has been taken that can be used to restore original data.</p> <p>Your system generates and transmits an AutoSupport or \"call home\" message to NetApp technical support and any configured destinations. AutoSupport message improves problem determination and resolution.</p> <p>Remediation</p> <p>Refer to the anti-ransomware documentation to take remedial measures for ransomware activity. If you need assistance, contact NetApp technical support.</p>"},{"location":"resources/ems-alert-runbook/#relocation-of-storage-pool-failed","title":"Relocation of Storage Pool Failed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>arl.netra.ca.check.failed</code></p> <p>This event occurs during the relocation of an storage pool (aggregate), when the destination node cannot reach the object stores.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Verify that your intercluster LIF is online and functional by using the \"network interface show\" command.</li> <li>Check network connectivity to the object store server by using the\"'ping\" command over the destination node intercluster LIF. </li> <li>Verify that the configuration of your object store has not changed and that login and connectivity information is still accurate by using the \"aggregate object-store config show\" command.</li> </ol> <p>Alternatively, you can override the error by using the \"override-destination-checks\" parameter of the relocation command.</p> <p>Contact NetApp technical support for more information or assistance.</p>"},{"location":"resources/ems-alert-runbook/#san-active-active-state-changed","title":"SAN \"active-active\" State Changed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>scsiblade.san.config.active</code></p> <p>The SAN pathing is no longer symmetric. Pathing should be asymmetric only on ASA, because AFF and FAS are both asymmetric.</p> <p>Remediation</p> <p>Try and enable the \"active-active\" state. Contact customer support if the problem persists.</p>"},{"location":"resources/ems-alert-runbook/#sfp-in-fc-target-adapter-receiving-low-power","title":"SFP in FC target adapter receiving low power","text":"<p>Impact: Availability</p> <p>EMS Event: <code>scsitarget.fct.sfpRxPowerLow</code></p> <p>This alert occurs when the power received (RX) by a small form-factor pluggable transceiver (SFP in FC target) is at a level below the defined threshold, which might indicate a failing or faulty part.</p> <p>Remediation</p> <p>Monitor the operating value. If it continues to decrease, then replace the SFP and/or the cables.</p>"},{"location":"resources/ems-alert-runbook/#sfp-in-fc-target-adapter-transmitting-low-power","title":"SFP in FC target adapter transmitting low power","text":"<p>Impact: Availability</p> <p>EMS Event: <code>scsitarget.fct.sfpTxPowerLow</code></p> <p>This alert occurs when the power transmitted (TX) by a small form-factor pluggable transceiver (SFP in FC target) is at a level below the defined threshold, which might indicate a failing or faulty part.</p> <p>Remediation</p> <p>Monitor the operating value. If it continues to decrease, then replace the SFP and/or the cables.</p>"},{"location":"resources/ems-alert-runbook/#service-processor-heartbeat-missed","title":"Service Processor Heartbeat Missed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>callhome.sp.hbt.missed</code></p> <p>This message occurs when ONTAP does not receive an expected \"heartbeat\" signal from the Service Processor (SP). Along with this message, log files from SP will be sent out for debugging. ONTAP will reset the SP to attempt to restore communication. The SP will be unavailable for up to two minutes while it reboots.</p> <p>Remediation</p> <p>Contact NetApp technical support.</p>"},{"location":"resources/ems-alert-runbook/#service-processor-heartbeat-stopped","title":"Service Processor Heartbeat Stopped","text":"<p>Impact: Availability</p> <p>EMS Event: <code>callhome.sp.hbt.stopped</code></p> <p>This message occurs when ONTAP is no longer receiving heartbeats from the Service Processor (SP). Depending on the hardware design, the system may continue to serve data or may determine to shut down to prevent data loss or hardware damage. The system continues to serve data, but because the SP might not be working, the system cannot send notifications of down appliances, boot errors, or Open Firmware (OFW) Power-On Self-Test (POST) errors. If your system is configured to do so, it generates and transmits an AutoSupport (or 'call home') message to NetApp technical support and to the configured destinations. Successful delivery of an AutoSupport message significantly improves problem determination and resolution.</p> <p>Remediation</p> <p>If the system has shut down, attempt a hard power cycle: Pull the controller out from the chassis, push it back in then power on the system. Contact NetApp technical support if the problem persists after the power cycle, or for any other condition that may warrant attention.</p>"},{"location":"resources/ems-alert-runbook/#service-processor-not-configured","title":"Service Processor Not Configured","text":"<p>Impact: Availability</p> <p>EMS Event: <code>sp.notConfigured</code></p> <p>This event occurs on a weekly basis, to remind you to configure the Service Processor (SP). The SP is a physical device that is incorporated into your system to provide remote access and remote management capabilities. You should configure the SP to use its full functionality.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Configure the SP by using the \"system service-processor network modify\" command.</li> <li>Optionally, obtain the MAC address of the SP by using the \"system service-processor network show\" command.</li> <li>Verify the SP network configuration by using the \"system service-processor network show\" command.</li> <li>Verify that the SP can send an AutoSupport email by using the \"system service-processor autosupport invoke\" command.     NOTE: AutoSupport email hosts and recipients should be configured in ONTAP before you issue this command.</li> </ol>"},{"location":"resources/ems-alert-runbook/#service-processor-offline","title":"Service Processor Offline","text":"<p>Impact: Availability</p> <p>EMS Event: <code>sp.ipmi.lost.shutdown</code></p> <p>ONTAP is no longer receiving heartbeats from the Service Processor (SP), even though all the SP recovery actions have been taken. ONTAP cannot monitor the health of the hardware without the SP.</p> <p>The system will shut down to prevent hardware damage and data loss. Set up a panic alert to be notified immediately if the SP goes offline.</p> <p>Remediation</p> <p>Power-cycle the system by performing the following actions:</p> <ol> <li>Pull the controller out from the chassis.</li> <li>Push the controller back in.</li> <li>Turn the controller back on. If the problem persists, replace the controller module.</li> </ol>"},{"location":"resources/ems-alert-runbook/#shadow-copy-failed","title":"Shadow Copy Failed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>cifs.shadowcopy.failure</code></p> <p>A Volume Shadow Copy Service (VSS), a Microsoft Server backup and restore service operation, has failed.</p> <p>Remediation</p> <p>Check the following using the information provided in the event message:</p> <ul> <li>Is shadow copy configuration enabled?</li> <li>Are the appropriate licenses installed? </li> <li>On which shares is the shadow copy operation performed?</li> <li>Is the share name correct?</li> <li>Does the share path exist?</li> <li>What are the states of the shadow copy set and its shadow copies?</li> </ul>"},{"location":"resources/ems-alert-runbook/#shelf-fan-failed","title":"Shelf Fan Failed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>ses.status.fanError</code></p> <p>The indicated cooling fan or fan module of the shelf has failed. The disks in the shelf might not receive enough cooling airflow, which might result in disk failure.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Verify that the fan module is fully seated and secured.     NOTE: The fan is integrated into the power supply module in some disk shelves.</li> <li>If the issue persists, replace the fan module.</li> <li>If the issue still persists, contact NetApp technical support for assistance.</li> </ol>"},{"location":"resources/ems-alert-runbook/#snapmirror-relationship-common-snapshot-failed","title":"SnapMirror Relationship Common Snapshot Failed","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sms.common.snapshot.failed</code></p> <p>This message occurs when there is a failure in creating a common Snapshot(tm) copy. The SnapMirror\u00ae Sync relationship continues to be in \"in-sync\" status. The latest common Snapshot copy is used for recovery in case the relationship status changes to \"out-of-sync.\" The common Snapshot copy should be created at scheduled intervals to decrease the recovery time of \"out-of-sync\" relationships.</p> <p>Remediation</p> <p>Create a common snapshot manually by using the \"snapmirror update\" command at the destination.</p>"},{"location":"resources/ems-alert-runbook/#snapmirror-relationship-initialization-failed","title":"SnapMirror Relationship Initialization Failed","text":"<p>Impact: Protection</p> <p>EMS Event: <code>smc.snapmir.init.fail</code></p> <p>This message occurs when a SnapMirror\u00ae 'initialize' command fails and no more retries will be attempted.</p> <p>Remediation</p> <p>Check the reason for the error, take action accordingly, and issue the command again.</p>"},{"location":"resources/ems-alert-runbook/#snapmirror-relationship-out-of-sync","title":"SnapMirror Relationship Out of Sync","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sms.status.out.of.sync</code></p> <p>This event occurs when a SnapMirror(R) Sync relationship status changes from \"in-sync\" to \"out-of-sync\". I/O restrictions are imposed on the source volume based on the mode of replication. Client read or write access to the volume is not allowed for relationships of the \"strict-sync-mirror\" policy type. Data protection is affected.</p> <p>Remediation</p> <p>Check the network connection between the source and destination volumes. Monitor the SnapMirror Sync relationship status using the \"snapmirror show\" command. \"Auto-resync\" attempts to bring the relationship back to the \"in-sync\" status.</p>"},{"location":"resources/ems-alert-runbook/#snapmirror-relationship-resync-attempt-failed","title":"SnapMirror Relationship Resync Attempt Failed","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sms.resync.attempt.failed</code></p> <p>This message occurs when a resynchronize operation between the source volume and destination volume fails. The SnapMirror\u00ae Sync relationship is in \"out-of-sync\" status. Data protection is impacted.</p> <p>Remediation</p> <p>Monitor SnapMirror Sync status using the \"snapmirror show\" command. If the auto-resync attempts fail, bring the relationship back to \"in-sync\" status manually by using the \"snapmirror resync\" command.</p>"},{"location":"resources/ems-alert-runbook/#snapmirror-relationship-snapshot-is-not-replicated","title":"SnapMirror Relationship Snapshot is not Replicated","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sms.snap.not.replicated</code></p> <p>This message occurs when a Snapshot(tm) copy for SnapMirror\u00ae Synchronous relationship is not successfully replicated.</p> <p>Remediation</p> <p>No remediation required. User can trigger another snap create request to create a snapshot that exists on both primary and secondary site.</p>"},{"location":"resources/ems-alert-runbook/#snapmirror-active-sync-automatic-unplanned-failover-completed","title":"SnapMirror active sync Automatic Unplanned Failover Completed","text":"<p>Impact: Protection</p> <p>EMS Event: <code>smbc.aufo.completed</code></p> <p>This message occurs when the SnapMirror\u00ae active sync automatic unplanned failover operation completes.</p>"},{"location":"resources/ems-alert-runbook/#snapmirror-active-sync-automatic-unplanned-failover-failed","title":"SnapMirror active sync Automatic Unplanned Failover Failed","text":"<p>Impact: Protection</p> <p>EMS Event: <code>smbc.aufo.failed</code></p> <p>This message occurs when the SnapMirror\u00ae active sync automatic unplanned failover operation fails.</p> <p>Remediation</p> <p>The automatic unplanned failover will be retried internally. However, operations will be suspended till the failover is complete. If AUFO is failing persistently and the customer wishes to continue servicing IO, they can perform \"snapmirror delete -destination-path destination_path\" followed by \"snapmirror break\" on the volumes. Doing so will affect protection as the relationship will be removed, customer will need to re-establish protection relationship.</p>"},{"location":"resources/ems-alert-runbook/#snapmirror-active-sync-planned-failover-completed","title":"SnapMirror active sync Planned Failover Completed","text":"<p>Impact: Protection</p> <p>EMS Event: <code>smbc.pfo.completed</code></p> <p>This message occurs when the SnapMirror\u00ae active sync planned failover operation completes.</p>"},{"location":"resources/ems-alert-runbook/#snapmirror-active-sync-planned-failover-failed","title":"SnapMirror active sync Planned Failover Failed","text":"<p>Impact: Protection</p> <p>EMS Event: <code>smbc.pfo.failed</code></p> <p>This message occurs when the SnapMirror\u00ae active sync planned failover operation fails.</p> <p>Remediation</p> <p>Determine the cause of the failure by using the \"snapmirror failover show -fields error-reason\" command. If the relationship is out-of-sync, wait till the relationship is brought to in-sync. Else, address the error causing planned failover failure and then retry the \"snapmirror failover start -destination-path destination_path\" command.</p>"},{"location":"resources/ems-alert-runbook/#snapmirror-active-sync-relationship-out-of-sync","title":"SnapMirror active sync Relationship Out of Sync","text":"<p>Impact: Protection</p> <p>EMS Event: <code>sms.status.out.of.sync.cg</code></p> <p>This message occurs when a SnapMirror for Business Continuity (SMBC) relationship changes status from \"in-sync\" to \"out-of-sync\". Due to this RPO=0 data protection will be disrupted.</p> <p>Remediation</p> <p>Check the network connection between the source and destination volumes. Monitor the SMBC relationship status by using the \"snapmirror show\" command on the destination, and by using the \"snapmirror list-destinations\" command on the source. Auto-resync will attempt to bring the relationship back to \"in-sync\" status. If the resync fails, verify that all the nodes in the cluster are in quorum and are healthy.</p>"},{"location":"resources/ems-alert-runbook/#storage-switch-power-supplies-failed","title":"Storage Switch Power Supplies Failed","text":"<p>Impact: Availability</p> <p>EMS Event: <code>cluster.switch.pwr.fail</code></p> <p>There is a missing power supply in the cluster switch. Redundancy is reduced, risk of outage with any further power failures.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Ensure that the power supply mains, which supplies power to the cluster switch, is turned on.</li> <li>Ensure that the power cord is connected to the power supply.</li> </ol> <p>Contact NetApp technical support if the issue persists.</p>"},{"location":"resources/ems-alert-runbook/#storage-vm-anti-ransomware-monitoring","title":"Storage VM Anti-ransomware Monitoring","text":"<p>Impact: Security</p> <p>EMS Event: <code>arw.vserver.state</code></p> <p>The anti-ransomware monitoring for the storage VM is disabled.</p> <p>Remediation</p> <p>Enable anti-ransomware to protect the storage VM.</p>"},{"location":"resources/ems-alert-runbook/#storage-vm-stop-succeeded","title":"Storage VM Stop Succeeded","text":"<p>Impact: Availability</p> <p>EMS Event: <code>vserver.stop.succeeded</code></p> <p>This message occurs when a 'vserver stop' operation succeeds.</p> <p>Remediation</p> <p>Use 'vserver start' command to start the data access on a storage VM.</p>"},{"location":"resources/ems-alert-runbook/#system-cannot-operate-due-to-main-unit-fan-failure","title":"System Cannot Operate Due to Main Unit Fan Failure","text":"<p>Impact: Availability</p> <p>EMS Event: <code>monitor.fan.critical</code></p> <p>One or more main unit fans have failed, disrupting system operation. This might lead to a potential data loss.</p> <p>Remediation</p> <p>Replace the failed fans.</p>"},{"location":"resources/ems-alert-runbook/#too-many-cifs-authentication","title":"Too Many CIFS Authentication","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.cifsManyAuths</code></p> <p>Many authentication negotiations have occurred simultaneously. There are 256 incomplete new session requests from this client.</p> <p>Remediation</p> <p>Investigate why the client has created 256 or more new connection requests. You might have to contact the vendor of the client or of the application to determine why the error occurred.</p>"},{"location":"resources/ems-alert-runbook/#unassigned-disks","title":"Unassigned Disks","text":"<p>Impact: Availability</p> <p>EMS Event: <code>unowned.disk.reminder</code></p> <p>System has unassigned disks - capacity is being wasted and your system may have some misconfiguration or partial configuration change applied.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Determine which disks are unassigned by using the \"disk show -n\" command.</li> <li>Assign the disks to a system by using the \"disk assign\" command.</li> </ol>"},{"location":"resources/ems-alert-runbook/#unauthorized-user-access-to-admin-share","title":"Unauthorized User Access to Admin Share","text":"<p>Impact: Security</p> <p>EMS Event: <code>Nblade.vscanBadUserPrivAccess</code></p> <p>A client has attempted to connect to the privileged ONTAP_ADMIN$ share even though their logged-in user is not an allowed user.</p> <p>Remediation</p> <p>Perform the following corrective actions:</p> <ol> <li>Ensure that the mentioned username and IP address is configured in one of the active Vscan scanner pools.</li> <li>Check the scanner pool configuration that is currently active by using the \"vserver vscan scanner pool show-active\" command.</li> </ol>"},{"location":"resources/ems-alert-runbook/#virus-detected","title":"Virus Detected","text":"<p>Impact: Availability</p> <p>EMS Event: <code>Nblade.vscanVirusDetected</code></p> <p>A Vscan server has reported an error to the storage system. This typically indicates that a virus has been found. However, other errors on the Vscan server can cause this event.</p> <p>Client access to the file is denied. The Vscan server might, depending on its settings and configuration, clean the file, quarantine it, or delete it.</p> <p>Remediation</p> <p>Check the log of the Vscan server reported in the \"syslog\" event to see if it was able to successfully clean, quarantine, or delete the infected file. If it was not able to do so, a system administrator might have to manually delete the file.</p>"},{"location":"resources/ems-alert-runbook/#volume-anti-ransomware-monitoring","title":"Volume Anti-ransomware Monitoring","text":"<p>Impact: Security</p> <p>EMS Event: <code>arw.volume.state</code></p> <p>The anti-ransomware monitoring for the volume is disabling.</p> <p>Remediation</p> <p>Enable anti-ransomware to protect the storage VM.</p>"},{"location":"resources/ems-alert-runbook/#volume-automatic-resizing-succeeded","title":"Volume Automatic Resizing Succeeded","text":"<p>Impact: Capacity</p> <p>EMS Event: <code>wafl.vol.autoSize.done</code></p> <p>This event occurs when the automatic resizing of a volume is successful. It happens when the \"autosize grow\" option is enabled, and the volume reaches the grow threshold percentage.</p>"},{"location":"resources/ems-alert-runbook/#volume-offline","title":"Volume Offline","text":"<p>Impact: Availability</p> <p>EMS Event: <code>wafl.vvol.offline</code></p> <p>This message indicates that a volume has been taken offline.</p> <p>Remediation</p> <p>Bring the volume back online.</p>"},{"location":"resources/ems-alert-runbook/#volume-restricted","title":"Volume Restricted","text":"<p>Impact: Availability</p> <p>EMS Event: <code>wafl.vvol.restrict</code></p> <p>This event indicates that a flexible volume is made restricted.</p> <p>Remediation</p> <p>Bring the volume back online.</p>"},{"location":"resources/matrix/","title":"Matrix","text":""},{"location":"resources/matrix/#matrix","title":"Matrix","text":"<p>The \u2133atri\u03c7 package provides the <code>matrix.Matrix</code> data-structure for storage, manipulation and transmission of both numeric and non-numeric (string) data. It is utilized by core components of Harvest, including collectors, plugins and exporters. It furthermore serves as an interface between these components, such that \"the left hand does not know what the right hand does\".</p> <p>Internally, the Matrix is a collection of metrics (<code>matrix.Metric</code>) and instances (<code>matrix.Instance</code>) in the form of a 2-dimensional array:</p> <p></p> <p>Since we use hash tables for accessing the elements of the array, all metrics and instances added to the matrix must have a unique key. Metrics are typed and contain the numeric data (i.e. rows) of the Matrix. Instances only serve as pointers to the columents of the Matrix, but they also store non-numeric data as labels (<code>*dict.Dict</code>).</p> <p>This package is the architectural backbone of Harvest, therefore understanding it is key for an advanced user or contributor.</p>"},{"location":"resources/matrix/#basic-usage","title":"Basic Usage","text":""},{"location":"resources/matrix/#initialize","title":"Initialize","text":"<p><pre><code>func matrix.New(name, object string, identifier string) *Matrix\n// always returns successfully pointer to (empty) Matrix \n</code></pre> This section describes how to properly initialize a new Matrix instance. Note that if you write a collector, a Matrix instance is already properly initialized for you (as <code>MyCollector.matrix</code>), and if you write a plugin or exporter, it is passed to you from the collector. That means most of the time you don't have to worry about initializing the Matrix.</p> <p><code>matrix.New()</code> requires three arguments: * <code>UUID</code> is by convention the collector name (e.g. <code>MyCollector</code>) if the Matrix comes from a collector, or the collector name and the plugin name concatenated with a <code>.</code> (e.g. <code>MyCollector.MyPlugin</code>) if the Matrix comes from a plugin. * <code>object</code> is a description of the instances of the Matrix. For example, if we collect data about cars and our instances are cars, a good name would be <code>car</code>. * <code>identifier</code> is a unique key used to identify a matrix instance</p> <p>Note that <code>identifier</code> should uniquely identify a Matrix instance. This is not a strict requirement, but guarantees that your data is properly handled by exporters.</p>"},{"location":"resources/matrix/#example","title":"Example","text":"<p>Here is an example from the point of view of a collector:</p> <pre><code>import \"github.com/netapp/harvest/v2/pkg/matrix\"\n\nvar myMatrix *matrix.Matrix\n\nmyMatrix = matrix.New(\"CarCollector\", \"car\", \"car\")\n</code></pre> <p>Next step is to add metrics and instances to our Matrix.</p>"},{"location":"resources/matrix/#add-instances-and-instance-labels","title":"Add instances and instance labels","text":"<pre><code>func (x *Matrix) NewInstance(key string) (*Instance, error)\n// returns pointer to a new Instance, or nil with error (if key is not unique)\n</code></pre> <p><pre><code>func (i *Instance) SetLabel(key, value string)\n// always successful, overwrites existing values\n</code></pre> <pre><code>func (i *Instance) GetLabel(key) string\n// always returns value, if label is not set, returns empty string\n</code></pre></p> <p>Once we have initialized a Matrix, we can add instances and add labels to our instances.</p>"},{"location":"resources/matrix/#example_1","title":"Example","text":"<pre><code>var (\n    instance *matrix.Instance\n    err error\n)\nif instance, err = myMatrix.NewInstance(\"SomeCarMark\"); err != nil {\n    return err\n    // or handle err, but beware that instance is nil\n}\ninstance.SetLabel(\"mark\", \"SomeCarMark\")\ninstance.SetLabel(\"color\", \"red\")\ninstance.SetLabel(\"style\", \"coupe\")\n// add as many labels as you like\ninstance.GetLabel(\"color\") // return \"red\"\ninstance.GetLabel(\"owner\") // returns \"\"\n</code></pre>"},{"location":"resources/matrix/#add-metrics","title":"Add Metrics","text":"<pre><code>func (x *Matrix) NewMetricInt64(key string) (Metric, error)\n// returns pointer to a new MetricInt64, or nil with error (if key is not unique)\n// note that Metric is an interface\n</code></pre> <p>Metrics are typed and there are currently 8 types, all can be created with the same signature as above: * <code>MetricUint8</code> * <code>MetricUint32</code> * <code>MetricUint64</code> * <code>MetricInt</code> * <code>MetricInt32</code> * <code>MetricInt64</code> * <code>MetricFloat32</code> * <code>MetricFloat64</code> * We are able to read from and write to a metric instance using different types (as displayed in the next section), however choosing a type wisely ensures that this is done efficiently and overflow does not occur.</p> <p>We can add labels to metrics just like instances. This is usually done when we deal with histograms:</p> <p><pre><code>func (m Metric) SetLabel(key, value string)\n// always successful, overwrites existing values\n</code></pre> <pre><code>func (m Metric) GetLabel(key) string\n// always returns value, if label is not set, returns empty string\n</code></pre></p>"},{"location":"resources/matrix/#example_2","title":"Example","text":"<p>Continuing our Matrix for collecting car-related data:</p> <pre><code>var (\n    speed, length matrix.Metric\n    err error\n)\n\nif speed, err = myMatrix.NewMetricUint32(\"max_speed\"); err != nil {\n    return err\n}\nif length, err = myMatrix.NewMetricFloat32(\"length_in_mm\"); err != nil {\n    return err\n}\n</code></pre>"},{"location":"resources/matrix/#write-numeric-data","title":"Write numeric data","text":"<p><pre><code>func (x *Matrix) Reset()\n// flush numeric data from previous poll\n</code></pre> <pre><code>func (m Metric) SetValueInt64(i *Instance, v int64) error\nfunc (m Metric) SetValueUint8(i *Instance, v uint8) error\nfunc (m Metric) SetValueUint64(i *Instance, v uint64) error\nfunc (m Metric) SetValueFloat64(i *Instance, v float64) error\nfunc (m Metric) SetValueBytes(i *Instance, v []byte) error\nfunc (m Metric) SetValueString(i *Instance, v []string) error\n// sets the numeric value for the instance i to v\n// returns error if v is invalid (explained below)\n</code></pre> <pre><code>func (m Metric) AddValueInt64(i *Instance, v int64) error\n// increments the numeric value for the instance i by v\n// same signatures for all the types defined above\n</code></pre></p> <p>When possible you should reuse a Matrix for each data poll, but to do that, you need to call <code>Reset()</code> to drop old data from the Matrix. It is safe to add new instances and metrics after calling this method.</p> <p>The <code>SetValue*()</code> and <code>AddValue*()</code> methods are typed same as the metrics. Even though you are not required to use the same type as the metric, it is the safest and most efficient way.</p> <p>Since most collectors get their data as bytes or strings, it is recommended to use the <code>SetValueString()</code> and <code>SetValueBytes()</code> methods.</p> <p>These methods return an error if value <code>v</code> can not be converted to the type of the metric. Error is always <code>nil</code> when the type of <code>v</code> matches the type of the metric.</p>"},{"location":"resources/matrix/#example_3","title":"Example","text":"<p>Continuing with the previous examples:</p> <pre><code>if err = myMatrix.Reset(); err != nil {\n    return\n}\n// write numbers to the matrix using the instance and the metrics we have created\n\n// let the metric do the conversion for us\nif err = speed.SetValueString(instance, \"500\"); err != nil {\n    logger.Error(me.Prefix, \"set speed value: \", err)\n}\n// here we ignore err since type is the metric type\nlength.SetValueFloat64(instance, 10000.00)\n\n// safe to add new instances\nvar instance2 matrix.Instance\nif instance2, err = myMatrix.NewInstance(\"SomeOtherCar\"); err != nil {\n    return err\n}\n\n// possible and safe even though speed has type Float32\n} if err = length.SetValueInt64(instance2, 13000); err != nil {\n    logger.Error(me.Prefix, \"set speed value:\", err)\n}\n\n// possible, but will overflow since speed is unsigned\n} if err = speed.SetValueInt64(instance2, -500); err != nil {\n    logger.Error(me.Prefix, \"set length value:\", err)\n}\n</code></pre>"},{"location":"resources/matrix/#read-metrics-and-instances","title":"Read metrics and instances","text":"<p>In this section we switch gears and look at the Matrix from the point of view of plugins and exporters. Both those components need to read from the Matrix and have no knowledge of its origin or contents.</p> <p><pre><code>func (x *Matrix) GetMetrics() map[string]Metric\n// returns all metrics in the Matrix\n</code></pre> <pre><code>func (x *Matrix) GetInstances() map[string]*Instance\n// returns all instances in the Matrix\n</code></pre></p> <p>Usually we will do a nested loop with these two methods to read all data in the Matrix. See examples below.</p>"},{"location":"resources/matrix/#example-iterate-over-instances","title":"Example: Iterate over instances","text":"<p>In this example the method <code>PrintKeys()</code> will iterate over a Matrix and print all metric and instance keys.</p> <pre><code>func PrintKeys(x *matrix.Matrix) {\n    for instanceKey, _ := range x.GetInstances() {\n        fmt.Println(\"instance key=\", instanceKey)\n    }\n}\n</code></pre>"},{"location":"resources/matrix/#example-read-instance-labels","title":"Example: Read instance labels","text":"<p>Each instance has a set of labels. We can iterate over these labels with the <code>GetLabel()</code> and <code>GetLabels()</code> method. In this example, we write a function that prints all labels of an instance:</p> <pre><code>func PrintLabels(instance *matrix.Instance) {\n    for label, value, := range instance.GetLabels().Map() {\n        fmt.Printf(\"%s=%s\\n\", label, value)\n    }\n}\n</code></pre>"},{"location":"resources/matrix/#example-read-metric-values-labels","title":"Example: Read metric values labels","text":"<p>Similar to the <code>SetValue*</code> and <code>AddValue*</code> methods, you can choose a type when reading from a metric. If you don't know the type of the metric, it is safe to read it as a string. In this example, we write a function that prints the value of a metric for all instances in a Matrix:</p> <pre><code>func PrintMetricValues(x *matrix.Matrix, m matrix.Metric) {\n    for key, instance := range x.GetInstances() {\n        if value, has := m.GetValueString(instance) {\n            fmt.Printf(\"instance %s = %s\\n\", key, value)\n        } else {\n            fmt.Printf(\"instance %s has no value\\n\", key)\n        }\n    }\n}\n</code></pre>"},{"location":"resources/power-algorithm/","title":"Power Algorithm","text":"<p>Gathering power metrics requires a cluster with:</p> <ul> <li>ONTAP versions 9.6+</li> <li>REST enabled, even when using the ZAPI collector. After granting REST permissions, restart Harvest.</li> </ul> <p>REST is required because it is the only way to collect chassis field-replaceable-unit (FRU) information via the REST API <code>/api/private/cli/system/chassis/fru</code>.</p>"},{"location":"resources/power-algorithm/#how-does-harvest-calculate-cluster-power","title":"How does Harvest calculate cluster power?","text":"<p>Cluster power is the sum of a cluster's node(s) power +  the sum of attached disk shelve(s) power.</p> <p>Redundant power supplies (PSU) load-share the total load. With n PSUs, each PSU does roughly (1/n) the work (the actual amount is slightly more than a single PSU due to additional fans.)</p>"},{"location":"resources/power-algorithm/#node-power","title":"Node power","text":"<p>Node power is calculated by collecting power supply unit (PSU) power, as reported by REST  <code>/api/private/cli/system/environment/sensors</code> or by ZAPI <code>environment-sensors-get-iter</code>.</p> <p>When a power supply is shared between controllers, the PSU's power will be evenly divided across the controllers due to load-sharing.</p> <p>For example:</p> <ul> <li>FAS2750 models have two power supplies that power both controllers. Each PSU is shared between the two controllers.</li> <li>A800 models have four power supplies. <code>PSU1</code> and <code>PSU2</code> power <code>Controller1</code> and <code>PSU3</code> and <code>PSU4</code> power <code>Controller2</code>. Each PSU provides power to a single controller.</li> </ul> <p>Harvest determines whether a PSU is shared between controllers by consulting the <code>connected_nodes</code> of each PSU, as reported by ONTAP via <code>/api/private/cli/system/chassis/fru</code></p>"},{"location":"resources/power-algorithm/#disk-shelf-power","title":"Disk shelf power","text":"<p>Disk shelf power is calculated by collecting <code>psu.power_drawn</code>, as reported by REST, via <code>/api/storage/shelves</code> or <code>sensor-reading</code>, as reported by ZAPI <code>storage-shelf-info-get-iter</code>.</p> <p>When shelves report both input and output rail sensors, Harvest classifies the rail from sensor labels and uses only input-rail pairs to calculate shelf power. If no input rail is detected, Harvest falls back to output-rail pairs, and if rail classification is unavailable, all pairs are summed.</p> <p>The power for embedded shelves is ignored, since that power is already accounted for in the controller's power draw.</p>"},{"location":"resources/power-algorithm/#examples","title":"Examples","text":""},{"location":"resources/power-algorithm/#fas2750","title":"FAS2750","text":"<pre><code># Power Metrics for 10.61.183.200\n\n## ONTAP version NetApp Release 9.8P16: Fri Dec 02 02:05:05 UTC 2022\n\n## Nodes\nsystem show\n       Node         |  Model  | SerialNumber  \n----------------------+---------+---------------\ncie-na2750-g1344-01 | FAS2750 | 621841000123  \ncie-na2750-g1344-02 | FAS2750 | 621841000124\n\n## Chassis\nsystem chassis fru show\n ChassisId   |      Name       |         Fru         |    Type    | Status | NumNodes |              ConnectedNodes               \n---------------+-----------------+---------------------+------------+--------+----------+-------------------------------------------\n021827030435 | 621841000123    | cie-na2750-g1344-01 | controller | ok     |        1 | cie-na2750-g1344-01                       \n021827030435 | 621841000124    | cie-na2750-g1344-02 | controller | ok     |        1 | cie-na2750-g1344-02                       \n021827030435 | PSQ094182201794 | PSU2 FRU            | psu        | ok     |        2 | cie-na2750-g1344-02, cie-na2750-g1344-01  \n021827030435 | PSQ094182201797 | PSU1 FRU            | psu        | ok     |        2 | cie-na2750-g1344-02, cie-na2750-g1344-01\n\n## Sensors\nsystem environment sensors show\n(filtered by power, voltage, current)\n       Node         |     Name      |  Type   | State  | Value | Units  \n----------------------+---------------+---------+--------+-------+--------\ncie-na2750-g1344-01 | PSU1 12V Curr | current | normal |  9920 | mA     \ncie-na2750-g1344-01 | PSU1 12V      | voltage | normal | 12180 | mV     \ncie-na2750-g1344-01 | PSU1 5V Curr  | current | normal |  4490 | mA     \ncie-na2750-g1344-01 | PSU1 5V       | voltage | normal |  5110 | mV     \ncie-na2750-g1344-01 | PSU2 12V Curr | current | normal |  9140 | mA     \ncie-na2750-g1344-01 | PSU2 12V      | voltage | normal | 12100 | mV     \ncie-na2750-g1344-01 | PSU2 5V Curr  | current | normal |  4880 | mA     \ncie-na2750-g1344-01 | PSU2 5V       | voltage | normal |  5070 | mV     \ncie-na2750-g1344-02 | PSU1 12V Curr | current | normal |  9920 | mA     \ncie-na2750-g1344-02 | PSU1 12V      | voltage | normal | 12180 | mV     \ncie-na2750-g1344-02 | PSU1 5V Curr  | current | normal |  4330 | mA     \ncie-na2750-g1344-02 | PSU1 5V       | voltage | normal |  5110 | mV     \ncie-na2750-g1344-02 | PSU2 12V Curr | current | normal |  9170 | mA     \ncie-na2750-g1344-02 | PSU2 12V      | voltage | normal | 12100 | mV     \ncie-na2750-g1344-02 | PSU2 5V Curr  | current | normal |  4720 | mA     \ncie-na2750-g1344-02 | PSU2 5V       | voltage | normal |  5070 | mV\n\n## Shelf PSUs\nstorage shelf show\nShelf | ProductId | ModuleType | PSUId | PSUIsEnabled | PSUPowerDrawn | Embedded  \n------+-----------+------------+-------+--------------+---------------+---------\n  1.0 | DS224-12  | iom12e     | 1,2   | true,true    | 1397,1318     | true\n\n### Controller Power From Sum(InVoltage * InCurrent)/NumNodes\nPower: 256W\n</code></pre>"},{"location":"resources/power-algorithm/#aff-a800","title":"AFF A800","text":"<pre><code># Power Metrics for 10.61.124.110\n\n## ONTAP version NetApp Release 9.13.1P1: Tue Jul 25 10:19:28 UTC 2023\n\n## Nodes\nsystem show\n  Node    |  Model   | SerialNumber  \n----------+----------+-------------\na800-1-01 | AFF-A800 | 941825000071  \na800-1-02 | AFF-A800 | 941825000072\n\n## Chassis\nsystem chassis fru show\n   ChassisId    |      Name      |    Fru    |    Type    | Status | NumNodes | ConnectedNodes  \n----------------+----------------+-----------+------------+--------+----------+---------------\nSHFFG1826000154 | 941825000071   | a800-1-01 | controller | ok     |        1 | a800-1-01       \nSHFFG1826000154 | 941825000072   | a800-1-02 | controller | ok     |        1 | a800-1-02       \nSHFFG1826000154 | EEQT1822002800 | PSU1 FRU  | psu        | ok     |        1 | a800-1-02       \nSHFFG1826000154 | EEQT1822002804 | PSU2 FRU  | psu        | ok     |        1 | a800-1-02       \nSHFFG1826000154 | EEQT1822002805 | PSU2 FRU  | psu        | ok     |        1 | a800-1-01       \nSHFFG1826000154 | EEQT1822002806 | PSU1 FRU  | psu        | ok     |        1 | a800-1-01\n\n## Sensors\nsystem environment sensors show\n(filtered by power, voltage, current)\n  Node    |     Name      |  Type   | State  | Value | Units  \n----------+---------------+---------+--------+-------+------\na800-1-01 | PSU1 Power In | unknown | normal |   376 | W      \na800-1-01 | PSU2 Power In | unknown | normal |   411 | W      \na800-1-02 | PSU1 Power In | unknown | normal |   383 | W      \na800-1-02 | PSU2 Power In | unknown | normal |   433 | W\n\n## Shelf PSUs\nstorage shelf show\nShelf |  ProductId  | ModuleType | PSUId | PSUIsEnabled | PSUPowerDrawn | Embedded  \n------+-------------+------------+-------+--------------+---------------+---------\n  1.0 | FS4483PSM3E | psm3e      |       |              |               | true      \n\n### Controller Power From Sum(InPower sensors)\nPower: 1603W\n</code></pre>"},{"location":"resources/rest-perf-metrics/","title":"REST Perf Metrics","text":"<p>This document describes implementation details about ONTAP's REST performance metrics endpoints, including how we built the Harvest RESTPerf collectors. </p> <p>Warning</p> <p>These are implementation details about ONTAP's REST performance metrics. You do not need to understand any of this to use Harvest. If you want to know how to use or configure Harvest's REST collectors, checkout the Rest Collector documentation instead. If you're interested in the gory details. Read on.</p>"},{"location":"resources/rest-perf-metrics/#introduction","title":"Introduction","text":"<p>ONTAP REST metrics were introduced in ONTAP <code>9.11.1</code> and included parity with Harvest-collected ZAPI performance metrics by ONTAP <code>9.12.1</code>.</p>"},{"location":"resources/rest-perf-metrics/#performance-rest-queries","title":"Performance REST queries","text":"<p>Mapping table</p> ZAPI REST Comment <code>perf-object-counter-list-info</code> <code>/api/cluster/counter/tables</code> returns counter tables and schemas <code>perf-object-instance-list-info-iter</code> <code>/api/cluster/counter/tables/{name}/rows</code> returns instances and counter values <code>perf-object-get-instances</code> <code>/api/cluster/counter/tables/{name}/rows</code> returns instances and counter values <p>Performance REST responses include <code>properties</code> and <code>counters</code>. Counters are metric-like, while properties include instance attributes.</p>"},{"location":"resources/rest-perf-metrics/#examples","title":"Examples","text":""},{"location":"resources/rest-perf-metrics/#ask-ontap-for-all-resources-that-report-performance-metrics","title":"Ask ONTAP for all resources that report performance metrics","text":"<pre><code>curl 'https://$clusterIP/api/cluster/counter/tables'\n</code></pre> Response <p> <pre><code>{\n  \"records\": [\n    {\n      \"name\": \"copy_manager\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/copy_manager\"\n        }\n      }\n    },\n    {\n      \"name\": \"copy_manager:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/copy_manager%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"disk\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/disk\"\n        }\n      }\n    },\n    {\n      \"name\": \"disk:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/disk%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"disk:raid_group\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/disk%3Araid_group\"\n        }\n      }\n    },\n    {\n      \"name\": \"external_cache\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/external_cache\"\n        }\n      }\n    },\n    {\n      \"name\": \"fcp\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/fcp\"\n        }\n      }\n    },\n    {\n      \"name\": \"fcp:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/fcp%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"fcp_lif\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/fcp_lif\"\n        }\n      }\n    },\n    {\n      \"name\": \"fcp_lif:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/fcp_lif%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"fcp_lif:port\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/fcp_lif%3Aport\"\n        }\n      }\n    },\n    {\n      \"name\": \"fcp_lif:svm\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/fcp_lif%3Asvm\"\n        }\n      }\n    },\n    {\n      \"name\": \"fcvi\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/fcvi\"\n        }\n      }\n    },\n    {\n      \"name\": \"headroom_aggregate\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/headroom_aggregate\"\n        }\n      }\n    },\n    {\n      \"name\": \"headroom_cpu\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/headroom_cpu\"\n        }\n      }\n    },\n    {\n      \"name\": \"host_adapter\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/host_adapter\"\n        }\n      }\n    },\n    {\n      \"name\": \"iscsi_lif\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/iscsi_lif\"\n        }\n      }\n    },\n    {\n      \"name\": \"iscsi_lif:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/iscsi_lif%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"iscsi_lif:svm\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/iscsi_lif%3Asvm\"\n        }\n      }\n    },\n    {\n      \"name\": \"lif\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/lif\"\n        }\n      }\n    },\n    {\n      \"name\": \"lif:svm\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/lif%3Asvm\"\n        }\n      }\n    },\n    {\n      \"name\": \"lun\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/lun\"\n        }\n      }\n    },\n    {\n      \"name\": \"lun:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/lun%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"lun:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/lun%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"namespace\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/namespace\"\n        }\n      }\n    },\n    {\n      \"name\": \"namespace:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/namespace%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"nfs_v4_diag\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/nfs_v4_diag\"\n        }\n      }\n    },\n    {\n      \"name\": \"nic_common\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/nic_common\"\n        }\n      }\n    },\n    {\n      \"name\": \"nvmf_lif\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/nvmf_lif\"\n        }\n      }\n    },\n    {\n      \"name\": \"nvmf_lif:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/nvmf_lif%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"nvmf_lif:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/nvmf_lif%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"nvmf_lif:port\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/nvmf_lif%3Aport\"\n        }\n      }\n    },\n    {\n      \"name\": \"object_store_client_op\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/object_store_client_op\"\n        }\n      }\n    },\n    {\n      \"name\": \"path\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/path\"\n        }\n      }\n    },\n    {\n      \"name\": \"processor\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/processor\"\n        }\n      }\n    },\n    {\n      \"name\": \"processor:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/processor%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"qos\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/qos\"\n        }\n      }\n    },\n    {\n      \"name\": \"qos:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/qos%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"qos:policy_group\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/qos%3Apolicy_group\"\n        }\n      }\n    },\n    {\n      \"name\": \"qos_detail\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/qos_detail\"\n        }\n      }\n    },\n    {\n      \"name\": \"qos_detail_volume\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/qos_detail_volume\"\n        }\n      }\n    },\n    {\n      \"name\": \"qos_volume\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/qos_volume\"\n        }\n      }\n    },\n    {\n      \"name\": \"qos_volume:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/qos_volume%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"qtree\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/qtree\"\n        }\n      }\n    },\n    {\n      \"name\": \"qtree:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/qtree%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_cifs\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_cifs\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_cifs:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_cifs%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_cifs:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_cifs%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v3\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v3\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v3:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v3%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v3:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v3%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v4\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v4\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v41\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v41\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v41:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v41%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v41:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v41%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v42\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v42\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v42:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v42%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v42:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v42%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v4:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v4%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"svm_nfs_v4:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/svm_nfs_v4%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"system\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/system\"\n        }\n      }\n    },\n    {\n      \"name\": \"system:constituent\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/system%3Aconstituent\"\n        }\n      }\n    },\n    {\n      \"name\": \"system:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/system%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"token_manager\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/token_manager\"\n        }\n      }\n    },\n    {\n      \"name\": \"volume\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/volume\"\n        }\n      }\n    },\n    {\n      \"name\": \"volume:node\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/volume%3Anode\"\n        }\n      }\n    },\n    {\n      \"name\": \"volume:svm\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/volume%3Asvm\"\n        }\n      }\n    },\n    {\n      \"name\": \"wafl\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/wafl\"\n        }\n      }\n    },\n    {\n      \"name\": \"wafl_comp_aggr_vol_bin\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/wafl_comp_aggr_vol_bin\"\n        }\n      }\n    },\n    {\n      \"name\": \"wafl_hya_per_aggregate\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/wafl_hya_per_aggregate\"\n        }\n      }\n    },\n    {\n      \"name\": \"wafl_hya_sizer\",\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/wafl_hya_sizer\"\n        }\n      }\n    }\n  ],\n  \"num_records\": 71,\n  \"_links\": {\n    \"self\": {\n      \"href\": \"/api/cluster/counter/tables/\"\n    }\n  }\n}\n</code></pre> </p>"},{"location":"resources/rest-perf-metrics/#node-performance-metrics-metadata","title":"Node performance metrics metadata","text":"<p>Ask ONTAP to return the schema for <code>system:node</code>. This will include the name, description, and metadata for all counters associated with <code>system:node</code>.</p> <pre><code>curl 'https://$clusterIP/api/cluster/counter/tables/system:node?return_records=true'\n</code></pre> Response <p> <pre><code>{\n  \"name\": \"system:node\",\n  \"description\": \"The System table reports general system activity. This includes global throughput for the main services, I/O latency, and CPU activity. The alias name for system:node is system_node.\",\n  \"counter_schemas\": [\n    {\n      \"name\": \"average_processor_busy_percent\",\n      \"description\": \"Average processor utilization across all processors in the system\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"cifs_ops\",\n      \"description\": \"Number of CIFS operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"cp\",\n      \"description\": \"CP time rate\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"cp_time\",\n      \"description\": \"Processor time in CP\",\n      \"type\": \"delta\",\n      \"unit\": \"microsec\"\n    },\n    {\n      \"name\": \"cpu_busy\",\n      \"description\": \"System CPU resource utilization. Returns a computed percentage for the default CPU field. Basically computes a 'cpu usage summary' value which indicates how 'busy' the system is based upon the most heavily utilized domain. The idea is to determine the amount of available CPU until we're limited by either a domain maxing out OR we exhaust all available idle CPU cycles, whichever occurs first.\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"cpu_elapsed_time\",\n      \"description\": \"Elapsed time since boot\",\n      \"type\": \"delta\",\n      \"unit\": \"microsec\"\n    },\n    {\n      \"name\": \"disk_data_read\",\n      \"description\": \"Number of disk kilobytes (KB) read per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"disk_data_written\",\n      \"description\": \"Number of disk kilobytes (KB) written per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"domain_busy\",\n      \"description\": \"Array of processor time in percentage spent in various domains\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"domain_shared\",\n      \"description\": \"Array of processor time in percentage spent in various shared domains\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"dswitchto_cnt\",\n      \"description\": \"Array of processor time in percentage spent in domain switch\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"fcp_data_received\",\n      \"description\": \"Number of FCP kilobytes (KB) received per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"fcp_data_sent\",\n      \"description\": \"Number of FCP kilobytes (KB) sent per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"fcp_ops\",\n      \"description\": \"Number of FCP operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"hard_switches\",\n      \"description\": \"Number of context switches per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"hdd_data_read\",\n      \"description\": \"Number of HDD Disk kilobytes (KB) read per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"hdd_data_written\",\n      \"description\": \"Number of HDD kilobytes (KB) written per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"idle\",\n      \"description\": \"Processor idle rate percentage\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"idle_time\",\n      \"description\": \"Processor idle time\",\n      \"type\": \"delta\",\n      \"unit\": \"microsec\"\n    },\n    {\n      \"name\": \"instance_name\",\n      \"description\": \"Node name\",\n      \"type\": \"string\",\n      \"unit\": \"none\"\n    },\n    {\n      \"name\": \"interrupt\",\n      \"description\": \"Processor interrupt rate percentage\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"interrupt_in_cp\",\n      \"description\": \"Processor interrupt rate percentage\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cp_time\"\n      }\n    },\n    {\n      \"name\": \"interrupt_in_cp_time\",\n      \"description\": \"Processor interrupt in CP time\",\n      \"type\": \"delta\",\n      \"unit\": \"microsec\"\n    },\n    {\n      \"name\": \"interrupt_num\",\n      \"description\": \"Processor interrupt number\",\n      \"type\": \"delta\",\n      \"unit\": \"none\"\n    },\n    {\n      \"name\": \"interrupt_num_in_cp\",\n      \"description\": \"Number of processor interrupts in CP\",\n      \"type\": \"delta\",\n      \"unit\": \"none\"\n    },\n    {\n      \"name\": \"interrupt_time\",\n      \"description\": \"Processor interrupt time\",\n      \"type\": \"delta\",\n      \"unit\": \"microsec\"\n    },\n    {\n      \"name\": \"intr_cnt\",\n      \"description\": \"Array of interrupt count per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"intr_cnt_ipi\",\n      \"description\": \"IPI interrupt count per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"intr_cnt_msec\",\n      \"description\": \"Millisecond interrupt count per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"intr_cnt_total\",\n      \"description\": \"Total interrupt count per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"iscsi_data_received\",\n      \"description\": \"iSCSI kilobytes (KB) received per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"iscsi_data_sent\",\n      \"description\": \"iSCSI kilobytes (KB) sent per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"iscsi_ops\",\n      \"description\": \"Number of iSCSI operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"memory\",\n      \"description\": \"Total memory in megabytes (MB)\",\n      \"type\": \"raw\",\n      \"unit\": \"none\"\n    },\n    {\n      \"name\": \"network_data_received\",\n      \"description\": \"Number of network kilobytes (KB) received per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"network_data_sent\",\n      \"description\": \"Number of network kilobytes (KB) sent per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"nfs_ops\",\n      \"description\": \"Number of NFS operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"non_interrupt\",\n      \"description\": \"Processor non-interrupt rate percentage\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"non_interrupt_time\",\n      \"description\": \"Processor non-interrupt time\",\n      \"type\": \"delta\",\n      \"unit\": \"microsec\"\n    },\n    {\n      \"name\": \"num_processors\",\n      \"description\": \"Number of active processors in the system\",\n      \"type\": \"raw\",\n      \"unit\": \"none\"\n    },\n    {\n      \"name\": \"nvme_fc_data_received\",\n      \"description\": \"NVMe/FC kilobytes (KB) received per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"nvme_fc_data_sent\",\n      \"description\": \"NVMe/FC kilobytes (KB) sent per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"nvme_fc_ops\",\n      \"description\": \"NVMe/FC operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"nvme_roce_data_received\",\n      \"description\": \"NVMe/RoCE kilobytes (KB) received per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"nvme_roce_data_sent\",\n      \"description\": \"NVMe/RoCE kilobytes (KB) sent per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"nvme_roce_ops\",\n      \"description\": \"NVMe/RoCE operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"nvme_tcp_data_received\",\n      \"description\": \"NVMe/TCP kilobytes (KB) received per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"nvme_tcp_data_sent\",\n      \"description\": \"NVMe/TCP kilobytes (KB) sent per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"nvme_tcp_ops\",\n      \"description\": \"NVMe/TCP operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"other_data\",\n      \"description\": \"Other throughput\",\n      \"type\": \"rate\",\n      \"unit\": \"b_per_sec\"\n    },\n    {\n      \"name\": \"other_latency\",\n      \"description\": \"Average latency for all other operations in the system in microseconds\",\n      \"type\": \"average\",\n      \"unit\": \"microsec\",\n      \"denominator\": {\n        \"name\": \"other_ops\"\n      }\n    },\n    {\n      \"name\": \"other_ops\",\n      \"description\": \"All other operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"partner_data_received\",\n      \"description\": \"SCSI Partner kilobytes (KB) received per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"partner_data_sent\",\n      \"description\": \"SCSI Partner kilobytes (KB) sent per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"processor_plevel\",\n      \"description\": \"Processor plevel rate percentage\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"processor_plevel_time\",\n      \"description\": \"Processor plevel rate percentage\",\n      \"type\": \"delta\",\n      \"unit\": \"none\"\n    },\n    {\n      \"name\": \"read_data\",\n      \"description\": \"Read throughput\",\n      \"type\": \"rate\",\n      \"unit\": \"b_per_sec\"\n    },\n    {\n      \"name\": \"read_latency\",\n      \"description\": \"Average latency for all read operations in the system in microseconds\",\n      \"type\": \"average\",\n      \"unit\": \"microsec\",\n      \"denominator\": {\n        \"name\": \"read_ops\"\n      }\n    },\n    {\n      \"name\": \"read_ops\",\n      \"description\": \"Read operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"sk_switches\",\n      \"description\": \"Number of sk switches per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"ssd_data_read\",\n      \"description\": \"Number of SSD Disk kilobytes (KB) read per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"ssd_data_written\",\n      \"description\": \"Number of SSD Disk kilobytes (KB) written per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"sys_read_data\",\n      \"description\": \"Network and FCP kilobytes (KB) received per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"sys_total_data\",\n      \"description\": \"Network and FCP kilobytes (KB) received and sent per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"sys_write_data\",\n      \"description\": \"Network and FCP kilobytes (KB) sent per second\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"tape_data_read\",\n      \"description\": \"Tape bytes read per millisecond\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"tape_data_written\",\n      \"description\": \"Tape bytes written per millisecond\",\n      \"type\": \"rate\",\n      \"unit\": \"kb_per_sec\"\n    },\n    {\n      \"name\": \"time\",\n      \"description\": \"Time in seconds since the Epoch (00:00:00 UTC January 1 1970)\",\n      \"type\": \"raw\",\n      \"unit\": \"sec\"\n    },\n    {\n      \"name\": \"time_per_interrupt\",\n      \"description\": \"Processor time per interrupt\",\n      \"type\": \"average\",\n      \"unit\": \"microsec\",\n      \"denominator\": {\n        \"name\": \"interrupt_num\"\n      }\n    },\n    {\n      \"name\": \"time_per_interrupt_in_cp\",\n      \"description\": \"Processor time per interrupt in CP\",\n      \"type\": \"average\",\n      \"unit\": \"microsec\",\n      \"denominator\": {\n        \"name\": \"interrupt_num_in_cp\"\n      }\n    },\n    {\n      \"name\": \"total_data\",\n      \"description\": \"Total throughput in bytes\",\n      \"type\": \"rate\",\n      \"unit\": \"b_per_sec\"\n    },\n    {\n      \"name\": \"total_latency\",\n      \"description\": \"Average latency for all operations in the system in microseconds\",\n      \"type\": \"average\",\n      \"unit\": \"microsec\",\n      \"denominator\": {\n        \"name\": \"total_ops\"\n      }\n    },\n    {\n      \"name\": \"total_ops\",\n      \"description\": \"Total number of operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    },\n    {\n      \"name\": \"total_processor_busy\",\n      \"description\": \"Total processor utilization of all processors in the system\",\n      \"type\": \"percent\",\n      \"unit\": \"percent\",\n      \"denominator\": {\n        \"name\": \"cpu_elapsed_time\"\n      }\n    },\n    {\n      \"name\": \"total_processor_busy_time\",\n      \"description\": \"Total processor time of all processors in the system\",\n      \"type\": \"delta\",\n      \"unit\": \"microsec\"\n    },\n    {\n      \"name\": \"uptime\",\n      \"description\": \"Time in seconds that the system has been up\",\n      \"type\": \"raw\",\n      \"unit\": \"sec\"\n    },\n    {\n      \"name\": \"wafliron\",\n      \"description\": \"Wafliron counters\",\n      \"type\": \"delta\",\n      \"unit\": \"none\"\n    },\n    {\n      \"name\": \"write_data\",\n      \"description\": \"Write throughput\",\n      \"type\": \"rate\",\n      \"unit\": \"b_per_sec\"\n    },\n    {\n      \"name\": \"write_latency\",\n      \"description\": \"Average latency for all write operations in the system in microseconds\",\n      \"type\": \"average\",\n      \"unit\": \"microsec\",\n      \"denominator\": {\n        \"name\": \"write_ops\"\n      }\n    },\n    {\n      \"name\": \"write_ops\",\n      \"description\": \"Write operations per second\",\n      \"type\": \"rate\",\n      \"unit\": \"per_sec\"\n    }\n  ],\n  \"_links\": {\n    \"self\": {\n      \"href\": \"/api/cluster/counter/tables/system:node\"\n    }\n  }\n}\n</code></pre> </p>"},{"location":"resources/rest-perf-metrics/#node-performance-metrics-with-all-instances-properties-and-counters","title":"Node performance metrics with all instances, properties, and counters","text":"<p>Ask ONTAP to return all instances of <code>system:node</code>. For each <code>system:node</code> include all of that node's properties and performance metrics.</p> <pre><code>curl 'https://$clusterIP/api/cluster/counter/tables/system:node/rows?fields=*&amp;return_records=true'\n</code></pre> Response <p> <pre><code>{\n  \"records\": [\n    {\n      \"counter_table\": {\n        \"name\": \"system:node\"\n      },\n      \"id\": \"umeng-aff300-01:28e14eab-0580-11e8-bd9d-00a098d39e12\",\n      \"properties\": [\n        {\n          \"name\": \"node.name\",\n          \"value\": \"umeng-aff300-01\"\n        },\n        {\n          \"name\": \"system_model\",\n          \"value\": \"AFF-A300\"\n        },\n        {\n          \"name\": \"ontap_version\",\n          \"value\": \"NetApp Release R9.12.1xN_221108_1315: Tue Nov  8 15:32:25 EST 2022 \"\n        },\n        {\n          \"name\": \"compile_flags\",\n          \"value\": \"1\"\n        },\n        {\n          \"name\": \"serial_no\",\n          \"value\": \"721802000260\"\n        },\n        {\n          \"name\": \"system_id\",\n          \"value\": \"0537124012\"\n        },\n        {\n          \"name\": \"hostname\",\n          \"value\": \"umeng-aff300-01\"\n        },\n        {\n          \"name\": \"name\",\n          \"value\": \"umeng-aff300-01\"\n        },\n        {\n          \"name\": \"uuid\",\n          \"value\": \"28e14eab-0580-11e8-bd9d-00a098d39e12\"\n        }\n      ],\n      \"counters\": [\n        {\n          \"name\": \"memory\",\n          \"value\": 88766\n        },\n        {\n          \"name\": \"nfs_ops\",\n          \"value\": 15991465\n        },\n        {\n          \"name\": \"cifs_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"fcp_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"iscsi_ops\",\n          \"value\": 355884195\n        },\n        {\n          \"name\": \"nvme_fc_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_tcp_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_roce_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"network_data_received\",\n          \"value\": 33454266379\n        },\n        {\n          \"name\": \"network_data_sent\",\n          \"value\": 9938586739\n        },\n        {\n          \"name\": \"fcp_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"fcp_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"iscsi_data_received\",\n          \"value\": 4543696942\n        },\n        {\n          \"name\": \"iscsi_data_sent\",\n          \"value\": 3058795391\n        },\n        {\n          \"name\": \"nvme_fc_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_fc_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_tcp_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_tcp_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_roce_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_roce_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"partner_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"partner_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"sys_read_data\",\n          \"value\": 33454266379\n        },\n        {\n          \"name\": \"sys_write_data\",\n          \"value\": 9938586739\n        },\n        {\n          \"name\": \"sys_total_data\",\n          \"value\": 43392853118\n        },\n        {\n          \"name\": \"disk_data_read\",\n          \"value\": 32083838540\n        },\n        {\n          \"name\": \"disk_data_written\",\n          \"value\": 21102507352\n        },\n        {\n          \"name\": \"hdd_data_read\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"hdd_data_written\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"ssd_data_read\",\n          \"value\": 32083838540\n        },\n        {\n          \"name\": \"ssd_data_written\",\n          \"value\": 21102507352\n        },\n        {\n          \"name\": \"tape_data_read\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"tape_data_written\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"read_ops\",\n          \"value\": 33495530\n        },\n        {\n          \"name\": \"write_ops\",\n          \"value\": 324699398\n        },\n        {\n          \"name\": \"other_ops\",\n          \"value\": 13680732\n        },\n        {\n          \"name\": \"total_ops\",\n          \"value\": 371875660\n        },\n        {\n          \"name\": \"read_latency\",\n          \"value\": 14728140707\n        },\n        {\n          \"name\": \"write_latency\",\n          \"value\": 1568830328022\n        },\n        {\n          \"name\": \"other_latency\",\n          \"value\": 2132691612\n        },\n        {\n          \"name\": \"total_latency\",\n          \"value\": 1585691160341\n        },\n        {\n          \"name\": \"read_data\",\n          \"value\": 3212301497187\n        },\n        {\n          \"name\": \"write_data\",\n          \"value\": 4787509093524\n        },\n        {\n          \"name\": \"other_data\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"total_data\",\n          \"value\": 7999810590711\n        },\n        {\n          \"name\": \"cpu_busy\",\n          \"value\": 790347800332\n        },\n        {\n          \"name\": \"cpu_elapsed_time\",\n          \"value\": 3979034040025\n        },\n        {\n          \"name\": \"average_processor_busy_percent\",\n          \"value\": 788429907770\n        },\n        {\n          \"name\": \"total_processor_busy\",\n          \"value\": 12614878524320\n        },\n        {\n          \"name\": \"total_processor_busy_time\",\n          \"value\": 12614878524320\n        },\n        {\n          \"name\": \"num_processors\",\n          \"value\": 16\n        },\n        {\n          \"name\": \"interrupt_time\",\n          \"value\": 118435504138\n        },\n        {\n          \"name\": \"interrupt\",\n          \"value\": 118435504138\n        },\n        {\n          \"name\": \"interrupt_num\",\n          \"value\": 1446537540\n        },\n        {\n          \"name\": \"time_per_interrupt\",\n          \"value\": 118435504138\n        },\n        {\n          \"name\": \"non_interrupt_time\",\n          \"value\": 12496443020182\n        },\n        {\n          \"name\": \"non_interrupt\",\n          \"value\": 12496443020182\n        },\n        {\n          \"name\": \"idle_time\",\n          \"value\": 51049666116080\n        },\n        {\n          \"name\": \"idle\",\n          \"value\": 51049666116080\n        },\n        {\n          \"name\": \"cp_time\",\n          \"value\": 221447740301\n        },\n        {\n          \"name\": \"cp\",\n          \"value\": 221447740301\n        },\n        {\n          \"name\": \"interrupt_in_cp_time\",\n          \"value\": 7969316828\n        },\n        {\n          \"name\": \"interrupt_in_cp\",\n          \"value\": 7969316828\n        },\n        {\n          \"name\": \"interrupt_num_in_cp\",\n          \"value\": 1639345044\n        },\n        {\n          \"name\": \"time_per_interrupt_in_cp\",\n          \"value\": 7969316828\n        },\n        {\n          \"name\": \"sk_switches\",\n          \"value\": 3830419593\n        },\n        {\n          \"name\": \"hard_switches\",\n          \"value\": 2786999477\n        },\n        {\n          \"name\": \"intr_cnt_msec\",\n          \"value\": 3978648113\n        },\n        {\n          \"name\": \"intr_cnt_ipi\",\n          \"value\": 1709054\n        },\n        {\n          \"name\": \"intr_cnt_total\",\n          \"value\": 1215253490\n        },\n        {\n          \"name\": \"time\",\n          \"value\": 1677516216\n        },\n        {\n          \"name\": \"uptime\",\n          \"value\": 3978648\n        },\n        {\n          \"name\": \"processor_plevel_time\",\n          \"values\": [\n            3405835479577,\n            2628275207938,\n            1916273074545,\n            1366761457118,\n            964863281216,\n            676002919489,\n            472533086045,\n            331487674159,\n            234447654307,\n            167247803300,\n            120098535891,\n            86312126550,\n            61675398266,\n            43549889374,\n            30176461104,\n            19891286233,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"0_CPU\",\n            \"1_CPU\",\n            \"2_CPU\",\n            \"3_CPU\",\n            \"4_CPU\",\n            \"5_CPU\",\n            \"6_CPU\",\n            \"7_CPU\",\n            \"8_CPU\",\n            \"9_CPU\",\n            \"10_CPU\",\n            \"11_CPU\",\n            \"12_CPU\",\n            \"13_CPU\",\n            \"14_CPU\",\n            \"15_CPU\",\n            \"16_CPU\",\n            \"17_CPU\",\n            \"18_CPU\",\n            \"19_CPU\",\n            \"20_CPU\",\n            \"21_CPU\",\n            \"22_CPU\",\n            \"23_CPU\",\n            \"24_CPU\",\n            \"25_CPU\",\n            \"26_CPU\",\n            \"27_CPU\",\n            \"28_CPU\",\n            \"29_CPU\",\n            \"30_CPU\",\n            \"31_CPU\",\n            \"32_CPU\",\n            \"33_CPU\",\n            \"34_CPU\",\n            \"35_CPU\",\n            \"36_CPU\",\n            \"37_CPU\",\n            \"38_CPU\",\n            \"39_CPU\",\n            \"40_CPU\",\n            \"41_CPU\",\n            \"42_CPU\",\n            \"43_CPU\",\n            \"44_CPU\",\n            \"45_CPU\",\n            \"46_CPU\",\n            \"47_CPU\",\n            \"48_CPU\",\n            \"49_CPU\",\n            \"50_CPU\",\n            \"51_CPU\",\n            \"52_CPU\",\n            \"53_CPU\",\n            \"54_CPU\",\n            \"55_CPU\",\n            \"56_CPU\",\n            \"57_CPU\",\n            \"58_CPU\",\n            \"59_CPU\",\n            \"60_CPU\",\n            \"61_CPU\",\n            \"62_CPU\",\n            \"63_CPU\",\n            \"64_CPU\",\n            \"65_CPU\",\n            \"66_CPU\",\n            \"67_CPU\",\n            \"68_CPU\",\n            \"69_CPU\",\n            \"70_CPU\",\n            \"71_CPU\",\n            \"72_CPU\",\n            \"73_CPU\",\n            \"74_CPU\",\n            \"75_CPU\",\n            \"76_CPU\",\n            \"77_CPU\",\n            \"78_CPU\",\n            \"79_CPU\",\n            \"80_CPU\",\n            \"81_CPU\",\n            \"82_CPU\",\n            \"83_CPU\",\n            \"84_CPU\",\n            \"85_CPU\",\n            \"86_CPU\",\n            \"87_CPU\",\n            \"88_CPU\",\n            \"89_CPU\",\n            \"90_CPU\",\n            \"91_CPU\",\n            \"92_CPU\",\n            \"93_CPU\",\n            \"94_CPU\",\n            \"95_CPU\",\n            \"96_CPU\",\n            \"97_CPU\",\n            \"98_CPU\",\n            \"99_CPU\",\n            \"100_CPU\",\n            \"101_CPU\",\n            \"102_CPU\",\n            \"103_CPU\",\n            \"104_CPU\",\n            \"105_CPU\",\n            \"106_CPU\",\n            \"107_CPU\",\n            \"108_CPU\",\n            \"109_CPU\",\n            \"110_CPU\",\n            \"111_CPU\",\n            \"112_CPU\",\n            \"113_CPU\",\n            \"114_CPU\",\n            \"115_CPU\",\n            \"116_CPU\",\n            \"117_CPU\",\n            \"118_CPU\",\n            \"119_CPU\",\n            \"120_CPU\",\n            \"121_CPU\",\n            \"122_CPU\",\n            \"123_CPU\",\n            \"124_CPU\",\n            \"125_CPU\",\n            \"126_CPU\",\n            \"127_CPU\"\n          ]\n        },\n        {\n          \"name\": \"processor_plevel\",\n          \"values\": [\n            3405835479577,\n            2628275207938,\n            1916273074545,\n            1366761457118,\n            964863281216,\n            676002919489,\n            472533086045,\n            331487674159,\n            234447654307,\n            167247803300,\n            120098535891,\n            86312126550,\n            61675398266,\n            43549889374,\n            30176461104,\n            19891286233,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"0_CPU\",\n            \"1_CPU\",\n            \"2_CPU\",\n            \"3_CPU\",\n            \"4_CPU\",\n            \"5_CPU\",\n            \"6_CPU\",\n            \"7_CPU\",\n            \"8_CPU\",\n            \"9_CPU\",\n            \"10_CPU\",\n            \"11_CPU\",\n            \"12_CPU\",\n            \"13_CPU\",\n            \"14_CPU\",\n            \"15_CPU\",\n            \"16_CPU\",\n            \"17_CPU\",\n            \"18_CPU\",\n            \"19_CPU\",\n            \"20_CPU\",\n            \"21_CPU\",\n            \"22_CPU\",\n            \"23_CPU\",\n            \"24_CPU\",\n            \"25_CPU\",\n            \"26_CPU\",\n            \"27_CPU\",\n            \"28_CPU\",\n            \"29_CPU\",\n            \"30_CPU\",\n            \"31_CPU\",\n            \"32_CPU\",\n            \"33_CPU\",\n            \"34_CPU\",\n            \"35_CPU\",\n            \"36_CPU\",\n            \"37_CPU\",\n            \"38_CPU\",\n            \"39_CPU\",\n            \"40_CPU\",\n            \"41_CPU\",\n            \"42_CPU\",\n            \"43_CPU\",\n            \"44_CPU\",\n            \"45_CPU\",\n            \"46_CPU\",\n            \"47_CPU\",\n            \"48_CPU\",\n            \"49_CPU\",\n            \"50_CPU\",\n            \"51_CPU\",\n            \"52_CPU\",\n            \"53_CPU\",\n            \"54_CPU\",\n            \"55_CPU\",\n            \"56_CPU\",\n            \"57_CPU\",\n            \"58_CPU\",\n            \"59_CPU\",\n            \"60_CPU\",\n            \"61_CPU\",\n            \"62_CPU\",\n            \"63_CPU\",\n            \"64_CPU\",\n            \"65_CPU\",\n            \"66_CPU\",\n            \"67_CPU\",\n            \"68_CPU\",\n            \"69_CPU\",\n            \"70_CPU\",\n            \"71_CPU\",\n            \"72_CPU\",\n            \"73_CPU\",\n            \"74_CPU\",\n            \"75_CPU\",\n            \"76_CPU\",\n            \"77_CPU\",\n            \"78_CPU\",\n            \"79_CPU\",\n            \"80_CPU\",\n            \"81_CPU\",\n            \"82_CPU\",\n            \"83_CPU\",\n            \"84_CPU\",\n            \"85_CPU\",\n            \"86_CPU\",\n            \"87_CPU\",\n            \"88_CPU\",\n            \"89_CPU\",\n            \"90_CPU\",\n            \"91_CPU\",\n            \"92_CPU\",\n            \"93_CPU\",\n            \"94_CPU\",\n            \"95_CPU\",\n            \"96_CPU\",\n            \"97_CPU\",\n            \"98_CPU\",\n            \"99_CPU\",\n            \"100_CPU\",\n            \"101_CPU\",\n            \"102_CPU\",\n            \"103_CPU\",\n            \"104_CPU\",\n            \"105_CPU\",\n            \"106_CPU\",\n            \"107_CPU\",\n            \"108_CPU\",\n            \"109_CPU\",\n            \"110_CPU\",\n            \"111_CPU\",\n            \"112_CPU\",\n            \"113_CPU\",\n            \"114_CPU\",\n            \"115_CPU\",\n            \"116_CPU\",\n            \"117_CPU\",\n            \"118_CPU\",\n            \"119_CPU\",\n            \"120_CPU\",\n            \"121_CPU\",\n            \"122_CPU\",\n            \"123_CPU\",\n            \"124_CPU\",\n            \"125_CPU\",\n            \"126_CPU\",\n            \"127_CPU\"\n          ]\n        },\n        {\n          \"name\": \"domain_busy\",\n          \"values\": [\n            51049666116086,\n            13419960088,\n            13297686377,\n            1735383373870,\n            39183250298,\n            6728050897,\n            28229793795,\n            17493622207,\n            122290467,\n            974721172619,\n            47944793823,\n            164946850,\n            4162377932,\n            407009733276,\n            128199854099,\n            9037374471285,\n            38911301970,\n            366749865,\n            732045734,\n            2997541695,\n            14,\n            18,\n            40\n          ],\n          \"labels\": [\n            \"idle\",\n            \"kahuna\",\n            \"storage\",\n            \"exempt\",\n            \"none\",\n            \"raid\",\n            \"raid_exempt\",\n            \"xor_exempt\",\n            \"target\",\n            \"wafl_exempt\",\n            \"wafl_mpcleaner\",\n            \"sm_exempt\",\n            \"protocol\",\n            \"nwk_exempt\",\n            \"network\",\n            \"hostOS\",\n            \"ssan_exempt\",\n            \"unclassified\",\n            \"kahuna_legacy\",\n            \"ha\",\n            \"ssan_exempt2\",\n            \"exempt_ise\",\n            \"zombie\"\n          ]\n        },\n        {\n          \"name\": \"domain_shared\",\n          \"values\": [\n            0,\n            685164024474,\n            0,\n            0,\n            0,\n            24684879894,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"idle\",\n            \"kahuna\",\n            \"storage\",\n            \"exempt\",\n            \"none\",\n            \"raid\",\n            \"raid_exempt\",\n            \"xor_exempt\",\n            \"target\",\n            \"wafl_exempt\",\n            \"wafl_mpcleaner\",\n            \"sm_exempt\",\n            \"protocol\",\n            \"nwk_exempt\",\n            \"network\",\n            \"hostOS\",\n            \"ssan_exempt\",\n            \"unclassified\",\n            \"kahuna_legacy\",\n            \"ha\",\n            \"ssan_exempt2\",\n            \"exempt_ise\",\n            \"zombie\"\n          ]\n        },\n        {\n          \"name\": \"dswitchto_cnt\",\n          \"values\": [\n            0,\n            322698663,\n            172936437,\n            446893016,\n            96971,\n            39788918,\n            5,\n            10,\n            10670440,\n            22,\n            7,\n            836,\n            2407967,\n            9798186907,\n            9802868991,\n            265242,\n            53,\n            2614118,\n            4430780,\n            66117706,\n            1,\n            1,\n            1\n          ],\n          \"labels\": [\n            \"idle\",\n            \"kahuna\",\n            \"storage\",\n            \"exempt\",\n            \"none\",\n            \"raid\",\n            \"raid_exempt\",\n            \"xor_exempt\",\n            \"target\",\n            \"wafl_exempt\",\n            \"wafl_mpcleaner\",\n            \"sm_exempt\",\n            \"protocol\",\n            \"nwk_exempt\",\n            \"network\",\n            \"hostOS\",\n            \"ssan_exempt\",\n            \"unclassified\",\n            \"kahuna_legacy\",\n            \"ha\",\n            \"ssan_exempt2\",\n            \"exempt_ise\",\n            \"zombie\"\n          ]\n        },\n        {\n          \"name\": \"intr_cnt\",\n          \"values\": [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            4191453008,\n            8181232,\n            1625052957,\n            0,\n            71854,\n            0,\n            71854,\n            0,\n            5,\n            0,\n            5,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"dev_0\",\n            \"dev_1\",\n            \"dev_2\",\n            \"dev_3\",\n            \"dev_4\",\n            \"dev_5\",\n            \"dev_6\",\n            \"dev_7\",\n            \"dev_8\",\n            \"dev_9\",\n            \"dev_10\",\n            \"dev_11\",\n            \"dev_12\",\n            \"dev_13\",\n            \"dev_14\",\n            \"dev_15\",\n            \"dev_16\",\n            \"dev_17\",\n            \"dev_18\",\n            \"dev_19\",\n            \"dev_20\",\n            \"dev_21\",\n            \"dev_22\",\n            \"dev_23\",\n            \"dev_24\",\n            \"dev_25\",\n            \"dev_26\",\n            \"dev_27\",\n            \"dev_28\",\n            \"dev_29\",\n            \"dev_30\",\n            \"dev_31\",\n            \"dev_32\",\n            \"dev_33\",\n            \"dev_34\",\n            \"dev_35\",\n            \"dev_36\",\n            \"dev_37\",\n            \"dev_38\",\n            \"dev_39\",\n            \"dev_40\",\n            \"dev_41\",\n            \"dev_42\",\n            \"dev_43\",\n            \"dev_44\",\n            \"dev_45\",\n            \"dev_46\",\n            \"dev_47\",\n            \"dev_48\",\n            \"dev_49\",\n            \"dev_50\",\n            \"dev_51\",\n            \"dev_52\",\n            \"dev_53\",\n            \"dev_54\",\n            \"dev_55\",\n            \"dev_56\",\n            \"dev_57\",\n            \"dev_58\",\n            \"dev_59\",\n            \"dev_60\",\n            \"dev_61\",\n            \"dev_62\",\n            \"dev_63\",\n            \"dev_64\",\n            \"dev_65\",\n            \"dev_66\",\n            \"dev_67\",\n            \"dev_68\",\n            \"dev_69\",\n            \"dev_70\",\n            \"dev_71\",\n            \"dev_72\",\n            \"dev_73\",\n            \"dev_74\",\n            \"dev_75\",\n            \"dev_76\",\n            \"dev_77\",\n            \"dev_78\",\n            \"dev_79\",\n            \"dev_80\",\n            \"dev_81\",\n            \"dev_82\",\n            \"dev_83\",\n            \"dev_84\",\n            \"dev_85\",\n            \"dev_86\",\n            \"dev_87\",\n            \"dev_88\",\n            \"dev_89\",\n            \"dev_90\",\n            \"dev_91\",\n            \"dev_92\",\n            \"dev_93\",\n            \"dev_94\",\n            \"dev_95\",\n            \"dev_96\",\n            \"dev_97\",\n            \"dev_98\",\n            \"dev_99\",\n            \"dev_100\",\n            \"dev_101\",\n            \"dev_102\",\n            \"dev_103\",\n            \"dev_104\",\n            \"dev_105\",\n            \"dev_106\",\n            \"dev_107\",\n            \"dev_108\",\n            \"dev_109\",\n            \"dev_110\",\n            \"dev_111\",\n            \"dev_112\",\n            \"dev_113\",\n            \"dev_114\",\n            \"dev_115\",\n            \"dev_116\",\n            \"dev_117\",\n            \"dev_118\",\n            \"dev_119\",\n            \"dev_120\",\n            \"dev_121\",\n            \"dev_122\",\n            \"dev_123\",\n            \"dev_124\",\n            \"dev_125\",\n            \"dev_126\",\n            \"dev_127\",\n            \"dev_128\",\n            \"dev_129\",\n            \"dev_130\",\n            \"dev_131\",\n            \"dev_132\",\n            \"dev_133\",\n            \"dev_134\",\n            \"dev_135\",\n            \"dev_136\",\n            \"dev_137\",\n            \"dev_138\",\n            \"dev_139\",\n            \"dev_140\",\n            \"dev_141\",\n            \"dev_142\",\n            \"dev_143\",\n            \"dev_144\",\n            \"dev_145\",\n            \"dev_146\",\n            \"dev_147\",\n            \"dev_148\",\n            \"dev_149\",\n            \"dev_150\",\n            \"dev_151\",\n            \"dev_152\",\n            \"dev_153\",\n            \"dev_154\",\n            \"dev_155\",\n            \"dev_156\",\n            \"dev_157\",\n            \"dev_158\",\n            \"dev_159\",\n            \"dev_160\",\n            \"dev_161\",\n            \"dev_162\",\n            \"dev_163\",\n            \"dev_164\",\n            \"dev_165\",\n            \"dev_166\",\n            \"dev_167\",\n            \"dev_168\",\n            \"dev_169\",\n            \"dev_170\",\n            \"dev_171\",\n            \"dev_172\",\n            \"dev_173\",\n            \"dev_174\",\n            \"dev_175\",\n            \"dev_176\",\n            \"dev_177\",\n            \"dev_178\",\n            \"dev_179\",\n            \"dev_180\",\n            \"dev_181\",\n            \"dev_182\",\n            \"dev_183\",\n            \"dev_184\",\n            \"dev_185\",\n            \"dev_186\",\n            \"dev_187\",\n            \"dev_188\",\n            \"dev_189\",\n            \"dev_190\",\n            \"dev_191\",\n            \"dev_192\",\n            \"dev_193\",\n            \"dev_194\",\n            \"dev_195\",\n            \"dev_196\",\n            \"dev_197\",\n            \"dev_198\",\n            \"dev_199\",\n            \"dev_200\",\n            \"dev_201\",\n            \"dev_202\",\n            \"dev_203\",\n            \"dev_204\",\n            \"dev_205\",\n            \"dev_206\",\n            \"dev_207\",\n            \"dev_208\",\n            \"dev_209\",\n            \"dev_210\",\n            \"dev_211\",\n            \"dev_212\",\n            \"dev_213\",\n            \"dev_214\",\n            \"dev_215\",\n            \"dev_216\",\n            \"dev_217\",\n            \"dev_218\",\n            \"dev_219\",\n            \"dev_220\",\n            \"dev_221\",\n            \"dev_222\",\n            \"dev_223\",\n            \"dev_224\",\n            \"dev_225\",\n            \"dev_226\",\n            \"dev_227\",\n            \"dev_228\",\n            \"dev_229\",\n            \"dev_230\",\n            \"dev_231\",\n            \"dev_232\",\n            \"dev_233\",\n            \"dev_234\",\n            \"dev_235\",\n            \"dev_236\",\n            \"dev_237\",\n            \"dev_238\",\n            \"dev_239\",\n            \"dev_240\",\n            \"dev_241\",\n            \"dev_242\",\n            \"dev_243\",\n            \"dev_244\",\n            \"dev_245\",\n            \"dev_246\",\n            \"dev_247\",\n            \"dev_248\",\n            \"dev_249\",\n            \"dev_250\",\n            \"dev_251\",\n            \"dev_252\",\n            \"dev_253\",\n            \"dev_254\",\n            \"dev_255\"\n          ]\n        },\n        {\n          \"name\": \"wafliron\",\n          \"values\": [\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"iron_totstarts\",\n            \"iron_nobackup\",\n            \"iron_usebackup\"\n          ]\n        }\n      ],\n      \"aggregation\": {\n        \"count\": 2,\n        \"complete\": true\n      },\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/system:node/rows/umeng-aff300-01%3A28e14eab-0580-11e8-bd9d-00a098d39e12\"\n        }\n      }\n    },\n    {\n      \"counter_table\": {\n        \"name\": \"system:node\"\n      },\n      \"id\": \"umeng-aff300-02:1524afca-0580-11e8-ae74-00a098d390f2\",\n      \"properties\": [\n        {\n          \"name\": \"node.name\",\n          \"value\": \"umeng-aff300-02\"\n        },\n        {\n          \"name\": \"system_model\",\n          \"value\": \"AFF-A300\"\n        },\n        {\n          \"name\": \"ontap_version\",\n          \"value\": \"NetApp Release R9.12.1xN_221108_1315: Tue Nov  8 15:32:25 EST 2022 \"\n        },\n        {\n          \"name\": \"compile_flags\",\n          \"value\": \"1\"\n        },\n        {\n          \"name\": \"serial_no\",\n          \"value\": \"721802000259\"\n        },\n        {\n          \"name\": \"system_id\",\n          \"value\": \"0537123843\"\n        },\n        {\n          \"name\": \"hostname\",\n          \"value\": \"umeng-aff300-02\"\n        },\n        {\n          \"name\": \"name\",\n          \"value\": \"umeng-aff300-02\"\n        },\n        {\n          \"name\": \"uuid\",\n          \"value\": \"1524afca-0580-11e8-ae74-00a098d390f2\"\n        }\n      ],\n      \"counters\": [\n        {\n          \"name\": \"memory\",\n          \"value\": 88766\n        },\n        {\n          \"name\": \"nfs_ops\",\n          \"value\": 2061227971\n        },\n        {\n          \"name\": \"cifs_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"fcp_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"iscsi_ops\",\n          \"value\": 183570559\n        },\n        {\n          \"name\": \"nvme_fc_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_tcp_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_roce_ops\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"network_data_received\",\n          \"value\": 28707362447\n        },\n        {\n          \"name\": \"network_data_sent\",\n          \"value\": 31199786274\n        },\n        {\n          \"name\": \"fcp_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"fcp_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"iscsi_data_received\",\n          \"value\": 2462501728\n        },\n        {\n          \"name\": \"iscsi_data_sent\",\n          \"value\": 962425592\n        },\n        {\n          \"name\": \"nvme_fc_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_fc_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_tcp_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_tcp_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_roce_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"nvme_roce_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"partner_data_received\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"partner_data_sent\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"sys_read_data\",\n          \"value\": 28707362447\n        },\n        {\n          \"name\": \"sys_write_data\",\n          \"value\": 31199786274\n        },\n        {\n          \"name\": \"sys_total_data\",\n          \"value\": 59907148721\n        },\n        {\n          \"name\": \"disk_data_read\",\n          \"value\": 27355740700\n        },\n        {\n          \"name\": \"disk_data_written\",\n          \"value\": 3426898232\n        },\n        {\n          \"name\": \"hdd_data_read\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"hdd_data_written\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"ssd_data_read\",\n          \"value\": 27355740700\n        },\n        {\n          \"name\": \"ssd_data_written\",\n          \"value\": 3426898232\n        },\n        {\n          \"name\": \"tape_data_read\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"tape_data_written\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"read_ops\",\n          \"value\": 29957410\n        },\n        {\n          \"name\": \"write_ops\",\n          \"value\": 2141657620\n        },\n        {\n          \"name\": \"other_ops\",\n          \"value\": 73183500\n        },\n        {\n          \"name\": \"total_ops\",\n          \"value\": 2244798530\n        },\n        {\n          \"name\": \"read_latency\",\n          \"value\": 43283636161\n        },\n        {\n          \"name\": \"write_latency\",\n          \"value\": 1437635703835\n        },\n        {\n          \"name\": \"other_latency\",\n          \"value\": 628457365\n        },\n        {\n          \"name\": \"total_latency\",\n          \"value\": 1481547797361\n        },\n        {\n          \"name\": \"read_data\",\n          \"value\": 1908711454978\n        },\n        {\n          \"name\": \"write_data\",\n          \"value\": 23562759645410\n        },\n        {\n          \"name\": \"other_data\",\n          \"value\": 0\n        },\n        {\n          \"name\": \"total_data\",\n          \"value\": 25471471100388\n        },\n        {\n          \"name\": \"cpu_busy\",\n          \"value\": 511050841704\n        },\n        {\n          \"name\": \"cpu_elapsed_time\",\n          \"value\": 3979039364919\n        },\n        {\n          \"name\": \"average_processor_busy_percent\",\n          \"value\": 509151403977\n        },\n        {\n          \"name\": \"total_processor_busy\",\n          \"value\": 8146422463632\n        },\n        {\n          \"name\": \"total_processor_busy_time\",\n          \"value\": 8146422463632\n        },\n        {\n          \"name\": \"num_processors\",\n          \"value\": 16\n        },\n        {\n          \"name\": \"interrupt_time\",\n          \"value\": 108155323601\n        },\n        {\n          \"name\": \"interrupt\",\n          \"value\": 108155323601\n        },\n        {\n          \"name\": \"interrupt_num\",\n          \"value\": 3369179127\n        },\n        {\n          \"name\": \"time_per_interrupt\",\n          \"value\": 108155323601\n        },\n        {\n          \"name\": \"non_interrupt_time\",\n          \"value\": 8038267140031\n        },\n        {\n          \"name\": \"non_interrupt\",\n          \"value\": 8038267140031\n        },\n        {\n          \"name\": \"idle_time\",\n          \"value\": 55518207375072\n        },\n        {\n          \"name\": \"idle\",\n          \"value\": 55518207375072\n        },\n        {\n          \"name\": \"cp_time\",\n          \"value\": 64306316680\n        },\n        {\n          \"name\": \"cp\",\n          \"value\": 64306316680\n        },\n        {\n          \"name\": \"interrupt_in_cp_time\",\n          \"value\": 2024956616\n        },\n        {\n          \"name\": \"interrupt_in_cp\",\n          \"value\": 2024956616\n        },\n        {\n          \"name\": \"interrupt_num_in_cp\",\n          \"value\": 2661183541\n        },\n        {\n          \"name\": \"time_per_interrupt_in_cp\",\n          \"value\": 2024956616\n        },\n        {\n          \"name\": \"sk_switches\",\n          \"value\": 2798598514\n        },\n        {\n          \"name\": \"hard_switches\",\n          \"value\": 1354185066\n        },\n        {\n          \"name\": \"intr_cnt_msec\",\n          \"value\": 3978642246\n        },\n        {\n          \"name\": \"intr_cnt_ipi\",\n          \"value\": 797281\n        },\n        {\n          \"name\": \"intr_cnt_total\",\n          \"value\": 905575861\n        },\n        {\n          \"name\": \"time\",\n          \"value\": 1677516216\n        },\n        {\n          \"name\": \"uptime\",\n          \"value\": 3978643\n        },\n        {\n          \"name\": \"processor_plevel_time\",\n          \"values\": [\n            2878770221447,\n            1882901052733,\n            1209134416474,\n            771086627192,\n            486829133301,\n            306387520688,\n            193706139760,\n            123419519944,\n            79080346535,\n            50459518003,\n            31714732122,\n            19476561954,\n            11616026278,\n            6666253598,\n            3623880168,\n            1790458071,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"0_CPU\",\n            \"1_CPU\",\n            \"2_CPU\",\n            \"3_CPU\",\n            \"4_CPU\",\n            \"5_CPU\",\n            \"6_CPU\",\n            \"7_CPU\",\n            \"8_CPU\",\n            \"9_CPU\",\n            \"10_CPU\",\n            \"11_CPU\",\n            \"12_CPU\",\n            \"13_CPU\",\n            \"14_CPU\",\n            \"15_CPU\",\n            \"16_CPU\",\n            \"17_CPU\",\n            \"18_CPU\",\n            \"19_CPU\",\n            \"20_CPU\",\n            \"21_CPU\",\n            \"22_CPU\",\n            \"23_CPU\",\n            \"24_CPU\",\n            \"25_CPU\",\n            \"26_CPU\",\n            \"27_CPU\",\n            \"28_CPU\",\n            \"29_CPU\",\n            \"30_CPU\",\n            \"31_CPU\",\n            \"32_CPU\",\n            \"33_CPU\",\n            \"34_CPU\",\n            \"35_CPU\",\n            \"36_CPU\",\n            \"37_CPU\",\n            \"38_CPU\",\n            \"39_CPU\",\n            \"40_CPU\",\n            \"41_CPU\",\n            \"42_CPU\",\n            \"43_CPU\",\n            \"44_CPU\",\n            \"45_CPU\",\n            \"46_CPU\",\n            \"47_CPU\",\n            \"48_CPU\",\n            \"49_CPU\",\n            \"50_CPU\",\n            \"51_CPU\",\n            \"52_CPU\",\n            \"53_CPU\",\n            \"54_CPU\",\n            \"55_CPU\",\n            \"56_CPU\",\n            \"57_CPU\",\n            \"58_CPU\",\n            \"59_CPU\",\n            \"60_CPU\",\n            \"61_CPU\",\n            \"62_CPU\",\n            \"63_CPU\",\n            \"64_CPU\",\n            \"65_CPU\",\n            \"66_CPU\",\n            \"67_CPU\",\n            \"68_CPU\",\n            \"69_CPU\",\n            \"70_CPU\",\n            \"71_CPU\",\n            \"72_CPU\",\n            \"73_CPU\",\n            \"74_CPU\",\n            \"75_CPU\",\n            \"76_CPU\",\n            \"77_CPU\",\n            \"78_CPU\",\n            \"79_CPU\",\n            \"80_CPU\",\n            \"81_CPU\",\n            \"82_CPU\",\n            \"83_CPU\",\n            \"84_CPU\",\n            \"85_CPU\",\n            \"86_CPU\",\n            \"87_CPU\",\n            \"88_CPU\",\n            \"89_CPU\",\n            \"90_CPU\",\n            \"91_CPU\",\n            \"92_CPU\",\n            \"93_CPU\",\n            \"94_CPU\",\n            \"95_CPU\",\n            \"96_CPU\",\n            \"97_CPU\",\n            \"98_CPU\",\n            \"99_CPU\",\n            \"100_CPU\",\n            \"101_CPU\",\n            \"102_CPU\",\n            \"103_CPU\",\n            \"104_CPU\",\n            \"105_CPU\",\n            \"106_CPU\",\n            \"107_CPU\",\n            \"108_CPU\",\n            \"109_CPU\",\n            \"110_CPU\",\n            \"111_CPU\",\n            \"112_CPU\",\n            \"113_CPU\",\n            \"114_CPU\",\n            \"115_CPU\",\n            \"116_CPU\",\n            \"117_CPU\",\n            \"118_CPU\",\n            \"119_CPU\",\n            \"120_CPU\",\n            \"121_CPU\",\n            \"122_CPU\",\n            \"123_CPU\",\n            \"124_CPU\",\n            \"125_CPU\",\n            \"126_CPU\",\n            \"127_CPU\"\n          ]\n        },\n        {\n          \"name\": \"processor_plevel\",\n          \"values\": [\n            2878770221447,\n            1882901052733,\n            1209134416474,\n            771086627192,\n            486829133301,\n            306387520688,\n            193706139760,\n            123419519944,\n            79080346535,\n            50459518003,\n            31714732122,\n            19476561954,\n            11616026278,\n            6666253598,\n            3623880168,\n            1790458071,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"0_CPU\",\n            \"1_CPU\",\n            \"2_CPU\",\n            \"3_CPU\",\n            \"4_CPU\",\n            \"5_CPU\",\n            \"6_CPU\",\n            \"7_CPU\",\n            \"8_CPU\",\n            \"9_CPU\",\n            \"10_CPU\",\n            \"11_CPU\",\n            \"12_CPU\",\n            \"13_CPU\",\n            \"14_CPU\",\n            \"15_CPU\",\n            \"16_CPU\",\n            \"17_CPU\",\n            \"18_CPU\",\n            \"19_CPU\",\n            \"20_CPU\",\n            \"21_CPU\",\n            \"22_CPU\",\n            \"23_CPU\",\n            \"24_CPU\",\n            \"25_CPU\",\n            \"26_CPU\",\n            \"27_CPU\",\n            \"28_CPU\",\n            \"29_CPU\",\n            \"30_CPU\",\n            \"31_CPU\",\n            \"32_CPU\",\n            \"33_CPU\",\n            \"34_CPU\",\n            \"35_CPU\",\n            \"36_CPU\",\n            \"37_CPU\",\n            \"38_CPU\",\n            \"39_CPU\",\n            \"40_CPU\",\n            \"41_CPU\",\n            \"42_CPU\",\n            \"43_CPU\",\n            \"44_CPU\",\n            \"45_CPU\",\n            \"46_CPU\",\n            \"47_CPU\",\n            \"48_CPU\",\n            \"49_CPU\",\n            \"50_CPU\",\n            \"51_CPU\",\n            \"52_CPU\",\n            \"53_CPU\",\n            \"54_CPU\",\n            \"55_CPU\",\n            \"56_CPU\",\n            \"57_CPU\",\n            \"58_CPU\",\n            \"59_CPU\",\n            \"60_CPU\",\n            \"61_CPU\",\n            \"62_CPU\",\n            \"63_CPU\",\n            \"64_CPU\",\n            \"65_CPU\",\n            \"66_CPU\",\n            \"67_CPU\",\n            \"68_CPU\",\n            \"69_CPU\",\n            \"70_CPU\",\n            \"71_CPU\",\n            \"72_CPU\",\n            \"73_CPU\",\n            \"74_CPU\",\n            \"75_CPU\",\n            \"76_CPU\",\n            \"77_CPU\",\n            \"78_CPU\",\n            \"79_CPU\",\n            \"80_CPU\",\n            \"81_CPU\",\n            \"82_CPU\",\n            \"83_CPU\",\n            \"84_CPU\",\n            \"85_CPU\",\n            \"86_CPU\",\n            \"87_CPU\",\n            \"88_CPU\",\n            \"89_CPU\",\n            \"90_CPU\",\n            \"91_CPU\",\n            \"92_CPU\",\n            \"93_CPU\",\n            \"94_CPU\",\n            \"95_CPU\",\n            \"96_CPU\",\n            \"97_CPU\",\n            \"98_CPU\",\n            \"99_CPU\",\n            \"100_CPU\",\n            \"101_CPU\",\n            \"102_CPU\",\n            \"103_CPU\",\n            \"104_CPU\",\n            \"105_CPU\",\n            \"106_CPU\",\n            \"107_CPU\",\n            \"108_CPU\",\n            \"109_CPU\",\n            \"110_CPU\",\n            \"111_CPU\",\n            \"112_CPU\",\n            \"113_CPU\",\n            \"114_CPU\",\n            \"115_CPU\",\n            \"116_CPU\",\n            \"117_CPU\",\n            \"118_CPU\",\n            \"119_CPU\",\n            \"120_CPU\",\n            \"121_CPU\",\n            \"122_CPU\",\n            \"123_CPU\",\n            \"124_CPU\",\n            \"125_CPU\",\n            \"126_CPU\",\n            \"127_CPU\"\n          ]\n        },\n        {\n          \"name\": \"domain_busy\",\n          \"values\": [\n            55518207375080,\n            8102895398,\n            12058227646,\n            991838747162,\n            28174147737,\n            6669066926,\n            14245801778,\n            9009875224,\n            118982762,\n            177496844302,\n            5888814259,\n            167280195,\n            3851617905,\n            484154906167,\n            91240285306,\n            6180138216837,\n            22111798640,\n            344700584,\n            266304074,\n            2388625825,\n            16,\n            21,\n            19\n          ],\n          \"labels\": [\n            \"idle\",\n            \"kahuna\",\n            \"storage\",\n            \"exempt\",\n            \"none\",\n            \"raid\",\n            \"raid_exempt\",\n            \"xor_exempt\",\n            \"target\",\n            \"wafl_exempt\",\n            \"wafl_mpcleaner\",\n            \"sm_exempt\",\n            \"protocol\",\n            \"nwk_exempt\",\n            \"network\",\n            \"hostOS\",\n            \"ssan_exempt\",\n            \"unclassified\",\n            \"kahuna_legacy\",\n            \"ha\",\n            \"ssan_exempt2\",\n            \"exempt_ise\",\n            \"zombie\"\n          ]\n        },\n        {\n          \"name\": \"domain_shared\",\n          \"values\": [\n            0,\n            153663450171,\n            0,\n            0,\n            0,\n            11834112384,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"idle\",\n            \"kahuna\",\n            \"storage\",\n            \"exempt\",\n            \"none\",\n            \"raid\",\n            \"raid_exempt\",\n            \"xor_exempt\",\n            \"target\",\n            \"wafl_exempt\",\n            \"wafl_mpcleaner\",\n            \"sm_exempt\",\n            \"protocol\",\n            \"nwk_exempt\",\n            \"network\",\n            \"hostOS\",\n            \"ssan_exempt\",\n            \"unclassified\",\n            \"kahuna_legacy\",\n            \"ha\",\n            \"ssan_exempt2\",\n            \"exempt_ise\",\n            \"zombie\"\n          ]\n        },\n        {\n          \"name\": \"dswitchto_cnt\",\n          \"values\": [\n            0,\n            178192633,\n            143964155,\n            286324250,\n            2365,\n            39684121,\n            5,\n            10,\n            10715325,\n            22,\n            7,\n            30,\n            2407970,\n            7865489299,\n            7870331008,\n            265242,\n            53,\n            2535145,\n            3252888,\n            53334340,\n            1,\n            1,\n            1\n          ],\n          \"labels\": [\n            \"idle\",\n            \"kahuna\",\n            \"storage\",\n            \"exempt\",\n            \"none\",\n            \"raid\",\n            \"raid_exempt\",\n            \"xor_exempt\",\n            \"target\",\n            \"wafl_exempt\",\n            \"wafl_mpcleaner\",\n            \"sm_exempt\",\n            \"protocol\",\n            \"nwk_exempt\",\n            \"network\",\n            \"hostOS\",\n            \"ssan_exempt\",\n            \"unclassified\",\n            \"kahuna_legacy\",\n            \"ha\",\n            \"ssan_exempt2\",\n            \"exempt_ise\",\n            \"zombie\"\n          ]\n        },\n        {\n          \"name\": \"intr_cnt\",\n          \"values\": [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            724698481,\n            8181275,\n            488080162,\n            0,\n            71856,\n            0,\n            71856,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"dev_0\",\n            \"dev_1\",\n            \"dev_2\",\n            \"dev_3\",\n            \"dev_4\",\n            \"dev_5\",\n            \"dev_6\",\n            \"dev_7\",\n            \"dev_8\",\n            \"dev_9\",\n            \"dev_10\",\n            \"dev_11\",\n            \"dev_12\",\n            \"dev_13\",\n            \"dev_14\",\n            \"dev_15\",\n            \"dev_16\",\n            \"dev_17\",\n            \"dev_18\",\n            \"dev_19\",\n            \"dev_20\",\n            \"dev_21\",\n            \"dev_22\",\n            \"dev_23\",\n            \"dev_24\",\n            \"dev_25\",\n            \"dev_26\",\n            \"dev_27\",\n            \"dev_28\",\n            \"dev_29\",\n            \"dev_30\",\n            \"dev_31\",\n            \"dev_32\",\n            \"dev_33\",\n            \"dev_34\",\n            \"dev_35\",\n            \"dev_36\",\n            \"dev_37\",\n            \"dev_38\",\n            \"dev_39\",\n            \"dev_40\",\n            \"dev_41\",\n            \"dev_42\",\n            \"dev_43\",\n            \"dev_44\",\n            \"dev_45\",\n            \"dev_46\",\n            \"dev_47\",\n            \"dev_48\",\n            \"dev_49\",\n            \"dev_50\",\n            \"dev_51\",\n            \"dev_52\",\n            \"dev_53\",\n            \"dev_54\",\n            \"dev_55\",\n            \"dev_56\",\n            \"dev_57\",\n            \"dev_58\",\n            \"dev_59\",\n            \"dev_60\",\n            \"dev_61\",\n            \"dev_62\",\n            \"dev_63\",\n            \"dev_64\",\n            \"dev_65\",\n            \"dev_66\",\n            \"dev_67\",\n            \"dev_68\",\n            \"dev_69\",\n            \"dev_70\",\n            \"dev_71\",\n            \"dev_72\",\n            \"dev_73\",\n            \"dev_74\",\n            \"dev_75\",\n            \"dev_76\",\n            \"dev_77\",\n            \"dev_78\",\n            \"dev_79\",\n            \"dev_80\",\n            \"dev_81\",\n            \"dev_82\",\n            \"dev_83\",\n            \"dev_84\",\n            \"dev_85\",\n            \"dev_86\",\n            \"dev_87\",\n            \"dev_88\",\n            \"dev_89\",\n            \"dev_90\",\n            \"dev_91\",\n            \"dev_92\",\n            \"dev_93\",\n            \"dev_94\",\n            \"dev_95\",\n            \"dev_96\",\n            \"dev_97\",\n            \"dev_98\",\n            \"dev_99\",\n            \"dev_100\",\n            \"dev_101\",\n            \"dev_102\",\n            \"dev_103\",\n            \"dev_104\",\n            \"dev_105\",\n            \"dev_106\",\n            \"dev_107\",\n            \"dev_108\",\n            \"dev_109\",\n            \"dev_110\",\n            \"dev_111\",\n            \"dev_112\",\n            \"dev_113\",\n            \"dev_114\",\n            \"dev_115\",\n            \"dev_116\",\n            \"dev_117\",\n            \"dev_118\",\n            \"dev_119\",\n            \"dev_120\",\n            \"dev_121\",\n            \"dev_122\",\n            \"dev_123\",\n            \"dev_124\",\n            \"dev_125\",\n            \"dev_126\",\n            \"dev_127\",\n            \"dev_128\",\n            \"dev_129\",\n            \"dev_130\",\n            \"dev_131\",\n            \"dev_132\",\n            \"dev_133\",\n            \"dev_134\",\n            \"dev_135\",\n            \"dev_136\",\n            \"dev_137\",\n            \"dev_138\",\n            \"dev_139\",\n            \"dev_140\",\n            \"dev_141\",\n            \"dev_142\",\n            \"dev_143\",\n            \"dev_144\",\n            \"dev_145\",\n            \"dev_146\",\n            \"dev_147\",\n            \"dev_148\",\n            \"dev_149\",\n            \"dev_150\",\n            \"dev_151\",\n            \"dev_152\",\n            \"dev_153\",\n            \"dev_154\",\n            \"dev_155\",\n            \"dev_156\",\n            \"dev_157\",\n            \"dev_158\",\n            \"dev_159\",\n            \"dev_160\",\n            \"dev_161\",\n            \"dev_162\",\n            \"dev_163\",\n            \"dev_164\",\n            \"dev_165\",\n            \"dev_166\",\n            \"dev_167\",\n            \"dev_168\",\n            \"dev_169\",\n            \"dev_170\",\n            \"dev_171\",\n            \"dev_172\",\n            \"dev_173\",\n            \"dev_174\",\n            \"dev_175\",\n            \"dev_176\",\n            \"dev_177\",\n            \"dev_178\",\n            \"dev_179\",\n            \"dev_180\",\n            \"dev_181\",\n            \"dev_182\",\n            \"dev_183\",\n            \"dev_184\",\n            \"dev_185\",\n            \"dev_186\",\n            \"dev_187\",\n            \"dev_188\",\n            \"dev_189\",\n            \"dev_190\",\n            \"dev_191\",\n            \"dev_192\",\n            \"dev_193\",\n            \"dev_194\",\n            \"dev_195\",\n            \"dev_196\",\n            \"dev_197\",\n            \"dev_198\",\n            \"dev_199\",\n            \"dev_200\",\n            \"dev_201\",\n            \"dev_202\",\n            \"dev_203\",\n            \"dev_204\",\n            \"dev_205\",\n            \"dev_206\",\n            \"dev_207\",\n            \"dev_208\",\n            \"dev_209\",\n            \"dev_210\",\n            \"dev_211\",\n            \"dev_212\",\n            \"dev_213\",\n            \"dev_214\",\n            \"dev_215\",\n            \"dev_216\",\n            \"dev_217\",\n            \"dev_218\",\n            \"dev_219\",\n            \"dev_220\",\n            \"dev_221\",\n            \"dev_222\",\n            \"dev_223\",\n            \"dev_224\",\n            \"dev_225\",\n            \"dev_226\",\n            \"dev_227\",\n            \"dev_228\",\n            \"dev_229\",\n            \"dev_230\",\n            \"dev_231\",\n            \"dev_232\",\n            \"dev_233\",\n            \"dev_234\",\n            \"dev_235\",\n            \"dev_236\",\n            \"dev_237\",\n            \"dev_238\",\n            \"dev_239\",\n            \"dev_240\",\n            \"dev_241\",\n            \"dev_242\",\n            \"dev_243\",\n            \"dev_244\",\n            \"dev_245\",\n            \"dev_246\",\n            \"dev_247\",\n            \"dev_248\",\n            \"dev_249\",\n            \"dev_250\",\n            \"dev_251\",\n            \"dev_252\",\n            \"dev_253\",\n            \"dev_254\",\n            \"dev_255\"\n          ]\n        },\n        {\n          \"name\": \"wafliron\",\n          \"values\": [\n            0,\n            0,\n            0\n          ],\n          \"labels\": [\n            \"iron_totstarts\",\n            \"iron_nobackup\",\n            \"iron_usebackup\"\n          ]\n        }\n      ],\n      \"aggregation\": {\n        \"count\": 2,\n        \"complete\": true\n      },\n      \"_links\": {\n        \"self\": {\n          \"href\": \"/api/cluster/counter/tables/system:node/rows/umeng-aff300-02%3A1524afca-0580-11e8-ae74-00a098d390f2\"\n        }\n      }\n    }\n  ],\n  \"num_records\": 2,\n  \"_links\": {\n    \"self\": {\n      \"href\": \"/api/cluster/counter/tables/system:node/rows?fields=*&amp;return_records=true\"\n    }\n  }\n}\n</code></pre> </p>"},{"location":"resources/rest-perf-metrics/#references","title":"References","text":"<ul> <li>Harvest REST Strategy</li> <li>ONTAP 9.11.1 ONTAPI-to-REST Counter Manager Mapping</li> <li>ONTAP REST API reference documentation</li> <li>ONTAP REST API </li> </ul>"},{"location":"resources/templates-and-metrics/","title":"Harvest Templates and Metrics","text":"<p>Harvest collects ONTAP counter information, augments it, and stores it in a time-series DB. Refer ONTAP Metrics for details about ONTAP metrics exposed by Harvest. Harvest collects StorageGrid counter information, augments it, and stores it in a time-series DB. Refer StorageGrid Metrics for details about StorageGrid metrics exposed by Harvest. Harvest collects Cisco Switch counter information, augments it, and stores it in a time-series DB. Refer Cisco Switch Metrics for details about Cisco Switch metrics exposed by Harvest.</p> <pre><code>flowchart RL\n    Harvest[Harvest&lt;br&gt;Get &amp; Augment] -- REST&lt;br&gt;ZAPI --&gt; ONTAP\n    id1[(Prometheus&lt;br&gt;Store)] -- Scrape --&gt; Harvest</code></pre> <p>Three concepts work in unison to collect ONTAP metrics data, prepare it and make it available to Prometheus.</p> <ul> <li>ZAPI/REST</li> <li>Harvest templates</li> <li>Exporters </li> </ul> <p>We're going to walk through an example from a running system, focusing on the <code>disk</code> object.</p> <p>At a high-level, Harvest templates describe what ZAPIs to send to ONTAP and how to interpret the responses.</p> <ul> <li>ONTAP defines twos ZAPIs to collect <code>disk</code> info<ul> <li>Config information is collected via <code>storage-disk-get-iter</code></li> <li>Performance counters are collected via <code>disk:constituent</code></li> </ul> </li> <li>These ZAPIs are found in their corresponding object template file <code>conf/zapi/cdot/9.8.0/disk.yaml</code>   and <code>conf/zapiperf/cdot/9.8.0/disk.yaml</code>. These files also describe how to map the ZAPI responses into a   time-series-friendly format</li> <li>Prometheus uniquely identifies a time series by its metric name and optional key-value pairs called labels.</li> </ul>"},{"location":"resources/templates-and-metrics/#handy-tools","title":"Handy Tools","text":"<ul> <li>dasel is useful to convert between XML, YAML, JSON, etc. We'll use it to make   displaying some of the data easier.</li> </ul>"},{"location":"resources/templates-and-metrics/#ontap-zapi-disk-example","title":"ONTAP ZAPI disk example","text":"<p>We'll use the <code>bin/harvest zapi</code> tool to interrogate the cluster and gather information about the counters. This is one way you can send ZAPIs to ONTAP and explore the return types and values.</p> <pre><code>bin/harvest zapi -p u2 show attrs --api storage-disk-get-iter\n</code></pre> <p>Output edited for brevity and line numbers added on left</p> <p>The hierarchy and return type of each counter is shown below. We'll use this hierarchy to build a matching Harvest template. For example, line <code>3</code> is the <code>bytes-per-sector</code> counter, which has an integer value, and is the child of <code>storage-disk-info &gt; disk-inventory-info</code>.</p> <p>To capture that counter's value as a metric in a Harvest, the ZAPI template must use the same hierarchical path. The matching path can be seen below.</p> <pre><code>building tree for attribute [attributes-list] =&gt; [storage-disk-info]\n\n 1 [storage-disk-info]            -               *\n 2   [disk-inventory-info]        -                \n 3     [bytes-per-sector]         -         integer\n 4     [capacity-sectors]         -         integer\n 5     [disk-type]                -          string\n 6     [is-shared]                -         boolean\n 7     [model]                    -          string\n 8     [serial-number]            -          string\n 9     [shelf]                    -          string\n10     [shelf-bay]                -          string\n11   [disk-name]                  -          string\n12   [disk-ownership-info]        -                \n13     [home-node-name]           -          string\n14     [is-failed]                -         boolean\n15     [owner-node-name]          -          string\n16   [disk-raid-info]             -                \n17     [container-type]           -          string\n18     [disk-outage-info]         -                \n19       [is-in-fdr]              -         boolean\n20       [reason]                 -          string  \n21   [disk-stats-info]            -                \n22     [average-latency]          -         integer\n23     [disk-io-kbps]             -         integer\n24     [power-on-time-interval]   -         integer\n25     [sectors-read]             -         integer\n26     [sectors-written]          -         integer\n27   [disk-uid]                   -          string\n28   [node-name]                  -          string\n29   [storage-disk-state]         -         integer\n30   [storage-disk-state-flags]   -         integer\n</code></pre>"},{"location":"resources/templates-and-metrics/#harvest-templates","title":"Harvest Templates","text":"<p>To understand templates, there are a few concepts to cover:</p> <p>There are three kinds of information included in templates that define what Harvest collects and exports:</p> <ol> <li>Configuration information is exported into the <code>_labels</code> metric (e.g. <code>disk_labels</code> see below)</li> <li>Metrics data is exported as <code>disk_\"metric name\"</code> e.g. <code>disk_bytes_per_sector</code>, <code>disk_sectors</code>, etc. Metrics are leaf    nodes that are not prefixed with a ^ or ^^. Metrics must be one of the number types: float or int.</li> <li>Plugins may add additional metrics, increasing the number of metrics exported in #2</li> </ol> <p>A resource will typically have multiple instances. Using disk as an example, that means there will be one <code>disk_labels</code> and a metric row per instance. If we have 24 disks and the disk template lists seven metrics to capture, Harvest will export a total of 192 rows of Prometheus data.</p> <p><code>24 instances * (7 metrics per instance + 1 label per instance) = 192 rows</code></p> <p>Sum of disk metrics that Harvest exports</p> <pre><code>curl -s 'http://localhost:14002/metrics' | grep ^disk | cut -d'{' -f1 | sort | uniq -c\n  24 disk_bytes_per_sector\n  24 disk_labels\n  24 disk_sectors\n  24 disk_stats_average_latency\n  24 disk_stats_io_kbps\n  24 disk_stats_sectors_read\n  24 disk_stats_sectors_written\n  24 disk_uptime\n# 192 rows \n</code></pre> <p>Read on to see how we control which labels from #1 and which metrics from #2 are included in the exported data.</p>"},{"location":"resources/templates-and-metrics/#instance-keys-and-labels","title":"Instance Keys and Labels","text":"<ul> <li>Instance key - An instance key defines the set of attributes Harvest uses to construct a key that uniquely identifies   an object. For example, the disk template uses the <code>node</code> + <code>disk</code> attributes to determine uniqueness. Using <code>node</code>   or <code>disk</code> alone wouldn't be sufficient since disks on separate nodes can have the same name. If a single label does   not uniquely identify an instance, combine multiple keys for uniqueness. Instance keys must refer to attributes that   are of type <code>string</code>.</li> </ul> <p>Because instance keys define uniqueness, these keys are also added to each metric as a key-value pair. (   see Control What Labels and Metrics are Exported for examples)</p> <ul> <li>Instance label - Labels are key-value pairs used to gather configuration information about each instance. All of the   key-value pairs are combined into a single metric named <code>disk_labels</code>. There will be one <code>disk_labels</code> for each   monitored instance. Here's an example reformatted so it's easier to read:</li> </ul> <pre><code>disk_labels{\n  datacenter=\"dc-1\",\n  cluster=\"umeng-aff300-05-06\",\n  node=\"umeng-aff300-06\",\n  disk=\"1.1.23\",\n  type=\"SSD\",\n  model=\"X371_S1643960ATE\",\n  outage=\"\",\n  owner_node=\"umeng-aff300-06\",\n  shared=\"true\",\n  shelf=\"1\",\n  shelf_bay=\"23\",\n  serial_number=\"S3SENE0K500532\",\n  failed=\"false\",\n  container_type=\"shared\"\n}\n</code></pre>"},{"location":"resources/templates-and-metrics/#harvest-object-template","title":"Harvest Object Template","text":"<p>Continuing with the disk example, below is the <code>conf/zapi/cdot/9.8.0/disk.yaml</code> that tells Harvest which ZAPI to send to ONTAP (<code>storage-disk-get-iter</code>) and describes how to interpret and export the response.</p> <ul> <li>Line <code>1</code> defines the name of this resource and is an exact match to the object defined in your <code>default.yaml</code>   or <code>custom.yaml</code> file. Eg.</li> </ul> <pre><code># default.yaml\nobjects:\n  Disk:  disk.yaml\n</code></pre> <ul> <li>Line <code>2</code> is the name of the ZAPI that Harvest will send to collect disk resources</li> <li>Line <code>3</code> is the prefix used to export metrics associated with this object. i.e. all metrics will be of the   form <code>disk_*</code></li> <li>Line <code>5</code> the counter section is where we define the   metrics, labels, and what constitutes instance uniqueness</li> <li>Line <code>7</code> the double hat prefix <code>^^</code> means this attribute is an instance key used to determine uniqueness. Instance   keys are also included as labels. Uuids are good choices for uniqueness</li> <li>Line <code>13</code> the single hat prefix <code>^</code> means this attribute should be stored as a label. That means we can include it in   the <code>export_options</code> section as one of the key-value pairs in <code>disk_labels</code></li> <li>Rows 10, 11, 23, 24, 25, 26, 27 - these are the metrics rows - metrics are leaf nodes that are not prefixed with a ^   or ^^. If you refer back to the ONTAP ZAPI disk example above, you'll notice each of these   attributes are integer types.</li> <li>Line 43 defines the set of labels to use when constructing the <code>disk_labels</code> metrics. As   mentioned above, these labels capture config-related attributes per instance.</li> </ul> <p>Output edited for brevity and line numbers added for reference.</p> <pre><code> 1  name:             Disk\n 2  query:            storage-disk-get-iter\n 3  object:           disk\n 4  \n 5  counters:\n 6    storage-disk-info:\n 7      - ^^disk-uid\n 8      - ^^disk-name               =&gt; disk\n 9      - disk-inventory-info:\n10        - bytes-per-sector        =&gt; bytes_per_sector        # notice this has the same hierarchical path we saw from bin/harvest zapi\n11        - capacity-sectors        =&gt; sectors\n12        - ^disk-type              =&gt; type\n13        - ^is-shared              =&gt; shared\n14        - ^model                  =&gt; model\n15        - ^serial-number          =&gt; serial_number\n16        - ^shelf                  =&gt; shelf\n17        - ^shelf-bay              =&gt; shelf_bay\n18      - disk-ownership-info:\n19        - ^home-node-name         =&gt; node\n20        - ^owner-node-name        =&gt; owner_node\n21        - ^is-failed              =&gt; failed\n22      - disk-stats-info:\n23        - average-latency\n24        - disk-io-kbps\n25        - power-on-time-interval  =&gt; uptime\n26        - sectors-read\n27        - sectors-written\n28      - disk-raid-info:\n29        - ^container-type         =&gt; container_type\n30        - disk-outage-info:\n31          - ^reason               =&gt; outage\n32  \n33  plugins:\n34    - LabelAgent:\n35      # metric label zapi_value rest_value `default_value`\n36      value_to_num:\n37        - new_status outage - - `0` #ok_value is empty value, '-' would be converted to blank while processing.\n38  \n39  export_options:\n40    instance_keys:\n41      - node\n42      - disk\n43    instance_labels:\n44      - type\n45      - model\n46      - outage\n47      - owner_node\n48      - shared\n49      - shelf\n50      - shelf_bay\n51      - serial_number\n52      - failed\n53      - container_type\n</code></pre>"},{"location":"resources/templates-and-metrics/#control-what-labels-and-metrics-are-exported","title":"Control What Labels and Metrics are Exported","text":"<p>Let's continue with <code>disk</code> and look at a few examples. We'll use <code>curl</code> to examine the Prometheus wire format that Harvest uses to export the metrics from <code>conf/zapi/cdot/9.8.0/disk.yaml</code>.</p> <p>The curl below shows all exported disk metrics. There are 24 disks on this cluster, Harvest is collecting seven metrics + one disk_labels + one plugin-created metric, <code>disk_new_status</code> for a total of 216 rows.</p> <pre><code>curl -s 'http://localhost:14002/metrics' | grep ^disk | cut -d'{' -f1 | sort | uniq -c\n  24 disk_bytes_per_sector           # metric\n  24 disk_labels                     # labels \n  24 disk_new_status                 # plugin created metric \n  24 disk_sectors                    # metric \n  24 disk_stats_average_latency      # metric   \n  24 disk_stats_io_kbps              # metric \n  24 disk_stats_sectors_read         # metric   \n  24 disk_stats_sectors_written      # metric  \n  24 disk_uptime                     # metric\n# sum = ((7 + 1 + 1) * 24 = 216 rows)\n</code></pre> <p>Here's a <code>disk_labels</code> for one instance, reformatted to make it easier to read.</p> <pre><code>curl -s 'http://localhost:14002/metrics' | grep ^disk_labels | head -1\n\ndisk_labels{\n  datacenter = \"dc-1\",                 # always included - value taken from datacenter in harvest.yml\n  cluster = \"umeng-aff300-05-06\",      # always included\n  node = \"umeng-aff300-06\",            # node is in the list of export_options instance_keys\n  disk = \"1.1.13\",                     # disk is in the list of export_options instance_keys\n  type = \"SSD\",                        # remainder are included because they are listed in the template's instance_labels\n  model = \"X371_S1643960ATE\",\n  outage = \"\",\n  owner_node = \"umeng-aff300-06\",\n  shared = \"true\",\n  shelf = \"1\",\n  shelf_bay = \"13\",\n  serial_number = \"S3SENE0K500572\",\n  failed = \"false\",\n  container_type = \"\",\n} 1.0\n</code></pre> <p>Here's the <code>disk_sectors</code> metric for a single instance.</p> <pre><code>curl -s 'http://localhost:14002/metrics' | grep ^disk_sectors | head -1\n\ndisk_sectors{                          # prefix of disk_ + metric name (line 11 in template)\n  datacenter = \"dc-1\",                 # always included - value taken from datacenter in harvest.yml\n  cluster = \"umeng-aff300-05-06\",      # always included\n  node = \"umeng-aff300-06\",            # node is in the list of export_options instance_keys\n  disk = \"1.1.17\",                     # disk is in the list of export_options instance_keys\n} 1875385008                           # metric value - number of sectors for this disk instance\n</code></pre> <pre><code>Number of rows for each template = number of instances * (number of metrics + 1 (for &lt;name&gt;_labels row) + plugin additions)\nNumber of metrics                = number of counters which are not labels or keys, those without a ^ or ^^\n</code></pre>"},{"location":"resources/templates-and-metrics/#common-errors-and-troubleshooting","title":"Common Errors and Troubleshooting","text":""},{"location":"resources/templates-and-metrics/#1-failed-to-parse-any-metrics","title":"1. Failed to parse any metrics","text":"<p>You add a new template to Harvest, restart your poller, and get an error message:</p> <pre><code>WRN ./poller.go:649 &gt; init collector-object (Zapi:NetPort): no metrics =&gt; failed to parse any\n</code></pre> <p>This means the collector, <code>Zapi NetPort</code>, was unable to find any metrics. Recall metrics are lines without prefixes. In cases where you don't have any metrics, but still want to collect labels, add the <code>collect_only_labels: true</code> key-value to your template. This flag tells Harvest to ignore that you don't have metrics and continue. Example.</p>"},{"location":"resources/templates-and-metrics/#2-missing-data","title":"2. Missing Data","text":"<ol> <li>What happens if an attribute is listed in the list of <code>instance_labels</code> (line 43 above), but that label is missing    from the list of counters captured at line 5?</li> </ol> <p>The label will still be written into <code>disk_labels</code>, but the value will be empty since it's missing. e.g if line 29 was deleted <code>container_type</code> would still be present in <code>disk_labels{container_type=\"\"}</code>.</p>"},{"location":"resources/templates-and-metrics/#prometheus-wire-format","title":"Prometheus Wire Format","text":"<p>https://prometheus.io/docs/instrumenting/exposition_formats/</p> <p>Keep in mind that Prometheus does not permit dashes (<code>-</code>) in labels. That's why Harvest templates use name replacement to convert dashed-names to underscored-names with <code>=&gt;</code>. e.g. <code>bytes-per-sector =&gt; bytes_per_sector</code> converts <code>bytes-per-sector</code> into the Prometheus accepted <code>bytes_per_sector</code>.</p> <p>Every time series is uniquely identified by its metric name and optional key-value pairs called labels.</p> <p>Labels enable Prometheus's dimensional data model: any combination of labels for the same metric name identifies a particular dimensional instantiation of that metric (for example: all HTTP requests that used the method POST to the /api/tracks handler). The query language allows filtering and aggregation based on these dimensions. Changing any label value, including adding or removing a label, will create a new time series.</p> <p><code>&lt;metric_name&gt;{&lt;label_name&gt;=&lt;label_value&gt;, ...} value [ timestamp ]</code></p> <ul> <li>metric_name and label_name carry the usual Prometheus expression language restrictions</li> <li>label_value can be any sequence of UTF-8 characters, but the backslash (), double-quote (\"), and line feed (\\n)     characters have to be escaped as \\, \\\", and \\n, respectively.</li> <li>value is a float represented as required by Go's ParseFloat() function. In addition to standard numerical values,     NaN, +Inf, and -Inf are valid values representing not a number, positive infinity, and negative infinity,     respectively.</li> <li>timestamp is an int64 (milliseconds since epoch, i.e. 1970-01-01 00:00:00 UTC, excluding leap seconds), represented     as required by Go's ParseInt() function</li> </ul> <p>Exposition formats </p>"},{"location":"resources/zapi-and-rest-gap/","title":"ZAPI and REST Gaps","text":""},{"location":"resources/zapi-and-rest-gap/#volume-count-difference","title":"Volume Count difference","text":"<p>The <code>REST</code> and <code>ZAPI</code> collectors return a different number of <code>volume_labels</code> depending on whether you have set up object store servers on your cluster.</p> <ul> <li>The <code>REST</code> collector does not include <code>volume_labels</code> for volumes associated with object store servers.</li> <li>The <code>ZAPI</code> collector includes <code>volume_labels</code> for volumes associated with object store servers. If you have not set up any object store servers on your cluster, both collectors will return the same number of <code>volume_labels</code>.</li> </ul>"}]}
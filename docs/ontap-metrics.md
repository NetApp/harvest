This document describes how Harvest metrics relate to their relevant ONTAP ZAPI and REST mappings, including:

- Details about which Harvest metrics each dashboard uses.
These can be generated on demand by running `bin/harvest grafana metrics`. See
[#1577](https://github.com/NetApp/harvest/issues/1577#issue-1471478260) for details.

- More information about ONTAP REST performance counters can be found [here](https://docs.netapp.com/us-en/ontap-pcmap-9121/index.html).

```
Creation Date : 2026-Feb-18
ONTAP Version: 9.16.1
```

??? "Navigate to Grafana dashboards"

    Add your Grafana instance to the following form and save it. When you click on dashboard links on this page, a link to your dashboard will be opened. NAbox hosts Grafana on a subdomain like so: https://localhost/grafana/

    <div>
        <label for="grafanaHost">Grafana Host</label>
        <input type="text" id="grafanaHost" name="grafanaHost" placeholder="e.g. http://localhost:3000" style="width: 80%;margin-left:1em">
        <button type="button" onclick="saveGrafanaHost()">Save</button>
    </div>

## Understanding the structure

Below is an <span class="key">annotated</span> example of how to interpret the structure of each of the [metrics](#metrics).

disk_io_queued <span class="key">Name of the metric exported by Harvest</span>

Number of I/Os queued to the disk but not yet issued <span class="key">Description of the ONTAP metric</span>

* <span class="key">API</span> will be one of REST or ZAPI depending on which collector is used to collect the metric
* <span class="key">Endpoint</span> name of the REST or ZAPI API used to collect this metric
* <span class="key">Metric</span> name of the ONTAP metric
* <span class="key">Template</span> path of the template that collects the metric

Performance related metrics also include:

- <span class="key">Unit</span> the unit of the metric
- <span class="key">Type</span> describes how to calculate a cooked metric from two consecutive ONTAP raw metrics
- <span class="key">Base</span> some counters require a `base counter` for post-processing. When required, this property lists the `base counter`

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
|REST | `api/cluster/counter/tables/disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml|
|ZAPI | `perf-object-get-instances disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml|

## Metrics


### aggr_disk_busy

The utilization percent of the disk. aggr_disk_busy is [disk_busy](#disk_busy) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `disk_busy_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_disk_busy` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Disk Utilization | table | [Top $TopResources Average Disk Utilization Per Aggregate](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=61) |
| ONTAP: Aggregate | Disk Utilization | timeseries | [Top $TopResources Average Disk Utilization Per Aggregate](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=63) |
| ONTAP: Cluster | Throughput | timeseries | [Average Disk Utilization by Aggregate](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=136) |
| ONTAP: Disk | Highlights | stat | [Raid Groups](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=47) |
| ONTAP: Disk | Highlights | stat | [Plexes](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=48) |
///



### aggr_disk_capacity

Disk capacity in MB. aggr_disk_capacity is [disk_capacity](#disk_capacity) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_cp_read_chain

Average number of blocks transferred in each consistency point read operation during a CP. aggr_disk_cp_read_chain is [disk_cp_read_chain](#disk_cp_read_chain) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_cp_read_latency

Average latency per block in microseconds for consistency point read operations. aggr_disk_cp_read_latency is [disk_cp_read_latency](#disk_cp_read_latency) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_cp_reads

Number of disk read operations initiated each second for consistency point processing. aggr_disk_cp_reads is [disk_cp_reads](#disk_cp_reads) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_io_pending

Average number of I/Os issued to the disk for which we have not yet received the response. aggr_disk_io_pending is [disk_io_pending](#disk_io_pending) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_io_queued

Number of I/Os queued to the disk but not yet issued. aggr_disk_io_queued is [disk_io_queued](#disk_io_queued) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_busy

The utilization percent of the disk. aggr_disk_max_busy is the maximum of [disk_busy](#disk_busy) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `disk_busy_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_disk_max_busy` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Highlights | table | [Top $TopResources Aggregates by Disk Utilization](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=54) |
| ONTAP: Disk | Highlights | timeseries | [Top $TopResources Aggregates by Max Disk Utilization](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=50) |
| ONTAP: MetroCluster | Highlights | gauge | [Max Disk Utilization Per Aggregate](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=35) |
///



### aggr_disk_max_capacity

Disk capacity in MB. aggr_disk_max_capacity is the maximum of [disk_capacity](#disk_capacity) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_cp_read_chain

Average number of blocks transferred in each consistency point read operation during a CP. aggr_disk_max_cp_read_chain is the maximum of [disk_cp_read_chain](#disk_cp_read_chain) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_cp_read_latency

Average latency per block in microseconds for consistency point read operations. aggr_disk_max_cp_read_latency is the maximum of [disk_cp_read_latency](#disk_cp_read_latency) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_cp_reads

Number of disk read operations initiated each second for consistency point processing. aggr_disk_max_cp_reads is the maximum of [disk_cp_reads](#disk_cp_reads) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_io_pending

Average number of I/Os issued to the disk for which we have not yet received the response. aggr_disk_max_io_pending is the maximum of [disk_io_pending](#disk_io_pending) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_io_queued

Number of I/Os queued to the disk but not yet issued. aggr_disk_max_io_queued is the maximum of [disk_io_queued](#disk_io_queued) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_total_data

Total throughput for user operations per second. aggr_disk_max_total_data is the maximum of [disk_total_data](#disk_total_data) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_total_transfers

Total number of disk operations involving data transfer initiated per second. aggr_disk_max_total_transfers is the maximum of [disk_total_transfers](#disk_total_transfers) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_transfer_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_transfers`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_disk_max_total_transfers` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Highlights | table | [Top $TopResources Aggregates by Disk Utilization](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=54) |
| ONTAP: Disk | Highlights | timeseries | [Top $TopResources Aggregates by Disk Transfers](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=51) |
///



### aggr_disk_max_user_read_blocks

Number of blocks transferred for user read operations per second. aggr_disk_max_user_read_blocks is the maximum of [disk_user_read_blocks](#disk_user_read_blocks) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_user_read_chain

Average number of blocks transferred in each user read operation. aggr_disk_max_user_read_chain is the maximum of [disk_user_read_chain](#disk_user_read_chain) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_disk_max_user_read_chain` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Highlights | timeseries | [Top $TopResources Aggregates by User Read Chain Length](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=56) |
///



### aggr_disk_max_user_read_latency

Average latency per block in microseconds for user read operations. aggr_disk_max_user_read_latency is the maximum of [disk_user_read_latency](#disk_user_read_latency) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_user_reads

Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. aggr_disk_max_user_reads is the maximum of [disk_user_reads](#disk_user_reads) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_user_write_blocks

Number of blocks transferred for user write operations per second. aggr_disk_max_user_write_blocks is the maximum of [disk_user_write_blocks](#disk_user_write_blocks) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_user_write_chain

Average number of blocks transferred in each user write operation. aggr_disk_max_user_write_chain is the maximum of [disk_user_write_chain](#disk_user_write_chain) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_writes | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_disk_max_user_write_chain` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Highlights | timeseries | [Top $TopResources Aggregates by User Write Chain Length](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=57) |
///



### aggr_disk_max_user_write_latency

Average latency per block in microseconds for user write operations. aggr_disk_max_user_write_latency is the maximum of [disk_user_write_latency](#disk_user_write_latency) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_max_user_writes

Number of disk write operations initiated each second for storing data or metadata associated with user requests. aggr_disk_max_user_writes is the maximum of [disk_user_writes](#disk_user_writes) for label `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_writes`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_total_data

Total throughput for user operations per second. aggr_disk_total_data is [disk_total_data](#disk_total_data) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_disk_total_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Aggregate | table | [Aggregates](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=91) |
///



### aggr_disk_total_transfers

Total number of disk operations involving data transfer initiated per second. aggr_disk_total_transfers is [disk_total_transfers](#disk_total_transfers) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_transfer_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_transfers`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_disk_total_transfers` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Aggregate | table | [Aggregates](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=91) |
///



### aggr_disk_user_read_blocks

Number of blocks transferred for user read operations per second. aggr_disk_user_read_blocks is [disk_user_read_blocks](#disk_user_read_blocks) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_user_read_chain

Average number of blocks transferred in each user read operation. aggr_disk_user_read_chain is [disk_user_read_chain](#disk_user_read_chain) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_user_read_latency

Average latency per block in microseconds for user read operations. aggr_disk_user_read_latency is [disk_user_read_latency](#disk_user_read_latency) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_user_reads

Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. aggr_disk_user_reads is [disk_user_reads](#disk_user_reads) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_disk_user_reads` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Highlights | timeseries | [Top $TopResources Aggregates by IOPS Per Power Consumed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=94) |
///



### aggr_disk_user_write_blocks

Number of blocks transferred for user write operations per second. aggr_disk_user_write_blocks is [disk_user_write_blocks](#disk_user_write_blocks) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_user_write_chain

Average number of blocks transferred in each user write operation. aggr_disk_user_write_chain is [disk_user_write_chain](#disk_user_write_chain) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_writes | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_user_write_latency

Average latency per block in microseconds for user write operations. aggr_disk_user_write_latency is [disk_user_write_latency](#disk_user_write_latency) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### aggr_disk_user_writes

Number of disk write operations initiated each second for storing data or metadata associated with user requests. aggr_disk_user_writes is [disk_user_writes](#disk_user_writes) aggregated by `aggr`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_writes`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_disk_user_writes` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Highlights | timeseries | [Top $TopResources Aggregates by IOPS Per Power Consumed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=94) |
///



### aggr_efficiency_savings

Space saved by storage efficiencies (logical_used - used)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.efficiency.savings` | conf/rest/9.12.0/aggr.yaml |



### aggr_efficiency_savings_wo_snapshots

Space saved by storage efficiencies (logical_used - used)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.efficiency_without_snapshots.savings` | conf/rest/9.12.0/aggr.yaml |



### aggr_efficiency_savings_wo_snapshots_flexclones

Space saved by storage efficiencies (logical_used - used)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.efficiency_without_snapshots_flexclones.savings` | conf/rest/9.12.0/aggr.yaml |



### aggr_hybrid_cache_size_total

Total usable space in bytes of SSD cache. Only provided when hybrid_cache.enabled is 'true'.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `block_storage.hybrid_cache.size` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.hybrid-cache-size-total` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_hybrid_disk_count

Number of disks used in the cache tier of the aggregate. Only provided when hybrid_cache.enabled is 'true'.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `block_storage.hybrid_cache.disk_count` | conf/rest/9.12.0/aggr.yaml |



### aggr_inode_files_private_used

Number of system metadata files used. If the referenced file system is restricted or offline, a value of 0 is returned.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `inode_attributes.files_private_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-inode-attributes.files-private-used` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_inode_files_total

Maximum number of user-visible files that this referenced file system can currently hold. If the referenced file system is restricted or offline, a value of 0 is returned.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `inode_attributes.files_total` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-inode-attributes.files-total` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_inode_files_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Inodes Files](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=35) |
///



### aggr_inode_files_used

Number of user-visible files used in the referenced file system. If the referenced file system is restricted or offline, a value of 0 is returned.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `inode_attributes.files_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-inode-attributes.files-used` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_inode_files_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Inodes Files](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=35) |
///



### aggr_inode_inodefile_private_capacity

Number of files that can currently be stored on disk for system metadata files. This number will dynamically increase as more system files are created.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `inode_attributes.file_private_capacity` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-inode-attributes.inodefile-private-capacity` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_inode_inodefile_private_capacity` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Inode Capacity](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=6) |
///



### aggr_inode_inodefile_public_capacity

Number of files that can currently be stored on disk for user-visible files.  This number will dynamically increase as more user-visible files are created.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `inode_attributes.file_public_capacity` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-inode-attributes.inodefile-public-capacity` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_inode_inodefile_public_capacity` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Inode Capacity](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=6) |
///



### aggr_inode_maxfiles_available

The count of the maximum number of user-visible files currently allowable on the referenced file system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `inode_attributes.max_files_available` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-inode-attributes.maxfiles-available` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_inode_maxfiles_possible

The largest value to which the maxfiles-available parameter can be increased by reconfiguration, on the referenced file system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `inode_attributes.max_files_possible` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-inode-attributes.maxfiles-possible` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_inode_maxfiles_used

The number of user-visible files currently in use on the referenced file system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `inode_attributes.max_files_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-inode-attributes.maxfiles-used` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_inode_used_percent

The percentage of disk space currently in use based on user-visible file count on the referenced file system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `inode_attributes.used_percent` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-inode-attributes.percent-inode-used-capacity` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_inode_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Inodes Used %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=4) |
///



### aggr_labels

This metric provides information about Aggregate

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `Harvest generated` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | stat | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=20) |
| ONTAP: Aggregate | Highlights | table | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=2) |
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: StorageGrid FabricPool | Highlights | stat | [Aggregates](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=20) |
| ONTAP: StorageGrid FabricPool | Highlights | table | [Aggregates](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=2) |
///



### aggr_logical_used_wo_snapshots

Logical used

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.efficiency_without_snapshots.logical_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-efficiency-get-iter` | `aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-logical-used-wo-snapshots` | conf/zapi/cdot/9.9.0/aggr_efficiency.yaml |

The `aggr_logical_used_wo_snapshots` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Storage Efficiency Ratios | stat | [Data Reduction with FlexClones](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=43) |
| ONTAP: Aggregate | Storage Efficiency Ratios | timeseries | [Top $TopResources Aggregates by Logical Used with FlexClones](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=51) |
| ONTAP: Cluster | Storage Efficiency Ratios | stat | [Data Reduction with FlexClones](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=189) |
| ONTAP: Cluster | Storage Efficiency Ratios | timeseries | [Logical Used with FlexClones](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=193) |
| ONTAP: Datacenter | Storage Efficiency | stat | [Data Reduction with FlexClones](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=189) |
| ONTAP: Datacenter | Storage Efficiency | timeseries | [Top $TopResources Logical Used with FlexClones by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=193) |
///



### aggr_logical_used_wo_snapshots_flexclones

Logical used

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.efficiency_without_snapshots_flexclones.logical_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-efficiency-get-iter` | `aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-logical-used-wo-snapshots-flexclones` | conf/zapi/cdot/9.9.0/aggr_efficiency.yaml |

The `aggr_logical_used_wo_snapshots_flexclones` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Storage Efficiency Ratios | stat | [Data Reduction](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=45) |
| ONTAP: Aggregate | Storage Efficiency Ratios | timeseries | [Top $TopResources Aggregates by Logical Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=55) |
| ONTAP: Cluster | Storage Efficiency Ratios | stat | [Data Reduction](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=188) |
| ONTAP: Cluster | Storage Efficiency Ratios | timeseries | [Logical Used](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=195) |
| ONTAP: Datacenter | Storage Efficiency | stat | [Data Reduction](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=188) |
| ONTAP: Datacenter | Storage Efficiency | timeseries | [Top $TopResources Logical Used by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=195) |
///



### aggr_new_status

This metric indicates a value of 1 if the aggregate state is online (indicating the aggregate is operational) and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_new_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | table | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=2) |
| ONTAP: Node | Highlights | stat | [Aggregates](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=16) |
| ONTAP: StorageGrid FabricPool | Highlights | table | [Aggregates](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=2) |
///



### aggr_object_store_logical_used

Logical space usage of aggregates in the attached object store.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/aggr/show-space` | `object_store_logical_used` | conf/rest/9.12.0/aggr.yaml |

The `aggr_object_store_logical_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by logical space usage in Object Store](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=822) |
///



### aggr_object_store_physical_used

Physical space usage of aggregates in the attached object store.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/aggr/show-space` | `object_store_physical_used` | conf/rest/9.12.0/aggr.yaml |

The `aggr_object_store_physical_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by physical space usage in Object Store](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=823) |
///



### aggr_other_data

Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.throughput_raw.other`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/aggr.yaml |



### aggr_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> aggr_statistics.iops_raw.other | conf/keyperf/9.15.0/aggr.yaml |



### aggr_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/aggr.yaml |



### aggr_physical_used_wo_snapshots

Total Data Reduction Physical Used Without Snapshots

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.efficiency_without_snapshots.logical_used, space.efficiency_without_snapshots.savings` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-efficiency-get-iter` | `aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-physical-used-wo-snapshots` | conf/zapi/cdot/9.9.0/aggr_efficiency.yaml |

The `aggr_physical_used_wo_snapshots` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Storage Efficiency Ratios | stat | [Data Reduction with FlexClones](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=43) |
| ONTAP: Aggregate | Storage Efficiency Ratios | timeseries | [Top $TopResources Aggregates by Physical Used with FlexClones](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=53) |
| ONTAP: Cluster | Storage Efficiency Ratios | stat | [Data Reduction with FlexClones](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=189) |
| ONTAP: Cluster | Storage Efficiency Ratios | timeseries | [Physical Used with FlexClones](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=196) |
| ONTAP: Datacenter | Storage Efficiency | stat | [Data Reduction with FlexClones](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=189) |
| ONTAP: Datacenter | Storage Efficiency | timeseries | [Top $TopResources Physical Used with FlexClones by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=196) |
///



### aggr_physical_used_wo_snapshots_flexclones

Total Data Reduction Physical Used without snapshots and flexclones

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.efficiency_without_snapshots_flexclones.logical_used, space.efficiency_without_snapshots_flexclones.savings` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-efficiency-get-iter` | `aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-physical-used-wo-snapshots-flexclones` | conf/zapi/cdot/9.9.0/aggr_efficiency.yaml |

The `aggr_physical_used_wo_snapshots_flexclones` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Storage Efficiency Ratios | stat | [Data Reduction](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=45) |
| ONTAP: Aggregate | Storage Efficiency Ratios | timeseries | [Top $TopResources Aggregates by Physical Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=57) |
| ONTAP: Cluster | Storage Efficiency Ratios | stat | [Data Reduction](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=188) |
| ONTAP: Cluster | Storage Efficiency Ratios | timeseries | [Physical Used](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=192) |
| ONTAP: Datacenter | Storage Efficiency | stat | [Data Reduction](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=188) |
| ONTAP: Datacenter | Storage Efficiency | timeseries | [Top $TopResources Physical Used by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=192) |
///



### aggr_power

Power consumed by aggregate in Watts.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `aggr_power` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Highlights | timeseries | [Top $TopResources Aggregates by Power Consumed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=87) |
| ONTAP: Power | Highlights | timeseries | [Aggregates Power by Disk Type](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=89) |
| ONTAP: Power | Aggregate | table | [Aggregates](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=91) |
///



### aggr_primary_disk_count

Number of disks used in the aggregate. This includes parity disks, but excludes disks in the hybrid cache.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `block_storage.primary.disk_count` | conf/rest/9.12.0/aggr.yaml |



### aggr_raid_disk_count

Number of disks in the aggregate.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `block_storage.primary.disk_count, block_storage.hybrid_cache.disk_count` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-raid-attributes.disk-count` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_raid_disk_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | stat | [Disks](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=21) |
| ONTAP: Aggregate | Highlights | table | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=2) |
| ONTAP: Disk | Highlights | stat | [Total Disks by Aggregate(s)](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=43) |
| ONTAP: Disk | Highlights | table | [Disk Capacity Per Aggregate](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=53) |
| ONTAP: StorageGrid FabricPool | Highlights | stat | [Disks](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=21) |
| ONTAP: StorageGrid FabricPool | Highlights | table | [Aggregates](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=2) |
///



### aggr_raid_plex_count

Number of plexes in the aggregate

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `block_storage.plexes.#` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-raid-attributes.plex-count` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_raid_size

Option to specify the maximum number of disks that can be included in a RAID group.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `block_storage.primary.raid_size` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-raid-attributes.raid-size` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_read_data

Performance metric for read I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/aggr.yaml |



### aggr_read_latency

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> aggr_statistics.iops_raw.read | conf/keyperf/9.15.0/aggr.yaml |



### aggr_read_ops

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/aggr.yaml |



### aggr_snapshot_files_total

Total files allowed in snapshots

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `snapshot.files_total` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.files-total` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_files_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Snapshot Files](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=5) |
///



### aggr_snapshot_files_used

Total files created in snapshots

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `snapshot.files_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.files-used` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_files_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Snapshot Files](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=5) |
///



### aggr_snapshot_inode_used_percent

The percentage of disk space currently in use based on user-visible file (inode) count on the referenced file system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.percent-inode-used-capacity` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_inode_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Snapshot Inodes Used %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=30) |
///



### aggr_snapshot_maxfiles_available

Maximum files available for snapshots

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `snapshot.max_files_available` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.maxfiles-available` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_maxfiles_available` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Snapshot MaxFiles](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=34) |
///



### aggr_snapshot_maxfiles_possible

The largest value to which the maxfiles-available parameter can be increased by reconfiguration, on the referenced file system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `snapshot.max_files_available, snapshot.max_files_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.maxfiles-possible` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_maxfiles_possible` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Snapshot MaxFiles](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=34) |
///



### aggr_snapshot_maxfiles_used

Files in use by snapshots

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `snapshot.max_files_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.maxfiles-used` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_maxfiles_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Snapshot MaxFiles](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=34) |
///



### aggr_snapshot_reserve_percent

Percentage of space reserved for snapshots

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.snapshot.reserve_percent` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.snapshot-reserve-percent` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_reserve_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Space Reserved for Snapshots %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=31) |
///



### aggr_snapshot_size_available

Available space for snapshots in bytes

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.snapshot.available` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.size-available` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_size_available` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Space Used by Snapshots](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=33) |
///



### aggr_snapshot_size_total

Total space for snapshots in bytes

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.snapshot.total` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.size-total` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_snapshot_size_used

Space used by snapshots in bytes

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.snapshot.used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.size-used` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_size_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Space Used by Snapshots](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=33) |
///



### aggr_snapshot_used_percent

Percentage of disk space used by snapshots

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.snapshot.used_percent` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-snapshot-attributes.percent-used-capacity` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_snapshot_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Space Used by Snapshots %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=32) |
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Space Used by Snapshots %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=819) |
///



### aggr_space_available

Space available in bytes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.available` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.size-available` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_available` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | stat | [Available Space](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=25) |
| ONTAP: Aggregate | Highlights | table | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=2) |
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Space Available](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=14) |
| ONTAP: Cluster | Highlights | stat | [Available Space](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=278) |
| ONTAP: Datacenter | Highlights | stat | [Available Space](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=613) |
| ONTAP: Datacenter | Highlights | timeseries | [Top $TopResources Available Space by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=657) |
| ONTAP: StorageGrid FabricPool | Highlights | stat | [Available Space](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=25) |
| ONTAP: StorageGrid FabricPool | Highlights | timeseries | [Space Available](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=14) |
///



### aggr_space_capacity_tier_used

Used space in bytes in the cloud store. Only applicable for aggregates with a cloud store tier.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.cloud_storage.used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.capacity-tier-used` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_capacity_tier_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Capacity Tier Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=15) |
| ONTAP: Aggregate | FabricPool | timeseries | [Top $TopResources Aggregates by Capacity Tier Footprint](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=199) |
| ONTAP: StorageGrid FabricPool | Highlights | timeseries | [Capacity Tier Used](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=15) |
///



### aggr_space_data_compacted_count

Amount of compacted data in bytes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.data_compacted_count` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.data-compacted-count` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_space_data_compaction_saved

Space saved in bytes by compacting the data.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.data_compaction_space_saved` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.data-compaction-space-saved` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_data_compaction_saved` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Data Compaction space saved](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=16) |
///



### aggr_space_data_compaction_saved_percent

Percentage saved by compacting the data.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.data_compaction_space_saved_percent` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.data-compaction-space-saved-percent` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_space_performance_tier_inactive_user_data

The size that is physically used in the block storage and has a cold temperature, in bytes. This property is only supported if the aggregate is either attached to a cloud store or can be attached to a cloud store.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either block_storage.inactive_user_data or **.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.inactive_user_data` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.performance-tier-inactive-user-data` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_performance_tier_inactive_user_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Inactive Data](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=820) |
///



### aggr_space_performance_tier_inactive_user_data_percent

The percentage of inactive user data in the block storage. This property is only supported if the aggregate is either attached to a cloud store or can be attached to a cloud store.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either block_storage.inactive_user_data_percent or **.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.inactive_user_data_percent` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.performance-tier-inactive-user-data-percent` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_performance_tier_inactive_user_data_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Inactive Data %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=821) |
///



### aggr_space_performance_tier_used

A summation of volume footprints (including volume guarantees), in bytes. This includes all of the volume footprints in the block_storage tier and the cloud_storage tier.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.footprint` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-space-get-iter` | `volume-footprints` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_performance_tier_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | FabricPool | timeseries | [Top $TopResources Aggregates by Performance Tier Footprint](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=195) |
///



### aggr_space_performance_tier_used_percent

A summation of volume footprints inside the aggregate,as a percentage. A volume's footprint is the amount of space being used for the volume in the aggregate.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.footprint_percent` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-space-get-iter` | `volume-footprints-percent` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_performance_tier_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | FabricPool | timeseries | [Top $TopResources Aggregates by Performance Tier Footprint %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=197) |
///



### aggr_space_physical_used

Total physical used size of an aggregate in bytes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.physical_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.physical-used` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_physical_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Physical Space Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=818) |
| ONTAP: Aggregate | Storage Efficiency Ratios | timeseries | [Top $TopResources Aggregates by Physical Used with Snapshots & FlexClones](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=49) |
| ONTAP: Cluster | Storage Efficiency Ratios | timeseries | [Physical Used with Snapshots & FlexClones](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=194) |
| ONTAP: Datacenter | Storage Efficiency | timeseries | [Top $TopResources Physical Used with Snapshots & FlexClones by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=194) |
| ONTAP: StorageGrid FabricPool | Highlights | timeseries | [Physical Space Used](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=13) |
///



### aggr_space_physical_used_percent

Physical used percentage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.physical_used_percent` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.physical-used-percent` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_physical_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  Physical Space Used %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=18) |
///



### aggr_space_reserved

The total disk space in bytes that is reserved on the referenced file system. The reserved space is already counted in the used space, so this element can be used to see what portion of the used space represents space reserved for future use.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.total-reserved-space` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_space_sis_saved

Amount of space saved in bytes by storage efficiency.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.volume_deduplication_space_saved` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.sis-space-saved` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_sis_saved` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by  SIS space saved](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=17) |
///



### aggr_space_sis_saved_percent

Percentage of space saved by storage efficiency.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.volume_deduplication_space_saved_percent` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.sis-space-saved-percent` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_space_sis_shared_count

Amount of shared bytes counted by storage efficiency.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.volume_deduplication_shared_count` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.sis-shared-count` | conf/zapi/cdot/9.8.0/aggr.yaml |



### aggr_space_total

Total usable space in bytes, not including WAFL reserve and aggregate snapshot reserve.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.size` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.size-total` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | stat | [Total Space](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=24) |
| ONTAP: Aggregate | Highlights | stat | [Space Used %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=26) |
| ONTAP: Aggregate | Highlights | table | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=2) |
| ONTAP: Aggregate | Highlights | timeseries | [Top $TopResources Aggregates by Total Space](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=13) |
| ONTAP: cDOT | Capacity Metrics | table | [Top $TopResources Aggregates by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=243) |
| ONTAP: cDOT | Capacity Metrics | timeseries | [Top $TopResources Aggregates by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=267) |
| ONTAP: Cluster | Highlights | stat | [Total Space](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=277) |
| ONTAP: Cluster | Highlights | stat | [Space Used %](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=281) |
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | bargauge | [Capacity used](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=158) |
| ONTAP: Datacenter | Highlights | stat | [Total Space](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=576) |
| ONTAP: Datacenter | Highlights | stat | [Space Used %](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=615) |
| ONTAP: Datacenter | Highlights | timeseries | [Top $TopResources Total Space by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=655) |
| ONTAP: Datacenter | Highlights | timeseries | [Top $TopResources Space Used % by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=658) |
| ONTAP: Disk | Highlights | table | [Disk Capacity Per Aggregate](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=53) |
| ONTAP: StorageGrid FabricPool | Highlights | stat | [Total Space](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=24) |
| ONTAP: StorageGrid FabricPool | Highlights | stat | [Space Used %](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=26) |
| ONTAP: StorageGrid FabricPool | Highlights | table | [Aggregates](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=2) |
///



### aggr_space_used

Space used or reserved in bytes. Includes volume guarantees and aggregate metadata.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.size-used` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | stat | [Used and Reserved Space](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=811) |
| ONTAP: Aggregate | Highlights | stat | [Space Used %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=26) |
| ONTAP: Aggregate | Highlights | table | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=2) |
| ONTAP: cDOT | Capacity Metrics | table | [Top $TopResources Aggregates by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=243) |
| ONTAP: cDOT | Capacity Metrics | timeseries | [Top $TopResources Aggregates by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=267) |
| ONTAP: Cluster | Highlights | stat | [Used and Reserved Space](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=279) |
| ONTAP: Cluster | Highlights | stat | [Space Used %](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=281) |
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | bargauge | [Capacity used](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=158) |
| ONTAP: Datacenter | Highlights | stat | [Space Used %](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=615) |
| ONTAP: Datacenter | Highlights | stat | [Used and Reserved Space](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=611) |
| ONTAP: Datacenter | Highlights | timeseries | [Top $TopResources Used and Reserved Space by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=656) |
| ONTAP: Datacenter | Highlights | timeseries | [Top $TopResources Space Used % by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=658) |
| ONTAP: Datacenter | Power and Temperature | stat | [Average Power/Used_TB](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=640) |
| ONTAP: Power | Highlights | stat | [Average Power/Used_TB](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=71) |
| ONTAP: StorageGrid FabricPool | Highlights | stat | [Space Used %](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=26) |
| ONTAP: StorageGrid FabricPool | Highlights | table | [Aggregates](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=2) |
///



### aggr_space_used_percent

The percentage of disk space currently in use on the referenced file system

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.block_storage.used, space.block_storage.size` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-space-attributes.percent-used-capacity` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_space_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | table | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=2) |
| ONTAP: Cluster | Throughput | timeseries | [Average Aggregate Space Used](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=180) |
| ONTAP: Disk | Highlights | table | [Disk Capacity Per Aggregate](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=53) |
| ONTAP: StorageGrid FabricPool | Highlights | table | [Aggregates](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=2) |
///



### aggr_total_data

Performance metric aggregated over all types of I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/aggr.yaml |



### aggr_total_latency

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> aggr_statistics.iops_raw.total | conf/keyperf/9.15.0/aggr.yaml |



### aggr_total_logical_used

Logical used

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.efficiency.logical_used` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-efficiency-get-iter` | `aggr-efficiency-info.aggr-efficiency-cumulative-info.total-logical-used` | conf/zapi/cdot/9.9.0/aggr_efficiency.yaml |

The `aggr_total_logical_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Storage Efficiency Ratios | stat | [Data Reduction with Snapshots & FlexClones](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=41) |
| ONTAP: Aggregate | Storage Efficiency Ratios | timeseries | [Top $TopResources Aggregates by Logical Used with Snapshots & FlexClones](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=47) |
| ONTAP: Aggregate | Growth Rate | timeseries | [Top $TopResources Aggregates Per Growth Rate of Logical Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=831) |
| ONTAP: Aggregate | Growth Rate | table | [Top $TopResources Aggregates by Logical Usage: Delta Report](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=834) |
| ONTAP: Cluster | Storage Efficiency Ratios | stat | [Data Reduction with Snapshots &FlexClones](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=187) |
| ONTAP: Cluster | Storage Efficiency Ratios | timeseries | [Logical Used with Snapshots & FlexClones](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=191) |
| ONTAP: Datacenter | Storage Efficiency | stat | [Data Reduction with Snapshots & FlexClones](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=187) |
| ONTAP: Datacenter | Storage Efficiency | timeseries | [Top $TopResources Logical Used with Snapshots & FlexClones by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=191) |
///



### aggr_total_ops

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/aggr.yaml |



### aggr_total_physical_used

Total Physical Used

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `space.efficiency.logical_used, space.efficiency.savings` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-efficiency-get-iter` | `aggr-efficiency-info.aggr-efficiency-cumulative-info.total-physical-used` | conf/zapi/cdot/9.9.0/aggr_efficiency.yaml |

The `aggr_total_physical_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Storage Efficiency Ratios | stat | [Data Reduction with Snapshots & FlexClones](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=41) |
| ONTAP: Aggregate | Growth Rate | timeseries | [Top $TopResources Aggregates Per Growth Rate of Physical Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=829) |
| ONTAP: Aggregate | Growth Rate | table | [Top $TopResources Aggregates by Physical Usage: Delta Report](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=833) |
| ONTAP: Cluster | Storage Efficiency Ratios | stat | [Data Reduction with Snapshots &FlexClones](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=187) |
| ONTAP: Datacenter | Storage Efficiency | stat | [Data Reduction with Snapshots & FlexClones](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=187) |
///



### aggr_volume_count

The aggregate's volume count, which includes both FlexVols and FlexGroup constituents.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/aggregates` | `volume_count` | conf/rest/9.12.0/aggr.yaml |
| ZAPI | `aggr-get-iter` | `aggr-attributes.aggr-volume-count-attributes.flexvol-count` | conf/zapi/cdot/9.8.0/aggr.yaml |

The `aggr_volume_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | stat | [Volumes](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=22) |
| ONTAP: Aggregate | Highlights | table | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=2) |
| ONTAP: StorageGrid FabricPool | Highlights | stat | [Volumes](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=22) |
| ONTAP: StorageGrid FabricPool | Highlights | table | [Aggregates](/d/cdot-storagegrid-fabricpool/ontap3a-storagegrid fabricpool?orgId=1&viewPanel=2) |
///



### aggr_write_data

Performance metric for write I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/aggr.yaml |



### aggr_write_latency

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> aggr_statistics.iops_raw.write | conf/keyperf/9.15.0/aggr.yaml |



### aggr_write_ops

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/aggregates` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/aggr.yaml |



### audit_log

Captures the operations such as create, update, and delete attempts on volumes via REST or ONTAP CLI commands

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/audit_log.yaml |

The `audit_log` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: AuditLog | Highlights | table | [Volume Changes](/d/cdot-auditlog/ontap3a-auditlog?orgId=1&viewPanel=295) |
///



### availability_zone_space_available



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/availability-zones` | `space.available` | conf/rest/asar2/9.16.0/availability_zone.yaml |



### availability_zone_space_physical_used



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/availability-zones` | `space.physical_used` | conf/rest/asar2/9.16.0/availability_zone.yaml |



### availability_zone_space_physical_used_percent



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/availability-zones` | `space.physical_used_percent` | conf/rest/asar2/9.16.0/availability_zone.yaml |



### availability_zone_space_size



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/availability-zones` | `space.size` | conf/rest/asar2/9.16.0/availability_zone.yaml |



### change_log

Detect and track changes related to the creation, modification, and deletion of an object of Node, SVM and Volume

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/node.yaml |
| REST | `NA` | `Harvest generated` | conf/rest/9.10.0/svm.yaml |
| REST | `NA` | `Harvest generated` | conf/rest/9.14.0/volume.yaml |

The `change_log` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Changelog Monitor | Node Changes | stat | [Create](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=285) |
| ONTAP: Changelog Monitor | Node Changes | stat | [Update](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=291) |
| ONTAP: Changelog Monitor | Node Changes | stat | [Delete](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=286) |
| ONTAP: Changelog Monitor | Node Changes | table | [Node Changes ](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=288) |
| ONTAP: Changelog Monitor | SVM Changes | stat | [Create](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=292) |
| ONTAP: Changelog Monitor | SVM Changes | stat | [Update](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=298) |
| ONTAP: Changelog Monitor | SVM Changes | stat | [Delete](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=300) |
| ONTAP: Changelog Monitor | SVM Changes | table | [SVM Changes ](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=301) |
| ONTAP: Changelog Monitor | Volume Changes | stat | [Create](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=299) |
| ONTAP: Changelog Monitor | Volume Changes | stat | [Update](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=284) |
| ONTAP: Changelog Monitor | Volume Changes | stat | [Delete](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=293) |
| ONTAP: Changelog Monitor | Volume Changes | table | [Volume Changes ](/d/cdot-changelog-monitor/ontap3a-changelog monitor?orgId=1&viewPanel=295) |
///



### cifs_session_connection_count

A counter used to track requests that are sent to the volumes to the node.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/cifs/sessions` | `connection_count` | conf/rest/9.8.0/cifs_session.yaml |
| ZAPI | `cifs-session-get-iter` | `cifs-session.connection-count` | conf/zapi/cdot/9.8.0/cifs_session.yaml |

The `cifs_session_connection_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | Highlights | timeseries | [Top $TopResources Connection Count](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=100) |
| ONTAP: SMB | Highlights | timeseries | [Connection Count By SMB version](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=101) |
///



### cifs_session_idle_duration

Specifies an ISO-8601 format of date and time used to retrieve the idle time duration in hours, minutes, and seconds format.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/cifs/sessions` | `idle_duration` | conf/rest/9.8.0/cifs_session.yaml |

The `cifs_session_idle_duration` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | Highlights | table | [CIFS Sessions](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=98) |
///



### cifs_session_labels

This metric provides information about CIFSSession

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/cifs/sessions` | `Harvest generated` | conf/rest/9.8.0/cifs_session.yaml |
| ZAPI | `cifs-session-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/cifs_session.yaml |

The `cifs_session_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | Highlights | table | [CIFS Sessions](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=98) |
///



### cifs_share_labels

This metric provides information about CIFSShare

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/vserver/cifs/share` | `Harvest generated` | conf/rest/9.6.0/cifs_share.yaml |
| ZAPI | `cifs-share-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/cifs_share.yaml |



### cloud_target_labels

This metric provides information about CloudTarget

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cloud/targets` | `Harvest generated` | conf/rest/9.12.0/cloud_target.yaml |
| ZAPI | `aggr-object-store-config-get-iter` | `Harvest generated` | conf/zapi/cdot/9.10.0/aggr_object_store_config.yaml |



### cloud_target_used

The amount of cloud space used by all the aggregates attached to the target, in bytes. This field is only populated for FabricPool targets. The value is recalculated once every 5 minutes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cloud/targets` | `used` | conf/rest/9.12.0/cloud_target.yaml |
| ZAPI | `aggr-object-store-config-get-iter` | `aggr-object-store-config-info.used-space` | conf/zapi/cdot/9.10.0/aggr_object_store_config.yaml |



### cluster_new_status

It is an indicator of the overall health status of the cluster, with a value of 1 indicating a healthy status and a value of 0 indicating an unhealthy status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/status.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/status.yaml |

The `cluster_new_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | table | [$Cluster](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=52) |
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | stat | [cluster health status](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=156) |
| ONTAP: Datacenter | Health | table | [Cluster Health](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=480) |
| ONTAP: Datacenter | System Manager | table | [System Manager](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=664) |
///



### cluster_other_data

Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.throughput_raw.other`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cluster.yaml |



### cluster_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cluster_statistics.iops_raw.other | conf/keyperf/9.15.0/cluster.yaml |



### cluster_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cluster.yaml |



### cluster_peer_labels

This metric provides information about ClusterPeer

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cluster/peers` | `Harvest generated` | conf/rest/9.12.0/clusterpeer.yaml |
| ZAPI | `cluster-peer-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/clusterpeer.yaml |

The `cluster_peer_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Cluster Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=214) |
| ONTAP: Security | Highlights | piechart | [Cluster Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=215) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### cluster_peer_non_encrypted

This metric indicates a value of 1 if the cluster peer encryption state is none (indicating the connection is not encrypted) and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/clusterpeer.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/clusterpeer.yaml |

The `cluster_peer_non_encrypted` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### cluster_read_data

Performance metric for read I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cluster.yaml |



### cluster_read_latency

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cluster_statistics.iops_raw.read | conf/keyperf/9.15.0/cluster.yaml |



### cluster_read_ops

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cluster.yaml |



### cluster_schedule_labels

This metric provides information about ClusterSchedule

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cluster/schedules` | `Harvest generated` | conf/rest/9.6.0/clusterschedule.yaml |

The `cluster_schedule_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Local Policy | table | [Schedules](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=102) |
///



### cluster_software_status

Displays the software job with its status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/clustersoftware.yaml |

The `cluster_software_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Software | table | [Cluster Software Status](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=529) |
///



### cluster_software_update

Displays the software update phase with its status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/clustersoftware.yaml |

The `cluster_software_update` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Software | table | [Cluster Software Update](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=528) |
///



### cluster_software_validation

Displays the software pre-validation checks with their status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/clustersoftware.yaml |

The `cluster_software_validation` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Software | table | [Cluster Software Validation](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=530) |
///



### cluster_space_available

Available space across the cluster.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/cluster` | `block_storage.available` | conf/rest/asar2/9.16.0/cluster.yaml |

The `cluster_space_available` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Highlights | stat | [Available](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=4) |
///



### cluster_subsystem_new_status

This metric indicates a value of 1 if the subsystem health is ok (indicating the subsystem is operational) and a value of 0 for any other health status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/subsystem.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/subsystem.yaml |

The `cluster_subsystem_new_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | table | [subsystems](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=177) |
///



### cluster_subsystem_outstanding_alerts

Number of outstanding alerts

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/system/health/subsystem` | `outstanding_alert_count` | conf/rest/9.12.0/subsystem.yaml |
| ZAPI | `diagnosis-subsystem-config-get-iter` | `diagnosis-subsystem-config-info.outstanding-alert-count` | conf/zapi/cdot/9.8.0/subsystem.yaml |

The `cluster_subsystem_outstanding_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | table | [subsystems](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=177) |
///



### cluster_subsystem_suppressed_alerts

Number of suppressed alerts

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/system/health/subsystem` | `suppressed_alert_count` | conf/rest/9.12.0/subsystem.yaml |
| ZAPI | `diagnosis-subsystem-config-get-iter` | `diagnosis-subsystem-config-info.suppressed-alert-count` | conf/zapi/cdot/9.8.0/subsystem.yaml |

The `cluster_subsystem_suppressed_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | table | [subsystems](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=177) |
///



### cluster_tags

Displays tags at the cluster level.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/status.yaml |



### cluster_total_data

Performance metric aggregated over all types of I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cluster.yaml |



### cluster_total_latency

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cluster_statistics.iops_raw.total | conf/keyperf/9.15.0/cluster.yaml |



### cluster_total_ops

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cluster.yaml |



### cluster_write_data

Performance metric for write I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cluster.yaml |



### cluster_write_latency

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cluster_statistics.iops_raw.write | conf/keyperf/9.15.0/cluster.yaml |



### cluster_write_ops

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/cluster` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cluster.yaml |



### copy_manager_bce_copy_count_curr

Current number of copy requests being processed by the Block Copy Engine.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/copy_manager` | `block_copy_engine_current_copy_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/copy_manager.yaml |
| ZapiPerf | `perf-object-get-instances copy_manager` | `bce_copy_count_curr`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/copy_manager.yaml |



### copy_manager_kb_copied

Sum of kilo-bytes copied.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/copy_manager` | `KB_copied`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/copy_manager.yaml |
| ZapiPerf | `perf-object-get-instances copy_manager` | `KB_copied`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/copy_manager.yaml |

The `copy_manager_kb_copied` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Copy Offload | timeseries | [Copy Offload Data Copied](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=206) |
///



### copy_manager_ocs_copy_count_curr

Current number of copy requests being processed by the ONTAP copy subsystem.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/copy_manager` | `ontap_copy_subsystem_current_copy_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/copy_manager.yaml |
| ZapiPerf | `perf-object-get-instances copy_manager` | `ocs_copy_count_curr`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/copy_manager.yaml |



### copy_manager_sce_copy_count_curr

Current number of copy requests being processed by the System Continuous Engineering.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/copy_manager` | `system_continuous_engineering_current_copy_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/copy_manager.yaml |
| ZapiPerf | `perf-object-get-instances copy_manager` | `sce_copy_count_curr`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/copy_manager.yaml |



### copy_manager_spince_copy_count_curr

Current number of copy requests being processed by the SpinCE.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/copy_manager` | `spince_current_copy_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/copy_manager.yaml |
| ZapiPerf | `perf-object-get-instances copy_manager` | `spince_copy_count_curr`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/copy_manager.yaml |



### disk_busy

The utilization percent of the disk

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `disk_busy_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_bytes_per_sector

Bytes per sector.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/disks` | `bytes_per_sector` | conf/rest/9.12.0/disk.yaml |
| ZAPI | `storage-disk-get-iter` | `storage-disk-info.disk-inventory-info.bytes-per-sector` | conf/zapi/cdot/9.8.0/disk.yaml |



### disk_capacity

Disk capacity in MB

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_cp_read_chain

Average number of blocks transferred in each consistency point read operation during a CP

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_cp_read_latency

Average latency per block in microseconds for consistency point read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_cp_reads

Number of disk read operations initiated each second for consistency point processing

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_io_pending

Average number of I/Os issued to the disk for which we have not yet received the response

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_io_queued

Number of I/Os queued to the disk but not yet issued

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_labels

This metric provides information about Disk

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/disks` | `Harvest generated` | conf/rest/9.12.0/disk.yaml |
| ZAPI | `storage-disk-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/disk.yaml |

The `disk_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: Disk | Highlights | stat | [Total Disks](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=41) |
| ONTAP: Disk | Highlights | stat | [Failed Disks](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=55) |
| ONTAP: Disk | List of Disks | table | [Disks in Cluster](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=18) |
| ONTAP: Health | Disks | table | [Disks Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=248) |
///



### disk_new_status

This metric indicates a value of 1 if the disk is not in an outage (i.e., the outage label is empty) and a value of 0 if the shelf is in an outage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/disk.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/disk.yaml |



### disk_power_on_hours

Hours powered on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/disks` | `stats.power_on_hours` | conf/rest/9.12.0/disk.yaml |



### disk_sectors

Number of sectors on the disk.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/disks` | `sector_count` | conf/rest/9.12.0/disk.yaml |
| ZAPI | `storage-disk-get-iter` | `storage-disk-info.disk-inventory-info.capacity-sectors` | conf/zapi/cdot/9.8.0/disk.yaml |

The `disk_sectors` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | List of Disks | table | [Disks in Cluster](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=18) |
///



### disk_stats_average_latency

Average I/O latency across all active paths, in milliseconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/disks` | `stats.average_latency` | conf/rest/9.12.0/disk.yaml |
| ZAPI | `storage-disk-get-iter` | `storage-disk-info.disk-stats-info.average-latency` | conf/zapi/cdot/9.8.0/disk.yaml |

The `disk_stats_average_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | List of Disks | table | [Disks in Cluster](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=18) |
///



### disk_stats_io_kbps

Total Disk Throughput in KBPS Across All Active Paths

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/disk` | `disk_io_kbps_total` | conf/rest/9.12.0/disk.yaml |
| ZAPI | `storage-disk-get-iter` | `storage-disk-info.disk-stats-info.disk-io-kbps` | conf/zapi/cdot/9.8.0/disk.yaml |

The `disk_stats_io_kbps` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | List of Disks | table | [Disks in Cluster](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=18) |
///



### disk_stats_sectors_read

Number of Sectors Read

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/disk` | `sectors_read` | conf/rest/9.12.0/disk.yaml |
| ZAPI | `storage-disk-get-iter` | `storage-disk-info.disk-stats-info.sectors-read` | conf/zapi/cdot/9.8.0/disk.yaml |



### disk_stats_sectors_written

Number of Sectors Written

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/disk` | `sectors_written` | conf/rest/9.12.0/disk.yaml |
| ZAPI | `storage-disk-get-iter` | `storage-disk-info.disk-stats-info.sectors-written` | conf/zapi/cdot/9.8.0/disk.yaml |



### disk_total_data

Total throughput for user operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_total_transfers

Total number of disk operations involving data transfer initiated per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_transfer_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_transfers`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_uptime

Number of seconds the drive has been powered on

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/disks` | `stats.power_on_hours, 60, 60` | conf/rest/9.12.0/disk.yaml |
| ZAPI | `storage-disk-get-iter` | `storage-disk-info.disk-stats-info.power-on-time-interval` | conf/zapi/cdot/9.8.0/disk.yaml |

The `disk_uptime` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | List of Disks | table | [Disks in Cluster](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=18) |
///



### disk_usable_size

Usable size of each disk, in bytes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/disks` | `usable_size` | conf/rest/9.12.0/disk.yaml |



### disk_user_read_blocks

Number of blocks transferred for user read operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_user_read_chain

Average number of blocks transferred in each user read operation

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_user_read_latency

Average latency per block in microseconds for user read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_user_reads

Number of disk read operations initiated each second for retrieving data or metadata associated with user requests

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_user_write_blocks

Number of blocks transferred for user write operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_user_write_chain

Average number of blocks transferred in each user write operation

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_writes | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_user_write_latency

Average latency per block in microseconds for user write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### disk_user_writes

Number of disk write operations initiated each second for storing data or metadata associated with user requests

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_writes`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### ems_destination_labels

This metric provides information about EmsDestination

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/support/ems/destinations` | `Harvest generated` | conf/rest/9.12.0/ems_destination.yaml |
| ZAPI | `ems-event-notification-destination-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/ems_destination.yaml |

The `ems_destination_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### ems_events

Indicates EMS events that have occurred in the ONTAP as configured in the ems.yaml.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/support/ems/events` | `Harvest generated` | conf/ems/9.6.0/ems.yaml |

The `ems_events` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | table | [Active Emergency EMS](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=625) |
///



### environment_sensor_average_ambient_temperature

Average temperature of all ambient sensors for node in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_average_ambient_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### environment_sensor_average_fan_speed

Average fan speed for node in rpm.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_average_fan_speed` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### environment_sensor_average_temperature

Average temperature of all non-ambient sensors for node in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_average_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Highlights | timeseries | [Top $TopResources Nodes by Average Temperature](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=74) |
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### environment_sensor_max_fan_speed

Maximum fan speed for node in rpm.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_max_fan_speed` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Power and Temperature | stat | [Max Node Fan Speed](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=650) |
| ONTAP: Power | Highlights | stat | [Max Node Fan Speed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=97) |
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### environment_sensor_max_temperature

Maximum temperature of all non-ambient sensors for node in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_max_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Power and Temperature | stat | [Max Node Temp](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=644) |
| ONTAP: Power | Highlights | stat | [Max Node Temp](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=68) |
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### environment_sensor_min_ambient_temperature

Minimum temperature of all ambient sensors for node in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_min_ambient_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### environment_sensor_min_fan_speed

Minimum fan speed for node in rpm.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_min_fan_speed` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### environment_sensor_min_temperature

Minimum temperature of all non-ambient sensors for node in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_min_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### environment_sensor_power

Power consumed by a node in Watts.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_power` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Power and Temperature | stat | [Total Power](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=638) |
| ONTAP: Datacenter | Power and Temperature | stat | [Average Power/Used_TB](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=640) |
| ONTAP: Datacenter | Power and Temperature | stat | [Average IOPs/Watt](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=642) |
| ONTAP: Datacenter | Power and Temperature | timeseries | [Total Power Consumed](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=652) |
| ONTAP: Power | Highlights | stat | [Total Power](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=21) |
| ONTAP: Power | Highlights | stat | [Average Power/Used_TB](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=71) |
| ONTAP: Power | Highlights | stat | [Average IOPs/Watt](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=96) |
| ONTAP: Power | Highlights | timeseries | [Total Power Consumed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=82) |
| ONTAP: Power | Highlights | timeseries | [Average Power Consumption (kWh) Over Last Hour](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=102) |
| ONTAP: Power | Highlights | timeseries | [Top $TopResources Nodes by Power Consumed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=73) |
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### environment_sensor_status

This metric indicates a value of 1 if the sensor threshold state is normal (indicating the sensor is operating within normal parameters) and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/sensor.yaml |



### environment_sensor_threshold_value

Provides the sensor reading.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cluster/sensors` | `value` | conf/rest/9.12.0/sensor.yaml |
| ZAPI | `environment-sensors-get-iter` | `environment-sensors-info.threshold-sensor-value` | conf/zapi/cdot/9.8.0/sensor.yaml |

The `environment_sensor_threshold_value` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Errors](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=621) |
| ONTAP: Health | Highlights | stat | [Total Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=277) |
| ONTAP: Health | Highlights | piechart | [Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=268) |
| ONTAP: Health | Sensor | table | [Sensor Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=285) |
| ONTAP: Power | Sensor Problems | table | [Sensor Problems](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=86) |
///



### ethernet_switch_port_new_status

Represent the status of the ethernet switch port

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| Rest | `NA` | `Harvest generated` | conf/rest/9.8.0/ethernet_switch_port.yaml |

The `ethernet_switch_port_new_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Switch | Highlights | table | [Switch Details](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=5) |
| ONTAP: Switch | Interfaces | stat | [Down (Last 24h)](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=37) |
| ONTAP: Switch | Interfaces | table | [Down (Last 24h)](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=39) |
| ONTAP: Switch | Interfaces | timeseries | [Down (Last 24h)](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=40) |
///



### ethernet_switch_port_receive_discards

Total number of discarded packets.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/network/ethernet/switch/ports` | `receive_raw.discards`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/ethernet_switch_port.yaml |

The `ethernet_switch_port_receive_discards` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Switch | Traffic | timeseries | [Top $TopResources Interface Drops](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=31) |
///



### ethernet_switch_port_receive_errors

Number of packet errors.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/network/ethernet/switch/ports` | `receive_raw.errors`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/ethernet_switch_port.yaml |

The `ethernet_switch_port_receive_errors` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Switch | Traffic | timeseries | [Top $TopResources Interface Errors](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=32) |
///



### ethernet_switch_port_receive_packets

Total packet count.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/network/ethernet/switch/ports` | `receive_raw.packets`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/ethernet_switch_port.yaml |

The `ethernet_switch_port_receive_packets` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Switch | Traffic | timeseries | [Top $TopResources Interface Receive Packets](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=29) |
///



### ethernet_switch_port_transmit_discards

Total number of discarded packets.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/network/ethernet/switch/ports` | `transmit_raw.discards`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/ethernet_switch_port.yaml |

The `ethernet_switch_port_transmit_discards` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Switch | Traffic | timeseries | [Top $TopResources Interface Drops](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=31) |
///



### ethernet_switch_port_transmit_errors

Number of packet errors.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/network/ethernet/switch/ports` | `transmit_raw.errors`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/ethernet_switch_port.yaml |

The `ethernet_switch_port_transmit_errors` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Switch | Traffic | timeseries | [Top $TopResources Interface Errors](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=32) |
///



### ethernet_switch_port_transmit_packets

Total packet count.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/network/ethernet/switch/ports` | `transmit_raw.packets`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/ethernet_switch_port.yaml |

The `ethernet_switch_port_transmit_packets` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Switch | Traffic | timeseries | [Top $TopResources Interface Transmit Packets](/d/cdot-switch/ontap3a-switch?orgId=1&viewPanel=28) |
///



### export_rule_labels

This metric provides information about ExportRule

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/vserver/export-policy/rule` | `Harvest generated` | conf/rest/9.8.0/exports.yaml |



### external_service_op_num_not_found_responses

Number of &apos;Not Found&apos; responses for calls to this operation.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances external_service_op` | `num_not_found_responses`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/external_service_operation.yaml |

The `external_service_op_num_not_found_responses` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: External Service Operation | Highlights | timeseries | [Top $TopResources Number of 'Not Found' Responses Per Operation](/d/cdot-external-service-operation/ontap3a-external service operation?orgId=1&viewPanel=83) |
///



### external_service_op_num_request_failures

A cumulative count of all request failures.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances external_service_op` | `num_request_failures`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/external_service_operation.yaml |

The `external_service_op_num_request_failures` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: External Service Operation | Highlights | timeseries | [Top $TopResources Number of Request Failures](/d/cdot-external-service-operation/ontap3a-external service operation?orgId=1&viewPanel=85) |
///



### external_service_op_num_requests_sent

Number of requests sent to this service.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances external_service_op` | `num_requests_sent`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/external_service_operation.yaml |

The `external_service_op_num_requests_sent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: External Service Operation | Highlights | timeseries | [Top $TopResources Number of Request Sent](/d/cdot-external-service-operation/ontap3a-external service operation?orgId=1&viewPanel=87) |
///



### external_service_op_num_responses_received

Number of responses received from the server (does not include timeouts).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances external_service_op` | `num_responses_received`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/external_service_operation.yaml |

The `external_service_op_num_responses_received` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: External Service Operation | Highlights | timeseries | [Top $TopResources Number of Responses Received](/d/cdot-external-service-operation/ontap3a-external service operation?orgId=1&viewPanel=89) |
///



### external_service_op_num_successful_responses

Number of successful responses to this operation.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances external_service_op` | `num_successful_responses`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/external_service_operation.yaml |

The `external_service_op_num_successful_responses` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: External Service Operation | Highlights | timeseries | [Top $TopResources Number of Successful Responses](/d/cdot-external-service-operation/ontap3a-external service operation?orgId=1&viewPanel=91) |
///



### external_service_op_num_timeouts

Number of times requests to the server for this operation timed out, meaning no response was recevied in a given time period.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances external_service_op` | `num_timeouts`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/external_service_operation.yaml |

The `external_service_op_num_timeouts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: External Service Operation | Highlights | timeseries | [Top $TopResources Number of Timeouts](/d/cdot-external-service-operation/ontap3a-external service operation?orgId=1&viewPanel=93) |
///



### external_service_op_request_latency

Average latency in microseconds of requests for operations of this type on this server.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances external_service_op` | `request_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> num_requests_sent | conf/zapiperf/cdot/9.8.0/external_service_operation.yaml |

The `external_service_op_request_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: External Service Operation | Highlights | timeseries | [Top $TopResources Request Latency to Server](/d/cdot-external-service-operation/ontap3a-external service operation?orgId=1&viewPanel=76) |
///



### external_service_op_request_latency_hist

This histogram holds the latency values for requests of this operation to the specified server.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances external_service_op` | `request_latency_hist`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/external_service_operation.yaml |



### fabricpool_average_latency

This counter is deprecated.Average latencies executed during various phases of command execution. The execution-start latency represents the average time taken to start executing an operation. The request-prepare latency represent the average time taken to prepare the commplete request that needs to be sent to the server. The send latency represents the average time taken to send requests to the server. The execution-start-to-send-complete represents the average time taken to send an operation out since its execution started. The execution-start-to-first-byte-received represent the average time taken to receive the first byte of a response since the command's request execution started. These counters can be used to identify performance bottlenecks within the object store client module.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances object_store_client_op` | `average_latency`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml |



### fabricpool_cloud_bin_op_latency_average

Cloud bin operation latency average in milliseconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_comp_aggr_vol_bin` | `cloud_bin_op_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_comp_aggr_vol_bin.yaml |
| ZapiPerf | `perf-object-get-instances wafl_comp_aggr_vol_bin` | `cloud_bin_op_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_comp_aggr_vol_bin.yaml |

The `fabricpool_cloud_bin_op_latency_average` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Object Storage | timeseries | [Top $TopResources Volumes by Object Storage GET Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=93) |
| ONTAP: Volume | Object Storage | timeseries | [Top $TopResources Volumes by Object Storage PUT Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=95) |
///



### fabricpool_cloud_bin_operation

Cloud bin operation counters.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_comp_aggr_vol_bin` | `cloud_bin_op`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_comp_aggr_vol_bin.yaml |
| ZapiPerf | `perf-object-get-instances wafl_comp_aggr_vol_bin` | `cloud_bin_operation`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_comp_aggr_vol_bin.yaml |

The `fabricpool_cloud_bin_operation` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Object Storage | timeseries | [Top $TopResources Volumes by Object Storage GET Request Count](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=94) |
| ONTAP: Volume | Object Storage | timeseries | [Top $TopResources Volumes by Object Storage PUT Request Count](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=96) |
| ONTAP: Volume | Object Storage | table | [Top $TopResources Volumes by Object Storage Requests](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=90) |
///



### fabricpool_get_throughput_bytes

This counter is deprecated. Counter that indicates the throughput for GET command in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances object_store_client_op` | `get_throughput_bytes`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml |



### fabricpool_put_throughput_bytes

This counter is deprecated. Counter that indicates the throughput for PUT command in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances object_store_client_op` | `put_throughput_bytes`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml |



### fabricpool_stats

This counter is deprecated. Counter that indicates the number of object store operations sent, and their success and failure counts. The objstore_client_op_name array indicate the operation name such as PUT, GET, etc. The objstore_client_op_stats_name array contain the total number of operations, their success and failure counter for each operation.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances object_store_client_op` | `stats`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml |



### fabricpool_throughput_ops

Counter that indicates the throughput for commands in ops per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances object_store_client_op` | `throughput_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml |



### fcp_avg_other_latency

Average latency in microseconds for operations other than read and write

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `average_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `avg_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.8.0/fcp.yaml |



### fcp_avg_read_latency

Average latency in microseconds for read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `avg_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_avg_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [Top $TopResources FCPs by Send Latency](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=77) |
///



### fcp_avg_write_latency

Average latency in microseconds for write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `avg_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_avg_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [Top $TopResources FCPs by Receive Latency](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=78) |
///



### fcp_discarded_frames_count

Number of discarded frames.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `discarded_frames_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `discarded_frames_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_discarded_frames_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=76) |
///



### fcp_fabric_connected_speed

The negotiated data rate between the target FC port and the fabric in gigabits per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/network/fc/ports` | `fabric.connected_speed` | conf/rest/9.6.0/fcp.yaml |

The `fcp_fabric_connected_speed` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | table | [FC ports with Fabric detail](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=115) |
///



### fcp_int_count

Number of interrupts

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `interrupt_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `int_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_int_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission interrupts](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=75) |
///



### fcp_invalid_crc

Number of invalid cyclic redundancy checks (CRC count)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `invalid.crc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `invalid_crc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_invalid_crc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission interrupts](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=75) |
///



### fcp_invalid_transmission_word

Number of invalid transmission words

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `invalid.transmission_word`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `invalid_transmission_word`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_invalid_transmission_word` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission interrupts](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=75) |
///



### fcp_isr_count

Number of interrupt responses

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `isr.count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `isr_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_isr_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission interrupts](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=75) |
///



### fcp_labels

This metric provides information about FCP

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/network/fc/ports` | `Harvest generated` | conf/rest/9.6.0/fcp.yaml |

The `fcp_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: Network | FibreChannel | table | [FC ports with Fabric detail](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=115) |
///



### fcp_lif_avg_latency

Average latency in microseconds for FCP operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | FCP Frontend | stat | [FCP Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=84) |
| ONTAP: Node | FCP Frontend | timeseries | [FCP Average Latency by Port / LIF](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=72) |
| ONTAP: SVM | FCP | stat | [SVM FCP Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=111) |
| ONTAP: SVM | FCP | timeseries | [SVM FCP Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=120) |
///



### fcp_lif_avg_other_latency

Average latency in microseconds for operations other than read and write

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `average_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `avg_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_avg_other_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | FCP | timeseries | [SVM FCP Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=120) |
///



### fcp_lif_avg_read_latency

Average latency in microseconds for read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `avg_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_avg_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | FCP | stat | [SVM FCP Average Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=112) |
| ONTAP: SVM | FCP | timeseries | [SVM FCP Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=120) |
| ONTAP: SVM | NVMe/FC | stat | [SVM FCP Average Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=180) |
///



### fcp_lif_avg_write_latency

Average latency in microseconds for write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `avg_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_avg_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | FCP | stat | [SVM FCP Average Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=113) |
| ONTAP: SVM | FCP | timeseries | [SVM FCP Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=120) |
| ONTAP: SVM | NVMe/FC | stat | [SVM FCP Average Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=181) |
///



### fcp_lif_other_ops

Number of operations that are not read or write.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_other_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | FCP | timeseries | [SVM FCP IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=115) |
///



### fcp_lif_read_data

Amount of data read from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | FCP | stat | [SVM FCP Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=116) |
| ONTAP: SVM | FCP | timeseries | [SVM FCP Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=121) |
| ONTAP: SVM | FCP | timeseries | [Top $TopResources FCP LIFs by Send Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=91) |
| ONTAP: SVM | NVMe/FC | stat | [SVM FCP Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=182) |
///



### fcp_lif_read_ops

Number of read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | FCP | stat | [SVM FCP Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=118) |
| ONTAP: SVM | FCP | timeseries | [SVM FCP IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=115) |
| ONTAP: SVM | NVMe/FC | stat | [SVM FCP Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=184) |
///



### fcp_lif_total_ops

Total number of operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | FCP Frontend | stat | [FCP IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=69) |
| ONTAP: Node | FCP Frontend | timeseries | [FCP IOPs by Port / LIF](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=74) |
| ONTAP: SVM | FCP | stat | [SVM FCP IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=122) |
///



### fcp_lif_write_data

Amount of data written to the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | FCP Frontend | timeseries | [FCP Throughput by Port / LIF](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=73) |
| ONTAP: SVM | FCP | stat | [SVM FCP Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=114) |
| ONTAP: SVM | FCP | stat | [SVM FCP Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=117) |
| ONTAP: SVM | FCP | timeseries | [SVM FCP Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=121) |
| ONTAP: SVM | FCP | timeseries | [Top $TopResources FCP LIFs by Receive Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=92) |
| ONTAP: SVM | NVMe/FC | stat | [SVM FCP Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=183) |
///



### fcp_lif_write_ops

Number of write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp_lif` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp_lif.yaml |
| ZapiPerf | `perf-object-get-instances fcp_lif` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp_lif.yaml |

The `fcp_lif_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | FCP | stat | [SVM FCP Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=119) |
| ONTAP: SVM | FCP | timeseries | [SVM FCP IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=115) |
| ONTAP: SVM | NVMe/FC | stat | [SVM FCP Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=185) |
///



### fcp_link_down

Number of times the Fibre Channel link was lost

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `link.down`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `link_down`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_link_down` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [Top $TopResources FCPs by Link Down](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=73) |
///



### fcp_link_failure

Number of link failures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `link_failure`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `link_failure`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_link_failure` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [Top $TopResources FCPs by Link Failure](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=74) |
///



### fcp_link_up

Number of times the Fibre Channel link was established

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `link.up`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `link_up`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_loss_of_signal

Number of times this port lost signal

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `loss_of_signal`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `loss_of_signal`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_loss_of_signal` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=76) |
///



### fcp_loss_of_sync

Number of times this port lost sync

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `loss_of_sync`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `loss_of_sync`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_loss_of_sync` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=76) |
///



### fcp_max_speed

The maximum speed supported by the FC port in gigabits per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/network/fc/ports` | `speed.maximum` | conf/rest/9.6.0/fcp.yaml |

The `fcp_max_speed` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | table | [FC ports with Fabric detail](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=115) |
///



### fcp_nvmf_avg_other_latency

Average latency in microseconds for operations other than read and write (FC-NVMe)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.average_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf.other_ops | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_avg_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf_other_ops | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_avg_read_latency

Average latency in microseconds for read operations (FC-NVMe)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf.read_ops | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_avg_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf_read_ops | conf/zapiperf/cdot/9.10.1/fcp.yaml |

The `fcp_nvmf_avg_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | NVMe/FC | timeseries | [Top $TopResources FCP_NVMFs by Send Latency](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=104) |
///



### fcp_nvmf_avg_remote_other_latency

Average latency in microseconds for remote operations other than read and write (FC-NVMe)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.average_remote_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf_remote.other_ops | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_avg_remote_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf_remote_other_ops | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_avg_remote_read_latency

Average latency in microseconds for remote read operations (FC-NVMe)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.average_remote_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf_remote.read_ops | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_avg_remote_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf_remote_read_ops | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_avg_remote_write_latency

Average latency in microseconds for remote write operations (FC-NVMe)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.average_remote_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf_remote.write_ops | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_avg_remote_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf_remote_write_ops | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_avg_write_latency

Average latency in microseconds for write operations (FC-NVMe)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf.write_ops | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_avg_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nvmf_write_ops | conf/zapiperf/cdot/9.10.1/fcp.yaml |

The `fcp_nvmf_avg_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | NVMe/FC | timeseries | [Top $TopResources FCP_NVMFs by Receive Latency](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=106) |
///



### fcp_nvmf_caw_data

Amount of CAW data sent to the storage system (FC-NVMe) in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.caw_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_caw_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_caw_ops

Number of FC-NVMe CAW operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.caw_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_caw_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_command_slots

Number of command slots that have been used by initiators logging into this port. This shows the command fan-in on the port.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.command_slots`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_command_slots`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_other_ops

Number of NVMF operations that are not read or write.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_read_data

Amount of data read from the storage system (FC-NVMe) in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |

The `fcp_nvmf_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Highlights | stat | [FC Read Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=92) |
| ONTAP: Network | NVMe/FC | table | [NVMe/FC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=98) |
| ONTAP: Network | NVMe/FC | timeseries | [Top $TopResources FCP_NVMFs by Send Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=100) |
| ONTAP: Node | Network Layer | timeseries | [Top $TopResources FC Ports by Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=112) |
///



### fcp_nvmf_read_ops

Number of FC-NVMe read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |

The `fcp_nvmf_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Highlights | stat | [FC Read Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=92) |
///



### fcp_nvmf_remote_caw_data

Amount of remote CAW data sent to the storage system (FC-NVMe) in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf_remote.caw_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_remote_caw_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_remote_caw_ops

Number of FC-NVMe remote CAW operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf_remote.caw_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_remote_caw_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_remote_other_ops

Number of NVMF remote operations that are not read or write.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf_remote.other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_remote_other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_remote_read_data

Amount of remote data read from the storage system (FC-NVMe) in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf_remote.read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_remote_read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_remote_read_ops

Number of FC-NVMe remote read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf_remote.read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_remote_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_remote_total_data

Amount of remote FC-NVMe traffic to and from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf_remote.total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_remote_total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_remote_total_ops

Total number of remote FC-NVMe operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf_remote.total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_remote_total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_remote_write_data

Amount of remote data written to the storage system (FC-NVMe) in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf_remote.write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_remote_write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_remote_write_ops

Number of FC-NVMe remote write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf_remote.write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_remote_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |



### fcp_nvmf_total_data

Amount of FC-NVMe traffic to and from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |

The `fcp_nvmf_total_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Highlights | stat | [FC Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=91) |
///



### fcp_nvmf_total_ops

Total number of FC-NVMe operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |

The `fcp_nvmf_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Highlights | stat | [FC Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=91) |
///



### fcp_nvmf_write_data

Amount of data written to the storage system (FC-NVMe) in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |

The `fcp_nvmf_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Highlights | stat | [FC Write Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=93) |
| ONTAP: Network | NVMe/FC | table | [NVMe/FC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=98) |
| ONTAP: Network | NVMe/FC | timeseries | [Top $TopResources FCP_NVMFs by Receive Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=102) |
| ONTAP: Node | Network Layer | timeseries | [Top $TopResources FC Ports by Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=112) |
///



### fcp_nvmf_write_ops

Number of FC-NVMe write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `nvmf.write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `nvmf_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/fcp.yaml |

The `fcp_nvmf_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Highlights | stat | [FC Write Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=93) |
///



### fcp_other_ops

Number of operations that are not read or write.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |



### fcp_prim_seq_err

Number of primitive sequence errors

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `primitive_seq_err`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `prim_seq_err`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_prim_seq_err` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=76) |
///



### fcp_queue_full

Number of times a queue full condition occurred.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `queue_full`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `queue_full`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_queue_full` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=76) |
///



### fcp_read_data

Amount of data read from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | table | [FC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=71) |
| ONTAP: Network | FibreChannel | timeseries | [Top $TopResources FCPs by Send Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=67) |
| ONTAP: Node | Network Layer | timeseries | [Top $TopResources FC Ports by Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=112) |
///



### fcp_read_ops

Number of read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |



### fcp_reset_count

Number of physical port resets

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `reset_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `reset_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |



### fcp_shared_int_count

Number of shared interrupts

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `shared_interrupt_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `shared_int_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |



### fcp_spurious_int_count

Number of spurious interrupts

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `spurious_interrupt_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `spurious_int_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_spurious_int_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission interrupts](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=75) |
///



### fcp_threshold_full

Number of times the total number of outstanding commands on the port exceeds the threshold supported by this port.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `threshold_full`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `threshold_full`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_threshold_full` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | timeseries | [FCPs Transmission errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=76) |
///



### fcp_total_data

Amount of FCP traffic to and from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |



### fcp_total_ops

Total number of FCP operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |



### fcp_util_percent

Represent the FCP utilization percentage

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |

The `fcp_util_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | table | [FC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=71) |
| ONTAP: Node | Network Layer | timeseries | [Top $TopResources FC Ports by Utilization %](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=110) |
///



### fcp_write_data

Amount of data written to the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |

The `fcp_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | FibreChannel | table | [FC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=71) |
| ONTAP: Network | FibreChannel | timeseries | [Top $TopResources FCPs by Receive Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=69) |
| ONTAP: Node | Network Layer | timeseries | [Top $TopResources FC Ports by Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=112) |
///



### fcp_write_ops

Number of write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcp` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcp.yaml |
| ZapiPerf | `perf-object-get-instances fcp_port` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcp.yaml |



### fcvi_firmware_invalid_crc_count

Firmware reported invalid CRC count

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `firmware.invalid_crc_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `fw_invalid_crc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_firmware_invalid_crc_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Invalid CRC Count](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=116) |
///



### fcvi_firmware_invalid_transmit_word_count

Firmware reported invalid transmit word count

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `firmware.invalid_transmit_word_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `fw_invalid_xmit_words`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_firmware_invalid_transmit_word_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Invalid Transmit Word Count](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=113) |
///



### fcvi_firmware_link_failure_count

Firmware reported link failure count

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `firmware.link_failure_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `fw_link_failure`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_firmware_link_failure_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Link Failure Count](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=114) |
///



### fcvi_firmware_loss_of_signal_count

Firmware reported loss of signal count

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `firmware.loss_of_signal_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `fw_loss_of_signal`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_firmware_loss_of_signal_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Loss of Signal Count](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=110) |
///



### fcvi_firmware_loss_of_sync_count

Firmware reported loss of sync count

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `firmware.loss_of_sync_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `fw_loss_of_sync`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_firmware_loss_of_sync_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Loss of Sync Count](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=117) |
///



### fcvi_firmware_systat_discard_frames

Firmware reported SyStatDiscardFrames value

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `firmware.systat.discard_frames`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `fw_SyStatDiscardFrames`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_firmware_systat_discard_frames` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [SyStatDiscardFrames Value](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=115) |
///



### fcvi_hard_reset_count

Number of times hard reset of FCVI adapter got issued.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `hard_reset_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `hard_reset_cnt`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_hard_reset_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Hard Reset Count](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=111) |
///



### fcvi_rdma_write_avg_latency

Average RDMA write I/O latency in microseconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `rdma.write_average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rdma.write_ops | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `rdma_write_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rdma_write_ops | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_rdma_write_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | Highlights | stat | [FCVI Write Latency](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=25) |
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Write Latency](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=62) |
///



### fcvi_rdma_write_ops

Number of RDMA write I/Os issued per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `rdma.write_ops`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `rdma_write_ops`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_rdma_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | Highlights | stat | [FCVI Write IOPs](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=22) |
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Write IOPs](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=64) |
///



### fcvi_rdma_write_throughput

RDMA write throughput in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `rdma.write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `rdma_write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_rdma_write_throughput` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | Highlights | stat | [FCVI Write Throughput](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=33) |
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Write Throughput](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=63) |
///



### fcvi_soft_reset_count

Number of times soft reset of FCVI adapter got issued.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/fcvi` | `soft_reset_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/fcvi.yaml |
| ZapiPerf | `perf-object-get-instances fcvi` | `soft_reset_cnt`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fcvi.yaml |

The `fcvi_soft_reset_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FCVI | timeseries | [Soft  Reset Count](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=112) |
///



### flashcache_accesses

External cache accesses per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `accesses`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `accesses`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_disk_reads_replaced

Estimated number of disk reads per second replaced by cache

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `disk_reads_replaced`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `disk_reads_replaced`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |

The `flashcache_disk_reads_replaced` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Disk Utilization | timeseries | [Flash Cache](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=36) |
///



### flashcache_evicts

Number of blocks evicted from the external cache to make room for new blocks

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `evicts`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `evicts`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_hit

Number of WAFL buffers served off the external cache

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `hit.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `hit`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_hit_directory

Number of directory buffers served off the external cache

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `hit.directory`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `hit_directory`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_hit_indirect

Number of indirect file buffers served off the external cache

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `hit.indirect`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `hit_indirect`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_hit_metadata_file

Number of metadata file buffers served off the external cache

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `hit.metadata_file`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `hit_metadata_file`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_hit_normal_lev0

Number of normal level 0 WAFL buffers served off the external cache

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `hit.normal_level_zero`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `hit_normal_lev0`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_hit_percent

External cache hit rate

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `hit.percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> average<br><span class="key">Base:</span> accesses | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `hit_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> accesses | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |

The `flashcache_hit_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Disk Utilization | timeseries | [Flash Cache](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=36) |
///



### flashcache_inserts

Number of WAFL buffers inserted into the external cache

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `inserts`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `inserts`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_invalidates

Number of blocks invalidated in the external cache

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `invalidates`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `invalidates`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_miss

External cache misses

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `miss.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `miss`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_miss_directory

External cache misses accessing directory buffers

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `miss.directory`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `miss_directory`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_miss_indirect

External cache misses accessing indirect file buffers

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `miss.indirect`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `miss_indirect`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_miss_metadata_file

External cache misses accessing metadata file buffers

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `miss.metadata_file`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `miss_metadata_file`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_miss_normal_lev0

External cache misses accessing normal level 0 buffers

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `miss.normal_level_zero`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `miss_normal_lev0`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashcache_usage

Percentage of blocks in external cache currently containing valid data

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/external_cache` | `usage`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/ext_cache_obj.yaml |
| ZapiPerf | `perf-object-get-instances ext_cache_obj` | `usage`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml |



### flashpool_cache_stats

Automated Working-set Analyzer (AWA) per-interval pseudo cache statistics for the most recent intervals. The number of intervals defined as recent is CM_WAFL_HYAS_INT_DIS_CNT. This array is a table with fields corresponding to the enum type of hyas_cache_stat_type_t.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_sizer` | `cache_stats`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_sizer.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_sizer` | `cache_stats`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_sizer.yaml |



### flashpool_evict_destage_rate

Number of block destage per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `evict_destage_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `evict_destage_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_evict_destage_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by Cache Removals](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=69) |
///



### flashpool_evict_remove_rate

Number of block free per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `evict_remove_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `evict_remove_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_evict_remove_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by Cache Removals](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=69) |
///



### flashpool_hya_read_hit_latency_average

Average of RAID I/O latency on read hit.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `hya_read_hit_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> hya_read_hit_latency_count | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `hya_read_hit_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> hya_read_hit_latency_count | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_hya_read_hit_latency_average` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by SSD and HDD Latency](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=70) |
///



### flashpool_hya_read_miss_latency_average

Average read miss latency.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `hya_read_miss_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> hya_read_miss_latency_count | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `hya_read_miss_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> hya_read_miss_latency_count | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_hya_read_miss_latency_average` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by SSD and HDD Latency](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=70) |
///



### flashpool_hya_write_hdd_latency_average

Average write latency to HDD.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `hya_write_hdd_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> hya_write_hdd_latency_count | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `hya_write_hdd_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> hya_write_hdd_latency_count | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |



### flashpool_hya_write_ssd_latency_average

Average of RAID I/O latency on write to SSD.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `hya_write_ssd_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> hya_write_ssd_latency_count | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `hya_write_ssd_latency_average`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> hya_write_ssd_latency_count | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |



### flashpool_read_cache_ins_rate

Cache insert rate blocks/sec.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `read_cache_insert_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `read_cache_ins_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_read_cache_ins_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by Cache Inserts](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=71) |
///



### flashpool_read_ops_replaced

Number of HDD read operations replaced by SSD reads per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `read_ops_replaced`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `read_ops_replaced`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_read_ops_replaced` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by Flash Pool Activity](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=67) |
| ONTAP: Disk | Disk Utilization | timeseries | [Flash Pool](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=38) |
///



### flashpool_read_ops_replaced_percent

Percentage of HDD read operations replace by SSD.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `read_ops_replaced_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_ops_total | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `read_ops_replaced_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_ops_total | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_read_ops_replaced_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by Flash Pool Activity](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=67) |
| ONTAP: Disk | Disk Utilization | timeseries | [Flash Pool](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=38) |
///



### flashpool_ssd_available

Total SSD blocks available.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `ssd_available`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `ssd_available`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |



### flashpool_ssd_read_cached

Total read cached SSD blocks.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `ssd_read_cached`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `ssd_read_cached`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_ssd_read_cached` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by Flash Pool Capacity Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=68) |
///



### flashpool_ssd_total

Total SSD blocks.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `ssd_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `ssd_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_ssd_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by Flash Pool Capacity Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=68) |
///



### flashpool_ssd_total_used

Total SSD blocks used.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `ssd_total_used`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `ssd_total_used`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |



### flashpool_ssd_write_cached

Total write cached SSD blocks.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `ssd_write_cached`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `ssd_write_cached`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_ssd_write_cached` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by Flash Pool Capacity Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=68) |
///



### flashpool_wc_write_blks_total

Number of write-cache blocks written per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `wc_write_blocks_total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `wc_write_blks_total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |

The `flashpool_wc_write_blks_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Flash Pool | timeseries | [Top $TopResources Aggregates by Cache Inserts](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=71) |
///



### flashpool_write_blks_replaced

Number of HDD write blocks replaced by SSD writes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `write_blocks_replaced`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `write_blks_replaced`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |



### flashpool_write_blks_replaced_percent

Percentage of blocks overwritten to write-cache among all disk writes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl_hya_per_aggregate` | `write_blocks_replaced_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> average<br><span class="key">Base:</span> estimated_write_blocks_total | conf/restperf/9.12.0/wafl_hya_per_aggr.yaml |
| ZapiPerf | `perf-object-get-instances wafl_hya_per_aggr` | `write_blks_replaced_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> average<br><span class="key">Base:</span> est_write_blks_total | conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml |



### flexcache_blocks_requested_from_client

Total blocks requested by the client.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.flexcache_raw.client_requested_blocks`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/flexcache.yaml |
| StatPerf | `flexcache_per_volume` | `blocks_requested_from_client`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `blocks_requested_from_client`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_blocks_requested_from_client` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Highlights | timeseries | [Top $TopResources Blocks requested from Client](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=109) |
///



### flexcache_blocks_retrieved_from_origin

Blocks retrieved from origin in case of a cache miss. This can be divided by the raw client_requested_blocks and multiplied by 100 to calculate the cache miss percentage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.flexcache_raw.cache_miss_blocks`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/flexcache.yaml |
| StatPerf | `flexcache_per_volume` | `blocks_retrieved_from_origin`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `blocks_retrieved_from_origin`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_blocks_retrieved_from_origin` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Highlights | timeseries | [Top $TopResources Blocks requested from Origin](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=110) |
///



### flexcache_evict_rw_cache_skipped_reason_disconnected

Total number of read-write cache evict operations skipped because cache is disconnected.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `evict_rw_cache_skipped_reason_disconnected`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `evict_rw_cache_skipped_reason_disconnected`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_evict_rw_cache_skipped_reason_disconnected` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Evict | timeseries | [Top $TopResources Read-Write Cache Evictions Skipped Due to Cache Disconnection](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=88) |
///



### flexcache_evict_skipped_reason_config_noent

Total number of evict operation is skipped because cache config is not available.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `evict_skipped_reason_config_noent`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `evict_skipped_reason_config_noent`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_evict_skipped_reason_config_noent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Evict | timeseries | [Top $TopResources Evictions Skipped Due to Configuration Issues](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=92) |
///



### flexcache_evict_skipped_reason_disconnected

Total number of evict operation is skipped because cache is disconnected.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `evict_skipped_reason_disconnected`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `evict_skipped_reason_disconnected`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_evict_skipped_reason_disconnected` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Evict | timeseries | [Top $TopResources Evictions Skipped Due to Cache Disconnection](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=93) |
///



### flexcache_evict_skipped_reason_offline

Total number of evict operation is skipped because cache volume is offline.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `evict_skipped_reason_offline`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `evict_skipped_reason_offline`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_evict_skipped_reason_offline` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Evict | timeseries | [Top $TopResources Evictions Skipped When Cache is Offline](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=94) |
| ONTAP: FlexCache | Invalidate | timeseries | [Top $TopResources Invalidate Operations Skipped When Cache Volume is Offline](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=95) |
///



### flexcache_invalidate_skipped_reason_config_noent

Total number of invalidate operation is skipped because cache config is not available.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `invalidate_skipped_reason_config_noent`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `invalidate_skipped_reason_config_noent`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_invalidate_skipped_reason_config_noent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Invalidate | timeseries | [Top $TopResources Invalidate Operations Skipped Due to Unavailable Cache Configuration](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=86) |
///



### flexcache_invalidate_skipped_reason_disconnected

Total number of invalidate operation is skipped because cache is disconnected.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `invalidate_skipped_reason_disconnected`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `invalidate_skipped_reason_disconnected`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_invalidate_skipped_reason_disconnected` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Invalidate | timeseries | [Top $TopResources Invalidate Operations Skipped Due to Cache Disconnection](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=87) |
///



### flexcache_invalidate_skipped_reason_offline

Total number of invalidate operation is skipped because cache volume is offline.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `invalidate_skipped_reason_offline`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `invalidate_skipped_reason_offline`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |



### flexcache_miss_percent

This metric represents the percentage of block requests from a client that resulted in a "miss" in the FlexCache. A "miss" occurs when the requested data is not found in the cache and has to be retrieved from the origin volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `blocks_retrieved_from_origin, blocks_requested_from_client`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/keyperf/9.15.0/flexcache.yaml |
| StatPerf | `flexcache_per_volume` | `blocks_retrieved_from_origin, blocks_requested_from_client`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `flexcache_per_volume` | `blocks_retrieved_from_origin, blocks_requested_from_client`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_miss_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Highlights | timeseries | [Top $TopResources Cache Miss percent](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=99) |
///



### flexcache_nix_retry_skipped_reason_initiator_retrieve

Total retry nix operations skipped because the initiator is retrieve operation.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `nix_retry_skipped_reason_initiator_retrieve`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `nix_retry_skipped_reason_initiator_retrieve`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_nix_retry_skipped_reason_initiator_retrieve` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Nix | timeseries | [Top $TopResources Retry Nix Operations Skipped Due to Retrieve Operation Initiator](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=79) |
///



### flexcache_nix_skipped_reason_config_noent

Total number of nix operation is skipped because cache config is not available.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `nix_skipped_reason_config_noent`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `nix_skipped_reason_config_noent`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_nix_skipped_reason_config_noent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Nix | timeseries | [Top $TopResources Nix Operations Skipped Due to Unavailable Cache Configuration](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=80) |
///



### flexcache_nix_skipped_reason_disconnected

Total number of nix operation is skipped because cache is disconnected.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `nix_skipped_reason_disconnected`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `nix_skipped_reason_disconnected`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_nix_skipped_reason_disconnected` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Nix | timeseries | [Top $TopResources Nix Operations Skipped Due to Cache Disconnection](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=81) |
///



### flexcache_nix_skipped_reason_in_progress

Total nix operations skipped because of an in-progress nix.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `nix_skipped_reason_in_progress`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `nix_skipped_reason_in_progress`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_nix_skipped_reason_in_progress` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Nix | timeseries | [Top $TopResources Nix Operations Skipped Due to In-Progress Nix Operation](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=62) |
///



### flexcache_nix_skipped_reason_offline

Total number of nix operation is skipped because cache volume is offline.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `nix_skipped_reason_offline`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `nix_skipped_reason_offline`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_nix_skipped_reason_offline` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Nix | timeseries | [Top $TopResources Nix Operations Skipped When Cache Volume is Offline](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=82) |
///



### flexcache_reconciled_data_entries

Total number of reconciled data entries at cache side.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `reconciled_data_entries`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `reconciled_data_entries`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_reconciled_data_entries` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Reconcile Metrics | timeseries | [Top $TopResources Reconciled data entries](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=100) |
///



### flexcache_reconciled_lock_entries

Total number of reconciled lock entries at cache side.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| StatPerf | `flexcache_per_volume` | `reconciled_lock_entries`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/flexcache.yaml |
| ZapiPerf | `perf-object-get-instances flexcache_per_volume` | `reconciled_lock_entries`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/flexcache.yaml |

The `flexcache_reconciled_lock_entries` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Reconcile Metrics | timeseries | [Top $TopResources Reconciled Lock Entries](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=101) |
///



### flexcache_size

Physical size of the volume, in bytes. The minimum size for a FlexVol volume is 20MB and the minimum size for a FlexGroup volume is 200MB per constituent. The recommended size for a FlexGroup volume is a minimum of 100GB per constituent. For all volumes, the default size is equal to the minimum size.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/flexcache/flexcaches` | `size` | conf/rest/9.12.0/flexcache.yaml |
| ZAPI | `flexcache-get-iter` | `flexcache-info.size` | conf/zapi/cdot/9.8.0/flexcache.yaml |

The `flexcache_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexCache | Highlights | table | [FlexCache Details](/d/cdot-flexcache/ontap3a-flexcache?orgId=1&viewPanel=112) |
///



### fpolicy_aborted_requests

Number of screen requests aborted

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_policy` | `aborted_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy.yaml |

The `fpolicy_aborted_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Highlights | timeseries | [Top $TopResources Policy by Aborted Requests](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=588) |
///



### fpolicy_denied_requests

Number of screen requests for which deny is received from fpolicy server

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_policy` | `denied_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy.yaml |

The `fpolicy_denied_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Highlights | timeseries | [Top $TopResources Policy by Denied Requests](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=589) |
///



### fpolicy_io_processing_latency

Average IO processing latency in microseconds for screen request

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_policy` | `io_processing_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> io_processing_latency_base | conf/zapiperf/cdot/9.8.0/fpolicy.yaml |

The `fpolicy_io_processing_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Highlights | timeseries | [Top $TopResources Policy by IO Processing Latency](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=442) |
///



### fpolicy_io_thread_wait_latency

Average IO thread wait latency in microseconds for the screen request

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_policy` | `io_thread_wait_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> io_thread_wait_latency_base | conf/zapiperf/cdot/9.8.0/fpolicy.yaml |

The `fpolicy_io_thread_wait_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Highlights | timeseries | [Top $TopResources Policy by IO Thread Wait Latency](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=586) |
///



### fpolicy_processed_requests

Number of screen requests went through fpolicy processing

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_policy` | `processed_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy.yaml |

The `fpolicy_processed_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Highlights | timeseries | [Top $TopResources Policy by Processed Requests](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=590) |
///



### fpolicy_processing_latency

Average policy processing latency in microseconds for screen request

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_policy` | `policy_processing_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> policy_processing_latency_base | conf/zapiperf/cdot/9.8.0/fpolicy.yaml |

The `fpolicy_processing_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Highlights | timeseries | [Top $TopResources Policy by Processing Latency](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=587) |
///



### fpolicy_server_cancelled_requests

Number of screen requests whose processing was cancelled (cancel timeout)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_server` | `cancelled_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml |

The `fpolicy_server_cancelled_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Server | timeseries | [Top $TopResources Servers by Cancelled Requests](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=606) |
///



### fpolicy_server_failed_requests

Number of screen requests the node failed to send to fpolicy server

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_server` | `failed_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml |

The `fpolicy_server_failed_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Server | timeseries | [Top $TopResources Servers by Failed Requests](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=607) |
///



### fpolicy_server_max_request_latency

Maximum latency in microseconds for a screen request

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_server` | `max_request_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml |

The `fpolicy_server_max_request_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Server | timeseries | [Top $TopResources Servers by Max Request Latency](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=603) |
///



### fpolicy_server_outstanding_requests

Total number of screen requests waiting for response

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_server` | `outstanding_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml |

The `fpolicy_server_outstanding_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Server | timeseries | [Top $TopResources Servers by Outstanding Requests](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=608) |
///



### fpolicy_server_processed_requests

Total number of screen requests processed(sync and async)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_server` | `processed_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml |

The `fpolicy_server_processed_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Server | timeseries | [Top $TopResources Servers by Processed Requests](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=609) |
///



### fpolicy_server_request_latency

Average latency in microseconds for screen request

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy_server` | `request_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> request_latency_base | conf/zapiperf/cdot/9.8.0/fpolicy_server.yaml |

The `fpolicy_server_request_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | Server | timeseries | [Top $TopResources Servers by Request Latency](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=604) |
///



### fpolicy_svm_aborted_requests

Number of screen requests aborted

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy` | `aborted_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml |

The `fpolicy_svm_aborted_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | SVM | timeseries | [Top $TopResources SVM by Aborted Requests](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=597) |
///



### fpolicy_svm_cifs_requests

Number of cifs screen requests sent to fpolicy server

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy` | `cifs_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml |

The `fpolicy_svm_cifs_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | SVM | timeseries | [Top $TopResources SVM by Cifs Requests](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=598) |
///



### fpolicy_svm_failedop_notifications

Number of failed file operation notifications sent to fpolicy server

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy` | `failedop_notifications`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml |

The `fpolicy_svm_failedop_notifications` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | SVM | timeseries | [Top $TopResources SVM by Failed File Operation](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=599) |
///



### fpolicy_svm_io_processing_latency

Average IO processing latency in microseconds for screen request

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy` | `io_processing_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> io_processing_latency_base | conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml |

The `fpolicy_svm_io_processing_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | SVM | timeseries | [Top $TopResources SVM by IO Processing Latency](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=594) |
///



### fpolicy_svm_io_thread_wait_latency

Average IO thread wait latency in microseconds for screen request

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances fpolicy` | `io_thread_wait_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> io_thread_wait_latency_base | conf/zapiperf/cdot/9.8.0/fpolicy_svm.yaml |

The `fpolicy_svm_io_thread_wait_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FPolicy | SVM | timeseries | [Top $TopResources SVM by IO Thread Wait Latency](/d/cdot-fpolicy/ontap3a-fpolicy?orgId=1&viewPanel=595) |
///



### fru_status

This metric indicates a value of 1 if the FRU status is ok and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/fru.yaml |

The `fru_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Power | Field Replaceable Unit (FRU) | table | [Field Replaceable Unit (FRU)](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=106) |
///



### headroom_aggr_current_latency

This is the storage aggregate average latency in microseconds per message at the disk level.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `current_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> current_ops | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `current_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> current_ops | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |

The `headroom_aggr_current_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | Aggregate Headroom | timeseries | [Current Latency](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=2) |
///



### headroom_aggr_current_ops

Total number of I/Os processed by the aggregate per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `current_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `current_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |

The `headroom_aggr_current_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | Highlights | timeseries | [Available Ops: Aggregate](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=45) |
| ONTAP: Headroom | Aggregate Headroom | timeseries | [Current IOP/s](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=5) |
///



### headroom_aggr_current_utilization

This is the storage aggregate average utilization of all the data disks in the aggregate.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `current_utilization`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> current_utilization_denominator | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `current_utilization`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> current_utilization_total | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |

The `headroom_aggr_current_utilization` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | Aggregate Headroom | timeseries | [Current Utilization](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=6) |
///



### headroom_aggr_ewma_daily

Daily exponential weighted moving average.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `ewma.daily`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `ewma_daily`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |



### headroom_aggr_ewma_hourly

Hourly exponential weighted moving average.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `ewma.hourly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `ewma_hourly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |



### headroom_aggr_ewma_monthly

Monthly exponential weighted moving average.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `ewma.monthly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `ewma_monthly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |



### headroom_aggr_ewma_weekly

Weekly exponential weighted moving average.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `ewma.weekly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `ewma_weekly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |



### headroom_aggr_optimal_point_confidence_factor

The confidence factor for the optimal point value based on the observed resource latency and utilization.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `optimal_point.confidence_factor`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point.samples | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `optimal_point_confidence_factor`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point_samples | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |



### headroom_aggr_optimal_point_latency

The latency in microseconds component of the optimal point of the latency/utilization curve.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `optimal_point.latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point.samples | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `optimal_point_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point_samples | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |

The `headroom_aggr_optimal_point_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | Aggregate Headroom | timeseries | [Optimal-Point Latency](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=7) |
///



### headroom_aggr_optimal_point_ops

The ops component of the optimal point derived from the latency/utilzation curve.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `optimal_point.ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point.samples | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `optimal_point_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point_samples | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |

The `headroom_aggr_optimal_point_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | Highlights | timeseries | [Available Ops: Aggregate](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=45) |
| ONTAP: Headroom | Aggregate Headroom | timeseries | [Optimal-Point IOP/s](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=10) |
///



### headroom_aggr_optimal_point_utilization

The utilization component of the optimal point of the latency/utilization curve.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_aggregate` | `optimal_point.utilization`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point.samples | conf/restperf/9.12.0/resource_headroom_aggr.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_aggr` | `optimal_point_utilization`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point_samples | conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml |

The `headroom_aggr_optimal_point_utilization` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | Aggregate Headroom | timeseries | [Optimal-Point Utilization](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=9) |
///



### headroom_cpu_current_latency

Current operation latency in microseconds of the resource.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `current_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> current_ops | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `current_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> current_ops | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |

The `headroom_cpu_current_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | CPU Headroom | timeseries | [Current Latency](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=16) |
///



### headroom_cpu_current_ops

Total number of operations per second (also referred to as dblade ops).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `current_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `current_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |

The `headroom_cpu_current_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | Highlights | timeseries | [Available Ops: CPU](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=46) |
| ONTAP: Headroom | CPU Headroom | timeseries | [Current CPU Ops](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=18) |
| ONTAP: NFS Troubleshooting | Highlights | table | [Headroom Overview (Average by Time Range)](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=8) |
///



### headroom_cpu_current_utilization

Average processor utilization across all processors in the system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `current_utilization`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> elapsed_time | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `current_utilization`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> current_utilization_total | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |

The `headroom_cpu_current_utilization` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | CPU Headroom | timeseries | [Current Utilization](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=20) |
| ONTAP: NFS Troubleshooting | Highlights | table | [Headroom Overview (Average by Time Range)](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=8) |
///



### headroom_cpu_ewma_daily

Daily exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `ewma.daily`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `ewma_daily`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |

The `headroom_cpu_ewma_daily` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [Weighted Avg Daily (Headroom)](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=10) |
///



### headroom_cpu_ewma_hourly

Hourly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `ewma.hourly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `ewma_hourly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |



### headroom_cpu_ewma_monthly

Monthly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `ewma.monthly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `ewma_monthly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |



### headroom_cpu_ewma_weekly

Weekly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `ewma.weekly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `ewma_weekly`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |

The `headroom_cpu_ewma_weekly` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [Weighted Avg Weekly (Headroom)](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=11) |
///



### headroom_cpu_optimal_point_confidence_factor

Confidence factor for the optimal point value based on the observed resource latency and utilization. The possible values are: 0 - unknown, 1 - low, 2 - medium, 3 - high. This counter can provide an average confidence factor over a range of time.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `optimal_point.confidence_factor`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point.samples | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `optimal_point_confidence_factor`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point_samples | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |



### headroom_cpu_optimal_point_latency

Latency component of the optimal point of the latency in microseconds/utilization curve. This counter can provide an average latency over a range of time.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `optimal_point.latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point.samples | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `optimal_point_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point_samples | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |

The `headroom_cpu_optimal_point_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | CPU Headroom | timeseries | [Optimal-Point Latency](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=22) |
///



### headroom_cpu_optimal_point_ops

Ops component of the optimal point derived from the latency/utilization curve. This counter can provide an average ops over a range of time.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `optimal_point.ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point.samples | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `optimal_point_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point_samples | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |

The `headroom_cpu_optimal_point_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | Highlights | timeseries | [Available Ops: CPU](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=46) |
| ONTAP: Headroom | CPU Headroom | timeseries | [Optimal-Point Ops](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=24) |
| ONTAP: NFS Troubleshooting | Highlights | table | [Headroom Overview (Average by Time Range)](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=8) |
///



### headroom_cpu_optimal_point_utilization

Utilization component of the optimal point of the latency/utilization curve. This counter can provide an average utilization over a range of time.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/headroom_cpu` | `optimal_point.utilization`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point.samples | conf/restperf/9.12.0/resource_headroom_cpu.yaml |
| ZapiPerf | `perf-object-get-instances resource_headroom_cpu` | `optimal_point_utilization`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> optimal_point_samples | conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml |

The `headroom_cpu_optimal_point_utilization` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Headroom | CPU Headroom | timeseries | [Optimal-Point Utilization](/d/cdot-headroom/ontap3a-headroom?orgId=1&viewPanel=26) |
| ONTAP: NFS Troubleshooting | Highlights | table | [Headroom Overview (Average by Time Range)](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=8) |
///



### health_disk_alerts

Provides any issues related to Disks health check if disks are broken or unassigned. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_disk_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Errors](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=621) |
| ONTAP: Datacenter | Issues | piechart | [Warnings](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=623) |
| ONTAP: Health | Highlights | stat | [Total Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=277) |
| ONTAP: Health | Highlights | stat | [Total Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=278) |
| ONTAP: Health | Highlights | piechart | [Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=268) |
| ONTAP: Health | Highlights | piechart | [Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=269) |
| ONTAP: Health | Disks | table | [Disks Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=248) |
///



### health_ems_alerts

The health_ems_alerts metric monitors EMS (Event Management System), providing a count based on their severity and other attributes. This metric includes labels such as node, message, source, and severity (e.g., emergency, alert, error). By default, it monitors alerts with emergency severity.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_ems_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Health | Highlights | stat | [Active Emergency EMS Alerts (Last 24 Hours)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=270) |
| ONTAP: Health | Highlights | table | [Active Emergency EMS Alerts (Last 24 Hours)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=272) |
| ONTAP: Health | Emergency EMS | table | [Active Emergency EMS Alerts (Last 24 Hours)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=237) |
///



### health_ha_alerts

Provides any issues related to HA health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_ha_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Errors](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=621) |
| ONTAP: Health | Highlights | stat | [Total Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=277) |
| ONTAP: Health | Highlights | piechart | [Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=268) |
| ONTAP: Health | HA | table | [HA Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=281) |
///



### health_license_alerts

Provides any issues related to License health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_license_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Errors](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=621) |
| ONTAP: Health | Highlights | stat | [Total Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=277) |
| ONTAP: Health | Highlights | piechart | [Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=268) |
| ONTAP: Health | License | table | [Non Compliant License](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=276) |
///



### health_lif_alerts

Provides any issues related to LIF health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_lif_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Warnings](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=623) |
| ONTAP: Health | Highlights | stat | [Total Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=278) |
| ONTAP: Health | Highlights | piechart | [Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=269) |
| ONTAP: Health | Lif | table | [Lif not at home port](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=280) |
///



### health_network_ethernet_port_alerts

Provides any issues related to Network Ethernet Port health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_network_ethernet_port_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Errors](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=621) |
| ONTAP: Health | Highlights | stat | [Total Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=277) |
| ONTAP: Health | Highlights | piechart | [Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=268) |
| ONTAP: Health | Network Port | table | [Ethernet ports are down](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=261) |
///



### health_network_fc_port_alerts

Provides any issues related to Network FC Port health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_network_fc_port_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Errors](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=621) |
| ONTAP: Health | Highlights | stat | [Total Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=277) |
| ONTAP: Health | Highlights | piechart | [Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=268) |
| ONTAP: Health | Network Port | table | [FC ports are down](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=262) |
///



### health_node_alerts

Provides any issues related to Node health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_node_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Errors](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=621) |
| ONTAP: Health | Highlights | stat | [Total Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=277) |
| ONTAP: Health | Highlights | piechart | [Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=268) |
| ONTAP: Health | Node | table | [Node Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=253) |
///



### health_shelf_alerts

Provides any issues related to Shelf health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_shelf_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Errors](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=621) |
| ONTAP: Datacenter | Issues | piechart | [Warnings](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=623) |
| ONTAP: Health | Highlights | stat | [Total Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=277) |
| ONTAP: Health | Highlights | stat | [Total Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=278) |
| ONTAP: Health | Highlights | piechart | [Errors](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=268) |
| ONTAP: Health | Highlights | piechart | [Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=269) |
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
///



### health_support_alerts

Provides any issues related to Support health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_support_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Warnings](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=623) |
| ONTAP: Health | Highlights | stat | [Total Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=278) |
| ONTAP: Health | Highlights | piechart | [Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=269) |
| ONTAP: Health | System Health Alerts | table | [System Alerts](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=249) |
///



### health_volume_move_alerts

Provides any issues related to Volume Move health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_volume_move_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Warnings](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=623) |
| ONTAP: Health | Highlights | stat | [Total Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=278) |
| ONTAP: Health | Highlights | piechart | [Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=269) |
| ONTAP: Health | Volume | table | [Volumes Move Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=271) |
///



### health_volume_ransomware_alerts

Provides any issues related to Volume Ransomware health check. Value of 1 means issue is happening and 0 means that issue is resolved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.6.0/health.yaml |

The `health_volume_ransomware_alerts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Issues | piechart | [Warnings](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=623) |
| ONTAP: Health | Highlights | stat | [Total Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=278) |
| ONTAP: Health | Highlights | piechart | [Warnings](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=269) |
| ONTAP: Health | Volume | table | [Volumes with Ransomware Issues (9.10+ Only)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=264) |
///



### hostadapter_bytes_read

Bytes read through a host adapter

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/host_adapter` | `bytes_read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/hostadapter.yaml |
| ZapiPerf | `perf-object-get-instances hostadapter` | `bytes_read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/hostadapter.yaml |

The `hostadapter_bytes_read` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Disk Utilization | timeseries | [Disk and Tape Drives Throughput by Node](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=34) |
| ONTAP: Disk | Disk Utilization | timeseries | [Top $TopResources Disk and Tape Drives Throughput by Host Adapter](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=62) |
| ONTAP: MetroCluster | Disk and Tape Adapter | timeseries | [Top $TopResources Adapters by Read Data](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=75) |
///



### hostadapter_bytes_written

Bytes written through a host adapter

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/host_adapter` | `bytes_written`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/hostadapter.yaml |
| ZapiPerf | `perf-object-get-instances hostadapter` | `bytes_written`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/hostadapter.yaml |

The `hostadapter_bytes_written` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Disk Utilization | timeseries | [Disk and Tape Drives Throughput by Node](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=34) |
| ONTAP: Disk | Disk Utilization | timeseries | [Top $TopResources Disk and Tape Drives Throughput by Host Adapter](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=62) |
| ONTAP: MetroCluster | Disk and Tape Adapter | timeseries | [Top $TopResources Adapters by Write Data](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=76) |
///



### igroup_labels

Details of Igroups in the cluster.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/san/igroups` | `Harvest generated` | conf/rest/asar2/9.16.0/igroup.yaml |

The `igroup_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Highlights | stat | [SCSI](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=7) |
| ASAr2: Overview | Hosts | table | [SAN initiator groups](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=13) |
| ONTAP: Hosts | Highlights | table | [SAN initiator groups](/d/cdot-host/ontap3a-hosts?orgId=1&viewPanel=2) |
///



### iscsi_lif_avg_latency

Average latency in microseconds for iSCSI operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cmd_transferred | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cmd_transfered | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |

The `iscsi_lif_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | iSCSI Frontend | stat | [iSCSI Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=77) |
| ONTAP: Node | iSCSI Frontend | timeseries | [Average Latency by LIF](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=79) |
| ONTAP: SVM | iSCSI | stat | [SVM iSCSI Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=191) |
///



### iscsi_lif_avg_other_latency

Average latency in microseconds for operations other than read and write (for example, Inquiry, Report LUNs, SCSI Task Management Functions)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `average_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> iscsi_other_ops | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `avg_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> iscsi_other_ops | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |

The `iscsi_lif_avg_other_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | iSCSI | timeseries | [SVM iSCSI Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=193) |
///



### iscsi_lif_avg_read_latency

Average latency in microseconds for read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> iscsi_read_ops | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `avg_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> iscsi_read_ops | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |

The `iscsi_lif_avg_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | iSCSI | stat | [SVM iSCSI Average Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=192) |
| ONTAP: SVM | iSCSI | timeseries | [SVM iSCSI Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=193) |
///



### iscsi_lif_avg_write_latency

Average latency in microseconds for write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> iscsi_write_ops | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `avg_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> iscsi_write_ops | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |

The `iscsi_lif_avg_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | iSCSI | stat | [SVM iSCSI Average Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=202) |
| ONTAP: SVM | iSCSI | timeseries | [SVM iSCSI Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=193) |
///



### iscsi_lif_cmd_transfered

Command transferred by this iSCSI connection

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `cmd_transferred`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `cmd_transfered`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |



### iscsi_lif_iscsi_other_ops

iSCSI other operations per second on this logical interface (LIF)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `iscsi_other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `iscsi_other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |

The `iscsi_lif_iscsi_other_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | iSCSI Frontend | stat | [iSCSI IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=78) |
| ONTAP: Node | iSCSI Frontend | timeseries | [IOPs by LIF](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=81) |
| ONTAP: SVM | iSCSI | stat | [SVM iSCSI IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=197) |
///



### iscsi_lif_iscsi_read_ops

iSCSI read operations per second on this logical interface (LIF)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `iscsi_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `iscsi_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |

The `iscsi_lif_iscsi_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | iSCSI | stat | [SVM iSCSI Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=198) |
///



### iscsi_lif_iscsi_write_ops

iSCSI write operations per second on this logical interface (LIF)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `iscsi_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `iscsi_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |

The `iscsi_lif_iscsi_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | iSCSI | stat | [SVM iSCSI Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=204) |
///



### iscsi_lif_protocol_errors

Number of protocol errors from iSCSI sessions on this logical interface (LIF)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `protocol_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `protocol_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |



### iscsi_lif_read_data

Performance metric for ISCSI LIF read I/O operations in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |

The `iscsi_lif_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | LIF | timeseries | [Top $TopResources iSCSI LIFs by Send Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=95) |
| ONTAP: SVM | iSCSI | stat | [SVM iSCSI Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=195) |
| ONTAP: SVM | iSCSI | timeseries | [Top $TopResources iSCSI LIFs by Send Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=194) |
| ONTAP: SVM | iSCSI | timeseries | [SVM iSCSI Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=201) |
///



### iscsi_lif_write_data

Performance metric for ISCSI LIF write I/O operations in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iscsi_lif` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/iscsi_lif.yaml |
| ZapiPerf | `perf-object-get-instances iscsi_lif` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml |

The `iscsi_lif_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | iSCSI Frontend | timeseries | [Throughput by LIF](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=80) |
| ONTAP: SVM | LIF | timeseries | [Top $TopResources iSCSI LIFs by Receive Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=96) |
| ONTAP: SVM | iSCSI | stat | [SVM iSCSI Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=196) |
| ONTAP: SVM | iSCSI | stat | [SVM iSCSI Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=203) |
| ONTAP: SVM | iSCSI | timeseries | [Top $TopResources iSCSI LIFs by Receive Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=200) |
| ONTAP: SVM | iSCSI | timeseries | [SVM iSCSI Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=201) |
///



### iw_avg_latency

Average RDMA I/O latency in microseconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iwarp` | `average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> ops | conf/restperf/9.14.1/iwarp.yaml |
| ZapiPerf | `perf-object-get-instances iwarp` | `iw_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> iw_ops | conf/zapiperf/cdot/9.8.0/iwarp.yaml |

The `iw_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Iwarp | timeseries | [Average Latency](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=103) |
///



### iw_ops

Number of RDMA I/Os issued.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iwarp` | `ops`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/iwarp.yaml |
| ZapiPerf | `perf-object-get-instances iwarp` | `iw_ops`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iwarp.yaml |



### iw_read_ops

Number of RDMA read I/Os issued.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iwarp` | `read_ops`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/iwarp.yaml |
| ZapiPerf | `perf-object-get-instances iwarp` | `iw_read_ops`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iwarp.yaml |



### iw_write_ops

Number of RDMA write I/Os issued.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/iwarp` | `write_ops`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/iwarp.yaml |
| ZapiPerf | `perf-object-get-instances iwarp` | `iw_write_ops`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/iwarp.yaml |

The `iw_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Iwarp | timeseries | [Write IOPs](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=109) |
///



### lif_labels

This metric provides information about LIF

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/network/ip/interfaces` | `Harvest generated` | conf/rest/9.12.0/lif.yaml |
| ZAPI | `net-interface-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/lif.yaml |

The `lif_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: Health | Lif | table | [Lif not at home port](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=280) |
| ONTAP: SVM | LIF | table | [LIF Details](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=252) |
///



### lif_recv_data

Number of bytes received per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lif` | `received_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lif.yaml |
| KeyPerf | `api/network/ip/interfaces` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lif.yaml |
| ZapiPerf | `perf-object-get-instances lif` | `recv_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lif.yaml |

The `lif_recv_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | LIF | timeseries | [Top $TopResources NAS LIFs by Receive Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=90) |
///



### lif_recv_errors

Number of received Errors per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lif` | `received_errors`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lif.yaml |
| ZapiPerf | `perf-object-get-instances lif` | `recv_errors`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lif.yaml |



### lif_recv_packet

Number of packets received per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lif` | `received_packets`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lif.yaml |
| ZapiPerf | `perf-object-get-instances lif` | `recv_packet`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lif.yaml |



### lif_sent_data

Number of bytes sent per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lif` | `sent_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lif.yaml |
| KeyPerf | `api/network/ip/interfaces` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lif.yaml |
| ZapiPerf | `perf-object-get-instances lif` | `sent_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lif.yaml |

The `lif_sent_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | LIF | timeseries | [Top $TopResources NAS LIFs by Send Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=89) |
///



### lif_sent_errors

Number of sent errors per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lif` | `sent_errors`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lif.yaml |
| ZapiPerf | `perf-object-get-instances lif` | `sent_errors`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lif.yaml |



### lif_sent_packet

Number of packets sent per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lif` | `sent_packets`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lif.yaml |
| ZapiPerf | `perf-object-get-instances lif` | `sent_packet`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lif.yaml |



### lif_total_data

Performance metric aggregated over all types of I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/network/ip/interfaces` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lif.yaml |



### lif_uptime

Interface up time

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lif` | `up_time`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lif.yaml |
| ZapiPerf | `perf-object-get-instances lif` | `up_time`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lif.yaml |

The `lif_uptime` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | LIF | table | [LIF Details](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=252) |
///



### lun_avg_read_latency

Average read latency in microseconds for all operations on the LUN

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.12.0/lun.yaml |
| KeyPerf | `api/storage/luns` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lun_statistics.iops_raw.read | conf/keyperf/9.15.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `avg_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_avg_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Highlights | stat | [Top $TopResources Luns by Average Read Latency](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=4) |
| ONTAP: LUN | LUN Table | table | [Top $TopResources Luns by Read Latency](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=42) |
| ONTAP: LUN | Top LUN Performance | timeseries | [Top $TopResources Luns by Average Read Latency](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=48) |
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [Latency](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=87) |
///



### lun_avg_write_latency

Average write latency in microseconds for all operations on the LUN

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.12.0/lun.yaml |
| KeyPerf | `api/storage/luns` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lun_statistics.iops_raw.write | conf/keyperf/9.15.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `avg_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_avg_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Highlights | stat | [Top $TopResources Luns by Average Write Latency](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=36) |
| ONTAP: LUN | LUN Table | table | [Top $TopResources Luns by Write Latency](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=43) |
| ONTAP: LUN | Top LUN Performance | timeseries | [Top $TopResources Luns by Average Write Latency](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=51) |
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [Latency](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=87) |
///



### lun_avg_xcopy_latency

Average latency in microseconds for xcopy requests

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `average_xcopy_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> xcopy_requests | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `avg_xcopy_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> xcopy_reqs | conf/zapiperf/cdot/9.8.0/lun.yaml |



### lun_block_size

Represents the block size being used

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `total_data, total_ops`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `lun` | `total_data, total_ops`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_block_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | LUN Table | table | [LUNS in Cluster](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=18) |
| ONTAP: LUN | Top LUN Performance | timeseries | [Top $TopResources Luns by Block Size](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=49) |
///



### lun_caw_reqs

Number of compare and write requests

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `caw_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `caw_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_caw_reqs` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [vStorage Offload Operations](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=91) |
///



### lun_enospc

Number of operations receiving ENOSPC errors

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `enospc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `enospc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |



### lun_labels

This metric provides information about Lun

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/lun` | `Harvest generated` | conf/rest/9.12.0/lun.yaml |
| ZAPI | `lun-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/lun.yaml |

The `lun_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: LUN | LUN Table | table | [LUNS in Cluster](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=18) |
///



### lun_new_status

This metric indicates a value of 1 if the LUN state is online (indicating the LUN is operational) and a value of 0 for any other state. and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/lun.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/lun.yaml |

The `lun_new_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | LUN Table | table | [LUNS in Cluster](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=18) |
///



### lun_other_data

Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/luns` | `statistics.throughput_raw.other`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lun.yaml |



### lun_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/luns` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lun_statistics.iops_raw.other | conf/keyperf/9.15.0/lun.yaml |



### lun_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/luns` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lun.yaml |



### lun_queue_full

Queue full responses

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `queue_full`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `queue_full`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |



### lun_read_align_histo

Histogram of WAFL read alignment (number sectors off WAFL block start)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `read_align_histogram`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_ops_sent | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `read_align_histo`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_ops_sent | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_read_align_histo` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Top LUN Performance Efficiency | timeseries | [Top $TopResources Luns by Read Misalignment Buckets](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=58) |
///



### lun_read_data

Performance metric for read I/O operations in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| KeyPerf | `api/storage/luns` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Highlights | stat | [Top $TopResources Luns by Read Throughput](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=23) |
| ONTAP: LUN | LUN Table | table | [Top $TopResources Luns by Read Throughput](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=44) |
| ONTAP: LUN | Top LUN Performance | timeseries | [Top $TopResources Luns by Read Throughput](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=31) |
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [Throughput](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=88) |
///



### lun_read_ops

Number of read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| KeyPerf | `api/storage/luns` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Highlights | stat | [Top $TopResources Luns by Read IOPs](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=5) |
| ONTAP: LUN | LUN Table | table | [Top $TopResources Luns by Read IOPS](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=46) |
| ONTAP: LUN | Top LUN Performance | timeseries | [Top $TopResources Luns by Read IOPs](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=32) |
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [IOPs](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=89) |
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [IO Size](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=93) |
///



### lun_read_partial_blocks

Percentage of reads whose size is not a multiple of WAFL block size

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `read_partial_blocks`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_ops | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `read_partial_blocks`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/lun.yaml |



### lun_remote_bytes

I/O to or from a LUN which is not owned by the storage system handling the I/O in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `remote_bytes`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `remote_bytes`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_remote_bytes` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [Indirect Access](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=90) |
///



### lun_remote_ops

Number of operations received by a storage system that does not own the LUN targeted by the operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `remote_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `remote_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_remote_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Top LUN Performance Efficiency | timeseries | [Top $TopResources Luns by Indirect Access IOPS](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=59) |
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [Indirect Access](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=90) |
///



### lun_size

The total provisioned size of the LUN. The LUN size can be increased but not decreased using the REST interface.<br/>The maximum and minimum sizes listed here are the absolute maximum and absolute minimum sizes, in bytes. The actual minimum and maximum sizes vary depending on the ONTAP version, ONTAP platform and the available space in the containing volume and aggregate.<br/>For more information, see _Size properties_ in the _docs_ section of the ONTAP REST API documentation.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/lun` | `size` | conf/rest/9.12.0/lun.yaml |
| ZAPI | `lun-get-iter` | `lun-info.size` | conf/zapi/cdot/9.8.0/lun.yaml |

The `lun_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | LUN Table | table | [LUNS in Cluster](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=18) |
///



### lun_size_used

The amount of space consumed by the main data stream of the LUN.<br/>This value is the total space consumed in the volume by the LUN, including filesystem overhead, but excluding prefix and suffix streams. Due to internal filesystem overhead and the many ways SAN filesystems and applications utilize blocks within a LUN, this value does not necessarily reflect actual consumption/availability from the perspective of the filesystem or application. Without specific knowledge of how the LUN blocks are utilized outside of ONTAP, this property should not be used as an indicator for an out-of-space condition.<br/>For more information, see _Size properties_ in the _docs_ section of the ONTAP REST API documentation.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/lun` | `size_used` | conf/rest/9.12.0/lun.yaml |
| ZAPI | `lun-get-iter` | `lun-info.size-used` | conf/zapi/cdot/9.8.0/lun.yaml |

The `lun_size_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | LUN Table | table | [LUNS in Cluster](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=18) |
///



### lun_size_used_percent

This metric represents the percentage of a LUN that is currently being used.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/lun` | `size_used, size` | conf/rest/9.12.0/lun.yaml |
| ZAPI | `lun-get-iter` | `size_used, size` | conf/zapi/cdot/9.8.0/lun.yaml |

The `lun_size_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Top Volume and LUN Capacity | timeseries | [Top $TopResources LUNs by Percent Most Filled](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=98) |
| ONTAP: LUN | Top Volume and LUN Capacity | timeseries | [Top $TopResources LUNs by Percent Least Filled](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=83) |
///



### lun_total_data

Performance metric for read and write I/O operations in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `read_data, write_data`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| KeyPerf | `api/storage/luns` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lun.yaml |
| ZapiPerf | `lun` | `read_data, write_data`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |



### lun_total_latency

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/luns` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lun_statistics.iops_raw.total | conf/keyperf/9.15.0/lun.yaml |



### lun_total_ops

Total number of read and write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `read_ops, write_ops`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| KeyPerf | `api/storage/luns` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lun.yaml |
| ZapiPerf | `lun` | `read_ops, write_ops`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |



### lun_unmap_reqs

Number of unmap command requests

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `unmap_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `unmap_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_unmap_reqs` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [vStorage Offload Operations](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=91) |
///



### lun_write_align_histo

Histogram of WAFL write alignment (number of sectors off WAFL block start)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `write_align_histogram`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> write_ops_sent | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `write_align_histo`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> write_ops_sent | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_write_align_histo` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Top LUN Performance Efficiency | timeseries | [Top $TopResources Luns by Write Misalignment Buckets](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=57) |
///



### lun_write_data

Performance metric for write I/O operations in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| KeyPerf | `api/storage/luns` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Highlights | stat | [Top $TopResources Luns by Write Throughput](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=37) |
| ONTAP: LUN | LUN Table | table | [Top $TopResources Luns by Write Throughput](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=45) |
| ONTAP: LUN | Top LUN Performance | timeseries | [Top $TopResources Luns by Write Throughput](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=52) |
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [Throughput](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=88) |
///



### lun_write_ops

Number of write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| KeyPerf | `api/storage/luns` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Highlights | stat | [Top $TopResources Luns by Write IOPs](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=38) |
| ONTAP: LUN | LUN Table | table | [Top $TopResources Luns by Write IOPS](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=47) |
| ONTAP: LUN | Top LUN Performance | timeseries | [Top $TopResources Luns by Write IOPs](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=53) |
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [IOPs](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=89) |
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [IO Size](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=93) |
///



### lun_write_partial_blocks

Percentage of writes whose size is not a multiple of WAFL block size

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `write_partial_blocks`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> write_ops | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `write_partial_blocks`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/lun.yaml |



### lun_writesame_reqs

Number of write same command requests

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `writesame_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `writesame_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_writesame_reqs` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [vStorage Offload Operations](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=91) |
///



### lun_writesame_unmap_reqs

Number of write same commands requests with unmap bit set

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `writesame_unmap_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `writesame_unmap_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_writesame_unmap_reqs` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [vStorage Offload Operations](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=91) |
///



### lun_xcopy_reqs

Total number of xcopy operations on the LUN

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/lun` | `xcopy_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/lun.yaml |
| ZapiPerf | `perf-object-get-instances lun` | `xcopy_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/lun.yaml |

The `lun_xcopy_reqs` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Per LUN (Must Select Cluster/SVM/Volume/LUN) | timeseries | [vStorage Offload Operations](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=91) |
///



### mav_request_approve_expiry_time

Shows the deadline by which approved operations must be approved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security/multi-admin-verify/requests` | `approve_expiry_time` | conf/rest/9.12.0/mav_request.yaml |

The `mav_request_approve_expiry_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MAV Request | Highlights | table | [MAV Requests](/d/cdot-mva/ontap3a-mav request?orgId=1&viewPanel=295) |
///



### mav_request_approve_time

Shows the date and time when requests were approved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security/multi-admin-verify/requests` | `approve_time` | conf/rest/9.12.0/mav_request.yaml |

The `mav_request_approve_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MAV Request | Highlights | table | [MAV Requests](/d/cdot-mva/ontap3a-mav request?orgId=1&viewPanel=295) |
///



### mav_request_create_time

Displays the date and time each MAV request was initiated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security/multi-admin-verify/requests` | `create_time` | conf/rest/9.12.0/mav_request.yaml |

The `mav_request_create_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MAV Request | Highlights | table | [MAV Requests](/d/cdot-mva/ontap3a-mav request?orgId=1&viewPanel=295) |
///



### mav_request_details

This metric provides information about MAV requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security/multi-admin-verify/requests` | `Harvest generated.` | conf/rest/9.12.0/mav_request.yaml |

The `mav_request_details` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MAV Request | Highlights | table | [MAV Requests](/d/cdot-mva/ontap3a-mav request?orgId=1&viewPanel=295) |
///



### mav_request_execution_expiry_time

Shows the deadline by which approved operations must be executed.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security/multi-admin-verify/requests` | `execution_expiry_time` | conf/rest/9.12.0/mav_request.yaml |

The `mav_request_execution_expiry_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MAV Request | Highlights | table | [MAV Requests](/d/cdot-mva/ontap3a-mav request?orgId=1&viewPanel=295) |
///



### mediator_labels

This metric provides information about Mediator

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cluster/mediators` | `Harvest generated` | conf/rest/9.12.0/mediator.yaml |



### metadata_collector_api_time

amount of time to collect data from monitored cluster object

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 

The `metadata_collector_api_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Collectors | timeseries | [API Time](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=166) |
///



### metadata_collector_bytesRx

The amount of data received by the collector from the monitored cluster.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> bytes | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> bytes | NA | 



### metadata_collector_calc_time

amount of time it took to compute metrics between two successive polls, specifically using properties like raw, delta, rate, average, and percent. This metric is available for ZapiPerf/RestPerf collectors.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 

The `metadata_collector_calc_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Collectors | timeseries | [Postprocessing Time](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=168) |
///



### metadata_collector_instances

number of objects collected from monitored cluster

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 

The `metadata_collector_instances` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Collectors | timeseries | [Instances Per Poll](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=186) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### metadata_collector_metrics

number of counters collected from monitored cluster

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 

The `metadata_collector_metrics` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Collectors | timeseries | [Data Points Per Poll](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=169) |
///



### metadata_collector_numCalls

The number of API calls made by the collector to the monitored cluster.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 



### metadata_collector_numPartials

The number of partial responses received by the collector from the monitored cluster.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 



### metadata_collector_parse_time

amount of time to parse XML, JSON, etc. for cluster object

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 

The `metadata_collector_parse_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Collectors | timeseries | [Parse Time](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=167) |
///



### metadata_collector_pluginInstances

The number of plugin instances generated by the collector.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 



### metadata_collector_plugin_time

amount of time for all plugins to post-process metrics

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 



### metadata_collector_poll_time

amount of time it took for the poll to finish

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 

The `metadata_collector_poll_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | timeseries | [Average Poll Time Per Poller](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=140) |
| Harvest Metadata | Highlights | timeseries | [Average Time Per Collector](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=158) |
| Harvest Metadata | Collectors | timeseries | [Time Per Data Poll](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=63) |
///



### metadata_collector_skips

number of metrics that were not calculated between two successive polls. This metric is available for ZapiPerf/RestPerf collectors.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 



### metadata_collector_task_time

amount of time it took for each collector's subtasks to complete

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 



### metadata_component_count

number of metrics collected for each object

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 

The `metadata_component_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | stat | [Collected/24h](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=69) |
| Harvest Metadata | Highlights | stat | [Collected/m](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=72) |
| Harvest Metadata | Highlights | stat | [Exported/m](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=157) |
| Harvest Metadata | Prometheus | timeseries | [Data Points Per Export](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=179) |
///



### metadata_component_status

status of the collector - 0 means running, 1 means standby, 2 means failed

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> enum | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> enum | NA | 

The `metadata_component_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | stat | [Total Object Count Across Collectors](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=70) |
| Harvest Metadata | Highlights | stat | [Failed Object Count Across Collectors](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=185) |
| Harvest Metadata | Highlights | stat | [Exporters](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=89) |
| Harvest Metadata | Highlights | table | [Collectors](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=128) |
| Harvest Metadata | Highlights | table | [Exporters](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=129) |
///



### metadata_exporter_count

number of metrics and labels exported

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 

The `metadata_exporter_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Prometheus | timeseries | [Data Points Per Export](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=179) |
///



### metadata_exporter_time

amount of time it took to render, export, and serve exported data

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> microseconds | NA | 

The `metadata_exporter_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | timeseries | [Average Time Per Exporter](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=159) |
| Harvest Metadata | Prometheus | timeseries | [Average Time Per Export](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=178) |
///



### metadata_target_goroutines

number of goroutines that exist within the poller

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> scalar | NA | 



### metadata_target_ping

The response time (in milliseconds) of the ping to the target system. If the ping is successful, the metric records the time it took for the ping to complete.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> milliseconds | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> milliseconds | NA | 

The `metadata_target_ping` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | table | [Target Systems](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=104) |
///



### metadata_target_status

status of the system being monitored. 0 means reachable, 1 means unreachable

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> enum | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> enum | NA | 

The `metadata_target_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | stat | [Datacenters](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=31) |
| Harvest Metadata | Highlights | table | [Target Systems](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=104) |
///



### metrocluster_check_aggr_status

Detail of the type of diagnostic operation run for the Aggregate with diagnostic operation result.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/metrocluster_check.yaml |

The `metrocluster_check_aggr_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Diagnostics | table | [Metrocluster Aggregate Diagnostics Check Details](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=101) |
///



### metrocluster_check_cluster_status

Detail of the type of diagnostic operation run for the Cluster with diagnostic operation result.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/metrocluster_check.yaml |

The `metrocluster_check_cluster_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Diagnostics | table | [Metrocluster Cluster Diagnostics Check Details](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=87) |
///



### metrocluster_check_node_status

Detail of the type of diagnostic operation run for the Node with diagnostic operation result.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/metrocluster_check.yaml |

The `metrocluster_check_node_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Diagnostics | table | [Metrocluster Node Diagnostics Check Details](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=88) |
///



### metrocluster_check_volume_status

Detail of the type of diagnostic operation run for the Volume with diagnostic operation result.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/metrocluster_check.yaml |

The `metrocluster_check_volume_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Diagnostics | table | [Metrocluster Volume Diagnostics Check Details](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=102) |
///



### namespace_avg_other_latency

Average other ops latency in microseconds for all operations on the Namespace

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `average_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/restperf/9.12.0/namespace.yaml |
| KeyPerf | `api/storage/namespaces` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> namespace_statistics.iops_raw.other | conf/keyperf/9.15.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `avg_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.10.1/namespace.yaml |



### namespace_avg_read_latency

Average read latency in microseconds for all operations on the Namespace

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.12.0/namespace.yaml |
| KeyPerf | `api/storage/namespaces` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> namespace_statistics.iops_raw.read | conf/keyperf/9.15.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `avg_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.10.1/namespace.yaml |

The `namespace_avg_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NVMe Namespaces | Highlights | timeseries | [Top $TopResources NVMe Namespaces by Average Read Latency](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=48) |
///



### namespace_avg_total_latency

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/namespaces` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> namespace_statistics.iops_raw.total | conf/keyperf/9.15.0/namespace.yaml |



### namespace_avg_write_latency

Average write latency in microseconds for all operations on the Namespace

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.12.0/namespace.yaml |
| KeyPerf | `api/storage/namespaces` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> namespace_statistics.iops_raw.write | conf/keyperf/9.15.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `avg_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.10.1/namespace.yaml |

The `namespace_avg_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NVMe Namespaces | Highlights | timeseries | [Top $TopResources NVMe Namespaces by Average Write Latency](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=51) |
///



### namespace_block_size

The size of blocks in the namespace in bytes. The default for namespaces with an `os_type` of _vmware_ is _512_. All other namespaces default to _4096_.<br/>Valid in POST when creating an NVMe namespace that is not a clone of another. Disallowed in POST when creating a namespace clone. Valid in POST.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/namespaces` | `space.block_size` | conf/rest/9.12.0/namespace.yaml |
| ZAPI | `nvme-namespace-get-iter` | `nvme-namespace-info.block-size` | conf/zapi/cdot/9.8.0/namespace.yaml |



### namespace_labels

This metric provides information about Namespace

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/namespaces` | `Harvest generated` | conf/rest/9.12.0/namespace.yaml |
| ZAPI | `nvme-namespace-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/namespace.yaml |

The `namespace_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: NVMe Namespaces | NVMe Namespaces Table | table | [NVMe Namespaces](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=99) |
///



### namespace_other_ops

Number of other operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| KeyPerf | `api/storage/namespaces` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |



### namespace_read_data

Performance metric for namespace read I/O operations in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| KeyPerf | `api/storage/namespaces` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |

The `namespace_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NVMe Namespaces | Highlights | timeseries | [Top $TopResources NVMe Namespaces by Read Throughput](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=31) |
///



### namespace_read_ops

Number of read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| KeyPerf | `api/storage/namespaces` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |

The `namespace_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NVMe Namespaces | Highlights | timeseries | [Top $TopResources NVMe Namespaces by Read IOPs](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=32) |
///



### namespace_remote_other_ops

Number of remote other operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `remote.other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `remote_other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |



### namespace_remote_read_data

Performance metric for namespace remote read I/O operations in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `remote.read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `remote_read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |



### namespace_remote_read_ops

Number of remote read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `remote.read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `remote_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |



### namespace_remote_write_data

Performance metric for namespace remote write I/O operations in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `remote.write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `remote_write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |



### namespace_remote_write_ops

Number of remote write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `remote.write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `remote_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |



### namespace_size

The total provisioned size of the NVMe namespace. Valid in POST and PATCH. The NVMe namespace size can be increased but not be made smaller using the REST interface.<br/>The maximum and minimum sizes listed here are the absolute maximum and absolute minimum sizes in bytes. The maximum size is variable with respect to large NVMe namespace support in ONTAP. If large namespaces are supported, the maximum size is 128 TB (140737488355328 bytes) and if not supported, the maximum size is just under 16 TB (17557557870592 bytes). The minimum size supported is always 4096 bytes.<br/>For more information, see _Size properties_ in the _docs_ section of the ONTAP REST API documentation.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/namespaces` | `space.size` | conf/rest/9.12.0/namespace.yaml |
| ZAPI | `nvme-namespace-get-iter` | `nvme-namespace-info.size` | conf/zapi/cdot/9.8.0/namespace.yaml |

The `namespace_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NVMe Namespaces | NVMe Namespaces Table | table | [NVMe Namespaces](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=99) |
///



### namespace_size_available

This metric represents the amount of available space in a namespace.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/namespaces` | `size, size_used` | conf/rest/9.12.0/namespace.yaml |
| ZAPI | `nvme-namespace-get-iter` | `size, size_used` | conf/zapi/cdot/9.8.0/namespace.yaml |



### namespace_size_available_percent

This metric represents the percentage of available space in a namespace.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/namespaces` | `size_available, size` | conf/rest/9.12.0/namespace.yaml |
| ZAPI | `nvme-namespace-get-iter` | `size_available, size` | conf/zapi/cdot/9.8.0/namespace.yaml |

The `namespace_size_available_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NVMe Namespaces | NVMe Namespaces Table | table | [NVMe Namespaces](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=99) |
///



### namespace_size_used

The amount of space consumed by the main data stream of the NVMe namespace.<br/>This value is the total space consumed in the volume by the NVMe namespace, including filesystem overhead, but excluding prefix and suffix streams. Due to internal filesystem overhead and the many ways NVMe filesystems and applications utilize blocks within a namespace, this value does not necessarily reflect actual consumption/availability from the perspective of the filesystem or application. Without specific knowledge of how the namespace blocks are utilized outside of ONTAP, this property should not be used and an indicator for an out-of-space condition.<br/>For more information, see _Size properties_ in the _docs_ section of the ONTAP REST API documentation.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/namespaces` | `space.used` | conf/rest/9.12.0/namespace.yaml |
| ZAPI | `nvme-namespace-get-iter` | `nvme-namespace-info.size-used` | conf/zapi/cdot/9.8.0/namespace.yaml |

The `namespace_size_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NVMe Namespaces | NVMe Namespaces Table | table | [NVMe Namespaces](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=99) |
///



### namespace_total_data

Performance metric aggregated over all types of I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/namespaces` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/namespace.yaml |



### namespace_total_ops

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/namespaces` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/namespace.yaml |



### namespace_write_data

Performance metric for namespace write I/O operations in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| KeyPerf | `api/storage/namespaces` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |

The `namespace_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NVMe Namespaces | Highlights | timeseries | [Top $TopResources NVMe Namespaces by Write Throughput](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=52) |
///



### namespace_write_ops

Number of write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/namespace` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/namespace.yaml |
| KeyPerf | `api/storage/namespaces` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/namespace.yaml |
| ZapiPerf | `perf-object-get-instances namespace` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/namespace.yaml |

The `namespace_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NVMe Namespaces | Highlights | timeseries | [Top $TopResources NVMe Namespaces by Write IOPs](/d/cdot-nvme-namespaces/ontap3a-nvme namespaces?orgId=1&viewPanel=53) |
///



### ndmp_session_data_bytes_processed

Indicates the NDMP data bytes processed.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/ndmp/sessions` | `data.bytes_processed` | conf/rest/9.7.0/ndmp_session.yaml |



### ndmp_session_mover_bytes_moved

Indicates the NDMP mover bytes moved.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/ndmp/sessions` | `mover.bytes_moved` | conf/rest/9.7.0/ndmp_session.yaml |



### net_connection_labels

This metric provides information about NetConnections

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/network/connections/active` | `Harvest generated` | conf/rest/9.12.0/netconnections.yaml |



### net_port_mtu

Maximum transmission unit, largest packet size on this network

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/network/ethernet/ports` | `mtu` | conf/rest/9.12.0/netport.yaml |
| ZAPI | `net-port-get-iter` | `net-port-info.mtu` | conf/zapi/cdot/9.8.0/netport.yaml |

The `net_port_mtu` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | table | [Ethernet ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=59) |
///



### net_port_status

This metric indicates a value of 1 if the port state is up and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/netport.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/netport.yaml |

The `net_port_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | table | [Ethernet ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=59) |
///



### net_route_labels

This metric provides information about NetRoute

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/network/ip/routes` | `Harvest generated` | conf/rest/9.8.0/netroute.yaml |

The `net_route_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Routes | table | [Routes](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=112) |
///



### netstat_bytes_recvd

Number of bytes received by a TCP connection

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances netstat` | `bytes_recvd`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/netstat.yaml |



### netstat_bytes_sent

Number of bytes sent by a TCP connection

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances netstat` | `bytes_sent`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/netstat.yaml |



### netstat_cong_win

Congestion window of a TCP connection

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances netstat` | `cong_win`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/netstat.yaml |



### netstat_cong_win_th

Congestion window threshold of a TCP connection

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances netstat` | `cong_win_th`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/netstat.yaml |



### netstat_ooorcv_pkts

Number of out-of-order packets received by this TCP connection

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances netstat` | `ooorcv_pkts`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/netstat.yaml |



### netstat_recv_window

Receive window size of a TCP connection

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances netstat` | `recv_window`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/netstat.yaml |



### netstat_rexmit_pkts

Number of packets retransmitted by this TCP connection

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances netstat` | `rexmit_pkts`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/netstat.yaml |



### netstat_send_window

Send window size of a TCP connection

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances netstat` | `send_window`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/netstat.yaml |



### nfs_clients_idle_duration

Specifies an ISO-8601 format of date and time to retrieve the idle time duration in hours, minutes, and seconds format.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/nfs/connected-clients` | `idle_duration` | conf/rest/9.7.0/nfs_clients.yaml |

The `nfs_clients_idle_duration` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFS Clients | Highlights | stat | [Total NFS Connections](/d/cdot-nfs-clients/ontap3a-nfs clients?orgId=1&viewPanel=23) |
| ONTAP: NFS Clients | Highlights | piechart | [NFS Connections by Protocol](/d/cdot-nfs-clients/ontap3a-nfs clients?orgId=1&viewPanel=27) |
| ONTAP: NFS Clients | Highlights | table | [NFS Clients (active in the past 48 hours)](/d/cdot-nfs-clients/ontap3a-nfs clients?orgId=1&viewPanel=25) |
///



### nfs_diag_storePool_ByteLockAlloc

Current number of byte range lock objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.byte_lock_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_ByteLockAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_ByteLockAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [ByteLockAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=36) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_ByteLockMax

Maximum number of byte range lock objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.byte_lock_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_ByteLockMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_ByteLockMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [ByteLockAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=36) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_ClientAlloc

Current number of client objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.client_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_ClientAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_ClientAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [ClientAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=39) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_ClientMax

Maximum number of client objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.client_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_ClientMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_ClientMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [ClientAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=39) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_ConnectionParentSessionReferenceAlloc

Current number of connection parent session reference objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.connection_parent_session_reference_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_ConnectionParentSessionReferenceAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_ConnectionParentSessionReferenceAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [ConnectionParentSessionReferenceAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=38) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_ConnectionParentSessionReferenceMax

Maximum number of connection parent session reference objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.connection_parent_session_reference_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_ConnectionParentSessionReferenceMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_ConnectionParentSessionReferenceMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [ConnectionParentSessionReferenceAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=38) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_CopyStateAlloc

Current number of copy state objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.copy_state_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_CopyStateAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_CopyStateAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [CopyStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=37) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_CopyStateMax

Maximum number of copy state objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.copy_state_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_CopyStateMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_CopyStateMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [CopyStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=37) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_DelegAlloc

Current number of delegation lock objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.delegation_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_DelegAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_DelegAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [DelegAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=40) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_DelegMax

Maximum number delegation lock objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.delegation_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_DelegMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_DelegMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [DelegAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=40) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_DelegStateAlloc

Current number of delegation state objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.delegation_state_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_DelegStateAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_DelegStateAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [DelegStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=41) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_DelegStateMax

Maximum number of delegation state objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.delegation_state_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_DelegStateMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_DelegStateMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [DelegStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=41) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_LayoutAlloc

Current number of layout objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.layout_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_LayoutAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_LayoutAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [LayoutAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=42) |
///



### nfs_diag_storePool_LayoutMax

Maximum number of layout objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.layout_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_LayoutMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_LayoutMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [LayoutAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=42) |
///



### nfs_diag_storePool_LayoutStateAlloc

Current number of layout state objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.layout_state_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_LayoutStateAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_LayoutStateAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [LayoutStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=43) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_LayoutStateMax

Maximum number of layout state objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.layout_state_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_LayoutStateMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_LayoutStateMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [LayoutStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=43) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_LockAlloc

Represent the current number of lock objects allocated

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_LockAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_LockMax

Represent the maximum number of lock objects

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_LockMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_LockStateAlloc

Current number of lock state objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.lock_state_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_LockStateAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_LockStateAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [LockStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=44) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_LockStateMax

Maximum number of lock state objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.lock_state_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_LockStateMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_LockStateMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [LockStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=44) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_OpenAlloc

Current number of share objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.open_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_OpenAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_OpenAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [OpenAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=45) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_OpenMax

Maximum number of share lock objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.open_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_OpenMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_OpenMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [OpenAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=45) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_OpenStateAlloc

Current number of open state objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.openstate_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_OpenStateAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_OpenStateAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [OpenStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=46) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_OpenStateMax

Maximum number of open state objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.openstate_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_OpenStateMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_OpenStateMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [OpenStateAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=46) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_OwnerAlloc

Current number of owner objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.owner_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_OwnerAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_OwnerAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [OwnerAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=47) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_OwnerMax

Maximum number of owner objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.owner_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_OwnerMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_OwnerMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [OwnerAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=47) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_SessionAlloc

Current number of session objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.session_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_SessionAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_SessionAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [SessionAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=50) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_SessionConnectionHolderAlloc

Current number of session connection holder objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.session_connection_holder_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_SessionConnectionHolderAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_SessionConnectionHolderAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [SessionConnectionHolderAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=48) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_SessionConnectionHolderMax

Maximum number of session connection holder objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.session_connection_holder_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_SessionConnectionHolderMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_SessionConnectionHolderMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [SessionConnectionHolderAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=48) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_SessionHolderAlloc

Current number of session holder objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.session_holder_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_SessionHolderAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_SessionHolderAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [SessionHolderAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=49) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_SessionHolderMax

Maximum number of session holder objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.session_holder_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_SessionHolderMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_SessionHolderMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [SessionHolderAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=49) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_SessionMax

Maximum number of session objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.session_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_SessionMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_SessionMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [SessionAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=50) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_StateRefHistoryAlloc

Current number of state reference callstack history objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.state_reference_history_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_StateRefHistoryAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_StateRefHistoryAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [StateRefHistoryAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=51) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_StateRefHistoryMax

Maximum number of state reference callstack history objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.state_reference_history_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_StateRefHistoryMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_StateRefHistoryMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [StateRefHistoryAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=51) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_StringAlloc

Current number of string objects allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.string_allocated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_StringAlloc`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_StringAlloc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [StringAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=52) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nfs_diag_storePool_StringMax

Maximum number of string objects.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nfs_v4_diag` | `storepool.string_maximum`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_pool.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_diag` | `storePool_StringMax`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml |

The `nfs_diag_storePool_StringMax` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFSv4 StorePool Monitors | Allocations over 50% | timeseries | [Allocations over 50%](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=53) |
| ONTAP: NFSv4 StorePool Monitors | Lock | timeseries | [StringAlloc](/d/cdot-nfsv4-storepool-monitors/ontap3a-nfsv4 storepool monitors?orgId=1&viewPanel=52) |
| ONTAP: NFS Troubleshooting | Highlights | timeseries | [All nodes with 1% or more allocations in $Datacenter](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=2) |
///



### nic_ifgrp_rx_bytes

Link Aggregation Group (LAG) Bytes received.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_ifgrp_rx_bytes` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Link Aggregation Group (LAG) | table | [Link Aggregation Groups](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=122) |
| ONTAP: Network | Link Aggregation Group (LAG) | timeseries | [Top $TopResources LAGs by Receive Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=124) |
///



### nic_ifgrp_rx_perc

Link Aggregation Group (LAG) Bytes received percentage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_ifgrp_rx_perc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Link Aggregation Group (LAG) | table | [Link Aggregation Groups](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=122) |
///



### nic_ifgrp_tx_bytes

Link Aggregation Group (LAG) Bytes sent.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_ifgrp_tx_bytes` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Link Aggregation Group (LAG) | table | [Link Aggregation Groups](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=122) |
| ONTAP: Network | Link Aggregation Group (LAG) | timeseries | [Top $TopResources LAGs by Send Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=123) |
///



### nic_ifgrp_tx_perc

Link Aggregation Group (LAG) Bytes sent percentage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_ifgrp_tx_perc` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Link Aggregation Group (LAG) | table | [Link Aggregation Groups](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=122) |
///



### nic_labels

This metric provides information about NicCommon

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZAPI | `nic_common` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | table | [NIC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=58) |
| ONTAP: Network | Ethernet | table | [Ethernet port errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=119) |
| ONTAP: NFS Troubleshooting | Network Port Table | table | [Ethernet ports](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=19) |
///



### nic_link_up_to_downs

Number of link state change from UP to DOWN.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `link_up_to_down`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `link_up_to_downs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_link_up_to_downs` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | table | [Ethernet port errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=119) |
| ONTAP: NFS Troubleshooting | Network Port Table | table | [Ethernet ports](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=19) |
///



### nic_new_status

This metric indicates a value of 1 if the NIC state is up (indicating the NIC is operational) and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_new_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | table | [NIC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=58) |
| ONTAP: NFS Troubleshooting | Network Port Table | table | [Ethernet ports](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=19) |
///



### nic_rx_alignment_errors

Alignment errors detected on received packets

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `receive_alignment_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `rx_alignment_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_rx_alignment_errors` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | timeseries | [NICs Receive Errors by Cluster](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=30) |
| ONTAP: Network | Ethernet | table | [Ethernet port errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=119) |
///



### nic_rx_bytes

Received in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `receive_bytes`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `rx_bytes`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_rx_bytes` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Highlights | stat | [Ethernet Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=26) |
| ONTAP: Network | Highlights | stat | [Ethernet Receive](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=27) |
| ONTAP: Network | Ethernet | table | [NIC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=58) |
| ONTAP: Network | Ethernet | timeseries | [Top $TopResources NICs by Receive Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=28) |
| ONTAP: NFS Troubleshooting | Network Port Table | table | [Ethernet ports](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=19) |
| ONTAP: Node | Network Layer | timeseries | [Top $TopResources Ethernet Ports by Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=111) |
///



### nic_rx_crc_errors

CRC errors detected on received packets

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `receive_crc_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `rx_crc_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_rx_crc_errors` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | timeseries | [NICs Receive Errors by Cluster](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=30) |
| ONTAP: Network | Ethernet | table | [Ethernet port errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=119) |
///



### nic_rx_errors

Receive errors in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `receive_errors`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `rx_errors`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |



### nic_rx_length_errors

Length errors detected on received packets

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `receive_length_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `rx_length_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_rx_length_errors` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | timeseries | [NICs Receive Errors by Cluster](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=30) |
| ONTAP: Network | Ethernet | table | [Ethernet port errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=119) |
///



### nic_rx_percent

Bytes received percentage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_rx_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | table | [NIC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=58) |
///



### nic_rx_total_errors

Total errors received

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `receive_total_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `rx_total_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_rx_total_errors` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | timeseries | [NICs Receive Errors by Cluster](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=30) |
| ONTAP: Network | Ethernet | table | [Ethernet port errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=119) |
| ONTAP: NFS Troubleshooting | Network Port Table | table | [Ethernet ports](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=19) |
///



### nic_tx_bytes

Sent in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `transmit_bytes`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `tx_bytes`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_tx_bytes` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Highlights | stat | [Ethernet Send](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=94) |
| ONTAP: Network | Ethernet | table | [NIC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=58) |
| ONTAP: Network | Ethernet | timeseries | [Top $TopResources NICs by Send Throughput](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=12) |
| ONTAP: NFS Troubleshooting | Network Port Table | table | [Ethernet ports](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=19) |
| ONTAP: Node | Network Layer | timeseries | [Top $TopResources Ethernet Ports by Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=111) |
///



### nic_tx_errors

Sent errors in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `transmit_errors`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `tx_errors`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |



### nic_tx_hw_errors

Transmit errors reported by hardware

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `transmit_hw_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `tx_hw_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_tx_hw_errors` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | timeseries | [NICs Send Errors by Cluster](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=29) |
| ONTAP: Network | Ethernet | table | [Ethernet port errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=119) |
///



### nic_tx_percent

Bytes sent percentage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_tx_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | table | [NIC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=58) |
///



### nic_tx_total_errors

Total errors sent

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nic_common` | `transmit_total_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `perf-object-get-instances nic_common` | `tx_total_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_tx_total_errors` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | timeseries | [NICs Send Errors by Cluster](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=29) |
| ONTAP: Network | Ethernet | table | [Ethernet port errors](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=119) |
| ONTAP: NFS Troubleshooting | Network Port Table | table | [Ethernet ports](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=19) |
///



### nic_util_percent

Max of Bytes received percentage and Bytes sent percentage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/nic_common.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nic_common.yaml |

The `nic_util_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Network | Ethernet | table | [NIC ports](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=58) |
| ONTAP: Network | Ethernet | timeseries | [Top $TopResources NICs by Port Utilization %](/d/cdot-network/ontap3a-network?orgId=1&viewPanel=61) |
| ONTAP: NFS Troubleshooting | Network Port Table | table | [Ethernet ports](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=19) |
| ONTAP: Node | Network Layer | timeseries | [Top $TopResources Ethernet Ports by Utilization %](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=109) |
///



### node_avg_processor_busy

Average processor utilization across active processors in the system

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `average_processor_busy_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> cpu_elapsed_time | conf/restperf/9.12.0/system_node.yaml |
| KeyPerf | `api/cluster/nodes` | `statistics.processor_utilization_raw`<br><span class="key">Unit:</span> statistics.processor_utilization_base<br><span class="key">Type:</span> percent<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/system_node.yaml |
| StatPerf | `system:node` | `avg_processor_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> <br><span class="key">Base:</span> cpu_elapsed_time | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `avg_processor_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> cpu_elapsed_time | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_avg_processor_busy` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | Cluster Metrics | timeseries | [Top $TopResources Clusters by Average CPU Utilization](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=247) |
| ONTAP: Cluster | Highlights | table | [Top $TopResources Nodes by Average CPU Utilization](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=236) |
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | bargauge | [Average CPU Utilization](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=240) |
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | timeseries | [Node Average CPU Utilization](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=242) |
| ONTAP: Cluster | Throughput | timeseries | [Average CPU Utilization](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=2) |
| ONTAP: Datacenter | Performance | timeseries | [Top $TopResources Average CPU Utilization by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=2) |
| ONTAP: MetroCluster | Highlights | gauge | [Average CPU Utilization](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=34) |
| ONTAP: Node | Highlights | bargauge | [Average CPU Utilization](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=129) |
| ONTAP: Node | CPU Layer | timeseries | [Average CPU Utilization](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=54) |
///



### node_cifs_connections

Number of connections

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |

The `node_cifs_connections` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | CIFS Frontend | timeseries | [CIFS Connections](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=65) |
///



### node_cifs_established_sessions

Number of established SMB and SMB2 sessions

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `established_sessions`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `established_sessions`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |

The `node_cifs_established_sessions` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | CIFS Frontend | timeseries | [CIFS Connections](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=65) |
///



### node_cifs_latency

Average latency in microseconds for CIFS operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> latency_base | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `cifs_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cifs_latency_base | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |

The `node_cifs_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | CIFS Frontend | stat | [CIFS Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=68) |
///



### node_cifs_op_count

Array of select CIFS operation counts

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `op_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `cifs_op_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |

The `node_cifs_op_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | CIFS Frontend | timeseries | [CIFS IOPs by Type](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=64) |
///



### node_cifs_open_files

Number of open files over SMB and SMB2

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `open_files`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `open_files`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |

The `node_cifs_open_files` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | CIFS Frontend | timeseries | [CIFS Connections](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=65) |
///



### node_cifs_ops

Number of CIFS operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `cifs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `cifs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `cifs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_cifs_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | Cluster Metrics | timeseries | [Top $TopResources CIFS IOPs by Cluster](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=239) |
| ONTAP: Node | Backend | timeseries | [Protocol Backend IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=42) |
| ONTAP: Node | CIFS Frontend | stat | [CIFS IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=63) |
///



### node_cifs_read_latency

Average latency in microseconds for CIFS read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_read_ops | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `cifs_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cifs_read_ops | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |



### node_cifs_read_ops

Total number of CIFS read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `total_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `cifs_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |



### node_cifs_total_ops

Total number of CIFS operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `cifs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |



### node_cifs_write_latency

Average latency in microseconds for CIFS write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_write_ops | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `cifs_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cifs_write_ops | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |



### node_cifs_write_ops

Total number of CIFS write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs:node` | `total_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_node.yaml |
| ZapiPerf | `perf-object-get-instances cifs:node` | `cifs_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_node.yaml |



### node_cpu_busy

System CPU resource utilization. Returns a computed percentage for the default CPU field. Basically computes a 'cpu usage summary' value which indicates how 'busy' the system is based upon the most heavily utilized domain. The idea is to determine the amount of available CPU until we're limited by either a domain maxing out OR we exhaust all available idle CPU cycles, whichever occurs first.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `cpu_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> cpu_elapsed_time | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `cpu_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> <br><span class="key">Base:</span> cpu_elapsed_time | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `cpu_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> cpu_elapsed_time | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_cpu_busy` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | Cluster Metrics | timeseries | [Top $TopResources Clusters by CPU busy](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=271) |
| ONTAP: Cluster | Highlights | table | [Top $TopResources Nodes by CPU busy](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=239) |
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | bargauge | [CPU busy](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=181) |
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | timeseries | [Node CPU Busy](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=222) |
| ONTAP: Cluster | Throughput | timeseries | [CPU Busy](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=241) |
| ONTAP: Datacenter | Performance | timeseries | [Top $TopResources CPU Busy by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=241) |
| ONTAP: Node | Highlights | bargauge | [CPU Busy](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=24) |
| ONTAP: Node | Backend | timeseries | [System Utilization](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=41) |
///



### node_cpu_busytime

The time (in hundredths of a second) that the CPU has been doing useful work since the last boot

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/node` | `cpu_busy_time` | conf/rest/9.12.0/node.yaml |
| ZAPI | `system-node-get-iter` | `node-details-info.cpu-busytime` | conf/zapi/cdot/9.8.0/node.yaml |



### node_cpu_domain_busy

Array of processor time in percentage spent in various domains

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `domain_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> cpu_elapsed_time | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `domain_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> array<br><span class="key">Base:</span> cpu_elapsed_time | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `domain_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> cpu_elapsed_time | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_cpu_domain_busy` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | CPU Layer | timeseries | [CPU Busy Domains](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=55) |
| ONTAP: Node | Backend | timeseries | [System Utilization](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=41) |
///



### node_cpu_elapsed_time

Elapsed time since boot

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `cpu_elapsed_time`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `cpu_elapsed_time`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `cpu_elapsed_time`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-display<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_disk_busy

The utilization percent of the disk. node_disk_busy is [disk_busy](#disk_busy) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `disk_busy_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `node_disk_busy` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Highlights | table | [Top $TopResources Nodes by Disk Utilization](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=237) |
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | bargauge | [Avg Disk Utilization by Cluster](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=220) |
| ONTAP: Node | Backend | timeseries | [System Utilization](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=41) |
///



### node_disk_capacity

Disk capacity in MB. node_disk_capacity is [disk_capacity](#disk_capacity) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_cp_read_chain

Average number of blocks transferred in each consistency point read operation during a CP. node_disk_cp_read_chain is [disk_cp_read_chain](#disk_cp_read_chain) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_cp_read_latency

Average latency per block in microseconds for consistency point read operations. node_disk_cp_read_latency is [disk_cp_read_latency](#disk_cp_read_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_cp_reads

Number of disk read operations initiated each second for consistency point processing. node_disk_cp_reads is [disk_cp_reads](#disk_cp_reads) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_data_read

Number of disk kilobytes (KB) read per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `disk_data_read`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `disk_data_read`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `disk_data_read`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_disk_data_read` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Disk Utilization | timeseries | [Disk Throughput by Node](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=32) |
///



### node_disk_data_written

Number of disk kilobytes (KB) written per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `disk_data_written`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `disk_data_written`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `disk_data_written`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_disk_data_written` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Disk Utilization | timeseries | [Disk Throughput by Node](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=32) |
///



### node_disk_io_pending

Average number of I/Os issued to the disk for which we have not yet received the response. node_disk_io_pending is [disk_io_pending](#disk_io_pending) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_io_queued

Number of I/Os queued to the disk but not yet issued. node_disk_io_queued is [disk_io_queued](#disk_io_queued) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_busy

The utilization percent of the disk. node_disk_max_busy is the maximum of [disk_busy](#disk_busy) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `disk_busy_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `node_disk_max_busy` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | bargauge | [Max Disk Utilization by Cluster](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=157) |
| ONTAP: Node | Highlights | bargauge | [Max Disk Utilization](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=40) |
///



### node_disk_max_capacity

Disk capacity in MB. node_disk_max_capacity is the maximum of [disk_capacity](#disk_capacity) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_cp_read_chain

Average number of blocks transferred in each consistency point read operation during a CP. node_disk_max_cp_read_chain is the maximum of [disk_cp_read_chain](#disk_cp_read_chain) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_cp_read_latency

Average latency per block in microseconds for consistency point read operations. node_disk_max_cp_read_latency is the maximum of [disk_cp_read_latency](#disk_cp_read_latency) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_cp_reads

Number of disk read operations initiated each second for consistency point processing. node_disk_max_cp_reads is the maximum of [disk_cp_reads](#disk_cp_reads) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_io_pending

Average number of I/Os issued to the disk for which we have not yet received the response. node_disk_max_io_pending is the maximum of [disk_io_pending](#disk_io_pending) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_io_queued

Number of I/Os queued to the disk but not yet issued. node_disk_max_io_queued is the maximum of [disk_io_queued](#disk_io_queued) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_total_data

Total throughput for user operations per second. node_disk_max_total_data is the maximum of [disk_total_data](#disk_total_data) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_total_transfers

Total number of disk operations involving data transfer initiated per second. node_disk_max_total_transfers is the maximum of [disk_total_transfers](#disk_total_transfers) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_transfer_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_transfers`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_user_read_blocks

Number of blocks transferred for user read operations per second. node_disk_max_user_read_blocks is the maximum of [disk_user_read_blocks](#disk_user_read_blocks) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_user_read_chain

Average number of blocks transferred in each user read operation. node_disk_max_user_read_chain is the maximum of [disk_user_read_chain](#disk_user_read_chain) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_user_read_latency

Average latency per block in microseconds for user read operations. node_disk_max_user_read_latency is the maximum of [disk_user_read_latency](#disk_user_read_latency) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_user_reads

Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. node_disk_max_user_reads is the maximum of [disk_user_reads](#disk_user_reads) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_user_write_blocks

Number of blocks transferred for user write operations per second. node_disk_max_user_write_blocks is the maximum of [disk_user_write_blocks](#disk_user_write_blocks) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_user_write_chain

Average number of blocks transferred in each user write operation. node_disk_max_user_write_chain is the maximum of [disk_user_write_chain](#disk_user_write_chain) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_writes | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_user_write_latency

Average latency per block in microseconds for user write operations. node_disk_max_user_write_latency is the maximum of [disk_user_write_latency](#disk_user_write_latency) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_max_user_writes

Number of disk write operations initiated each second for storing data or metadata associated with user requests. node_disk_max_user_writes is the maximum of [disk_user_writes](#disk_user_writes) for label `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_writes`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_total_data

Total throughput for user operations per second. node_disk_total_data is [disk_total_data](#disk_total_data) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_total_transfers

Total number of disk operations involving data transfer initiated per second. node_disk_total_transfers is [disk_total_transfers](#disk_total_transfers) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_transfer_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_transfers`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_user_read_blocks

Number of blocks transferred for user read operations per second. node_disk_user_read_blocks is [disk_user_read_blocks](#disk_user_read_blocks) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_user_read_chain

Average number of blocks transferred in each user read operation. node_disk_user_read_chain is [disk_user_read_chain](#disk_user_read_chain) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_user_read_latency

Average latency per block in microseconds for user read operations. node_disk_user_read_latency is [disk_user_read_latency](#disk_user_read_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_user_reads

Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. node_disk_user_reads is [disk_user_reads](#disk_user_reads) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_user_write_blocks

Number of blocks transferred for user write operations per second. node_disk_user_write_blocks is [disk_user_write_blocks](#disk_user_write_blocks) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_user_write_chain

Average number of blocks transferred in each user write operation. node_disk_user_write_chain is [disk_user_write_chain](#disk_user_write_chain) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_writes | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_user_write_latency

Average latency per block in microseconds for user write operations. node_disk_user_write_latency is [disk_user_write_latency](#disk_user_write_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_disk_user_writes

Number of disk write operations initiated each second for storing data or metadata associated with user requests. node_disk_user_writes is [disk_user_writes](#disk_user_writes) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_writes`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### node_failed_fan

Specifies a count of the number of chassis fans that are not operating within the recommended RPM range.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cluster/nodes` | `controller.failed_fan.count` | conf/rest/9.12.0/node.yaml |
| ZAPI | `system-node-get-iter` | `node-details-info.env-failed-fan-count` | conf/zapi/cdot/9.8.0/node.yaml |

The `node_failed_fan` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Highlights | table | [Node Details](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=138) |
///



### node_failed_power

Number of failed power supply units.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cluster/nodes` | `controller.failed_power_supply.count` | conf/rest/9.12.0/node.yaml |
| ZAPI | `system-node-get-iter` | `node-details-info.env-failed-power-supply-count` | conf/zapi/cdot/9.8.0/node.yaml |

The `node_failed_power` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Highlights | table | [Node Details](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=138) |
///



### node_fcp_data_recv

Number of FCP kilobytes (KB) received per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `fcp_data_received`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `fcp_data_recv`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `fcp_data_recv`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_fcp_data_sent

Number of FCP kilobytes (KB) sent per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `fcp_data_sent`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `fcp_data_sent`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `fcp_data_sent`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_fcp_ops

Number of FCP operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `fcp_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `fcp_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `fcp_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_fcp_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Backend | timeseries | [Protocol Backend IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=42) |
///



### node_hdd_data_read

Number of HDD Disk kilobytes (KB) read per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `hdd_data_read`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `hdd_data_read`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `hdd_data_read`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_hdd_data_written

Number of HDD kilobytes (KB) written per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `hdd_data_written`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `hdd_data_written`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `hdd_data_written`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_iscsi_ops

Number of iSCSI operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `iscsi_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `iscsi_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `iscsi_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_iscsi_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Backend | timeseries | [Protocol Backend IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=42) |
///



### node_labels

This metric provides information about Node

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cluster/nodes` | `Harvest generated` | conf/rest/9.12.0/node.yaml |
| ZAPI | `system-node-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/node.yaml |

The `node_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Highlights | table | [Aggregates](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=2) |
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | table | [$Cluster](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=52) |
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: Datacenter | Health | table | [Node Health](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=528) |
| ONTAP: Datacenter | Power and Temperature | stat | [Average Power/Used_TB](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=640) |
| ONTAP: Health | HA | table | [HA Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=281) |
| ONTAP: Health | Node | table | [Node Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=253) |
| ONTAP: Node | Highlights | table | [Node Details](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=138) |
| ONTAP: Power | Highlights | stat | [Average Power/Used_TB](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=71) |
| ONTAP: Power | Nodes | table | [Storage Nodes](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=33) |
///



### node_memory

Total memory in megabytes (MB)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `memory`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `memory`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `memory`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_net_data_recv

Number of network kilobytes (KB) received per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `network_data_received`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `net_data_recv`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `net_data_recv`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_net_data_sent

Number of network kilobytes (KB) sent per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `network_data_sent`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `net_data_sent`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `net_data_sent`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_new_status

This metric indicates a value of 1 if the node is healthy (true or up, indicating the node is operational) and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/node.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/node.yaml |

The `node_new_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Nodes & Subsystems - $Cluster | table | [$Cluster](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=52) |
| ONTAP: Datacenter | Health | table | [Node Health](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=528) |
| ONTAP: Node | Highlights | stat | [Nodes](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=15) |
///



### node_nfs_access_avg_latency

Average latency in microseconds of Access procedure requests. The counter keeps track of the average response time of Access requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `access.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> access.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `access.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> access.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `access.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> access.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `access.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> access.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `access_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> access_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `access_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> access_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `access_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> access_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `access_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> access_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_access_total

Total number of Access procedure requests. It is the total number of access success and access error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `access.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `access.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `access.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `access.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `access_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `access_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `access_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `access_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_backchannel_ctl_avg_latency

Average latency in microseconds of BACKCHANNEL_CTL operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `backchannel_ctl.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> backchannel_ctl.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `backchannel_ctl.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> backchannel_ctl.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `backchannel_ctl_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> backchannel_ctl_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `backchannel_ctl_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> backchannel_ctl_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_backchannel_ctl_total

Total number of BACKCHANNEL_CTL operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `backchannel_ctl.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `backchannel_ctl.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `backchannel_ctl_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `backchannel_ctl_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_bind_conn_to_session_avg_latency

Average latency in microseconds of BIND_CONN_TO_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `bind_connections_to_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> bind_connections_to_session.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `bind_conn_to_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> bind_conn_to_session.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `bind_conn_to_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> bind_conn_to_session_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `bind_conn_to_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> bind_conn_to_session_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_bind_conn_to_session_total

Total number of BIND_CONN_TO_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `bind_connections_to_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `bind_conn_to_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `bind_conn_to_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `bind_conn_to_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_close_avg_latency

Average latency in microseconds of CLOSE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `close.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> close.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `close.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> close.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `close.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> close.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `close_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> close_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `close_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> close_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `close_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> close_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_close_total

Total number of CLOSE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `close.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `close.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `close.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `close_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `close_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `close_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_commit_avg_latency

Average latency in microseconds of Commit procedure requests. The counter keeps track of the average response time of Commit requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `commit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> commit.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `commit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> commit.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `commit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> commit.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `commit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> commit.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `commit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> commit_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `commit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> commit_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `commit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> commit_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `commit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> commit_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_commit_total

Total number of Commit procedure requests. It is the total number of Commit success and Commit error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `commit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `commit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `commit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `commit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `commit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `commit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `commit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `commit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_create_avg_latency

Average latency in microseconds of Create procedure requests. The counter keeps track of the average response time of Create requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `create.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `create.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `create.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `create.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `create_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `create_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `create_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `create_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_create_session_avg_latency

Average latency in microseconds of CREATE_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `create_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create_session.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `create_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create_session.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `create_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_session_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `create_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_session_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_create_session_total

Total number of CREATE_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `create_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `create_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `create_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `create_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_create_total

Total number Create of procedure requests. It is the total number of create success and create error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `create.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `create.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `create.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `create.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `create_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `create_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `create_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `create_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_delegpurge_avg_latency

Average latency in microseconds of DELEGPURGE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `delegpurge.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegpurge.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `delegpurge.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegpurge.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `delegpurge.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegpurge.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `delegpurge_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegpurge_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `delegpurge_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegpurge_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `delegpurge_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegpurge_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_delegpurge_total

Total number of DELEGPURGE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `delegpurge.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `delegpurge.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `delegpurge.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `delegpurge_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `delegpurge_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `delegpurge_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_delegreturn_avg_latency

Average latency in microseconds of DELEGRETURN operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `delegreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegreturn.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `delegreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegreturn.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `delegreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegreturn.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `delegreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegreturn_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `delegreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegreturn_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `delegreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegreturn_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_delegreturn_total

Total number of DELEGRETURN operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `delegreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `delegreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `delegreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `delegreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `delegreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `delegreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_destroy_clientid_avg_latency

Average latency in microseconds of DESTROY_CLIENTID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `destroy_clientid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> destroy_clientid.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `destroy_clientid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> destroy_clientid.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `destroy_clientid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> destroy_clientid_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `destroy_clientid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> destroy_clientid_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_destroy_clientid_total

Total number of DESTROY_CLIENTID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `destroy_clientid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `destroy_clientid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `destroy_clientid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `destroy_clientid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_destroy_session_avg_latency

Average latency in microseconds of DESTROY_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `destroy_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> destroy_session.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `destroy_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> destroy_session.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `destroy_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> destroy_session_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `destroy_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> destroy_session_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_destroy_session_total

Total number of DESTROY_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `destroy_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `destroy_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `destroy_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `destroy_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_exchange_id_avg_latency

Average latency in microseconds of EXCHANGE_ID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `exchange_id.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> exchange_id.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `exchange_id.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> exchange_id.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `exchange_id_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> exchange_id_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `exchange_id_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> exchange_id_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_exchange_id_total

Total number of EXCHANGE_ID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `exchange_id.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `exchange_id.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `exchange_id_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `exchange_id_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_free_stateid_avg_latency

Average latency in microseconds of FREE_STATEID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `free_stateid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> free_stateid.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `free_stateid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> free_stateid.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `free_stateid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> free_stateid_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `free_stateid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> free_stateid_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_free_stateid_total

Total number of FREE_STATEID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `free_stateid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `free_stateid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `free_stateid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `free_stateid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_fsinfo_avg_latency

Average latency in microseconds of FSInfo procedure requests. The counter keeps track of the average response time of FSInfo requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `fsinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> fsinfo.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `fsinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> fsinfo_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_fsinfo_total

Total number FSInfo of procedure requests. It is the total number of FSInfo success and FSInfo error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `fsinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `fsinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_fsstat_avg_latency

Average latency in microseconds of FSStat procedure requests. The counter keeps track of the average response time of FSStat requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `fsstat.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> fsstat.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `fsstat_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> fsstat_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_fsstat_total

Total number FSStat of procedure requests. It is the total number of FSStat success and FSStat error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `fsstat.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `fsstat_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_get_dir_delegation_avg_latency

Average latency in microseconds of GET_DIR_DELEGATION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `get_dir_delegation.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> get_dir_delegation.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `get_dir_delegation.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> get_dir_delegation.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `get_dir_delegation_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> get_dir_delegation_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `get_dir_delegation_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> get_dir_delegation_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_get_dir_delegation_total

Total number of GET_DIR_DELEGATION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `get_dir_delegation.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `get_dir_delegation.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `get_dir_delegation_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `get_dir_delegation_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_getattr_avg_latency

Average latency in microseconds of GetAttr procedure requests. This counter keeps track of the average response time of GetAttr requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `getattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getattr.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `getattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getattr.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `getattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getattr.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `getattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getattr.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `getattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getattr_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `getattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getattr_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `getattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getattr_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `getattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getattr_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_getattr_total

Total number of Getattr procedure requests. It is the total number of getattr success and getattr error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `getattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `getattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `getattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `getattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `getattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `getattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `getattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `getattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_getdeviceinfo_avg_latency

Average latency in microseconds of GETDEVICEINFO operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `getdeviceinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getdeviceinfo.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `getdeviceinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getdeviceinfo.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `getdeviceinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getdeviceinfo_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `getdeviceinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getdeviceinfo_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_getdeviceinfo_total

Total number of GETDEVICEINFO operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `getdeviceinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `getdeviceinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `getdeviceinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `getdeviceinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_getdevicelist_avg_latency

Average latency in microseconds of GETDEVICELIST operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `getdevicelist.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getdevicelist.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `getdevicelist.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getdevicelist.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `getdevicelist_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getdevicelist_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `getdevicelist_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getdevicelist_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_getdevicelist_total

Total number of GETDEVICELIST operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `getdevicelist.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `getdevicelist.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `getdevicelist_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `getdevicelist_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_getfh_avg_latency

Average latency in microseconds of GETFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `getfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getfh.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `getfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getfh.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `getfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getfh.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `getfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `getfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getfh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `getfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_getfh_total

Total number of GETFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `getfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `getfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `getfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `getfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `getfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `getfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_latency

Average latency in microseconds of NFSv3 requests. This counter keeps track of the average response time of NFSv3 requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |

The `node_nfs_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NFSv3 Frontend | stat | [NFSv3 Avg Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=92) |
///



### node_nfs_layoutcommit_avg_latency

Average latency in microseconds of LAYOUTCOMMIT operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `layoutcommit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutcommit.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `layoutcommit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutcommit.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `layoutcommit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutcommit_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `layoutcommit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutcommit_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_layoutcommit_total

Total number of LAYOUTCOMMIT operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `layoutcommit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `layoutcommit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `layoutcommit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `layoutcommit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_layoutget_avg_latency

Average latency in microseconds of LAYOUTGET operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `layoutget.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutget.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `layoutget.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutget.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `layoutget_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutget_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `layoutget_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutget_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_layoutget_total

Total number of LAYOUTGET operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `layoutget.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `layoutget.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `layoutget_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `layoutget_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_layoutreturn_avg_latency

Average latency in microseconds of LAYOUTRETURN operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `layoutreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutreturn.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `layoutreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutreturn.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `layoutreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutreturn_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `layoutreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutreturn_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_layoutreturn_total

Total number of LAYOUTRETURN operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `layoutreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `layoutreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `layoutreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `layoutreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_link_avg_latency

Average latency in microseconds of Link procedure requests. The counter keeps track of the average response time of Link requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `link.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> link.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `link.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> link.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `link.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> link.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `link.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> link.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `link_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> link_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `link_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> link_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `link_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> link_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `link_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> link_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_link_total

Total number Link of procedure requests. It is the total number of Link success and Link error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `link.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `link.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `link.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `link.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `link_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `link_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `link_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `link_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_lock_avg_latency

Average latency in microseconds of LOCK operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `lock.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lock.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `lock.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lock.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `lock.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lock.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `lock_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lock_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `lock_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lock_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `lock_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lock_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_lock_total

Total number of LOCK operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `lock.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `lock.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `lock.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `lock_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `lock_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `lock_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_lockt_avg_latency

Average latency in microseconds of LOCKT operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `lockt.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lockt.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `lockt.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lockt.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `lockt.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lockt.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `lockt_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lockt_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `lockt_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lockt_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `lockt_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lockt_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_lockt_total

Total number of LOCKT operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `lockt.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `lockt.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `lockt.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `lockt_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `lockt_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `lockt_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_locku_avg_latency

Average latency in microseconds of LOCKU operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `locku.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> locku.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `locku.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> locku.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `locku.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> locku.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `locku_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> locku_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `locku_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> locku_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `locku_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> locku_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_locku_total

Total number of LOCKU operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `locku.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `locku.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `locku.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `locku_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `locku_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `locku_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_lookup_avg_latency

Average latency in microseconds of LookUp procedure requests. This shows the average time it takes for the LookUp operation to reply to the request.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `lookup.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookup.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `lookup.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookup.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `lookup.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookup.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `lookup.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookup.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `lookup_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookup_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `lookup_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookup_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `lookup_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookup_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `lookup_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookup_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_lookup_total

Total number of Lookup procedure requests. It is the total number of lookup success and lookup error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `lookup.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `lookup.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `lookup.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `lookup.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `lookup_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `lookup_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `lookup_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `lookup_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_lookupp_avg_latency

Average latency in microseconds of LOOKUPP operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `lookupp.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookupp.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `lookupp.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookupp.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `lookupp.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookupp.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `lookupp_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookupp_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `lookupp_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookupp_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `lookupp_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookupp_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_lookupp_total

Total number of LOOKUPP operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `lookupp.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `lookupp.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `lookupp.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `lookupp_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `lookupp_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `lookupp_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_mkdir_avg_latency

Average latency in microseconds of MkDir procedure requests. The counter keeps track of the average response time of MkDir requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `mkdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> mkdir.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `mkdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> mkdir_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_mkdir_total

Total number MkDir of procedure requests. It is the total number of MkDir success and MkDir error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `mkdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `mkdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_mknod_avg_latency

Average latency in microseconds of MkNod procedure requests. The counter keeps track of the average response time of MkNod requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `mknod.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> mknod.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `mknod_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> mknod_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_mknod_total

Total number MkNod of procedure requests. It is the total number of MkNod success and MkNod error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `mknod.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `mknod_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_null_avg_latency

Average latency in microseconds of Null procedure requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `null.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> null.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `null.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> null.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `null.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> null.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `null.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> null.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `null_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> null_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `null_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> null_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `null_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> null_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `null_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> null_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_null_total

Total number of Null procedure requests. It is the total of null success and null error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `null.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `null.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `null.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `null.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `null_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `null_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `null_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `null_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_nverify_avg_latency

Average latency in microseconds of NVERIFY operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `nverify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nverify.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `nverify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nverify.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `nverify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nverify.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `nverify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> nverify_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `nverify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> nverify_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `nverify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> nverify_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_nverify_total

Total number of NVERIFY operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `nverify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `nverify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `nverify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `nverify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `nverify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `nverify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_open_avg_latency

Average latency in microseconds of OPEN operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `open.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `open.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `open.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `open_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `open_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `open_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_open_confirm_avg_latency

Average latency in microseconds of OPEN_CONFIRM procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `open_confirm.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open_confirm.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `open_confirm_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_confirm_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_open_confirm_total

Total number of OPEN_CONFIRM procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `open_confirm.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `open_confirm_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_open_downgrade_avg_latency

Average latency in microseconds of OPEN_DOWNGRADE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `open_downgrade.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open_downgrade.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `open_downgrade.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open_downgrade.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `open_downgrade.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open_downgrade.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `open_downgrade_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_downgrade_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `open_downgrade_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_downgrade_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `open_downgrade_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_downgrade_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_open_downgrade_total

Total number of OPEN_DOWNGRADE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `open_downgrade.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `open_downgrade.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `open_downgrade.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `open_downgrade_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `open_downgrade_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `open_downgrade_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_open_total

Total number of OPEN operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `open.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `open.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `open.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `open_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `open_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `open_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_openattr_avg_latency

Average latency in microseconds of OPENATTR operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `openattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> openattr.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `openattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> openattr.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `openattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> openattr.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `openattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> openattr_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `openattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> openattr_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `openattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> openattr_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_openattr_total

Total number of OPENATTR operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `openattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `openattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `openattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `openattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `openattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `openattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_ops

Number of NFS operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `nfs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `nfs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `nfs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_nfs_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | Cluster Metrics | timeseries | [Top $TopResources NFS IOPs by Cluster](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=241) |
| ONTAP: Node | Backend | timeseries | [Protocol Backend IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=42) |
| ONTAP: Node | NFSv3 Frontend | table | [NFS Avg IOPS](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=97) |
///



### node_nfs_pathconf_avg_latency

Average latency in microseconds of PathConf procedure requests. The counter keeps track of the average response time of PathConf requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `pathconf.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> pathconf.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `pathconf_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> pathconf_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_pathconf_total

Total number PathConf of procedure requests. It is the total number of PathConf success and PathConf error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `pathconf.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `pathconf_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_putfh_avg_latency

The number of successful PUTPUBFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `putfh.average_latency`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `putfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putfh.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `putfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putfh.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `putfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `putfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putfh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `putfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_putfh_total

Total number of PUTFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `putfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `putfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `putfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `putfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `putfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `putfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_putpubfh_avg_latency

Average latency in microseconds of PUTPUBFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `putpubfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putpubfh.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `putpubfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putpubfh.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `putpubfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putpubfh.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `putpubfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putpubfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `putpubfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putpubfh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `putpubfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putpubfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_putpubfh_total

Total number of PUTPUBFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `putpubfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `putpubfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `putpubfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `putpubfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `putpubfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `putpubfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_putrootfh_avg_latency

Average latency in microseconds of PUTROOTFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `putrootfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putrootfh.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `putrootfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putrootfh.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `putrootfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putrootfh.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `putrootfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putrootfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `putrootfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putrootfh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `putrootfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putrootfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_putrootfh_total

Total number of PUTROOTFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `putrootfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `putrootfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `putrootfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `putrootfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `putrootfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `putrootfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_read_avg_latency

Average latency in microseconds of Read procedure requests. The counter keeps track of the average response time of Read requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `read.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `read.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `read.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `read.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `read_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `read_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `read_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `read_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |

The `node_nfs_read_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NFSv3 Frontend | stat | [NFSv3 Avg Read Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=90) |
| ONTAP: Node | NFSv3 Frontend | timeseries | [NFSv3 Read and Write Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=99) |
///



### node_nfs_read_ops

Total observed NFSv3 read operations per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `nfsv3_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |

The `node_nfs_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NFSv3 Frontend | table | [NFS Avg IOPS](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=97) |
| ONTAP: Node | NFSv3 Frontend | timeseries | [NFSv3 Read and Write IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=101) |
///



### node_nfs_read_symlink_avg_latency

Average latency in microseconds of ReadSymLink procedure requests. The counter keeps track of the average response time of ReadSymLink requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `read_symlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_symlink.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `read_symlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_symlink_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_read_symlink_total

Total number of ReadSymLink procedure requests. It is the total number of read symlink success and read symlink error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `read_symlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `read_symlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_read_throughput

Rate of NFSv3 read data transfers in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `total.read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `total.read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `total.read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `nfsv3_read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `nfs41_read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `nfs42_read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `nfs4_read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |

The `node_nfs_read_throughput` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NFSv3 Frontend | table | [NFSv3 Avg Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=95) |
| ONTAP: Node | NFSv3 Frontend | timeseries | [NFSv3 Read and Write Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=100) |
///



### node_nfs_read_total

Total number Read of procedure requests. It is the total number of read success and read error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `read.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `read.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `read.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `read.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `read_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `read_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `read_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `read_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_readdir_avg_latency

Average latency in microseconds of ReadDir procedure requests. The counter keeps track of the average response time of ReadDir requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `readdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdir.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `readdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdir.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `readdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdir.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `readdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdir.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `readdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdir_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `readdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdir_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `readdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdir_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `readdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdir_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_readdir_total

Total number ReadDir of procedure requests. It is the total number of ReadDir success and ReadDir error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `readdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `readdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `readdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `readdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `readdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `readdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `readdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `readdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_readdirplus_avg_latency

Average latency in microseconds of ReadDirPlus procedure requests. The counter keeps track of the average response time of ReadDirPlus requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `readdirplus.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdirplus.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `readdirplus_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdirplus_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_readdirplus_total

Total number ReadDirPlus of procedure requests. It is the total number of ReadDirPlus success and ReadDirPlus error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `readdirplus.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `readdirplus_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_readlink_avg_latency

Average latency in microseconds of READLINK operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `readlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readlink.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `readlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readlink.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `readlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readlink.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `readlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readlink_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `readlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readlink_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `readlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readlink_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_readlink_total

Total number of READLINK operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `readlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `readlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `readlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `readlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `readlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `readlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_reclaim_complete_avg_latency

Average latency in microseconds of RECLAIM_COMPLETE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `reclaim_complete.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> reclaim_complete.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `reclaim_complete.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> reclaim_complete.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `reclaim_complete_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> reclaim_complete_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `reclaim_complete_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> reclaim_complete_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_reclaim_complete_total

Total number of RECLAIM_COMPLETE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `reclaim_complete.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `reclaim_complete.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `reclaim_complete_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `reclaim_complete_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_release_lock_owner_avg_latency

Average Latency of RELEASE_LOCKOWNER procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `release_lock_owner.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> release_lock_owner.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `release_lock_owner_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> release_lock_owner_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_release_lock_owner_total

Total number of RELEASE_LOCKOWNER procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `release_lock_owner.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `release_lock_owner_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_remove_avg_latency

Average latency in microseconds of Remove procedure requests. The counter keeps track of the average response time of Remove requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `remove.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> remove.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `remove.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> remove.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `remove.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> remove.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `remove.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> remove.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `remove_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> remove_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `remove_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> remove_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `remove_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> remove_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `remove_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> remove_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_remove_total

Total number Remove of procedure requests. It is the total number of Remove success and Remove error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `remove.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `remove.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `remove.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `remove.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `remove_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `remove_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `remove_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `remove_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_rename_avg_latency

Average latency in microseconds of Rename procedure requests. The counter keeps track of the average response time of Rename requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `rename.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rename.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `rename.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rename.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `rename.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rename.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `rename.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rename.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `rename_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rename_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `rename_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rename_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `rename_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rename_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `rename_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rename_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_rename_total

Total number Rename of procedure requests. It is the total number of Rename success and Rename error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `rename.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `rename.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `rename.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `rename.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `rename_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `rename_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `rename_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `rename_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_renew_avg_latency

Average latency in microseconds of RENEW procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `renew.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> renew.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `renew_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> renew_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_renew_total

Total number of RENEW procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `renew.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `renew_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_restorefh_avg_latency

Average latency in microseconds of RESTOREFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `restorefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> restorefh.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `restorefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> restorefh.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `restorefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> restorefh.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `restorefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> restorefh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `restorefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> restorefh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `restorefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> restorefh_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_restorefh_total

Total number of RESTOREFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `restorefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `restorefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `restorefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `restorefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `restorefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `restorefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_rmdir_avg_latency

Average latency in microseconds of RmDir procedure requests. The counter keeps track of the average response time of RmDir requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `rmdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rmdir.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `rmdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rmdir_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_rmdir_total

Total number RmDir of procedure requests. It is the total number of RmDir success and RmDir error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `rmdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `rmdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_savefh_avg_latency

Average latency in microseconds of SAVEFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `savefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> savefh.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `savefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> savefh.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `savefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> savefh.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `savefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> savefh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `savefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> savefh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `savefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> savefh_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_savefh_total

Total number of SAVEFH operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `savefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `savefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `savefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `savefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `savefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `savefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_secinfo_avg_latency

Average latency in microseconds of SECINFO operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `secinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `secinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `secinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `secinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `secinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `secinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_secinfo_no_name_avg_latency

Average latency in microseconds of SECINFO_NO_NAME operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `secinfo_no_name.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo_no_name.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `secinfo_no_name.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo_no_name.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `secinfo_no_name_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_no_name_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `secinfo_no_name_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_no_name_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_secinfo_no_name_total

Total number of SECINFO_NO_NAME operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `secinfo_no_name.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `secinfo_no_name.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `secinfo_no_name_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `secinfo_no_name_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_secinfo_total

Total number of SECINFO operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `secinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `secinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `secinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `secinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `secinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `secinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_sequence_avg_latency

Average latency in microseconds of SEQUENCE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `sequence.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> sequence.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `sequence.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> sequence.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `sequence_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> sequence_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `sequence_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> sequence_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_sequence_total

Total number of SEQUENCE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `sequence.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `sequence.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `sequence_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `sequence_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_set_ssv_avg_latency

Average latency in microseconds of SET_SSV operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `set_ssv.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> set_ssv.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `set_ssv.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> set_ssv.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `set_ssv_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> set_ssv_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `set_ssv_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> set_ssv_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_set_ssv_total

Total number of SET_SSV operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `set_ssv.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `set_ssv.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `set_ssv_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `set_ssv_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_setattr_avg_latency

Average latency in microseconds of SetAttr procedure requests. The counter keeps track of the average response time of SetAttr requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `setattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setattr.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `setattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setattr.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `setattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setattr.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `setattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setattr.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `setattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setattr_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `setattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setattr_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `setattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setattr_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `setattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setattr_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_setattr_total

Total number of Setattr procedure requests. It is the total number of Setattr success and setattr error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `setattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `setattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `setattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `setattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `setattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `setattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `setattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `setattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_setclientid_avg_latency

Average latency in microseconds of SETCLIENTID procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `setclientid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setclientid.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `setclientid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setclientid_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_setclientid_confirm_avg_latency

Average latency in microseconds of SETCLIENTID_CONFIRM procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `setclientid_confirm.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setclientid_confirm.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `setclientid_confirm_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setclientid_confirm_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_setclientid_confirm_total

Total number of SETCLIENTID_CONFIRM procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `setclientid_confirm.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `setclientid_confirm_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_setclientid_total

Total number of SETCLIENTID procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `setclientid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `setclientid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_symlink_avg_latency

Average latency in microseconds of SymLink procedure requests. The counter keeps track of the average response time of SymLink requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `symlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> symlink.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `symlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> symlink_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_symlink_total

Total number SymLink of procedure requests. It is the total number of SymLink success and create SymLink requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `symlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `symlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |



### node_nfs_test_stateid_avg_latency

Average latency in microseconds of TEST_STATEID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `test_stateid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> test_stateid.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `test_stateid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> test_stateid.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `test_stateid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> test_stateid_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `test_stateid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> test_stateid_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_test_stateid_total

Total number of TEST_STATEID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `test_stateid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `test_stateid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `test_stateid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `test_stateid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_throughput

Rate of NFSv3 data transfers in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `total.throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `total.throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `total.throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `nfsv3_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `nfs41_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `nfs42_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `nfs4_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |

The `node_nfs_throughput` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NFSv3 Frontend | table | [NFSv3 Avg Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=95) |
///



### node_nfs_total_ops

Total number of NFSv3 procedure requests per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `nfsv3_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |

The `node_nfs_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NFSv3 Frontend | timeseries | [NFSv3 Read and Write IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=101) |
///



### node_nfs_verify_avg_latency

Average latency in microseconds of VERIFY operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `verify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> verify.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `verify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> verify.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `verify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> verify.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `verify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> verify_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `verify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> verify_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `verify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> verify_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_verify_total

Total number of VERIFY operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `verify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `verify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `verify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `verify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `verify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `verify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nfs_want_delegation_avg_latency

Average latency in microseconds of WANT_DELEGATION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `want_delegation.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> want_delegation.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `want_delegation.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> want_delegation.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `want_delegation_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> want_delegation_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `want_delegation_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> want_delegation_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_want_delegation_total

Total number of WANT_DELEGATION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `want_delegation.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `want_delegation.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `want_delegation_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `want_delegation_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |



### node_nfs_write_avg_latency

Average latency in microseconds of Write procedure requests. The counter keeps track of the average response time of Write requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `write.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write.total | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `write.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write.total | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `write.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write.total | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `write.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write.total | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `write_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> write_total | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `write_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> write_total | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `write_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> write_total | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `write_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> write_total | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |

The `node_nfs_write_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NFSv3 Frontend | stat | [NFSv3 Avg Write Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=93) |
| ONTAP: Node | NFSv3 Frontend | timeseries | [NFSv3 Read and Write Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=99) |
///



### node_nfs_write_ops

Total observed NFSv3 write operations per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `nfsv3_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |

The `node_nfs_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NFSv3 Frontend | table | [NFS Avg IOPS](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=97) |
| ONTAP: Node | NFSv3 Frontend | timeseries | [NFSv3 Read and Write IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=101) |
///



### node_nfs_write_throughput

Rate of NFSv3 write data transfers in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `total.write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `total.write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `total.write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `nfsv3_write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `nfs41_write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `nfs42_write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `nfs4_write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |

The `node_nfs_write_throughput` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NFSv3 Frontend | table | [NFSv3 Avg Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=95) |
| ONTAP: Node | NFSv3 Frontend | timeseries | [NFSv3 Read and Write Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=100) |
///



### node_nfs_write_total

Total number of Write procedure requests. It is the total number of write success and write error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3:node` | `write.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41:node` | `write.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42:node` | `write.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2_node.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4:node` | `write.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3:node` | `write_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1:node` | `write_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2:node` | `write_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2_node.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4:node` | `write_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml |



### node_nvme_fc_data_recv

NVMe/FC kilobytes (KB) received per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `nvme_fc_data_received`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `nvme_fc_data_recv`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.15.1/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `nvme_fc_data_recv`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.15.1/system_node.yaml |



### node_nvme_fc_data_sent

NVMe/FC kilobytes (KB) sent per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `nvme_fc_data_sent`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `nvme_fc_data_sent`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `nvme_fc_data_sent`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.15.1/system_node.yaml |



### node_nvme_fc_ops

NVMe/FC operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `nvme_fc_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `nvme_fc_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `nvme_fc_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.15.1/system_node.yaml |



### node_nvmf_data_recv

NVMe/FC kilobytes (KB) received per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `nvme_fc_data_received, 1`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `nvme_fc_data_recv, 1`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.15.1/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `nvmf_data_recv`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_nvmf_data_sent

NVMe/FC kilobytes (KB) sent per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `nvme_fc_data_sent, 1`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `nvme_fc_data_sent, 1`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.15.1/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `nvmf_data_sent`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_nvmf_ops

NVMe/FC operations per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `nvme_fc_ops, 1`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `nvme_fc_ops, 1`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.15.1/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `nvmf_ops`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_nvmf_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Backend | timeseries | [Protocol Backend IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=42) |
///



### node_other_data

Other throughput in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `other_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `other_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `other_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_other_latency

Average latency for all other operations in the system in microseconds

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> <br><span class="key">Base:</span> other_ops | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_other_ops

All other operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_read_data

Read throughput in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_read_latency

Average latency for all read operations in the system in microseconds

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> <br><span class="key">Base:</span> read_ops | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_read_ops

Read operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_ssd_data_read

Number of SSD Disk kilobytes (KB) read per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `ssd_data_read`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `ssd_data_read`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `ssd_data_read`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_ssd_data_written

Number of SSD Disk kilobytes (KB) written per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `ssd_data_written`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `ssd_data_written`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `ssd_data_written`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_total_data

Performance metric for total data throughput in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_total_latency

Average latency for all operations in the system in microseconds

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `total_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `total_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> <br><span class="key">Base:</span> total_ops | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `total_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_total_ops

Total number of operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |

The `node_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Power and Temperature | stat | [Average IOPs/Watt](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=642) |
| ONTAP: Power | Highlights | stat | [Average IOPs/Watt](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=96) |
///



### node_uptime

The total time, in seconds, that the node has been up.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cluster/nodes` | `uptime` | conf/rest/9.12.0/node.yaml |
| ZAPI | `system-node-get-iter` | `node-details-info.node-uptime` | conf/zapi/cdot/9.8.0/node.yaml |

The `node_uptime` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Highlights | table | [Node Details](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=138) |
///



### node_volume_avg_latency

Performance metric aggregated over all types of I/O operations. node_volume_avg_latency in microseconds is [volume_avg_latency](#volume_avg_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.total | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | Cluster Metrics | timeseries | [Top $TopResources Clusters by Max Node Latency](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=234) |
| ONTAP: Cluster | Highlights | timeseries | [Top $TopResources Nodes by Latency](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=234) |
| ONTAP: Node | Highlights | stat | [Average Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=4) |
| ONTAP: Node | Highlights | timeseries | [Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=10) |
///



### node_volume_nfs_access_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_access_latency is [volume_nfs_access_latency](#volume_nfs_access_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.access.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.access.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_access_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_access_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_access_ops

Number of operations of the given type performed on this volume. node_volume_nfs_access_ops is [volume_nfs_access_ops](#volume_nfs_access_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.access.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_access_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_getattr_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_getattr_latency is [volume_nfs_getattr_latency](#volume_nfs_getattr_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.getattr.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.getattr.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_getattr_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_getattr_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_getattr_ops

Number of operations of the given type performed on this volume. node_volume_nfs_getattr_ops is [volume_nfs_getattr_ops](#volume_nfs_getattr_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.getattr.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_getattr_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_lookup_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_lookup_latency is [volume_nfs_lookup_latency](#volume_nfs_lookup_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.lookup.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.lookup.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_lookup_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_lookup_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_lookup_ops

Number of operations of the given type performed on this volume. node_volume_nfs_lookup_ops is [volume_nfs_lookup_ops](#volume_nfs_lookup_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.lookup.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_lookup_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_other_latency

Average time for the WAFL filesystem to process other NFS operations to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency in microseconds (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_other_latency is [volume_nfs_other_latency](#volume_nfs_other_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_other_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_other_ops

Number of other NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_other_ops is [volume_nfs_other_ops](#volume_nfs_other_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_punch_hole_latency

Average time for the WAFL filesystem to process NFS protocol hole-punch requests to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_punch_hole_latency in microseconds is [volume_nfs_punch_hole_latency](#volume_nfs_punch_hole_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_punch_hole_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_punch_hole_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_punch_hole_ops

Number of NFS hole-punch requests per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_punch_hole_ops is [volume_nfs_punch_hole_ops](#volume_nfs_punch_hole_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_punch_hole_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_read_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_read_latency is [volume_nfs_read_latency](#volume_nfs_read_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.read.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.read.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_read_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_read_ops

Number of operations of the given type performed on this volume. node_volume_nfs_read_ops is [volume_nfs_read_ops](#volume_nfs_read_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.read.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_setattr_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_setattr_latency is [volume_nfs_setattr_latency](#volume_nfs_setattr_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.setattr.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.setattr.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_setattr_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_setattr_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_setattr_ops

Number of operations of the given type performed on this volume. node_volume_nfs_setattr_ops is [volume_nfs_setattr_ops](#volume_nfs_setattr_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.setattr.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_setattr_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_total_ops

Number of total NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). node_volume_nfs_total_ops is [volume_nfs_total_ops](#volume_nfs_total_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_write_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. node_volume_nfs_write_latency is [volume_nfs_write_latency](#volume_nfs_write_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.write.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.write.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_write_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_nfs_write_ops

Number of operations of the given type performed on this volume. node_volume_nfs_write_ops is [volume_nfs_write_ops](#volume_nfs_write_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.write.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### node_volume_other_data

Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on. node_volume_other_data is [volume_other_data](#volume_other_data) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.other`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |



### node_volume_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on. node_volume_other_latency in microseconds is [volume_other_latency](#volume_other_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.other | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_other_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Backend | timeseries | [Average Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=30) |
///



### node_volume_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on. node_volume_other_ops is [volume_other_ops](#volume_other_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_other_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Backend | timeseries | [IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=32) |
///



### node_volume_read_data

Performance metric for read I/O operations in bytes per seconds. node_volume_read_data is [volume_read_data](#volume_read_data) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Highlights | table | [Top $TopResources Nodes by Throughput](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=232) |
| ONTAP: Cluster | Highlights | timeseries | [Top $TopResources Nodes by Throughput](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=231) |
| ONTAP: Node | Highlights | timeseries | [Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=11) |
| ONTAP: Node | Backend | timeseries | [Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=31) |
///



### node_volume_read_latency

Performance metric for read I/O operations. node_volume_read_latency in microseconds is [volume_read_latency](#volume_read_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.read | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Highlights | table | [Top $TopResources Nodes by Read Latency](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=238) |
| ONTAP: Node | Backend | timeseries | [Average Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=30) |
///



### node_volume_read_ops

Performance metric for read I/O operations. node_volume_read_ops is [volume_read_ops](#volume_read_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Backend | timeseries | [IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=32) |
///



### node_volume_total_data

Performance metric for the aggregated total data throughput in bytes per second across all volumes on a node. This metric is calculated by Harvest by summing the data read from and written to each volume on the node.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/volume.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_total_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Highlights | stat | [Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=23) |
///



### node_volume_total_ops

Performance metric aggregated over all types of I/O operations. node_volume_total_ops is [volume_total_ops](#volume_total_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Highlights | table | [Top $TopResources Nodes by IOPs](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=230) |
| ONTAP: Cluster | Highlights | timeseries | [Top $TopResources Nodes by IOPs](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=235) |
| ONTAP: Node | Highlights | stat | [IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=5) |
| ONTAP: Node | Highlights | timeseries | [Top Average IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=12) |
///



### node_volume_write_data

Performance metric for write I/O operations in bytes per seconds. node_volume_write_data is [volume_write_data](#volume_write_data) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Highlights | table | [Top $TopResources Nodes by Throughput](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=232) |
| ONTAP: Cluster | Highlights | timeseries | [Top $TopResources Nodes by Throughput](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=231) |
| ONTAP: Node | Highlights | timeseries | [Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=11) |
| ONTAP: Node | Backend | timeseries | [Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=31) |
///



### node_volume_write_latency

Performance metric for write I/O operations. node_volume_write_latency in microseconds is [volume_write_latency](#volume_write_latency) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.write | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Highlights | table | [Top $TopResources Nodes by Write Latency](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=233) |
| ONTAP: Disk | Disk Utilization | timeseries | [Write Latency by Node](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=64) |
| ONTAP: Node | Backend | timeseries | [Average Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=30) |
///



### node_volume_write_ops

Performance metric for write I/O operations. node_volume_write_ops is [volume_write_ops](#volume_write_ops) aggregated by `node`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `node_volume_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Backend | timeseries | [IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=32) |
///



### node_write_data

Write throughput in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_write_latency

Average latency for all write operations in the system in microseconds

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> <br><span class="key">Base:</span> write_ops | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### node_write_ops

Write operations per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/system:node` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/system_node.yaml |
| StatPerf | `system:node` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/statperf/9.8.0/system_node.yaml |
| ZapiPerf | `perf-object-get-instances system:node` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/system_node.yaml |



### ntpserver_labels

This metric provides information about NtpServer

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/cluster/ntp/servers` | `Harvest generated` | conf/rest/9.12.0/ntpserver.yaml |
| ZAPI | `ntp-server-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/ntpserver.yaml |

The `ntpserver_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Cluster Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=214) |
| ONTAP: Security | Highlights | piechart | [Cluster Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=215) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### nvm_mirror_write_throughput

Mirror throughput in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances nvm_mirror` | `write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvm_mirror.yaml |

The `nvm_mirror_write_throughput` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster NVM Mirror | timeseries | [Write Throughput](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=104) |
///



### nvme_lif_avg_latency

Average latency in microseconds for NVMF operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NVMe/FC Frontend | stat | [NVMe/FC Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=122) |
| ONTAP: Node | NVMe/FC Frontend | timeseries | [NVMe/FC Average Latency by Port / LIF](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=120) |
| ONTAP: SVM | NVMe/FC | stat | [SVM NVMe/FC Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=177) |
| ONTAP: SVM | NVMe/FC | timeseries | [SVM NVMe/FC Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=186) |
///



### nvme_lif_avg_other_latency

Average latency in microseconds for operations other than read, write, compare or compare-and-write.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `average_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `avg_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_avg_other_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NVMe/FC | timeseries | [SVM NVMe/FC Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=186) |
///



### nvme_lif_avg_read_latency

Average latency in microseconds for read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `avg_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_avg_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NVMe/FC | timeseries | [SVM NVMe/FC Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=186) |
///



### nvme_lif_avg_write_latency

Average latency in microseconds for write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `avg_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_avg_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NVMe/FC | timeseries | [SVM NVMe/FC Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=186) |
///



### nvme_lif_other_ops

Number of operations that are not read, write, compare or compare-and-write.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_other_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NVMe/FC | timeseries | [SVM NVMe/FC IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=188) |
///



### nvme_lif_read_data

Amount of data read from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | LIF | timeseries | [Top $TopResources NVMe/FC LIFs by Send Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=93) |
| ONTAP: SVM | NVMe/FC | timeseries | [SVM NVMe/FC Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=187) |
| ONTAP: SVM | NVMe/FC | timeseries | [Top $TopResources SVM NVMe/FC LIFs by Send Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=189) |
///



### nvme_lif_read_ops

Number of read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NVMe/FC | timeseries | [SVM NVMe/FC IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=188) |
///



### nvme_lif_total_ops

Total number of operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NVMe/FC Frontend | stat | [NVMe/FC IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=124) |
| ONTAP: Node | NVMe/FC Frontend | timeseries | [NVMe/FC IOPs by Port / LIF](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=128) |
| ONTAP: SVM | NVMe/FC | stat | [SVM NVMe/FC IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=179) |
///



### nvme_lif_write_data

Amount of data written to the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | NVMe/FC Frontend | timeseries | [NVMe/FC Throughput by Port / LIF](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=126) |
| ONTAP: SVM | LIF | timeseries | [Top $TopResources NVMe/FC LIFs by Receive Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=94) |
| ONTAP: SVM | NVMe/FC | stat | [SVM NVMe/FC Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=178) |
| ONTAP: SVM | NVMe/FC | timeseries | [SVM NVMe/FC Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=187) |
| ONTAP: SVM | NVMe/FC | timeseries | [Top $TopResources SVM NVMe/FC LIFs by Receive Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=190) |
///



### nvme_lif_write_ops

Number of write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_lif` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nvmf_lif.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_fc_lif` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml |

The `nvme_lif_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NVMe/FC | timeseries | [SVM NVMe/FC IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=188) |
///



### nvmf_rdma_port_avg_latency

Average latency in microseconds for NVMF operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_avg_other_latency

Average latency in microseconds for operations other than read, write, compare or compare-and-write

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `average_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `avg_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_avg_read_latency

Average latency in microseconds for read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `avg_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_avg_write_latency

Average latency in microseconds for write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `avg_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_other_ops

Number of operations that are not read, write, compare or compare-and-right.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_read_data

Amount of data read from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_read_ops

Number of read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_total_data

Amount of NVMF traffic to and from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_total_ops

Total number of operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_write_data

Amount of data written to the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_rdma_port_write_ops

Number of write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_rdma_port` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_rdma_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_rdma_port` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_rdma_port.yaml |



### nvmf_tcp_port_avg_latency

Average latency in microseconds for NVMF operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_avg_other_latency

Average latency in microseconds for operations other than read, write, compare or compare-and-write

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `average_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `avg_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_avg_read_latency

Average latency in microseconds for read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `avg_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_avg_write_latency

Average latency in microseconds for write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `avg_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_other_ops

Number of operations that are not read, write, compare or compare-and-write.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_read_data

Amount of data read from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_read_ops

Number of read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_total_data

Amount of NVMF traffic to and from the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_total_ops

Total number of operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_write_data

Amount of data written to the storage system in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### nvmf_tcp_port_write_ops

Number of write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/nvmf_tcp_port` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/nvmf_tcp_port.yaml |
| ZapiPerf | `perf-object-get-instances nvmf_tcp_port` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nvmf_tcp_port.yaml |



### ontaps3_labels

This metric provides information about OntapS3

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/s3/buckets` | `Harvest generated` | conf/rest/9.7.0/ontap_s3.yaml |

The `ontaps3_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Bucket protection | stat | [Total Buckets](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=104) |
| ONTAP: Data Protection | Bucket protection | stat | [Unprotected Buckets](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=105) |
| ONTAP: Data Protection | Bucket protection | stat | [Not Backed up to Cloud](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=106) |
| ONTAP: Data Protection | Bucket protection | table | [Buckets](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=103) |
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: S3 Object Storage | Highlights | table | [Bucket Overview](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=5) |
///



### ontaps3_logical_used_size

Specifies the bucket logical used size up to this point. This field cannot be specified using a POST or PATCH method.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/s3/buckets` | `logical_used_size` | conf/rest/9.7.0/ontap_s3.yaml |

The `ontaps3_logical_used_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Bucket protection | table | [Buckets](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=103) |
| ONTAP: S3 Object Storage | Highlights | table | [Bucket Overview](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=5) |
| ONTAP: S3 Object Storage | Highlights | timeseries | [Top $TopResources Buckets by Used Size](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=127) |
///



### ontaps3_object_count



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/vserver/object-store-server/bucket` | `object_count` | conf/rest/9.7.0/ontap_s3.yaml |

The `ontaps3_object_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | Highlights | table | [Bucket Overview](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=5) |
///



### ontaps3_policy_labels

This metric provides information about OntapS3Policy

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/vserver/object-store-server/bucket/policy` | `Harvest generated` | conf/rest/9.7.0/ontap_s3_policy.yaml |

The `ontaps3_policy_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | Highlights | table | [Bucket Permission](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=6) |
///



### ontaps3_size

Specifies the bucket size in bytes; ranges from 190MB to 62PB.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/s3/buckets` | `size` | conf/rest/9.7.0/ontap_s3.yaml |

The `ontaps3_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Bucket protection | table | [Buckets](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=103) |
| ONTAP: S3 Object Storage | Highlights | table | [Bucket Overview](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=5) |
///



### ontaps3_svm_abort_multipart_upload_failed

Number of failed Abort Multipart Upload operations. ontaps3_svm_abort_multipart_upload_failed is [ontaps3_svm_abort_multipart_upload_failed](#ontaps3_svm_abort_multipart_upload_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `abort_multipart_upload_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `abort_multipart_upload_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_abort_multipart_upload_failed_client_close

Number of times Abort Multipart Upload operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_abort_multipart_upload_failed_client_close is [ontaps3_svm_abort_multipart_upload_failed_client_close](#ontaps3_svm_abort_multipart_upload_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `abort_multipart_upload_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `abort_multipart_upload_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_abort_multipart_upload_latency

Average latency in microseconds for Abort Multipart Upload operations. ontaps3_svm_abort_multipart_upload_latency is [ontaps3_svm_abort_multipart_upload_latency](#ontaps3_svm_abort_multipart_upload_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `abort_multipart_upload_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> abort_multipart_upload_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `abort_multipart_upload_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> abort_multipart_upload_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_abort_multipart_upload_rate

Number of Abort Multipart Upload operations per second. ontaps3_svm_abort_multipart_upload_rate is [ontaps3_svm_abort_multipart_upload_rate](#ontaps3_svm_abort_multipart_upload_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `abort_multipart_upload_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `abort_multipart_upload_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_abort_multipart_upload_total

Number of Abort Multipart Upload operations. ontaps3_svm_abort_multipart_upload_total is [ontaps3_svm_abort_multipart_upload_total](#ontaps3_svm_abort_multipart_upload_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `abort_multipart_upload_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `abort_multipart_upload_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_allow_access

Number of times access was allowed. ontaps3_svm_allow_access is [ontaps3_svm_allow_access](#ontaps3_svm_allow_access) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `allow_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `allow_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_anonymous_access

Number of times anonymous access was allowed. ontaps3_svm_anonymous_access is [ontaps3_svm_anonymous_access](#ontaps3_svm_anonymous_access) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `anonymous_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `anonymous_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_anonymous_deny_access

Number of times anonymous access was denied. ontaps3_svm_anonymous_deny_access is [ontaps3_svm_anonymous_deny_access](#ontaps3_svm_anonymous_deny_access) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `anonymous_deny_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `anonymous_deny_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_authentication_failures

Number of authentication failures. ontaps3_svm_authentication_failures is [ontaps3_svm_authentication_failures](#ontaps3_svm_authentication_failures) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `authentication_failures`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `authentication_failures`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_chunked_upload_reqs

Total number of object store server chunked object upload requests. ontaps3_svm_chunked_upload_reqs is [ontaps3_svm_chunked_upload_reqs](#ontaps3_svm_chunked_upload_reqs) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `chunked_upload_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `chunked_upload_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_complete_multipart_upload_failed

Number of failed Complete Multipart Upload operations. ontaps3_svm_complete_multipart_upload_failed is [ontaps3_svm_complete_multipart_upload_failed](#ontaps3_svm_complete_multipart_upload_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `complete_multipart_upload_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `complete_multipart_upload_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_complete_multipart_upload_failed_client_close

Number of times Complete Multipart Upload operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_complete_multipart_upload_failed_client_close is [ontaps3_svm_complete_multipart_upload_failed_client_close](#ontaps3_svm_complete_multipart_upload_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `complete_multipart_upload_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `complete_multipart_upload_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_complete_multipart_upload_latency

Average latency in microseconds for Complete Multipart Upload operations. ontaps3_svm_complete_multipart_upload_latency is [ontaps3_svm_complete_multipart_upload_latency](#ontaps3_svm_complete_multipart_upload_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `complete_multipart_upload_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> complete_multipart_upload_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `complete_multipart_upload_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> complete_multipart_upload_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_complete_multipart_upload_rate

Number of Complete Multipart Upload operations per second. ontaps3_svm_complete_multipart_upload_rate is [ontaps3_svm_complete_multipart_upload_rate](#ontaps3_svm_complete_multipart_upload_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `complete_multipart_upload_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `complete_multipart_upload_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_complete_multipart_upload_total

Number of Complete Multipart Upload operations. ontaps3_svm_complete_multipart_upload_total is [ontaps3_svm_complete_multipart_upload_total](#ontaps3_svm_complete_multipart_upload_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `complete_multipart_upload_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `complete_multipart_upload_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_connected_connections

Number of object store server connections currently established. ontaps3_svm_connected_connections is [ontaps3_svm_connected_connections](#ontaps3_svm_connected_connections) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `connected_connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `connected_connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_connections

Total number of object store server connections. ontaps3_svm_connections is [ontaps3_svm_connections](#ontaps3_svm_connections) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_connections` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | table | [Requests & Connections stats](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=137) |
///



### ontaps3_svm_create_bucket_failed

Number of failed Create Bucket operations. ontaps3_svm_create_bucket_failed is [ontaps3_svm_create_bucket_failed](#ontaps3_svm_create_bucket_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `create_bucket_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `create_bucket_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_create_bucket_failed_client_close

Number of times Create Bucket operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_create_bucket_failed_client_close is [ontaps3_svm_create_bucket_failed_client_close](#ontaps3_svm_create_bucket_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `create_bucket_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `create_bucket_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_create_bucket_latency

Average latency in microseconds for Create Bucket operations. ontaps3_svm_create_bucket_latency is [ontaps3_svm_create_bucket_latency](#ontaps3_svm_create_bucket_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `create_bucket_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create_bucket_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `create_bucket_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_bucket_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_create_bucket_rate

Number of Create Bucket operations per second. ontaps3_svm_create_bucket_rate is [ontaps3_svm_create_bucket_rate](#ontaps3_svm_create_bucket_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `create_bucket_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `create_bucket_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_create_bucket_total

Number of Create Bucket operations. ontaps3_svm_create_bucket_total is [ontaps3_svm_create_bucket_total](#ontaps3_svm_create_bucket_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `create_bucket_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `create_bucket_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_default_deny_access

Number of times access was denied by default and not through any policy statement. ontaps3_svm_default_deny_access is [ontaps3_svm_default_deny_access](#ontaps3_svm_default_deny_access) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `default_deny_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `default_deny_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_bucket_failed

Number of failed Delete Bucket operations. ontaps3_svm_delete_bucket_failed is [ontaps3_svm_delete_bucket_failed](#ontaps3_svm_delete_bucket_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_bucket_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_bucket_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_bucket_failed_client_close

Number of times Delete Bucket operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_delete_bucket_failed_client_close is [ontaps3_svm_delete_bucket_failed_client_close](#ontaps3_svm_delete_bucket_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_bucket_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_bucket_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_bucket_latency

Average latency in microseconds for Delete Bucket operations. ontaps3_svm_delete_bucket_latency is [ontaps3_svm_delete_bucket_latency](#ontaps3_svm_delete_bucket_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_bucket_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delete_bucket_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_bucket_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delete_bucket_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_bucket_rate

Number of Delete Bucket operations per second. ontaps3_svm_delete_bucket_rate is [ontaps3_svm_delete_bucket_rate](#ontaps3_svm_delete_bucket_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_bucket_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_bucket_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_bucket_total

Number of Delete Bucket operations. ontaps3_svm_delete_bucket_total is [ontaps3_svm_delete_bucket_total](#ontaps3_svm_delete_bucket_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_bucket_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_bucket_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_object_failed

Number of failed DELETE object operations. ontaps3_svm_delete_object_failed is [ontaps3_svm_delete_object_failed](#ontaps3_svm_delete_object_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_object_failed_client_close

Number of times DELETE object operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_delete_object_failed_client_close is [ontaps3_svm_delete_object_failed_client_close](#ontaps3_svm_delete_object_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_object_latency

Average latency in microseconds for DELETE object operations. ontaps3_svm_delete_object_latency is [ontaps3_svm_delete_object_latency](#ontaps3_svm_delete_object_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delete_object_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delete_object_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_delete_object_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Latency](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=130) |
///



### ontaps3_svm_delete_object_rate

Number of DELETE object operations per second. ontaps3_svm_delete_object_rate is [ontaps3_svm_delete_object_rate](#ontaps3_svm_delete_object_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_delete_object_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Rate](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=129) |
///



### ontaps3_svm_delete_object_tagging_failed

Number of failed DELETE object tagging operations. ontaps3_svm_delete_object_tagging_failed is [ontaps3_svm_delete_object_tagging_failed](#ontaps3_svm_delete_object_tagging_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_tagging_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_tagging_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_object_tagging_failed_client_close

Number of times DELETE object tagging operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_delete_object_tagging_failed_client_close is [ontaps3_svm_delete_object_tagging_failed_client_close](#ontaps3_svm_delete_object_tagging_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_tagging_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_tagging_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_object_tagging_latency

Average latency in microseconds for DELETE object tagging operations. ontaps3_svm_delete_object_tagging_latency is [ontaps3_svm_delete_object_tagging_latency](#ontaps3_svm_delete_object_tagging_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_tagging_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delete_object_tagging_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_tagging_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delete_object_tagging_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_object_tagging_rate

Number of DELETE object tagging operations per second. ontaps3_svm_delete_object_tagging_rate is [ontaps3_svm_delete_object_tagging_rate](#ontaps3_svm_delete_object_tagging_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_tagging_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_tagging_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_object_tagging_total

Number of DELETE object tagging operations. ontaps3_svm_delete_object_tagging_total is [ontaps3_svm_delete_object_tagging_total](#ontaps3_svm_delete_object_tagging_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_tagging_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_tagging_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_delete_object_total

Number of DELETE object operations. ontaps3_svm_delete_object_total is [ontaps3_svm_delete_object_total](#ontaps3_svm_delete_object_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `delete_object_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `delete_object_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_delete_object_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Operations](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=132) |
///



### ontaps3_svm_explicit_deny_access

Number of times access was denied explicitly by a policy statement. ontaps3_svm_explicit_deny_access is [ontaps3_svm_explicit_deny_access](#ontaps3_svm_explicit_deny_access) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `explicit_deny_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `explicit_deny_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_bucket_acl_failed

Number of failed GET Bucket ACL operations. ontaps3_svm_get_bucket_acl_failed is [ontaps3_svm_get_bucket_acl_failed](#ontaps3_svm_get_bucket_acl_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_bucket_acl_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_bucket_acl_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_bucket_acl_total

Number of GET Bucket ACL operations. ontaps3_svm_get_bucket_acl_total is [ontaps3_svm_get_bucket_acl_total](#ontaps3_svm_get_bucket_acl_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_bucket_acl_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_bucket_acl_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_bucket_versioning_failed

Number of failed Get Bucket Versioning operations. ontaps3_svm_get_bucket_versioning_failed is [ontaps3_svm_get_bucket_versioning_failed](#ontaps3_svm_get_bucket_versioning_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_bucket_versioning_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_bucket_versioning_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_bucket_versioning_total

Number of Get Bucket Versioning operations. ontaps3_svm_get_bucket_versioning_total is [ontaps3_svm_get_bucket_versioning_total](#ontaps3_svm_get_bucket_versioning_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_bucket_versioning_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_bucket_versioning_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_data

Rate of GET object data transfers in bytes per second. ontaps3_svm_get_data is [ontaps3_svm_get_data](#ontaps3_svm_get_data) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_get_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Data Transfer](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=131) |
///



### ontaps3_svm_get_object_acl_failed

Number of failed GET Object ACL operations. ontaps3_svm_get_object_acl_failed is [ontaps3_svm_get_object_acl_failed](#ontaps3_svm_get_object_acl_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_acl_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_acl_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_acl_total

Number of GET Object ACL operations. ontaps3_svm_get_object_acl_total is [ontaps3_svm_get_object_acl_total](#ontaps3_svm_get_object_acl_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_acl_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_acl_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_failed

Number of failed GET object operations. ontaps3_svm_get_object_failed is [ontaps3_svm_get_object_failed](#ontaps3_svm_get_object_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_failed_client_close

Number of times GET object operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_get_object_failed_client_close is [ontaps3_svm_get_object_failed_client_close](#ontaps3_svm_get_object_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_lastbyte_latency

Average last-byte latency in microseconds for GET object operations. ontaps3_svm_get_object_lastbyte_latency is [ontaps3_svm_get_object_lastbyte_latency](#ontaps3_svm_get_object_lastbyte_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_lastbyte_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> get_object_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_lastbyte_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> get_object_lastbyte_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_latency

Average first-byte latency in microseconds for GET object operations. ontaps3_svm_get_object_latency is [ontaps3_svm_get_object_latency](#ontaps3_svm_get_object_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> get_object_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> get_object_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_get_object_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Latency](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=130) |
///



### ontaps3_svm_get_object_rate

Number of GET object operations per second. ontaps3_svm_get_object_rate is [ontaps3_svm_get_object_rate](#ontaps3_svm_get_object_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_get_object_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Rate](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=129) |
///



### ontaps3_svm_get_object_tagging_failed

Number of failed GET object tagging operations. ontaps3_svm_get_object_tagging_failed is [ontaps3_svm_get_object_tagging_failed](#ontaps3_svm_get_object_tagging_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_tagging_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_tagging_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_tagging_failed_client_close

Number of times GET object tagging operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_get_object_tagging_failed_client_close is [ontaps3_svm_get_object_tagging_failed_client_close](#ontaps3_svm_get_object_tagging_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_tagging_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_tagging_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_tagging_latency

Average latency in microseconds for GET object tagging operations. ontaps3_svm_get_object_tagging_latency is [ontaps3_svm_get_object_tagging_latency](#ontaps3_svm_get_object_tagging_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_tagging_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> get_object_tagging_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_tagging_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> get_object_tagging_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_tagging_rate

Number of GET object tagging operations per second. ontaps3_svm_get_object_tagging_rate is [ontaps3_svm_get_object_tagging_rate](#ontaps3_svm_get_object_tagging_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_tagging_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_tagging_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_tagging_total

Number of GET object tagging operations. ontaps3_svm_get_object_tagging_total is [ontaps3_svm_get_object_tagging_total](#ontaps3_svm_get_object_tagging_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_tagging_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_tagging_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_get_object_total

Number of GET object operations. ontaps3_svm_get_object_total is [ontaps3_svm_get_object_total](#ontaps3_svm_get_object_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `get_object_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `get_object_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_get_object_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Operations](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=132) |
///



### ontaps3_svm_group_policy_evaluated

Number of times group policies were evaluated. ontaps3_svm_group_policy_evaluated is [ontaps3_svm_group_policy_evaluated](#ontaps3_svm_group_policy_evaluated) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `group_policy_evaluated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `group_policy_evaluated`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_head_bucket_failed

Number of failed HEAD bucket operations. ontaps3_svm_head_bucket_failed is [ontaps3_svm_head_bucket_failed](#ontaps3_svm_head_bucket_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_bucket_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_bucket_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_head_bucket_failed_client_close

Number of times HEAD bucket operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_head_bucket_failed_client_close is [ontaps3_svm_head_bucket_failed_client_close](#ontaps3_svm_head_bucket_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_bucket_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_bucket_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_head_bucket_latency

Average latency in microseconds for HEAD bucket operations. ontaps3_svm_head_bucket_latency is [ontaps3_svm_head_bucket_latency](#ontaps3_svm_head_bucket_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_bucket_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> head_bucket_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_bucket_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> head_bucket_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_head_bucket_rate

Number of HEAD bucket operations per second. ontaps3_svm_head_bucket_rate is [ontaps3_svm_head_bucket_rate](#ontaps3_svm_head_bucket_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_bucket_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_bucket_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_head_bucket_total

Number of HEAD bucket operations. ontaps3_svm_head_bucket_total is [ontaps3_svm_head_bucket_total](#ontaps3_svm_head_bucket_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_bucket_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_bucket_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_head_object_failed

Number of failed HEAD Object operations. ontaps3_svm_head_object_failed is [ontaps3_svm_head_object_failed](#ontaps3_svm_head_object_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_object_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_object_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_head_object_failed_client_close

Number of times HEAD object operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_head_object_failed_client_close is [ontaps3_svm_head_object_failed_client_close](#ontaps3_svm_head_object_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_object_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_object_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_head_object_latency

Average latency in microseconds for HEAD object operations. ontaps3_svm_head_object_latency is [ontaps3_svm_head_object_latency](#ontaps3_svm_head_object_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_object_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> head_object_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_object_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> head_object_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_head_object_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Latency](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=130) |
///



### ontaps3_svm_head_object_rate

Number of HEAD Object operations per second. ontaps3_svm_head_object_rate is [ontaps3_svm_head_object_rate](#ontaps3_svm_head_object_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_object_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_object_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_head_object_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Rate](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=129) |
///



### ontaps3_svm_head_object_total

Number of HEAD Object operations. ontaps3_svm_head_object_total is [ontaps3_svm_head_object_total](#ontaps3_svm_head_object_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `head_object_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `head_object_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_head_object_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Operations](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=132) |
///



### ontaps3_svm_initiate_multipart_upload_failed

Number of failed Initiate Multipart Upload operations. ontaps3_svm_initiate_multipart_upload_failed is [ontaps3_svm_initiate_multipart_upload_failed](#ontaps3_svm_initiate_multipart_upload_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `initiate_multipart_upload_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `initiate_multipart_upload_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_initiate_multipart_upload_failed_client_close

Number of times Initiate Multipart Upload operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_initiate_multipart_upload_failed_client_close is [ontaps3_svm_initiate_multipart_upload_failed_client_close](#ontaps3_svm_initiate_multipart_upload_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `initiate_multipart_upload_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `initiate_multipart_upload_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_initiate_multipart_upload_latency

Average latency in microseconds for Initiate Multipart Upload operations. ontaps3_svm_initiate_multipart_upload_latency is [ontaps3_svm_initiate_multipart_upload_latency](#ontaps3_svm_initiate_multipart_upload_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `initiate_multipart_upload_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> initiate_multipart_upload_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `initiate_multipart_upload_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> initiate_multipart_upload_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_initiate_multipart_upload_rate

Number of Initiate Multipart Upload operations per second. ontaps3_svm_initiate_multipart_upload_rate is [ontaps3_svm_initiate_multipart_upload_rate](#ontaps3_svm_initiate_multipart_upload_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `initiate_multipart_upload_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `initiate_multipart_upload_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_initiate_multipart_upload_total

Number of Initiate Multipart Upload operations. ontaps3_svm_initiate_multipart_upload_total is [ontaps3_svm_initiate_multipart_upload_total](#ontaps3_svm_initiate_multipart_upload_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `initiate_multipart_upload_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `initiate_multipart_upload_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_input_flow_control_entry

Number of times input flow control was entered. ontaps3_svm_input_flow_control_entry is [ontaps3_svm_input_flow_control_entry](#ontaps3_svm_input_flow_control_entry) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `input_flow_control_entry`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `input_flow_control_entry`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_input_flow_control_exit

Number of times input flow control was exited. ontaps3_svm_input_flow_control_exit is [ontaps3_svm_input_flow_control_exit](#ontaps3_svm_input_flow_control_exit) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `input_flow_control_exit`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `input_flow_control_exit`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_buckets_failed

Number of failed LIST Buckets operations. ontaps3_svm_list_buckets_failed is [ontaps3_svm_list_buckets_failed](#ontaps3_svm_list_buckets_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_buckets_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_buckets_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_buckets_failed_client_close

Number of times LIST Bucket operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_list_buckets_failed_client_close is [ontaps3_svm_list_buckets_failed_client_close](#ontaps3_svm_list_buckets_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_buckets_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_buckets_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_buckets_latency

Average latency in microseconds for LIST Buckets operations. ontaps3_svm_list_buckets_latency is [ontaps3_svm_list_buckets_latency](#ontaps3_svm_list_buckets_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_buckets_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> list_buckets_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_buckets_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> head_object_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_buckets_rate

Number of LIST Buckets operations per second. ontaps3_svm_list_buckets_rate is [ontaps3_svm_list_buckets_rate](#ontaps3_svm_list_buckets_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_buckets_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_buckets_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_buckets_total

Number of LIST Buckets operations. ontaps3_svm_list_buckets_total is [ontaps3_svm_list_buckets_total](#ontaps3_svm_list_buckets_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_buckets_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_buckets_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_object_versions_failed

Number of failed LIST object versions operations. ontaps3_svm_list_object_versions_failed is [ontaps3_svm_list_object_versions_failed](#ontaps3_svm_list_object_versions_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_object_versions_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_object_versions_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_object_versions_failed_client_close

Number of times LIST object versions operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_list_object_versions_failed_client_close is [ontaps3_svm_list_object_versions_failed_client_close](#ontaps3_svm_list_object_versions_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_object_versions_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_object_versions_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_object_versions_latency

Average latency in microseconds for LIST Object versions operations. ontaps3_svm_list_object_versions_latency is [ontaps3_svm_list_object_versions_latency](#ontaps3_svm_list_object_versions_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_object_versions_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> list_object_versions_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_object_versions_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> list_object_versions_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_object_versions_rate

Number of LIST Object Versions operations per second. ontaps3_svm_list_object_versions_rate is [ontaps3_svm_list_object_versions_rate](#ontaps3_svm_list_object_versions_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_object_versions_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_object_versions_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_object_versions_total

Number of LIST Object Versions operations. ontaps3_svm_list_object_versions_total is [ontaps3_svm_list_object_versions_total](#ontaps3_svm_list_object_versions_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_object_versions_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_object_versions_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_objects_failed

Number of failed LIST objects operations. ontaps3_svm_list_objects_failed is [ontaps3_svm_list_objects_failed](#ontaps3_svm_list_objects_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_objects_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_objects_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_objects_failed_client_close

Number of times LIST objects operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_list_objects_failed_client_close is [ontaps3_svm_list_objects_failed_client_close](#ontaps3_svm_list_objects_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_objects_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_objects_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_objects_latency

Average latency in microseconds for LIST Objects operations. ontaps3_svm_list_objects_latency is [ontaps3_svm_list_objects_latency](#ontaps3_svm_list_objects_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_objects_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> list_objects_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_objects_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> list_objects_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_objects_rate

Number of LIST Objects operations per second. ontaps3_svm_list_objects_rate is [ontaps3_svm_list_objects_rate](#ontaps3_svm_list_objects_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_objects_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_objects_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_objects_total

Number of LIST Objects operations. ontaps3_svm_list_objects_total is [ontaps3_svm_list_objects_total](#ontaps3_svm_list_objects_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_objects_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_objects_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_uploads_failed

Number of failed LIST Upload operations. ontaps3_svm_list_uploads_failed is [ontaps3_svm_list_uploads_failed](#ontaps3_svm_list_uploads_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_uploads_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_uploads_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_uploads_failed_client_close

Number of times LIST Upload operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_list_uploads_failed_client_close is [ontaps3_svm_list_uploads_failed_client_close](#ontaps3_svm_list_uploads_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_uploads_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_uploads_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_uploads_latency

Average latency in microseconds for LIST Upload operations. ontaps3_svm_list_uploads_latency is [ontaps3_svm_list_uploads_latency](#ontaps3_svm_list_uploads_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_uploads_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> list_uploads_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_uploads_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> list_uploads_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_uploads_rate

Number of LIST Upload operations per second. ontaps3_svm_list_uploads_rate is [ontaps3_svm_list_uploads_rate](#ontaps3_svm_list_uploads_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_uploads_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_uploads_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_list_uploads_total

Number of LIST Upload operations. ontaps3_svm_list_uploads_total is [ontaps3_svm_list_uploads_total](#ontaps3_svm_list_uploads_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `list_uploads_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `list_uploads_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_max_cmds_per_connection

Maximum commands pipelined at any instance on a connection. ontaps3_svm_max_cmds_per_connection is [ontaps3_svm_max_cmds_per_connection](#ontaps3_svm_max_cmds_per_connection) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `maximum_commands_per_connection`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `max_cmds_per_connection`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_max_cmds_per_connection` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | table | [Requests & Connections stats](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=137) |
///



### ontaps3_svm_max_connected_connections

Maximum number of object store server connections established at one time. ontaps3_svm_max_connected_connections is [ontaps3_svm_max_connected_connections](#ontaps3_svm_max_connected_connections) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `maximum_connected_connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `max_connected_connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_max_connected_connections` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | table | [Requests & Connections stats](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=137) |
///



### ontaps3_svm_max_requests_outstanding

Maximum number of object store server requests in process at one time. ontaps3_svm_max_requests_outstanding is [ontaps3_svm_max_requests_outstanding](#ontaps3_svm_max_requests_outstanding) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `maximum_requests_outstanding`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `max_requests_outstanding`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_max_requests_outstanding` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | table | [Requests & Connections stats](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=137) |
///



### ontaps3_svm_multi_delete_reqs

Total number of object store server multiple object delete requests. ontaps3_svm_multi_delete_reqs is [ontaps3_svm_multi_delete_reqs](#ontaps3_svm_multi_delete_reqs) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `multiple_delete_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `multi_delete_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_output_flow_control_entry

Number of output flow control was entered. ontaps3_svm_output_flow_control_entry is [ontaps3_svm_output_flow_control_entry](#ontaps3_svm_output_flow_control_entry) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `output_flow_control_entry`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `output_flow_control_entry`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_output_flow_control_exit

Number of times output flow control was exited. ontaps3_svm_output_flow_control_exit is [ontaps3_svm_output_flow_control_exit](#ontaps3_svm_output_flow_control_exit) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `output_flow_control_exit`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `output_flow_control_exit`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_presigned_url_reqs

Total number of presigned object store server URL requests. ontaps3_svm_presigned_url_reqs is [ontaps3_svm_presigned_url_reqs](#ontaps3_svm_presigned_url_reqs) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `presigned_url_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `presigned_url_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_bucket_versioning_failed

Number of failed Put Bucket Versioning operations. ontaps3_svm_put_bucket_versioning_failed is [ontaps3_svm_put_bucket_versioning_failed](#ontaps3_svm_put_bucket_versioning_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_bucket_versioning_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_bucket_versioning_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_bucket_versioning_total

Number of Put Bucket Versioning operations. ontaps3_svm_put_bucket_versioning_total is [ontaps3_svm_put_bucket_versioning_total](#ontaps3_svm_put_bucket_versioning_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_bucket_versioning_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_bucket_versioning_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_data

Rate of PUT object data transfers in bytes per second. ontaps3_svm_put_data is [ontaps3_svm_put_data](#ontaps3_svm_put_data) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_put_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Data Transfer](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=131) |
///



### ontaps3_svm_put_object_failed

Number of failed PUT object operations. ontaps3_svm_put_object_failed is [ontaps3_svm_put_object_failed](#ontaps3_svm_put_object_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_object_failed_client_close

Number of times PUT object operation failed due to the case where client closed the connection while the operation was still pending on server. ontaps3_svm_put_object_failed_client_close is [ontaps3_svm_put_object_failed_client_close](#ontaps3_svm_put_object_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_object_latency

Average latency in microseconds for PUT object operations. ontaps3_svm_put_object_latency is [ontaps3_svm_put_object_latency](#ontaps3_svm_put_object_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> put_object_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> put_object_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_put_object_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Latency](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=130) |
///



### ontaps3_svm_put_object_rate

Number of PUT object operations per second. ontaps3_svm_put_object_rate is [ontaps3_svm_put_object_rate](#ontaps3_svm_put_object_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_put_object_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Rate](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=129) |
///



### ontaps3_svm_put_object_tagging_failed

Number of failed PUT object tagging operations. ontaps3_svm_put_object_tagging_failed is [ontaps3_svm_put_object_tagging_failed](#ontaps3_svm_put_object_tagging_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_tagging_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_tagging_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_object_tagging_failed_client_close

Number of times PUT object tagging operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_put_object_tagging_failed_client_close is [ontaps3_svm_put_object_tagging_failed_client_close](#ontaps3_svm_put_object_tagging_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_tagging_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_tagging_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_object_tagging_latency

Average latency in microseconds for PUT object tagging operations. ontaps3_svm_put_object_tagging_latency is [ontaps3_svm_put_object_tagging_latency](#ontaps3_svm_put_object_tagging_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_tagging_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> put_object_tagging_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_tagging_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> put_object_tagging_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_object_tagging_rate

Number of PUT object tagging operations per second. ontaps3_svm_put_object_tagging_rate is [ontaps3_svm_put_object_tagging_rate](#ontaps3_svm_put_object_tagging_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_tagging_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_tagging_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_object_tagging_total

Number of PUT object tagging operations. ontaps3_svm_put_object_tagging_total is [ontaps3_svm_put_object_tagging_total](#ontaps3_svm_put_object_tagging_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_tagging_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_tagging_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_put_object_total

Number of PUT object operations. ontaps3_svm_put_object_total is [ontaps3_svm_put_object_total](#ontaps3_svm_put_object_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `put_object_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `put_object_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_put_object_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | timeseries | [Top $TopResources SVMs by Operations](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=132) |
///



### ontaps3_svm_request_parse_errors

Number of request parser errors due to malformed requests. ontaps3_svm_request_parse_errors is [ontaps3_svm_request_parse_errors](#ontaps3_svm_request_parse_errors) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `request_parse_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `request_parse_errors`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_requests

Total number of object store server requests. ontaps3_svm_requests is [ontaps3_svm_requests](#ontaps3_svm_requests) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |

The `ontaps3_svm_requests` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | S3 Object Storage SVM | table | [Requests & Connections stats](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=137) |
///



### ontaps3_svm_requests_outstanding

Number of object store server requests in process. ontaps3_svm_requests_outstanding is [ontaps3_svm_requests_outstanding](#ontaps3_svm_requests_outstanding) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `requests_outstanding`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `requests_outstanding`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_root_user_access

Number of times access was done by root user. ontaps3_svm_root_user_access is [ontaps3_svm_root_user_access](#ontaps3_svm_root_user_access) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `root_user_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `root_user_access`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_server_connection_close

Number of connection closes triggered by server due to fatal errors. ontaps3_svm_server_connection_close is [ontaps3_svm_server_connection_close](#ontaps3_svm_server_connection_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `server_connection_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `server_connection_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_signature_v2_reqs

Total number of object store server signature V2 requests. ontaps3_svm_signature_v2_reqs is [ontaps3_svm_signature_v2_reqs](#ontaps3_svm_signature_v2_reqs) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `signature_v2_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `signature_v2_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_signature_v4_reqs

Total number of object store server signature V4 requests. ontaps3_svm_signature_v4_reqs is [ontaps3_svm_signature_v4_reqs](#ontaps3_svm_signature_v4_reqs) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `signature_v4_requests`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `signature_v4_reqs`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_tagging

Number of requests with tagging specified. ontaps3_svm_tagging is [ontaps3_svm_tagging](#ontaps3_svm_tagging) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `tagging`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `tagging`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_upload_part_failed

Number of failed Upload Part operations. ontaps3_svm_upload_part_failed is [ontaps3_svm_upload_part_failed](#ontaps3_svm_upload_part_failed) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `upload_part_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `upload_part_failed`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_upload_part_failed_client_close

Number of times Upload Part operation failed because client terminated connection while the operation was still pending on server. ontaps3_svm_upload_part_failed_client_close is [ontaps3_svm_upload_part_failed_client_close](#ontaps3_svm_upload_part_failed_client_close) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `upload_part_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `upload_part_failed_client_close`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_upload_part_latency

Average latency in microseconds for Upload Part operations. ontaps3_svm_upload_part_latency is [ontaps3_svm_upload_part_latency](#ontaps3_svm_upload_part_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `upload_part_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> upload_part_total | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `upload_part_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> upload_part_latency_base | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_upload_part_rate

Number of Upload Part operations per second. ontaps3_svm_upload_part_rate is [ontaps3_svm_upload_part_rate](#ontaps3_svm_upload_part_rate) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `upload_part_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `upload_part_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_svm_upload_part_total

Number of Upload Part operations. ontaps3_svm_upload_part_total is [ontaps3_svm_upload_part_total](#ontaps3_svm_upload_part_total) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/object_store_server` | `upload_part_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.14.1/ontap_s3_svm.yaml |
| ZapiPerf | `perf-object-get-instances object_store_server` | `upload_part_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/ontap_s3_svm.yaml |



### ontaps3_used_percent

The used_percent metric the percentage of a bucket's total capacity that is currently being used.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/s3/buckets` | `logical_used_size, size` | conf/rest/9.7.0/ontap_s3.yaml |

The `ontaps3_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: S3 Object Storage | Highlights | table | [Bucket Overview](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=5) |
| ONTAP: S3 Object Storage | Highlights | timeseries | [Top $TopResources Buckets by Used Size Percent](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=128) |
///



### path_read_data

The average read throughput in kilobytes per second read from the indicated target port by the controller.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/path` | `read_data`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/path.yaml |
| ZapiPerf | `perf-object-get-instances path` | `read_data`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/path.yaml |

The `path_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FibreBridge/Array | timeseries | [Read Data from FibreBridge/Array WWPN](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=100) |
///



### path_read_iops

The number of I/O read operations sent from the initiator port to the indicated target port.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/path` | `read_iops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/path.yaml |
| ZapiPerf | `perf-object-get-instances path` | `read_iops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/path.yaml |

The `path_read_iops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FibreBridge/Array | timeseries | [Read IOPs from FibreBridge/Array WWPN](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=97) |
///



### path_read_latency

The average latency in microseconds of I/O read operations sent from this controller to the indicated target port.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/path` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_iops | conf/restperf/9.12.0/path.yaml |
| ZapiPerf | `perf-object-get-instances path` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_iops | conf/zapiperf/cdot/9.8.0/path.yaml |

The `path_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FibreBridge/Array | timeseries | [Read Latency from FibreBridge/Array WWPN](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=86) |
///



### path_total_data

The average throughput in kilobytes per second read and written from/to the indicated target port by the controller.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/path` | `total_data`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/path.yaml |
| ZapiPerf | `perf-object-get-instances path` | `total_data`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/path.yaml |



### path_total_iops

The number of total read/write I/O operations sent from the initiator port to the indicated target port.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/path` | `total_iops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/path.yaml |
| ZapiPerf | `perf-object-get-instances path` | `total_iops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/path.yaml |



### path_write_data

The average write throughput in kilobytes per second written to the indicated target port by the controller.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/path` | `write_data`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/path.yaml |
| ZapiPerf | `perf-object-get-instances path` | `write_data`<br><span class="key">Unit:</span> kb_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/path.yaml |

The `path_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FibreBridge/Array | timeseries | [Write Data to FibreBridge/Array WWPN](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=96) |
///



### path_write_iops

The number of I/O write operations sent from the initiator port to the indicated target port.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/path` | `write_iops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/path.yaml |
| ZapiPerf | `perf-object-get-instances path` | `write_iops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/path.yaml |

The `path_write_iops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FibreBridge/Array | timeseries | [Write IOPs to FibreBridge/Array WWPN](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=99) |
///



### path_write_latency

The average latency in microseconds of I/O write operations sent from this controller to the indicated target port.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/path` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_iops | conf/restperf/9.12.0/path.yaml |
| ZapiPerf | `perf-object-get-instances path` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_iops | conf/zapiperf/cdot/9.8.0/path.yaml |

The `path_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster FibreBridge/Array | timeseries | [Write Latency to FibreBridge/Array WWPN](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=98) |
///



### plex_disk_busy

The utilization percent of the disk. plex_disk_busy is [disk_busy](#disk_busy) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `disk_busy_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `plex_disk_busy` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Disk | timeseries | [Top $TopResources Plexes by Disk Utilization](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=17) |
| ONTAP: MetroCluster | MetroCluster Disk | table | [Top $TopResources Plexes by Disk Utilization](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=95) |
///



### plex_disk_capacity

Disk capacity in MB. plex_disk_capacity is [disk_capacity](#disk_capacity) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_cp_read_chain

Average number of blocks transferred in each consistency point read operation during a CP. plex_disk_cp_read_chain is [disk_cp_read_chain](#disk_cp_read_chain) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_cp_read_latency

Average latency per block in microseconds for consistency point read operations. plex_disk_cp_read_latency is [disk_cp_read_latency](#disk_cp_read_latency) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_cp_reads

Number of disk read operations initiated each second for consistency point processing. plex_disk_cp_reads is [disk_cp_reads](#disk_cp_reads) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_io_pending

Average number of I/Os issued to the disk for which we have not yet received the response. plex_disk_io_pending is [disk_io_pending](#disk_io_pending) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_io_queued

Number of I/Os queued to the disk but not yet issued. plex_disk_io_queued is [disk_io_queued](#disk_io_queued) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_total_data

Total throughput for user operations per second. plex_disk_total_data is [disk_total_data](#disk_total_data) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_total_transfers

Total number of disk operations involving data transfer initiated per second. plex_disk_total_transfers is [disk_total_transfers](#disk_total_transfers) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_transfer_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_transfers`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_user_read_blocks

Number of blocks transferred for user read operations per second. plex_disk_user_read_blocks is [disk_user_read_blocks](#disk_user_read_blocks) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_user_read_chain

Average number of blocks transferred in each user read operation. plex_disk_user_read_chain is [disk_user_read_chain](#disk_user_read_chain) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_user_read_latency

Average latency per block in microseconds for user read operations. plex_disk_user_read_latency is [disk_user_read_latency](#disk_user_read_latency) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `plex_disk_user_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Disk | timeseries | [Top $TopResources Plexes by User Read Latency per 4KB IO](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=71) |
///



### plex_disk_user_reads

Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. plex_disk_user_reads is [disk_user_reads](#disk_user_reads) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `plex_disk_user_reads` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Disk | timeseries | [Top $TopResources Plexes by User Reads](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=73) |
///



### plex_disk_user_write_blocks

Number of blocks transferred for user write operations per second. plex_disk_user_write_blocks is [disk_user_write_blocks](#disk_user_write_blocks) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_user_write_chain

Average number of blocks transferred in each user write operation. plex_disk_user_write_chain is [disk_user_write_chain](#disk_user_write_chain) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_writes | conf/zapiperf/cdot/9.8.0/disk.yaml |



### plex_disk_user_write_latency

Average latency per block in microseconds for user write operations. plex_disk_user_write_latency is [disk_user_write_latency](#disk_user_write_latency) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `plex_disk_user_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Disk | timeseries | [Top $TopResources Plexes by User Write Latency per 4KB IO](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=69) |
///



### plex_disk_user_writes

Number of disk write operations initiated each second for storing data or metadata associated with user requests. plex_disk_user_writes is [disk_user_writes](#disk_user_writes) aggregated by `plex`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_writes`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `plex_disk_user_writes` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: MetroCluster | MetroCluster Disk | timeseries | [Top $TopResources Plexes by User Writes](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=74) |
///



### poller_concurrent_collectors

Tracks the number of concurrent collectors running.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> bytes | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> bytes | NA | 



### poller_cpu_percent

Tracks the percentage of cpu usage of concurrent collectors running.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | NA |

The `poller_cpu_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | timeseries | [% CPU Used](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=183) |
///



### poller_memory

Tracks the memory usage of the poller process, including Resident Set Size (RSS), swap memory, and Virtual Memory Size (VMS).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> bytes | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> bytes | NA | 

The `poller_memory` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | table | [Pollers](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=156) |
| Harvest Metadata | Highlights | timeseries | [Poller RSS Memory](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=181) |
///



### poller_memory_percent

Indicates the percentage of memory used by the poller process relative to the total available memory.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated`<br><span class="key">Unit:</span> percent | NA | 
| ZAPI | `NA` | `Harvest generated`<br><span class="key">Unit:</span> percent | NA | 

The `poller_memory_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | timeseries | [% Memory Used](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=184) |
///



### poller_status

Indicates the operational status of the poller process, where 1 means operational and 0 means not operational.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | NA |
| ZAPI | `NA` | `Harvest generated` | NA |

The `poller_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| Harvest Metadata | Highlights | stat | [Pollers](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=33) |
| Harvest Metadata | Highlights | table | [Pollers](/d/cdot-metadata/harvest metadata?orgId=1&viewPanel=156) |
///



### qos_concurrency

This is the average number of concurrent requests for the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `concurrency`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `concurrency`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_concurrency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Concurrency](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=173) |
///



### qos_latency

This is the average response time for requests that were initiated by the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> ops | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> ops | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | stat | [Average QoS Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=209) |
| ONTAP: Volume | QoS | stat | [Top $TopResources QoS Volumes Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=103) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources QoS Volumes by Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=111) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Average Latency](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=170) |
///



### qos_ops

This field is the workload's rate of operations that completed during the measurement interval; measured per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | stat | [QoS IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=210) |
| ONTAP: Volume | QoS | stat | [Top $TopResources QoS Volumes by Total IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=109) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources QoS Volumes by Total IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=117) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Total IOPS](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=165) |
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | timeseries | [Top $TopResources Fixed QoS Shared Policy IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=245) |
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | table | [Fixed QoS Shared Policy IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=249) |
| ONTAP: Workload | Fixed QoS Workload Utilization | timeseries | [Top $TopResources Fixed QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=219) |
| ONTAP: Workload | Fixed QoS Workload Utilization | table | [Fixed QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=223) |
| ONTAP: Workload | Adaptive QoS Workload Utilization | timeseries | [Top $TopResources Adaptive QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=228) |
| ONTAP: Workload | Adaptive QoS Workload Utilization | table | [Adaptive QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=239) |
///



### qos_other_ops

This is the rate of this workload's other operations that completed during the measurement interval.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/workload.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_other_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Other IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=118) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Other IOPS](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=166) |
///



### qos_policy_adaptive_absolute_min_iops

Specifies the absolute minimum IOPS that is used as an override when the expected_iops is less than this value.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_policy_adaptive.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_policy_adaptive.yaml |

The `qos_policy_adaptive_absolute_min_iops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Adaptive QoS Workload Utilization | table | [Adaptive QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=239) |
///



### qos_policy_adaptive_expected_iops

Specifies the size to be used to calculate expected IOPS per TB.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_policy_adaptive.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_policy_adaptive.yaml |



### qos_policy_adaptive_labels

This metric provides information about QosPolicyAdaptive

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/qos/adaptive-policy-group` | `Harvest generated` | conf/rest/9.12.0/qos_policy_adaptive.yaml |
| ZAPI | `qos-adaptive-policy-group-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_policy_adaptive.yaml |



### qos_policy_adaptive_peak_iops

Specifies the maximum possible IOPS per TB allocated based on the storage object allocated size or the storage object used size.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_policy_adaptive.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_policy_adaptive.yaml |

The `qos_policy_adaptive_peak_iops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Adaptive QoS Workload Utilization | table | [Adaptive QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=239) |
///



### qos_policy_fixed_labels

This metric provides information about QosPolicyFixed

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/qos/policy-group` | `Harvest generated` | conf/rest/9.12.0/qos_policy_fixed.yaml |
| ZAPI | `qos-policy-group-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml |

The `qos_policy_fixed_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | timeseries | [Top $TopResources Fixed QoS Shared Policy IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=245) |
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | timeseries | [Top $TopResources Fixed QoS Shared Policy Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=247) |
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | table | [Fixed QoS Shared Policy IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=249) |
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | table | [Fixed QoS Shared Policy Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=251) |
| ONTAP: Workload | Fixed QoS Workload Utilization | table | [Fixed QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=223) |
| ONTAP: Workload | Fixed QoS Workload Utilization | table | [Fixed QoS Workload Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=224) |
///



### qos_policy_fixed_max_throughput_iops

Maximum throughput defined by this policy. It is specified in terms of IOPS. 0 means no maximum throughput is enforced.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_policy_fixed.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml |

The `qos_policy_fixed_max_throughput_iops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | timeseries | [Top $TopResources Fixed QoS Shared Policy IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=245) |
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | table | [Fixed QoS Shared Policy IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=249) |
| ONTAP: Workload | Fixed QoS Workload Utilization | timeseries | [Top $TopResources Fixed QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=219) |
///



### qos_policy_fixed_max_throughput_mbps

Maximum throughput defined by this policy. It is specified in terms of Mbps. 0 means no maximum throughput is enforced.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_policy_fixed.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml |

The `qos_policy_fixed_max_throughput_mbps` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | timeseries | [Top $TopResources Fixed QoS Shared Policy Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=247) |
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | table | [Fixed QoS Shared Policy Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=251) |
| ONTAP: Workload | Fixed QoS Workload Utilization | timeseries | [Top $TopResources Fixed QoS Workload Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=221) |
///



### qos_policy_fixed_min_throughput_iops

Minimum throughput defined by this policy. It is specified in terms of IOPS. 0 means no minimum throughput is enforced. These floors are not guaranteed on non-AFF platforms or when FabricPool tiering policies are set.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_policy_fixed.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml |



### qos_policy_fixed_min_throughput_mbps

Minimum throughput defined by this policy. It is specified in terms of Mbps. 0 means no minimum throughput is enforced.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_policy_fixed.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_policy_fixed.yaml |



### qos_read_data

This is the amount of data read in bytes per second from the filer by the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | stat | [QoS Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=214) |
| ONTAP: SVM | QoS Policy Group | timeseries | [Top $TopResources SVMs by QoS Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=219) |
| ONTAP: Volume | QoS | stat | [Top $TopResources Qos Volumes Total Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=104) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources QoS Volumes by Average Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=113) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=74) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Read Throughput](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=167) |
///



### qos_read_io_type

This is the percentage of read requests served from various components (such as buffer cache, ext_cache, disk, etc.).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `read_io_type_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_io_type_base | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `read_io_type`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_io_type_base | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_read_io_type` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type bamboo_ssd](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=172) |
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type cache](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=176) |
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type cloud](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=177) |
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type cloud_s2c](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=178) |
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type disk](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=179) |
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type ext_cache](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=180) |
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type fc_miss](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=181) |
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type hya_cache](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=182) |
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type hya_hdd](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=183) |
| ONTAP: Workload | Read IO Type | timeseries | [Top $TopResources Workloads by Read IO Type hya_non_cache](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=184) |
///



### qos_read_latency

This is the average response time for read requests that were initiated by the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | stat | [Average QoS Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=211) |
| ONTAP: SVM | QoS Policy Group | timeseries | [Top $TopResources SVMs by QoS Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=218) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Read Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=72) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Average Read Latency](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=169) |
///



### qos_read_ops

This is the rate of this workload's read operations that completed during the measurement interval.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | stat | [QoS Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=216) |
| ONTAP: SVM | QoS Policy Group | timeseries | [Top $TopResources SVMs by QoS IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=220) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Read IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=76) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Read IOPS](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=163) |
///



### qos_sequential_reads

This is the percentage of reads, performed on behalf of the workload, that were sequential.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `sequential_reads_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> sequential_reads_base | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `sequential_reads`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent,no-zero-values<br><span class="key">Base:</span> sequential_reads_base | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_sequential_reads` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | timeseries | [Top $TopResources Workloads by Sequential Reads (%)](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=222) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Sequential Reads](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=79) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Sequential Reads (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=253) |
///



### qos_sequential_writes

This is the percentage of writes, performed on behalf of the workload, that were sequential. This counter is only available on platforms with more than 4GB of NVRAM.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `sequential_writes_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> sequential_writes_base | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `sequential_writes`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent,no-zero-values<br><span class="key">Base:</span> sequential_writes_base | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_sequential_writes` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | timeseries | [Top $TopResources Workloads by Sequential Writes (%)](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=585) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Sequential Writes](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=80) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Sequential Writes (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=254) |
///



### qos_total_data

This is the total amount of data read/written in bytes per second from/to the filer by the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_total_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | timeseries | [Top $TopResources Fixed QoS Shared Policy Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=247) |
| ONTAP: Workload | Fixed QoS Shared Policy Utilization | table | [Fixed QoS Shared Policy Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=251) |
| ONTAP: Workload | Fixed QoS Workload Utilization | timeseries | [Top $TopResources Fixed QoS Workload Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=221) |
| ONTAP: Workload | Fixed QoS Workload Utilization | table | [Fixed QoS Workload Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=224) |
| ONTAP: Workload | Adaptive QoS Workload Utilization | timeseries | [Top $TopResources Adaptive QoS Workload Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=229) |
| ONTAP: Workload | Adaptive QoS Workload Utilization | table | [Adaptive QoS Workload Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=241) |
///



### qos_workload_labels

This metric provides information about QosWorkload

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/qos/workload` | `Harvest generated` | conf/rest/9.12.0/qos_workload.yaml |
| ZAPI | `qos-workload-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_workload.yaml |

The `qos_workload_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: Workload | Adaptive QoS Workload Utilization | table | [Adaptive QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=239) |
| ONTAP: Workload | Adaptive QoS Workload Utilization | table | [Adaptive QoS Workload Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=241) |
///



### qos_workload_max_throughput_iops

Maximum throughput IOPs allowed for the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_workload.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_workload.yaml |

The `qos_workload_max_throughput_iops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Adaptive QoS Workload Utilization | timeseries | [Top $TopResources Adaptive QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=228) |
| ONTAP: Workload | Adaptive QoS Workload Utilization | table | [Adaptive QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=239) |
///



### qos_workload_max_throughput_mbps

Maximum throughput Mbps allowed for the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_workload.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_workload.yaml |

The `qos_workload_max_throughput_mbps` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Adaptive QoS Workload Utilization | timeseries | [Top $TopResources Adaptive QoS Workload Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=229) |
| ONTAP: Workload | Adaptive QoS Workload Utilization | table | [Adaptive QoS Workload Bandwidth Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=241) |
///



### qos_workload_min_throughput_iops

Minimum throughput IOPs allowed for the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_workload.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_workload.yaml |

The `qos_workload_min_throughput_iops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Workload | Adaptive QoS Workload Utilization | table | [Adaptive QoS Workload IOPs Utilization (%)](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=239) |
///



### qos_workload_min_throughput_mbps

Minimum throughput Mbps allowed for the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/qos_workload.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/qos_workload.yaml |



### qos_write_data

This is the amount of data written in bytes per second to the filer by the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | stat | [QoS Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=212) |
| ONTAP: SVM | QoS Policy Group | stat | [QoS Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=215) |
| ONTAP: SVM | QoS Policy Group | timeseries | [Top $TopResources SVMs by QoS Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=219) |
| ONTAP: Volume | QoS | stat | [Top $TopResources Qos Volumes Total Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=104) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources QoS Volumes by Average Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=113) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=75) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Write Throughput](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=168) |
///



### qos_write_latency

This is the average response time for write requests that were initiated by the workload.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | stat | [Average QoS Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=213) |
| ONTAP: SVM | QoS Policy Group | timeseries | [Top $TopResources SVMs by QoS Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=218) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Write Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=73) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Average Write Latency](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=171) |
///



### qos_write_ops

This is the workload's write operations that completed during the measurement interval; measured per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qos_volume` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/workload_volume.yaml |
| ZapiPerf | `perf-object-get-instances workload_volume` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/workload_volume.yaml |

The `qos_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | QoS Policy Group | stat | [QoS Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=217) |
| ONTAP: SVM | QoS Policy Group | timeseries | [Top $TopResources SVMs by QoS IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=220) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Write IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=77) |
| ONTAP: Workload | Highlights | timeseries | [Top $TopResources Workloads by Write IOPS](/d/cdot-workload/ontap3a-workload?orgId=1&viewPanel=164) |
///



### qtree_cifs_ops

Number of CIFS operations per second to the qtree

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qtree` | `cifs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/qtree.yaml |
| ZapiPerf | `perf-object-get-instances qtree` | `cifs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/qtree.yaml |

The `qtree_cifs_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Qtree | Highlights | timeseries | [Top $TopResources CIFS by IOPs](/d/cdot-qtree/ontap3a-qtree?orgId=1&viewPanel=120) |
///



### qtree_id

The identifier for the qtree, unique within the qtree's volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/qtrees` | `id` | conf/rest/9.12.0/qtree.yaml |



### qtree_internal_ops

Number of internal operations generated by activites such as snapmirror and backup per second to the qtree

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qtree` | `internal_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/qtree.yaml |
| ZapiPerf | `perf-object-get-instances qtree` | `internal_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/qtree.yaml |

The `qtree_internal_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Qtree | Highlights | timeseries | [Top $TopResources Qtrees by Internal IOPs](/d/cdot-qtree/ontap3a-qtree?orgId=1&viewPanel=123) |
///



### qtree_labels

This metric provides information about Qtree

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/qtrees` | `Harvest generated` | conf/rest/9.12.0/qtree.yaml |
| ZAPI | `qtree-list-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/qtree.yaml |

The `qtree_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
///



### qtree_nfs_ops

Number of NFS operations per second to the qtree

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qtree` | `nfs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/qtree.yaml |
| ZapiPerf | `perf-object-get-instances qtree` | `nfs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/qtree.yaml |

The `qtree_nfs_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Qtree | Highlights | timeseries | [Top $TopResources NFSs by IOPs](/d/cdot-qtree/ontap3a-qtree?orgId=1&viewPanel=92) |
///



### qtree_other_data

Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.throughput_raw.other`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.16.0/qtree.yaml |



### qtree_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> qtree_statistics.iops_raw.other | conf/keyperf/9.16.0/qtree.yaml |



### qtree_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.16.0/qtree.yaml |



### qtree_read_data

Performance metric for read I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.16.0/qtree.yaml |



### qtree_read_latency

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> qtree_statistics.iops_raw.read | conf/keyperf/9.16.0/qtree.yaml |



### qtree_read_ops

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.16.0/qtree.yaml |



### qtree_total_data

Performance metric aggregated over all types of I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.16.0/qtree.yaml |



### qtree_total_latency

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> qtree_statistics.iops_raw.total | conf/keyperf/9.16.0/qtree.yaml |



### qtree_total_ops

Summation of NFS ops, CIFS ops, CSS ops and internal ops

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/qtree` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/qtree.yaml |
| KeyPerf | `api/storage/qtrees` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.16.0/qtree.yaml |
| ZapiPerf | `perf-object-get-instances qtree` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/qtree.yaml |

The `qtree_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Qtree | Highlights | timeseries | [Top $TopResources Qtrees by IOPs](/d/cdot-qtree/ontap3a-qtree?orgId=1&viewPanel=122) |
| ONTAP: Volume Deep Dive | Highlights | timeseries | [Qtrees by IOPs](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=2) |
///



### qtree_write_data

Performance metric for write I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.16.0/qtree.yaml |



### qtree_write_latency

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> qtree_statistics.iops_raw.write | conf/keyperf/9.16.0/qtree.yaml |



### qtree_write_ops

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/qtrees` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.16.0/qtree.yaml |



### quota_disk_limit

Maximum amount of disk space, in kilobytes, allowed for the quota target (hard disk space limit). The value is -1 if the limit is unlimited.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `space.hard_limit` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `disk-limit` | conf/zapi/cdot/9.8.0/qtree.yaml |

The `quota_disk_limit` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: Quota | Highlights | table | [Reports](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=5) |
///



### quota_disk_used

Current amount of disk space, in kilobytes, used by the quota target.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `space.used.total` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `disk-used` | conf/zapi/cdot/9.8.0/qtree.yaml |

The `quota_disk_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Qtree | Usage | timeseries | [Top $TopResources Qtrees by Disk Used](/d/cdot-qtree/ontap3a-qtree?orgId=1&viewPanel=125) |
| ONTAP: Qtree | Usage | timeseries | [Top $TopResources Qtrees by Disk Used Growth](/d/cdot-qtree/ontap3a-qtree?orgId=1&viewPanel=129) |
| ONTAP: Quota | Highlights | table | [Reports](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=5) |
| ONTAP: Quota | Space Usage | timeseries | [Top $TopResources Quotas by Space Used](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=7) |
///



### quota_disk_used_pct_disk_limit

Current disk space used expressed as a percentage of hard disk limit.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `space.used.hard_limit_percent` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `disk-used-pct-disk-limit` | conf/zapi/cdot/9.8.0/qtree.yaml |

The `quota_disk_used_pct_disk_limit` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Quota | Highlights | table | [Reports](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=5) |
| ONTAP: Quota | Space Usage | timeseries | [Top $TopResources Quotas by Space Used %](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=8) |
///



### quota_disk_used_pct_soft_disk_limit

Current disk space used expressed as a percentage of soft disk limit.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `space.used.soft_limit_percent` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `disk-used-pct-soft-disk-limit` | conf/zapi/cdot/9.8.0/qtree.yaml |



### quota_disk_used_pct_threshold

Current disk space used expressed as a percentage of threshold.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZAPI | `quota-report-iter` | `disk-used-pct-threshold` | conf/zapi/cdot/9.8.0/qtree.yaml |



### quota_file_limit

Maximum number of files allowed for the quota target (hard files limit). The value is -1 if the limit is unlimited.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `files.hard_limit` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `file-limit` | conf/zapi/cdot/9.8.0/qtree.yaml |

The `quota_file_limit` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Quota | Highlights | table | [Reports](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=5) |
///



### quota_files_used

Current number of files used by the quota target.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `files.used.total` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `files-used` | conf/zapi/cdot/9.8.0/qtree.yaml |

The `quota_files_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Qtree | Usage | timeseries | [Top $TopResources Qtrees by Files Used](/d/cdot-qtree/ontap3a-qtree?orgId=1&viewPanel=126) |
| ONTAP: Quota | Highlights | table | [Reports](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=5) |
| ONTAP: Quota | Space Usage | timeseries | [Top $TopResources Quotas by Files Used](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=9) |
///



### quota_files_used_pct_file_limit

Current number of files used expressed as a percentage of hard file limit.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `files.used.hard_limit_percent` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `files-used-pct-file-limit` | conf/zapi/cdot/9.8.0/qtree.yaml |

The `quota_files_used_pct_file_limit` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Quota | Highlights | table | [Reports](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=5) |
| ONTAP: Quota | Space Usage | timeseries | [Top $TopResources Quotas by Files Used %](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=10) |
///



### quota_files_used_pct_soft_file_limit

Current number of files used expressed as a percentage of soft file limit.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `files.used.soft_limit_percent` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `files-used-pct-soft-file-limit` | conf/zapi/cdot/9.8.0/qtree.yaml |



### quota_soft_disk_limit

soft disk space limit, in kilobytes, for the quota target. The value is -1 if the limit is unlimited.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `space.soft_limit` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `soft-disk-limit` | conf/zapi/cdot/9.8.0/qtree.yaml |

The `quota_soft_disk_limit` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Quota | Highlights | table | [Reports](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=5) |
///



### quota_soft_file_limit

Soft file limit, in number of files, for the quota target. The value is -1 if the limit is unlimited.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/quota/reports` | `files.soft_limit` | conf/rest/9.12.0/quota.yaml |
| ZAPI | `quota-report-iter` | `soft-file-limit` | conf/zapi/cdot/9.8.0/qtree.yaml |

The `quota_soft_file_limit` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Quota | Highlights | table | [Reports](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=5) |
///



### quota_threshold

Disk space threshold, in kilobytes, for the quota target. The value is -1 if the limit is unlimited.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZAPI | `quota-report-iter` | `threshold` | conf/zapi/cdot/9.8.0/qtree.yaml |
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/quota.yaml |



### raid_disk_busy

The utilization percent of the disk. raid_disk_busy is [disk_busy](#disk_busy) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `disk_busy_percent`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_busy`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `raid_disk_busy` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Top Disks: Raid-level Overview | timeseries | [Top $TopResources Disks by Disk Busy](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=59) |
///



### raid_disk_capacity

Disk capacity in MB. raid_disk_capacity is [disk_capacity](#disk_capacity) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `disk_capacity`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_cp_read_chain

Average number of blocks transferred in each consistency point read operation during a CP. raid_disk_cp_read_chain is [disk_cp_read_chain](#disk_cp_read_chain) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_cp_read_latency

Average latency per block in microseconds for consistency point read operations. raid_disk_cp_read_latency is [disk_cp_read_latency](#disk_cp_read_latency) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cp_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_cp_reads

Number of disk read operations initiated each second for consistency point processing. raid_disk_cp_reads is [disk_cp_reads](#disk_cp_reads) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `cp_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `cp_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_io_pending

Average number of I/Os issued to the disk for which we have not yet received the response. raid_disk_io_pending is [disk_io_pending](#disk_io_pending) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_pending`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_io_queued

Number of I/Os queued to the disk but not yet issued. raid_disk_io_queued is [disk_io_queued](#disk_io_queued) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `io_queued`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> base_for_disk_busy | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_total_data

Total throughput for user operations per second. raid_disk_total_data is [disk_total_data](#disk_total_data) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_total_transfers

Total number of disk operations involving data transfer initiated per second. raid_disk_total_transfers is [disk_total_transfers](#disk_total_transfers) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `total_transfer_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `total_transfers`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `raid_disk_total_transfers` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Top Disks: Raid-level Overview | timeseries | [Top $TopResources Disks by Total Transfers](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=60) |
///



### raid_disk_user_read_blocks

Number of blocks transferred for user read operations per second. raid_disk_user_read_blocks is [disk_user_read_blocks](#disk_user_read_blocks) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_user_read_chain

Average number of blocks transferred in each user read operation. raid_disk_user_read_chain is [disk_user_read_chain](#disk_user_read_chain) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_reads | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_user_read_latency

Average latency per block in microseconds for user read operations. raid_disk_user_read_latency is [disk_user_read_latency](#disk_user_read_latency) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_read_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `raid_disk_user_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Top Disks: Raid-level Overview | timeseries | [Top $TopResources Disks by User Read Latency](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=58) |
///



### raid_disk_user_reads

Number of disk read operations initiated each second for retrieving data or metadata associated with user requests. raid_disk_user_reads is [disk_user_reads](#disk_user_reads) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_read_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_reads`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_user_write_blocks

Number of blocks transferred for user write operations per second. raid_disk_user_write_blocks is [disk_user_write_blocks](#disk_user_write_blocks) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_block_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_blocks`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_user_write_chain

Average number of blocks transferred in each user write operation. raid_disk_user_write_chain is [disk_user_write_chain](#disk_user_write_chain) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_chain`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_writes | conf/zapiperf/cdot/9.8.0/disk.yaml |



### raid_disk_user_write_latency

Average latency per block in microseconds for user write operations. raid_disk_user_write_latency is [disk_user_write_latency](#disk_user_write_latency) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_block_count | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> user_write_blocks | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `raid_disk_user_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Top Disks: Raid-level Overview | timeseries | [Top $TopResources Disks by User Write Latency](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=61) |
///



### raid_disk_user_writes

Number of disk write operations initiated each second for storing data or metadata associated with user requests. raid_disk_user_writes is [disk_user_writes](#disk_user_writes) aggregated by `raid`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/disk:constituent` | `user_write_count`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `perf-object-get-instances disk:constituent` | `user_writes`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### rw_ctx_cifs_giveups

Array of number of give-ups of CIFS ops because they rewind more than a certain threshold, categorized by their rewind reasons.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/rewind_context` | `cifs_give_ups`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.16.0/rwctx.yaml |
| ZapiPerf | `perf-object-get-instances rw_ctx` | `cifs_giveups`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/rwctx.yaml |

The `rw_ctx_cifs_giveups` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Rewind View | timeseries | [Top $TopResources Metric by CIFS Giveups](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=86) |
///



### rw_ctx_cifs_rewinds

Array of number of rewinds for CIFS ops based on their reasons.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/rewind_context` | `cifs_rewinds`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.16.0/rwctx.yaml |
| ZapiPerf | `perf-object-get-instances rw_ctx` | `cifs_rewinds`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/rwctx.yaml |

The `rw_ctx_cifs_rewinds` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Rewind View | timeseries | [Top $TopResources Metric by CIFS Rewinds](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=85) |
///



### rw_ctx_nfs_giveups

Array of number of give-ups of NFS ops because they rewind more than a certain threshold, categorized by their rewind reasons.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/rewind_context` | `nfs_give_ups`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.16.0/rwctx.yaml |
| ZapiPerf | `perf-object-get-instances rw_ctx` | `nfs_giveups`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/rwctx.yaml |

The `rw_ctx_nfs_giveups` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Rewind View | timeseries | [Top $TopResources Metric by NFS Giveups](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=83) |
///



### rw_ctx_nfs_rewinds

Array of number of rewinds for NFS ops based on their reasons.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/rewind_context` | `nfs_rewinds`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.16.0/rwctx.yaml |
| ZapiPerf | `perf-object-get-instances rw_ctx` | `nfs_rewinds`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/rwctx.yaml |

The `rw_ctx_nfs_rewinds` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Rewind View | timeseries | [Top $TopResources Metric by NFS Rewinds](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=82) |
///



### rw_ctx_qos_flowcontrol

The number of times QoS limiting has enabled stream flowcontrol.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances rw_ctx` | `qos_flowcontrol`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/rwctx.yaml |

The `rw_ctx_qos_flowcontrol` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Rewind View | timeseries | [Top $TopResources Node by QoS Flowcontrol](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=89) |
///



### rw_ctx_qos_rewinds

The number of restarts after a rewind because of QoS limiting.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances rw_ctx` | `qos_rewinds`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/rwctx.yaml |

The `rw_ctx_qos_rewinds` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Rewind View | timeseries | [Top $TopResources Node by QoS Rewinds](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=87) |
///



### security_account_activediruser

Represent the Active directory user in security account

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| Rest | `NA` | `Harvest generated` | conf/rest/9.12.0/security_account.yaml |

The `security_account_activediruser` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [AD/LDAP](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=192) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_account_certificateuser

Represent the Certificate user in security account

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| Rest | `NA` | `Harvest generated` | conf/rest/9.12.0/security_account.yaml |

The `security_account_certificateuser` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Certificate](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=190) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_account_labels

This metric provides information about SecurityAccount

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security/accounts` | `Harvest generated` | conf/rest/9.12.0/security_account.yaml |
| ZAPI | `security-login-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/security_account.yaml |

The `security_account_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Cluster Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=214) |
| ONTAP: Security | Highlights | piechart | [Cluster Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=215) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_account_ldapuser

Represent the LDAP user in security account

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| Rest | `NA` | `Harvest generated` | conf/rest/9.12.0/security_account.yaml |

The `security_account_ldapuser` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [AD/LDAP](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=192) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_account_localuser

Represent the Local user in security account

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| Rest | `NA` | `Harvest generated` | conf/rest/9.12.0/security_account.yaml |

The `security_account_localuser` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Local](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=194) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_account_samluser

Represent the SAML user in security account

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| Rest | `NA` | `Harvest generated` | conf/rest/9.12.0/security_account.yaml |

The `security_account_samluser` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [SAML](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=158) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_audit_destination_port

The destination port used to forward the message.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZAPI | `cluster-log-forward-get-iter` | `cluster-log-forward-info.port` | conf/zapi/cdot/9.8.0/security_audit_dest.yaml |



### security_audit_destination_status

Represent the security audit protocol in security audit destinations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| Rest | `NA` | `Harvest generated` | conf/rest/9.12.0/security_audit_dest.yaml |

The `security_audit_destination_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Cluster Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=214) |
| ONTAP: Security | Highlights | piechart | [Cluster Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=215) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_certificate_expiry_time



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/security/certificate` | `expiration` | conf/rest/9.12.0/security_certificate.yaml |
| ZAPI | `security-certificate-get-iter` | `certificate-info.expiration-date` | conf/zapi/cdot/9.8.0/security_certificate.yaml |

The `security_certificate_expiry_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | table | [SSL Certificates Expiration](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=228) |
///



### security_certificate_labels

This metric provides information about SecurityCert

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/security/certificate` | `Harvest generated` | conf/rest/9.12.0/security_certificate.yaml |
| ZAPI | `security-certificate-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/security_certificate.yaml |

The `security_certificate_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Expiring in < 60 days](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=193) |
| ONTAP: Security | Highlights | stat | [Expired](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=191) |
| ONTAP: Security | Highlights | table | [SSL Certificates Expiration](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=228) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_labels

This metric provides information about Security

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security` | `Harvest generated` | conf/rest/9.12.0/security.yaml |
| ZAPI | `cluster-identity-get` | `Harvest generated` | conf/zapi/cdot/9.8.0/security.yaml |

The `security_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Cluster Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=214) |
| ONTAP: Security | Highlights | piechart | [Cluster Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=215) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_login_labels

This metric provides information about SecurityLogin

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security/login/messages` | `Harvest generated` | conf/rest/9.12.0/security_login.yaml |
| ZAPI | `vserver-login-banner-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/security_login.yaml |

The `security_login_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Cluster Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=214) |
| ONTAP: Security | Highlights | stat | [SVM Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=216) |
| ONTAP: Security | Highlights | piechart | [Cluster Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=215) |
| ONTAP: Security | Highlights | piechart | [SVM Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=217) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
| ONTAP: Security | SVM Compliance | table | [SVM Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=225) |
///



### security_ssh_labels

This metric provides information about SecuritySsh

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security/ssh` | `Harvest generated` | conf/rest/9.12.0/security_ssh.yaml |

The `security_ssh_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Cluster Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=214) |
| ONTAP: Security | Highlights | piechart | [Cluster Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=215) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### security_ssh_max_instances

Maximum possible simultaneous connections.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/security/ssh` | `max_instances` | conf/rest/9.12.0/security_ssh.yaml |



### shelf_average_ambient_temperature

Average temperature of all ambient sensors for shelf in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_average_ambient_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_average_fan_speed

Average fan speed for shelf in rpm.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_average_fan_speed` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_average_temperature

Average temperature of all non-ambient sensors for shelf in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_average_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Highlights | timeseries | [Top $TopResources Shelves by Average Temperature](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=88) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | timeseries | [Top $TopResources Shelves by Average Temperature](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=73) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_disk_count

Disk count in a shelf.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/shelves` | `disk_count` | conf/rest/9.12.0/shelf.yaml |
| ZAPI | `storage-shelf-info-get-iter` | `storage-shelf-info.disk-count` | conf/zapi/cdot/9.8.0/shelf.yaml |

The `shelf_disk_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_fan_labels

This metric provides information about shelf fans.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### shelf_fan_rpm

Fan Rotation Per Minute.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_fan_rpm` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Shelf | Highlights | stat | [Fan RPM Avg](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=65) |
| ONTAP: Shelf | Temperature Sensors | bargauge | [Cooling Sensors](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=29) |
///



### shelf_fan_status

Fan Operational Status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### shelf_labels

This metric provides information about Shelf

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/shelves` | `Harvest generated` | conf/rest/9.12.0/shelf.yaml |
| ZAPI | `storage-shelf-info-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/shelf.yaml |

The `shelf_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: Datacenter | Power and Temperature | stat | [Total Power](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=638) |
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Highlights | stat | [Total Power](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=21) |
| ONTAP: Power | Highlights | stat | [Average Power/Used_TB](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=71) |
| ONTAP: Power | Highlights | stat | [Average IOPs/Watt](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=96) |
| ONTAP: Power | Highlights | timeseries | [Total Power Consumed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=82) |
| ONTAP: Power | Highlights | timeseries | [Average Power Consumption (kWh) Over Last Hour](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=102) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | stat | [Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=21) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_max_fan_speed

Maximum fan speed for shelf in rpm.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_max_fan_speed` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Power and Temperature | stat | [Max Shelf Fan Speed](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=648) |
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Highlights | stat | [Max Shelf Fan Speed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=70) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | stat | [Max Shelf Fan Speed](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_max_temperature

Maximum temperature of all non-ambient sensors for shelf in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_max_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Power and Temperature | stat | [Max Shelf Temp](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=646) |
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Highlights | stat | [Max Shelf Temp](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=64) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | stat | [Max Shelf temp](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=75) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_min_ambient_temperature

Minimum temperature of all ambient sensors for shelf in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_min_ambient_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_min_fan_speed

Minimum fan speed for shelf in rpm.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_min_fan_speed` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_min_temperature

Minimum temperature of all non-ambient sensors for shelf in Celsius.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_min_temperature` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_module_labels

This metric provides information about shelf module.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_module_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Shelf | Module | table | [Storage Shelf Modules](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=72) |
///



### shelf_module_status

Displays the shelf module labels with their status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### shelf_new_status

This metric indicates a value of 1 if the shelf state is online or ok (indicating the shelf is operational) and a value of 0 for any other state (indicating the shelf is not operational).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/shelf.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/shelf.yaml |

The `shelf_new_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_power

Power consumed by shelf in Watts.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_power` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Power and Temperature | stat | [Total Power](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=638) |
| ONTAP: Datacenter | Power and Temperature | timeseries | [Total Power Consumed](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=652) |
| ONTAP: Health | Shelves | table | [Storage Shelf Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=243) |
| ONTAP: Power | Highlights | stat | [Total Power](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=21) |
| ONTAP: Power | Highlights | stat | [Average Power/Used_TB](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=71) |
| ONTAP: Power | Highlights | stat | [Average IOPs/Watt](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=96) |
| ONTAP: Power | Highlights | timeseries | [Total Power Consumed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=82) |
| ONTAP: Power | Highlights | timeseries | [Average Power Consumption (kWh) Over Last Hour](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=102) |
| ONTAP: Power | Highlights | timeseries | [Top $TopResources Shelves by Power Consumed](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=79) |
| ONTAP: Power | Shelves | table | [Storage Shelves](/d/cdot-power/ontap3a-power?orgId=1&viewPanel=81) |
| ONTAP: Shelf | Highlights | stat | [Total Power (Shelf)](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=79) |
| ONTAP: Shelf | Highlights | timeseries | [Top $TopResources Shelves by Power Consumed](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=71) |
| ONTAP: Shelf | Highlights | table | [Storage Shelves](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=69) |
///



### shelf_psu_labels

This metric provides information about shelf psu.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_psu_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Shelf | PSU | table | [Storage Shelf PSUs](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=70) |
///



### shelf_psu_power_drawn

Power Drawn From PSU In Watts.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_psu_power_drawn` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Shelf | PSU | table | [Storage Shelf PSUs](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=70) |
| ONTAP: Shelf | Voltage & PSU Sensors | bargauge | [Energy Drawn from PSUs](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=48) |
///



### shelf_psu_power_rating

Power Supply Power Ratings In Watts.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_psu_power_rating` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Shelf | PSU | table | [Storage Shelf PSUs](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=70) |
| ONTAP: Shelf | Voltage & PSU Sensors | bargauge | [Power Rating from PSUs](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=53) |
///



### shelf_psu_status

Operational Status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### shelf_sensor_labels

This metric provides information about shelf sensor.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### shelf_sensor_reading

Current Sensor Reading.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_sensor_reading` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Shelf | Highlights | stat | [Custom Sensor Avg](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=67) |
| ONTAP: Shelf | Custom Sensors | bargauge | [Custom Sensors - Shelf](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=31) |
///



### shelf_sensor_status

Operational Status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### shelf_temperature_labels

This metric provides information about shelf temperature.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### shelf_temperature_reading

Temperature Reading.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_temperature_reading` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Shelf | Highlights | stat | [Temperature Avg](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=64) |
| ONTAP: Shelf | Temperature Sensors | bargauge | [Temperature Sensors](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=28) |
///



### shelf_temperature_status

Operational Status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### shelf_voltage_labels

This metric provides information about shelf voltage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### shelf_voltage_reading

Voltage Current Reading.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |

The `shelf_voltage_reading` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Shelf | Voltage & PSU Sensors | bargauge | [Voltage Sensors](/d/cdot-shelf/ontap3a-shelf?orgId=1&viewPanel=30) |
///



### shelf_voltage_status

Operational Status.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/disk.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/disk.yaml |



### smb2_close_latency

Average latency in microseconds for SMB2_COM_CLOSE operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `close_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> close_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `close_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> close_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_close_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_close_ops

Number of SMB2_COM_CLOSE operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `close_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `close_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_close_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_create_latency

Average latency in microseconds for SMB2_COM_CREATE operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `create_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `create_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_create_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_create_ops

Number of SMB2_COM_CREATE operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `create_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `create_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_create_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_lock_latency

Average latency in microseconds for SMB2_COM_LOCK operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `lock_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lock_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `lock_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lock_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_lock_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_lock_ops

Number of SMB2_COM_LOCK operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `lock_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `lock_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_lock_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_negotiate_latency

Average latency in microseconds for SMB2_COM_NEGOTIATE operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `negotiate_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> negotiate_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `negotiate_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> negotiate_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_negotiate_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_negotiate_ops

Number of SMB2_COM_NEGOTIATE operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `negotiate_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `negotiate_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_negotiate_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_oplock_break_latency

Average latency in microseconds for SMB2_COM_OPLOCK_BREAK operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `oplock_break_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> oplock_break_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `oplock_break_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> oplock_break_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_oplock_break_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_oplock_break_ops

Number of SMB2_COM_OPLOCK_BREAK operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `oplock_break_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `oplock_break_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_oplock_break_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_query_directory_latency

Average latency in microseconds for SMB2_COM_QUERY_DIRECTORY operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `query_directory_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> query_directory_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `query_directory_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> query_directory_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_query_directory_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_query_directory_ops

Number of SMB2_COM_QUERY_DIRECTORY operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `query_directory_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `query_directory_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_query_directory_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_query_info_latency

Average latency in microseconds for SMB2_COM_QUERY_INFO operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `query_info_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> query_info_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `query_info_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> query_info_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_query_info_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_query_info_ops

Number of SMB2_COM_QUERY_INFO operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `query_info_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `query_info_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_query_info_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_read_latency

Average latency in microseconds for SMB2_COM_READ operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Read / Write Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=76) |
///



### smb2_read_ops

Number of SMB2_COM_READ operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Read / Write IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=75) |
///



### smb2_session_setup_latency

Average latency in microseconds for SMB2_COM_SESSION_SETUP operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `session_setup_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> session_setup_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `session_setup_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> session_setup_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_session_setup_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_session_setup_ops

Number of SMB2_COM_SESSION_SETUP operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `session_setup_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `session_setup_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_session_setup_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_set_info_latency

Average latency in microseconds for SMB2_COM_SET_INFO operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `set_info_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> set_info_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `set_info_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> set_info_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_set_info_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_set_info_ops

Number of SMB2_COM_SET_INFO operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `set_info_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `set_info_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_set_info_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_tree_connect_latency

Average latency in microseconds for SMB2_COM_TREE_CONNECT operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `tree_connect_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> tree_connect_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `tree_connect_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> tree_connect_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_tree_connect_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=78) |
///



### smb2_tree_connect_ops

Number of SMB2_COM_TREE_CONNECT operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `tree_connect_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `tree_connect_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_tree_connect_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Other IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=77) |
///



### smb2_write_latency

Average latency in microseconds for SMB2_COM_WRITE operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_latency_base | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Read / Write Latency](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=76) |
///



### smb2_write_ops

Number of SMB2_COM_WRITE operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/smb2` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.14.1/smb2.yaml |
| ZapiPerf | `perf-object-get-instances smb2` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/smb2.yaml |

The `smb2_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SMB | SMB Performance | timeseries | [Read / Write IOPS](/d/cdot-smb/ontap3a-smb?orgId=1&viewPanel=75) |
///



### snapmirror_break_failed_count

The number of failed SnapMirror break operations for the relationship

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `break_failed_count` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.break-failed-count` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_break_failed_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SnapMirror Sources | Highlights | stat | [Number of Failed SnapMirror Transfers](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=138) |
///



### snapmirror_break_successful_count

The number of successful SnapMirror break operations for the relationship

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `break_successful_count` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.break-successful-count` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_break_successful_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SnapMirror Sources | Highlights | stat | [Number of Successful SnapMirror Transfers](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=137) |
///



### snapmirror_labels

This metric provides information about SnapMirror

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `Harvest generated` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: SnapMirror Sources | Highlights | stat | [Unhealthy Relationships](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=139) |
| ONTAP: SnapMirror Sources | Highlights | table | [Relationships](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=134) |
| ONTAP: SnapMirror Sources | Policy and Lag details | piechart | [Relationships by Protection Policy](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=144) |
| ONTAP: SnapMirror Sources | Policy and Lag details | table | [Protection Policy and Lag detail](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=127) |
| ONTAP: SnapMirror Destinations | Highlights | stat | [Unhealthy](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=142) |
| ONTAP: SnapMirror Destinations | Highlights | piechart | [Relationships by Protection Policy](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=111) |
| ONTAP: SnapMirror Destinations | Highlights | stat | [Healthy](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=143) |
| ONTAP: SnapMirror Destinations | Highlights | table | [Relationships](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=28) |
| ONTAP: SnapMirror Destinations | Highlights | timeseries | [Top $TopResources Destination Volumes by Average Throughput](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=147) |
| ONTAP: SnapMirror Destinations | Consistency Group Data Protection | stat | [Unhealthy](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=144) |
| ONTAP: SnapMirror Destinations | Consistency Group Data Protection | piechart | [Consistency Group relationships by relationship type](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=113) |
| ONTAP: SnapMirror Destinations | Consistency Group Data Protection | stat | [Healthy](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=145) |
| ONTAP: SnapMirror Destinations | Consistency Group Data Protection | table | [Consistency Group Relationships](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=132) |
///



### snapmirror_lag_time

Amount of time since the last snapmirror transfer in seconds

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `lag_time` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.lag-time` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_lag_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SnapMirror Sources | Highlights | table | [Relationships](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=134) |
| ONTAP: SnapMirror Sources | Highlights | timeseries | [Top $TopResources Relationships by Lag Time](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=141) |
| ONTAP: SnapMirror Sources | Policy and Lag details | piechart | [Relationships by Lag time](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=125) |
| ONTAP: SnapMirror Sources | Policy and Lag details | table | [Protection Policy and Lag detail](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=127) |
| ONTAP: SnapMirror Destinations | Highlights | table | [Relationships](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=28) |
| ONTAP: SnapMirror Destinations | Highlights | timeseries | [Top $TopResources Relationships by Lag Time](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=134) |
///



### snapmirror_last_transfer_duration

Duration of the last SnapMirror transfer in seconds

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `last_transfer_duration` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.last-transfer-duration` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_last_transfer_duration` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SnapMirror Sources | Highlights | table | [Relationships](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=134) |
| ONTAP: SnapMirror Sources | Highlights | timeseries | [Top $TopResources Relationships by Transfer Duration](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=140) |
| ONTAP: SnapMirror Destinations | Highlights | table | [Relationships](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=28) |
| ONTAP: SnapMirror Destinations | Highlights | timeseries | [Top $TopResources Relationships by Transfer Duration](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=136) |
///



### snapmirror_last_transfer_end_timestamp

The Timestamp of the end of the last transfer

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `last_transfer_end_timestamp` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.last-transfer-end-timestamp` | conf/zapi/cdot/9.8.0/snapmirror.yaml |



### snapmirror_last_transfer_size

Size in kilobytes (1024 bytes) of the last transfer

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `last_transfer_size` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.last-transfer-size` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_last_transfer_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SnapMirror Sources | Highlights | table | [Relationships](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=134) |
| ONTAP: SnapMirror Sources | Highlights | timeseries | [Top $TopResources Relationships by Transfer Data](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=142) |
| ONTAP: SnapMirror Destinations | Highlights | table | [Relationships](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=28) |
| ONTAP: SnapMirror Destinations | Highlights | timeseries | [Top $TopResources Relationships by Transfer Data](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=135) |
///



### snapmirror_newest_snapshot_timestamp

The timestamp of the newest Snapshot copy on the destination volume

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `newest_snapshot_timestamp` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.newest-snapshot-timestamp` | conf/zapi/cdot/9.8.0/snapmirror.yaml |



### snapmirror_policy_labels

This metric provides information about SnapMirrorPolicy

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/snapmirror/policies` | `Harvest generated` | conf/rest/9.6.0/snapmirrorpolicy.yaml |

The `snapmirror_policy_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Local Policy | table | [Protection policies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=100) |
///



### snapmirror_resync_failed_count

The number of failed SnapMirror resync operations for the relationship

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `resync_failed_count` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.resync-failed-count` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_resync_failed_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SnapMirror Sources | Highlights | stat | [Number of Failed SnapMirror Transfers](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=138) |
///



### snapmirror_resync_successful_count

The number of successful SnapMirror resync operations for the relationship

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `resync_successful_count` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.resync-successful-count` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_resync_successful_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SnapMirror Sources | Highlights | stat | [Number of Successful SnapMirror Transfers](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=137) |
///



### snapmirror_total_transfer_bytes

Cumulative bytes transferred for the relationship

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `total_transfer_bytes` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.total-transfer-bytes` | conf/zapi/cdot/9.8.0/snapmirror.yaml |



### snapmirror_total_transfer_time_secs

Cumulative total transfer time in seconds for the relationship

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `total_transfer_time_secs` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.total-transfer-time-secs` | conf/zapi/cdot/9.8.0/snapmirror.yaml |



### snapmirror_update_failed_count

The number of successful SnapMirror update operations for the relationship

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `update_failed_count` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.update-failed-count` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_update_failed_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SnapMirror Sources | Highlights | stat | [Number of Failed SnapMirror Transfers](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=138) |
///



### snapmirror_update_successful_count

Number of Successful Updates

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapmirror` | `update_successful_count` | conf/rest/9.12.0/snapmirror.yaml |
| ZAPI | `snapmirror-get-iter` | `snapmirror-info.update-successful-count` | conf/zapi/cdot/9.8.0/snapmirror.yaml |

The `snapmirror_update_successful_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SnapMirror Sources | Highlights | stat | [Number of Successful SnapMirror Transfers](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=137) |
///



### snapshot_create_time

Creation time of the snapshot. It is the volume access time when the snapshot was created.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapshot` | `create_time` | conf/rest/9.6.0/snapshot.yaml |

The `snapshot_create_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Volume Encryption & Autonomous Ransomware Protection | table | [Anti-ransomware Snapshots](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=156) |
///



### snapshot_labels

This metric provides information about Snapshot

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapshot` | `Harvest generated` | conf/rest/9.6.0/snapshot.yaml |

The `snapshot_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Volume Encryption & Autonomous Ransomware Protection | table | [Anti-ransomware Snapshots](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=156) |
///



### snapshot_policy_labels

This metric provides information about SnapshotPolicy

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/snapshot-policies` | `Harvest generated` | conf/rest/9.12.0/snapshotpolicy.yaml |
| ZAPI | `snapshot-policy-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/snapshotpolicy.yaml |

The `snapshot_policy_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Snapshot Copies | stat | [<10 Copies ](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=96) |
| ONTAP: Data Protection | Snapshot Copies | stat | [10-100 Copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=97) |
| ONTAP: Data Protection | Snapshot Copies | stat | [101-500 Copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=98) |
| ONTAP: Data Protection | Snapshot Copies | stat | [>500 Copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=99) |
| ONTAP: Data Protection | Snapshot Copies | table | [Volume count by the number of Snapshot copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=94) |
| ONTAP: Data Protection | Local Policy | table | [Snapshot policies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=101) |
| ONTAP: Datacenter | Snapshots | piechart | [Snapshot Copies](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=632) |
///



### snapshot_restore_size

Size of the active file system at the time the snapshot is captured. The actual size of the snapshot also includes those blocks trapped by other snapshots.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/snapshot` | `afs_used` | conf/rest/9.6.0/snapshot.yaml |

The `snapshot_restore_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Volume Encryption & Autonomous Ransomware Protection | table | [Anti-ransomware Snapshots](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=156) |
///



### snapshot_volume_violation_count

This metric represents the total number of snapshots that exist on volumes without being created through an applied snapshot policy.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/snapshotpolicy.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/snapshotpolicy.yaml |

The `snapshot_volume_violation_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Snapshot Policy Violations | stat | [Total Snapshots Violations](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=170) |
| ONTAP: Data Protection | Snapshot Policy Violations | table | [Snapshot Violation Details](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=168) |
///



### snapshot_volume_violation_total_size

This metric captures the total size of all snapshots that were not created through an applied snapshot policy.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/snapshotpolicy.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/snapshotpolicy.yaml |

The `snapshot_volume_violation_total_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Snapshot Policy Violations | stat | [Snapshot Violations Total Size](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=171) |
| ONTAP: Data Protection | Snapshot Policy Violations | table | [Snapshot Violation Details](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=168) |
///



### storage_unit_avg_latency

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> storage_unit_statistics.iops_raw.total | conf/keyperf/asar2/9.16.0/storage_unit.yaml |

The `storage_unit_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Storage Units | table | [Storage Units in Cluster](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=10) |
///



### storage_unit_labels

This metric provides information about StorageUnit

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/storage-units` | `Harvest generated` | conf/rest/asar2/9.16.0/storage_unit.yaml |

The `storage_unit_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Highlights | stat | [Storage Units](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=3) |
| ASAr2: Overview | Highlights | stat | [Online](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=5) |
| ASAr2: Overview | Highlights | stat | [Offline](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=6) |
| ASAr2: Overview | Storage Units | table | [Storage Units in Cluster](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=10) |
///



### storage_unit_other_data

Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.throughput_raw.other`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/asar2/9.16.0/storage_unit.yaml |



### storage_unit_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> storage_unit_statistics.iops_raw.other | conf/keyperf/asar2/9.16.0/storage_unit.yaml |



### storage_unit_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/asar2/9.16.0/storage_unit.yaml |



### storage_unit_read_data

Performance metric for read I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/asar2/9.16.0/storage_unit.yaml |



### storage_unit_read_latency

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> storage_unit_statistics.iops_raw.read | conf/keyperf/asar2/9.16.0/storage_unit.yaml |



### storage_unit_read_ops

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/asar2/9.16.0/storage_unit.yaml |



### storage_unit_space_efficiency_ratio



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/storage-units` | `space.efficiency_ratio` | conf/rest/asar2/9.16.0/storage_unit.yaml |

The `storage_unit_space_efficiency_ratio` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Storage Units | table | [Storage Units in Cluster](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=10) |
///



### storage_unit_space_physical_used

The number of bytes consumed on the disk by the storage unit, excluding snapshots.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/storage-units` | `space.physical_used` | conf/rest/asar2/9.16.0/storage_unit.yaml |

The `storage_unit_space_physical_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Storage Units | table | [Storage Units in Cluster](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=10) |
///



### storage_unit_space_size



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/storage-units` | `space.size` | conf/rest/asar2/9.16.0/storage_unit.yaml |

The `storage_unit_space_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Storage Units | table | [Storage Units in Cluster](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=10) |
///



### storage_unit_space_used



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/storage-units` | `space.used` | conf/rest/asar2/9.16.0/storage_unit.yaml |

The `storage_unit_space_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Storage Units | table | [Storage Units in Cluster](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=10) |
///



### storage_unit_total_data

Performance metric aggregated over all types of I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/asar2/9.16.0/storage_unit.yaml |

The `storage_unit_total_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Storage Units | table | [Storage Units in Cluster](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=10) |
///



### storage_unit_total_ops

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/asar2/9.16.0/storage_unit.yaml |

The `storage_unit_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ASAr2: Overview | Storage Units | table | [Storage Units in Cluster](/d/asar2-overview/asar23a-overview?orgId=1&viewPanel=10) |
///



### storage_unit_write_data

Performance metric for write I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/asar2/9.16.0/storage_unit.yaml |



### storage_unit_write_latency

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec | conf/rest/asar2/9.16.0/storage_unit.yaml | 



### storage_unit_write_ops

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/storage-units` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/asar2/9.16.0/storage_unit.yaml |



### support_auto_update_labels

This metric provides information about SupportAutoUpdate

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/support/auto-update` | `Harvest generated` | conf/rest/9.12.0/support_auto_update.yaml |

The `support_auto_update_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### support_labels

This metric provides information about Support

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/support/autosupport` | `Harvest generated` | conf/rest/9.12.0/support.yaml |
| ZAPI | `autosupport-config-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/support.yaml |

The `support_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Highlights | stat | [Cluster Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=214) |
| ONTAP: Security | Highlights | piechart | [Cluster Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=215) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### svm_cifs_connections

Number of connections

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `connections`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |

The `svm_cifs_connections` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | timeseries | [SVM CIFS Connections and Open Files](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=107) |
///



### svm_cifs_established_sessions

Number of established SMB and SMB2 sessions

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `established_sessions`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `established_sessions`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |



### svm_cifs_latency

Average latency in microseconds for CIFS operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> latency_base | conf/restperf/9.12.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `cifs_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cifs_latency_base | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |

The `svm_cifs_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | stat | [SVM CIFS Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=99) |
| ONTAP: SVM | CIFS | timeseries | [SVM CIFS Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=102) |
///



### svm_cifs_op_count

Array of select CIFS operation counts

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `op_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `cifs_op_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |

The `svm_cifs_op_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | stat | [SVM CIFS IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=103) |
| ONTAP: SVM | CIFS | timeseries | [SVM CIFS IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=106) |
| ONTAP: SVM | CIFS | timeseries | [SVM CIFS IOP by Type](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=108) |
///



### svm_cifs_open_files

Number of open files over SMB and SMB2

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `open_files`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `open_files`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |

The `svm_cifs_open_files` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | timeseries | [SVM CIFS Connections and Open Files](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=107) |
///



### svm_cifs_ops

Total number of CIFS operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `cifs_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |



### svm_cifs_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/protocols/cifs/services` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_cifs_statistics.iops_raw.other | conf/keyperf/9.15.0/cifs_vserver.yaml |



### svm_cifs_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/protocols/cifs/services` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cifs_vserver.yaml |



### svm_cifs_read_data

Performance metric for read I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/protocols/cifs/services` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cifs_vserver.yaml |



### svm_cifs_read_latency

Average latency in microseconds for CIFS read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `average_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_read_ops | conf/restperf/9.12.0/cifs_vserver.yaml |
| KeyPerf | `api/protocols/cifs/services` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_cifs_statistics.iops_raw.read | conf/keyperf/9.15.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `cifs_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cifs_read_ops | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |

The `svm_cifs_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | stat | [SVM CIFS Average Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=100) |
| ONTAP: SVM | CIFS | timeseries | [SVM CIFS Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=102) |
///



### svm_cifs_read_ops

Total number of CIFS read operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `total_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_vserver.yaml |
| KeyPerf | `api/protocols/cifs/services` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `cifs_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |

The `svm_cifs_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | stat | [SVM CIFS Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=104) |
| ONTAP: SVM | CIFS | timeseries | [SVM CIFS IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=106) |
///



### svm_cifs_signed_sessions

Number of signed SMB and SMB2 sessions.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `signed_sessions`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `signed_sessions`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |



### svm_cifs_total_latency

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/protocols/cifs/services` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_cifs_statistics.iops_raw.total | conf/keyperf/9.15.0/cifs_vserver.yaml |



### svm_cifs_total_ops

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/protocols/cifs/services` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cifs_vserver.yaml |



### svm_cifs_write_data

Performance metric for write I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/protocols/cifs/services` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cifs_vserver.yaml |



### svm_cifs_write_latency

Average latency in microseconds for CIFS write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `average_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_write_ops | conf/restperf/9.12.0/cifs_vserver.yaml |
| KeyPerf | `api/protocols/cifs/services` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_cifs_statistics.iops_raw.write | conf/keyperf/9.15.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `cifs_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> cifs_write_ops | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |

The `svm_cifs_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | stat | [SVM CIFS Average Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=101) |
| ONTAP: SVM | CIFS | timeseries | [SVM CIFS Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=102) |
///



### svm_cifs_write_ops

Total number of CIFS write operations

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_cifs` | `total_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/cifs_vserver.yaml |
| KeyPerf | `api/protocols/cifs/services` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/cifs_vserver.yaml |
| ZapiPerf | `perf-object-get-instances cifs:vserver` | `cifs_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml |

The `svm_cifs_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | stat | [SVM CIFS Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=105) |
| ONTAP: SVM | CIFS | timeseries | [SVM CIFS IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=106) |
///



### svm_labels

This metric provides information about SVM

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/vserver` | `Harvest generated` | conf/rest/9.9.0/svm.yaml |
| ZAPI | `vserver-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/svm.yaml |

The `svm_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | Capacity Metrics | table | [Top $TopResources SVMs by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=233) |
| ONTAP: cDOT | Capacity Metrics | timeseries | [Top $TopResources SVMs by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=272) |
| ONTAP: cDOT | SVM Metrics | timeseries | [Top $TopResources Average Throughput by SVMs](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=269) |
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: Security | Highlights | stat | [Cluster Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=214) |
| ONTAP: Security | Highlights | stat | [SVM Compliant %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=216) |
| ONTAP: Security | Highlights | stat | [SVM Autonomous Ransomware Protection %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=210) |
| ONTAP: Security | Highlights | piechart | [Cluster Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=215) |
| ONTAP: Security | Highlights | piechart | [SVM Compliant](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=217) |
| ONTAP: Security | Highlights | piechart | [SVM Autonomous Ransomware Protection](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=209) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
| ONTAP: Security | SVM Compliance | table | [SVM Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=225) |
///



### svm_ldap_encrypted

This metric indicates a LDAP session security has been sealed

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.10.0/svm.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/svm.yaml |

The `svm_ldap_encrypted` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | SVM Compliance | table | [SVM Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=225) |
///



### svm_ldap_signed

This metric indicates a LDAP session security has been signed

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.10.0/svm.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/svm.yaml |

The `svm_ldap_signed` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | SVM Compliance | table | [SVM Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=225) |
///



### svm_new_status

This metric indicates a value of 1 if the SVM state is online (indicating the SVM is operational) and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.10.0/svm.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/svm.yaml |



### svm_nfs_access_avg_latency

Average latency in microseconds of Access procedure requests. The counter keeps track of the average response time of Access requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `access.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> access.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `access.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> access.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `access.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> access.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `access.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> access.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `access_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> access_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `access_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> access_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `access_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> access_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `access_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> access_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_access_total

Total number of Access procedure requests. It is the total number of access success and access error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `access.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `access.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `access.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `access.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `access_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `access_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `access_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `access_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_backchannel_ctl_avg_latency

Average latency in microseconds of BACKCHANNEL_CTL operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `backchannel_ctl.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> backchannel_ctl.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `backchannel_ctl.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> backchannel_ctl.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `backchannel_ctl_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> backchannel_ctl_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `backchannel_ctl_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> backchannel_ctl_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_backchannel_ctl_total

Total number of BACKCHANNEL_CTL operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `backchannel_ctl.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `backchannel_ctl.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `backchannel_ctl_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `backchannel_ctl_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_bind_conn_to_session_avg_latency

Average latency in microseconds of BIND_CONN_TO_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `bind_connections_to_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> bind_connections_to_session.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `bind_conn_to_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> bind_conn_to_session.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `bind_conn_to_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> bind_conn_to_session_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `bind_conn_to_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> bind_conn_to_session_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_bind_conn_to_session_total

Total number of BIND_CONN_TO_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `bind_connections_to_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `bind_conn_to_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `bind_conn_to_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `bind_conn_to_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_close_avg_latency

Average latency in microseconds of CLOSE procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `close.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> close.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `close.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> close.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `close.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> close.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `close_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> close_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `close_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> close_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `close_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> close_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_close_total

Total number of CLOSE procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `close.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `close.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `close.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `close_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `close_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `close_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_commit_avg_latency

Average latency in microseconds of Commit procedure requests. The counter keeps track of the average response time of Commit requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `commit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> commit.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `commit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> commit.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `commit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> commit.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `commit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> commit.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `commit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> commit_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `commit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> commit_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `commit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> commit_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `commit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> commit_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_commit_total

Total number of Commit procedure requests. It is the total number of Commit success and Commit error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `commit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `commit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `commit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `commit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `commit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `commit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `commit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `commit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_create_avg_latency

Average latency in microseconds of Create procedure requests. The counter keeps track of the average response time of Create requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `create.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `create.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `create.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `create.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `create_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `create_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `create_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `create_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_create_session_avg_latency

Average latency in microseconds of CREATE_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `create_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create_session.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `create_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> create_session.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `create_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_session_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `create_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> create_session_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_create_session_total

Total number of CREATE_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `create_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `create_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `create_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `create_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_create_total

Total number Create of procedure requests. It is the total number of create success and create error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `create.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `create.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `create.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `create.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `create_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `create_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `create_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `create_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_delegpurge_avg_latency

Average latency in microseconds of DELEGPURGE procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `delegpurge.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegpurge.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `delegpurge.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegpurge.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `delegpurge.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegpurge.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `delegpurge_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegpurge_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `delegpurge_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegpurge_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `delegpurge_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegpurge_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_delegpurge_total

Total number of DELEGPURGE procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `delegpurge.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `delegpurge.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `delegpurge.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `delegpurge_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `delegpurge_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `delegpurge_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_delegreturn_avg_latency

Average latency in microseconds of DELEGRETURN procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `delegreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegreturn.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `delegreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegreturn.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `delegreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> delegreturn.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `delegreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegreturn_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `delegreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegreturn_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `delegreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> delegreturn_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_delegreturn_total

Total number of DELEGRETURN procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `delegreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `delegreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `delegreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `delegreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `delegreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `delegreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_destroy_clientid_avg_latency

Average latency in microseconds of DESTROY_CLIENTID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `destroy_clientid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> destroy_clientid.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `destroy_clientid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> destroy_clientid.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `destroy_clientid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> destroy_clientid_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `destroy_clientid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> destroy_clientid_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_destroy_clientid_total

Total number of DESTROY_CLIENTID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `destroy_clientid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `destroy_clientid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `destroy_clientid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `destroy_clientid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_destroy_session_avg_latency

Average latency in microseconds of DESTROY_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `destroy_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> destroy_session.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `destroy_session.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> destroy_session.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `destroy_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> destroy_session_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `destroy_session_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> destroy_session_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_destroy_session_total

Total number of DESTROY_SESSION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `destroy_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `destroy_session.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `destroy_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `destroy_session_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_exchange_id_avg_latency

Average latency in microseconds of EXCHANGE_ID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `exchange_id.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> exchange_id.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `exchange_id.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> exchange_id.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `exchange_id_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> exchange_id_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `exchange_id_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> exchange_id_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_exchange_id_total

Total number of EXCHANGE_ID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `exchange_id.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `exchange_id.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `exchange_id_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `exchange_id_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_free_stateid_avg_latency

Average latency in microseconds of FREE_STATEID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `free_stateid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> free_stateid.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `free_stateid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> free_stateid.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `free_stateid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> free_stateid_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `free_stateid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> free_stateid_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_free_stateid_total

Total number of FREE_STATEID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `free_stateid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `free_stateid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `free_stateid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `free_stateid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_fsinfo_avg_latency

Average latency in microseconds of FSInfo procedure requests. The counter keeps track of the average response time of FSInfo requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `fsinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> fsinfo.total | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `fsinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> fsinfo_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_fsinfo_total

Total number FSInfo of procedure requests. It is the total number of FSInfo success and FSInfo error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `fsinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `fsinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_fsstat_avg_latency

Average latency in microseconds of FSStat procedure requests. The counter keeps track of the average response time of FSStat requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `fsstat.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> fsstat.total | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `fsstat_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> fsstat_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_fsstat_total

Total number FSStat of procedure requests. It is the total number of FSStat success and FSStat error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `fsstat.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `fsstat_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_get_dir_delegation_avg_latency

Average latency in microseconds of GET_DIR_DELEGATION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `get_dir_delegation.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> get_dir_delegation.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `get_dir_delegation.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> get_dir_delegation.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `get_dir_delegation_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> get_dir_delegation_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `get_dir_delegation_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> get_dir_delegation_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_get_dir_delegation_total

Total number of GET_DIR_DELEGATION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `get_dir_delegation.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `get_dir_delegation.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `get_dir_delegation_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `get_dir_delegation_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_getattr_avg_latency

Average latency in microseconds of GetAttr procedure requests. This counter keeps track of the average response time of GetAttr requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `getattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getattr.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `getattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getattr.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `getattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getattr.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `getattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getattr.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `getattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getattr_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `getattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getattr_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `getattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getattr_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `getattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getattr_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_getattr_total

Total number of Getattr procedure requests. It is the total number of getattr success and getattr error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `getattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `getattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `getattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `getattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `getattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `getattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `getattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `getattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_getdeviceinfo_avg_latency

Average latency in microseconds of GETDEVICEINFO operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `getdeviceinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getdeviceinfo.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `getdeviceinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getdeviceinfo.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `getdeviceinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getdeviceinfo_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `getdeviceinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getdeviceinfo_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_getdeviceinfo_total

Total number of GETDEVICEINFO operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `getdeviceinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `getdeviceinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `getdeviceinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `getdeviceinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_getdevicelist_avg_latency

Average latency in microseconds of GETDEVICELIST operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `getdevicelist.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getdevicelist.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `getdevicelist.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getdevicelist.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `getdevicelist_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getdevicelist_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `getdevicelist_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getdevicelist_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_getdevicelist_total

Total number of GETDEVICELIST operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `getdevicelist.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `getdevicelist.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `getdevicelist_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `getdevicelist_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_getfh_avg_latency

Average latency in microseconds of GETFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `getfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getfh.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `getfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getfh.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `getfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> getfh.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `getfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getfh_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `getfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `getfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> getfh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_getfh_total

Total number of GETFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `getfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `getfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `getfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `getfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `getfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `getfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_latency

Average latency in microseconds of NFSv3 requests. This counter keeps track of the average response time of NFSv3 requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v3.iops_raw.total | conf/keyperf/9.15.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v4.iops_raw.total | conf/keyperf/9.15.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/nfsv4_1.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v41.iops_raw.total | conf/keyperf/9.15.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |

The `svm_nfs_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv3 | stat | [NFSv3 Avg Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=39) |
| ONTAP: SVM | NFSv4 | stat | [NFSv4 Avg Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=154) |
| ONTAP: SVM | NFSv4.1 | stat | [NFSv4.1 Avg Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=164) |
| ONTAP: SVM | NFSv4.2 | stat | [NFSv4.2 Avg Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=528) |
///



### svm_nfs_layoutcommit_avg_latency

Average latency in microseconds of LAYOUTCOMMIT operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `layoutcommit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutcommit.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `layoutcommit.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutcommit.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `layoutcommit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutcommit_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `layoutcommit_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutcommit_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_layoutcommit_total

Total number of LAYOUTCOMMIT operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `layoutcommit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `layoutcommit.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `layoutcommit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `layoutcommit_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_layoutget_avg_latency

Average latency in microseconds of LAYOUTGET operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `layoutget.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutget.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `layoutget.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutget.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `layoutget_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutget_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `layoutget_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutget_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_layoutget_total

Total number of LAYOUTGET operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `layoutget.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `layoutget.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `layoutget_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `layoutget_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_layoutreturn_avg_latency

Average latency in microseconds of LAYOUTRETURN operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `layoutreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutreturn.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `layoutreturn.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> layoutreturn.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `layoutreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutreturn_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `layoutreturn_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> layoutreturn_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_layoutreturn_total

Total number of LAYOUTRETURN operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `layoutreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `layoutreturn.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `layoutreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `layoutreturn_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_link_avg_latency

Average latency in microseconds of Link procedure requests. The counter keeps track of the average response time of Link requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `link.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> link.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `link.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> link.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `link.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> link.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `link.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> link.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `link_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> link_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `link_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> link_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `link_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> link_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `link_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> link_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_link_total

Total number Link of procedure requests. It is the total number of Link success and Link error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `link.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `link.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `link.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `link.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `link_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `link_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `link_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `link_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_lock_avg_latency

Average latency in microseconds of LOCK procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `lock.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lock.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `lock.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lock.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `lock.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lock.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `lock_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lock_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `lock_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lock_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `lock_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lock_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_lock_total

Total number of LOCK procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `lock.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `lock.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `lock.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `lock_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `lock_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `lock_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_lockt_avg_latency

Average latency in microseconds of LOCKT procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `lockt.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lockt.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `lockt.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lockt.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `lockt.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lockt.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `lockt_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lockt_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `lockt_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lockt_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `lockt_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lockt_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_lockt_total

Total number of LOCKT procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `lockt.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `lockt.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `lockt.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `lockt_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `lockt_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `lockt_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_locku_avg_latency

Average latency in microseconds of LOCKU procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `locku.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> locku.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `locku.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> locku.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `locku.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> locku.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `locku_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> locku_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `locku_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> locku_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `locku_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> locku_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_locku_total

Total number of LOCKU procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `locku.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `locku.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `locku.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `locku_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `locku_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `locku_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_lookup_avg_latency

Average latency in microseconds of LookUp procedure requests. This shows the average time it takes for the LookUp operation to reply to the request.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `lookup.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookup.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `lookup.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookup.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `lookup.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookup.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `lookup.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookup.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `lookup_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookup_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `lookup_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookup_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `lookup_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookup_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `lookup_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookup_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_lookup_total

Total number of Lookup procedure requests. It is the total number of lookup success and lookup error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `lookup.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `lookup.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `lookup.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `lookup.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `lookup_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `lookup_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `lookup_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `lookup_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_lookupp_avg_latency

Average latency in microseconds of LOOKUPP procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `lookupp.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookupp.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `lookupp.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookupp.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `lookupp.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> lookupp.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `lookupp_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookupp_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `lookupp_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookupp_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `lookupp_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> lookupp_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_lookupp_total

Total number of LOOKUPP procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `lookupp.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `lookupp.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `lookupp.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `lookupp_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `lookupp_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `lookupp_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_mkdir_avg_latency

Average latency in microseconds of MkDir procedure requests. The counter keeps track of the average response time of MkDir requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `mkdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> mkdir.total | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `mkdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> mkdir_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_mkdir_total

Total number MkDir of procedure requests. It is the total number of MkDir success and MkDir error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `mkdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `mkdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_mknod_avg_latency

Average latency in microseconds of MkNod procedure requests. The counter keeps track of the average response time of MkNod requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `mknod.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> mknod.total | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `mknod_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> mknod_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_mknod_total

Total number MkNod of procedure requests. It is the total number of MkNod success and MkNod error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `mknod.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `mknod_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_null_avg_latency

Average latency in microseconds of Null procedure requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `null.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> null.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `null.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> null.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `null.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> null.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `null.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> null.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `null_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> null_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `null_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> null_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `null_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> null_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `null_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> null_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_null_total

Total number of Null procedure requests. It is the total of null success and null error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `null.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `null.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `null.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `null.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `null_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `null_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `null_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `null_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_nverify_avg_latency

Average latency in microseconds of NVERIFY procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `nverify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nverify.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `nverify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nverify.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `nverify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nverify.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `nverify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> nverify_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `nverify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> nverify_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `nverify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> nverify_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_nverify_total

Total number of NVERIFY procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `nverify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `nverify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `nverify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `nverify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `nverify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `nverify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_open_avg_latency

Average latency in microseconds of OPEN procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `open.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `open.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `open.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `open_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `open_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `open_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_open_confirm_avg_latency

Average latency in microseconds of OPEN_CONFIRM procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `open_confirm.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open_confirm.total | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `open_confirm_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_confirm_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_open_confirm_total

Total number of OPEN_CONFIRM procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `open_confirm.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `open_confirm_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_open_downgrade_avg_latency

Average latency in microseconds of OPEN_DOWNGRADE procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `open_downgrade.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open_downgrade.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `open_downgrade.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open_downgrade.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `open_downgrade.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> open_downgrade.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `open_downgrade_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_downgrade_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `open_downgrade_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_downgrade_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `open_downgrade_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> open_downgrade_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_open_downgrade_total

Total number of OPEN_DOWNGRADE procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `open_downgrade.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `open_downgrade.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `open_downgrade.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `open_downgrade_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `open_downgrade_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `open_downgrade_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_open_total

Total number of OPEN procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `open.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `open.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `open.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `open_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `open_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `open_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_openattr_avg_latency

Average latency in microseconds of OPENATTR procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `openattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> openattr.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `openattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> openattr.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `openattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> openattr.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `openattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> openattr_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `openattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> openattr_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `openattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> openattr_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_openattr_total

Total number of OPENATTR procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `openattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `openattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `openattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `openattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `openattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `openattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_ops

Total number of NFSv3 procedure requests per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `nfsv3_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |

The `svm_nfs_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv3 | stat | [NFSv3 IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=46) |
| ONTAP: SVM | NFSv3 | timeseries | [Top $TopResources NFSv3 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=42) |
| ONTAP: SVM | NFSv4 | stat | [NFSv4 IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=166) |
| ONTAP: SVM | NFSv4 | timeseries | [Top $TopResources NFSv4 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=147) |
| ONTAP: SVM | NFSv4.1 | stat | [NFSv4.1 IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=156) |
| ONTAP: SVM | NFSv4.1 | timeseries | [Top $TopResources NFSv4.1 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=174) |
| ONTAP: SVM | NFSv4.2 | stat | [NFSv4.2 IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=532) |
| ONTAP: SVM | NFSv4.2 | timeseries | [Top $TopResources NFSv4.2 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=550) |
///



### svm_nfs_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v3.iops_raw.other | conf/keyperf/9.15.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v4.iops_raw.other | conf/keyperf/9.15.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v41.iops_raw.other | conf/keyperf/9.15.0/nfsv4_1.yaml |



### svm_nfs_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4_1.yaml |



### svm_nfs_pathconf_avg_latency

Average latency in microseconds of PathConf procedure requests. The counter keeps track of the average response time of PathConf requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `pathconf.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> pathconf.total | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `pathconf_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> pathconf_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_pathconf_total

Total number PathConf of procedure requests. It is the total number of PathConf success and PathConf error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `pathconf.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `pathconf_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_putfh_avg_latency

Average latency in microseconds of PUTFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `putfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putfh.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `putfh.average_latency`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `putfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putfh.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `putfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putfh_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `putfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `putfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putfh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_putfh_total

Total number of PUTFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `putfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `putfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `putfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `putfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `putfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `putfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_putpubfh_avg_latency

Average latency in microseconds of PUTPUBFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `putpubfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putpubfh.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `putpubfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putpubfh.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `putpubfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putpubfh.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `putpubfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putpubfh_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `putpubfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putpubfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `putpubfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putpubfh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_putpubfh_total

Total number of PUTPUBFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `putpubfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `putpubfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `putpubfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `putpubfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `putpubfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `putpubfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_putrootfh_avg_latency

Average latency in microseconds of PUTROOTFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `putrootfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putrootfh.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `putrootfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putrootfh.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `putrootfh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> putrootfh.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `putrootfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putrootfh_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `putrootfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putrootfh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `putrootfh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> putrootfh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_putrootfh_total

Total number of PUTROOTFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `putrootfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `putrootfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `putrootfh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `putrootfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `putrootfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `putrootfh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_read_avg_latency

Average latency in microseconds of Read procedure requests. The counter keeps track of the average response time of Read requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `read.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read.total | conf/restperf/9.12.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v3.iops_raw.read | conf/keyperf/9.15.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `read.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read.total | conf/restperf/9.12.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v4.iops_raw.read | conf/keyperf/9.15.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `read.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v41.iops_raw.read | conf/keyperf/9.15.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `read.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `read_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `read_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `read_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `read_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |

The `svm_nfs_read_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv3 | stat | [NFSv3 Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=48) |
| ONTAP: SVM | NFSv3 | timeseries | [Top $TopResources NFSv3 SVMs by Read and Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=51) |
| ONTAP: SVM | NFSv4 | stat | [NFSv4 Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=158) |
| ONTAP: SVM | NFSv4 | timeseries | [Top $TopResources NFSv4 SVMs by Read and Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=145) |
| ONTAP: SVM | NFSv4.1 | stat | [NFSv4.1 Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=140) |
| ONTAP: SVM | NFSv4.1 | timeseries | [Top $TopResources NFSv4.1 SVMs by Read and Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=172) |
| ONTAP: SVM | NFSv4.2 | stat | [NFSv4.2 Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=534) |
| ONTAP: SVM | NFSv4.2 | timeseries | [Top $TopResources NFSv4.2 SVMs by Read and Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=546) |
///



### svm_nfs_read_ops

Total observed NFSv3 read operations per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `nfsv3_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |

The `svm_nfs_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv3 | stat | [NFSv3 Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=152) |
| ONTAP: SVM | NFSv3 | timeseries | [Top $TopResources NFSv3 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=42) |
///



### svm_nfs_read_symlink_avg_latency

Average latency in microseconds of ReadSymLink procedure requests. The counter keeps track of the average response time of ReadSymLink requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `read_symlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_symlink.total | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `read_symlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> read_symlink_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_read_symlink_total

Total number of ReadSymLink procedure requests. It is the total number of read symlink success and read symlink error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `read_symlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `read_symlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_read_throughput

Rate of NFSv3 read data transfers in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `total.read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `total.read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `total.read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `nfsv3_read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `nfs4_read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `nfs41_read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `nfs42_read_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |

The `svm_nfs_read_throughput` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv3 | stat | [NFSv3 Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=150) |
| ONTAP: SVM | NFSv3 | timeseries | [Top $TopResources NFSv3 SVMs by Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=53) |
| ONTAP: SVM | NFSv4 | stat | [NFSv4 Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=160) |
| ONTAP: SVM | NFSv4 | timeseries | [Top $TopResources NFSv4 SVMs by Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=146) |
| ONTAP: SVM | NFSv4.1 | stat | [NFSv4.1 Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=168) |
| ONTAP: SVM | NFSv4.1 | timeseries | [Top $TopResources NFSv4.1 SVMs by Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=173) |
| ONTAP: SVM | NFSv4.2 | stat | [NFSv4.2 Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=538) |
| ONTAP: SVM | NFSv4.2 | timeseries | [Top $TopResources NFSv4.2 SVMs by Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=548) |
///



### svm_nfs_read_total

Total number Read of procedure requests. It is the total number of read success and read error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `read.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `read.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `read.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `read.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `read_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `read_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `read_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `read_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |

The `svm_nfs_read_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv4 | stat | [NFSv4 Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=162) |
| ONTAP: SVM | NFSv4 | timeseries | [Top $TopResources NFSv4 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=147) |
| ONTAP: SVM | NFSv4.1 | stat | [NFSv4.1 Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=170) |
| ONTAP: SVM | NFSv4.1 | timeseries | [Top $TopResources NFSv4.1 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=174) |
| ONTAP: SVM | NFSv4.2 | stat | [NFSv4.2 Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=542) |
| ONTAP: SVM | NFSv4.2 | timeseries | [Top $TopResources NFSv4.2 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=550) |
///



### svm_nfs_readdir_avg_latency

Average latency in microseconds of ReadDir procedure requests. The counter keeps track of the average response time of ReadDir requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `readdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdir.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `readdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdir.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `readdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdir.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `readdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdir.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `readdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdir_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `readdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdir_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `readdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdir_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `readdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdir_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_readdir_total

Total number ReadDir of procedure requests. It is the total number of ReadDir success and ReadDir error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `readdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `readdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `readdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `readdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `readdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `readdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `readdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `readdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_readdirplus_avg_latency

Average latency in microseconds of ReadDirPlus procedure requests. The counter keeps track of the average response time of ReadDirPlus requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `readdirplus.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readdirplus.total | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `readdirplus_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readdirplus_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_readdirplus_total

Total number ReadDirPlus of procedure requests. It is the total number of ReadDirPlus success and ReadDirPlus error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `readdirplus.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `readdirplus_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_readlink_avg_latency

Average latency in microseconds of READLINK procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `readlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readlink.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `readlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readlink.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `readlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> readlink.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `readlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readlink_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `readlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readlink_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `readlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> readlink_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_readlink_total

Total number of READLINK procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `readlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `readlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `readlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `readlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `readlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `readlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_reclaim_complete_avg_latency

Average latency in microseconds of RECLAIM_COMPLETE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `reclaim_complete.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> reclaim_complete.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `reclaim_complete.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> reclaim_complete.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `reclaim_complete_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> reclaim_complete_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `reclaim_complete_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> reclaim_complete_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_reclaim_complete_total

Total number of RECLAIM_COMPLETE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `reclaim_complete.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `reclaim_complete.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `reclaim_complete_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `reclaim_complete_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_release_lock_owner_avg_latency

Average Latency of RELEASE_LOCKOWNER procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `release_lock_owner.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> release_lock_owner.total | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `release_lock_owner_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> release_lock_owner_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_release_lock_owner_total

Total number of RELEASE_LOCKOWNER procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `release_lock_owner.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `release_lock_owner_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_remove_avg_latency

Average latency in microseconds of Remove procedure requests. The counter keeps track of the average response time of Remove requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `remove.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> remove.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `remove.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> remove.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `remove.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> remove.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `remove.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> remove.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `remove_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> remove_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `remove_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> remove_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `remove_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> remove_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `remove_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> remove_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_remove_total

Total number Remove of procedure requests. It is the total number of Remove success and Remove error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `remove.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `remove.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `remove.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `remove.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `remove_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `remove_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `remove_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `remove_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_rename_avg_latency

Average latency in microseconds of Rename procedure requests. The counter keeps track of the average response time of Rename requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `rename.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rename.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `rename.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rename.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `rename.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rename.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `rename.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rename.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `rename_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rename_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `rename_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rename_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `rename_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rename_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `rename_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rename_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_rename_total

Total number Rename of procedure requests. It is the total number of Rename success and Rename error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `rename.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `rename.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `rename.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `rename.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `rename_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `rename_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `rename_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `rename_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_renew_avg_latency

Average latency in microseconds of RENEW procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `renew.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> renew.total | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `renew_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> renew_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_renew_total

Total number of RENEW procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `renew.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `renew_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_restorefh_avg_latency

Average latency in microseconds of RESTOREFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `restorefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> restorefh.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `restorefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> restorefh.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `restorefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> restorefh.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `restorefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> restorefh_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `restorefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> restorefh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `restorefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> restorefh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_restorefh_total

Total number of RESTOREFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `restorefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `restorefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `restorefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `restorefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `restorefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `restorefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_rmdir_avg_latency

Average latency in microseconds of RmDir procedure requests. The counter keeps track of the average response time of RmDir requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `rmdir.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> rmdir.total | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `rmdir_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> rmdir_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_rmdir_total

Total number RmDir of procedure requests. It is the total number of RmDir success and RmDir error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `rmdir.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `rmdir_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_savefh_avg_latency

Average latency in microseconds of SAVEFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `savefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> savefh.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `savefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> savefh.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `savefh.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> savefh.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `savefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> savefh_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `savefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> savefh_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `savefh_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> savefh_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_savefh_total

Total number of SAVEFH procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `savefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `savefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `savefh.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `savefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `savefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `savefh_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_secinfo_avg_latency

Average latency in microseconds of SECINFO procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `secinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `secinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `secinfo.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `secinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `secinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `secinfo_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_secinfo_no_name_avg_latency

Average latency in microseconds of SECINFO_NO_NAME operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `secinfo_no_name.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo_no_name.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `secinfo_no_name.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> secinfo_no_name.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `secinfo_no_name_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_no_name_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `secinfo_no_name_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> secinfo_no_name_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_secinfo_no_name_total

Total number of SECINFO_NO_NAME operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `secinfo_no_name.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `secinfo_no_name.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `secinfo_no_name_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `secinfo_no_name_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_secinfo_total

Total number of SECINFO procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `secinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `secinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `secinfo.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `secinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `secinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `secinfo_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_sequence_avg_latency

Average latency in microseconds of SEQUENCE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `sequence.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> sequence.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `sequence.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> sequence.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `sequence_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> sequence_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `sequence_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> sequence_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_sequence_total

Total number of SEQUENCE operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `sequence.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `sequence.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `sequence_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `sequence_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_set_ssv_avg_latency

Average latency in microseconds of SET_SSV operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `set_ssv.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> set_ssv.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `set_ssv.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> set_ssv.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `set_ssv_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> set_ssv_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `set_ssv_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> set_ssv_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_set_ssv_total

Total number of SET_SSV operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `set_ssv.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `set_ssv.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `set_ssv_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `set_ssv_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_setattr_avg_latency

Average latency in microseconds of SetAttr procedure requests. The counter keeps track of the average response time of SetAttr requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `setattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setattr.total | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `setattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setattr.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `setattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setattr.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `setattr.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setattr.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `setattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setattr_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `setattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setattr_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `setattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setattr_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `setattr_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setattr_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_setattr_total

Total number of Setattr procedure requests. It is the total number of Setattr success and setattr error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `setattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `setattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `setattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `setattr.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `setattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `setattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `setattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `setattr_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_setclientid_avg_latency

Average latency in microseconds of SETCLIENTID procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `setclientid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setclientid.total | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `setclientid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setclientid_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_setclientid_confirm_avg_latency

Average latency in microseconds of SETCLIENTID_CONFIRM procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `setclientid_confirm.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> setclientid_confirm.total | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `setclientid_confirm_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> setclientid_confirm_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_setclientid_confirm_total

Total number of SETCLIENTID_CONFIRM procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `setclientid_confirm.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `setclientid_confirm_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_setclientid_total

Total number of SETCLIENTID procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `setclientid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `setclientid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |



### svm_nfs_symlink_avg_latency

Average latency in microseconds of SymLink procedure requests. The counter keeps track of the average response time of SymLink requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `symlink.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> symlink.total | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `symlink_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> symlink_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_symlink_total

Total number SymLink of procedure requests. It is the total number of SymLink success and create SymLink requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `symlink.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `symlink_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |



### svm_nfs_test_stateid_avg_latency

Average latency in microseconds of TEST_STATEID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `test_stateid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> test_stateid.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `test_stateid.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> test_stateid.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `test_stateid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> test_stateid_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `test_stateid_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> test_stateid_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_test_stateid_total

Total number of TEST_STATEID operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `test_stateid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `test_stateid.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `test_stateid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `test_stateid_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_throughput

Rate of NFSv3 data transfers in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `total.throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `total.write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `total.write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `nfsv3_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `nfs4_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `nfs41_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `nfs42_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |

The `svm_nfs_throughput` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv3 | stat | [NFSv3 Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=50) |
| ONTAP: SVM | NFSv3 | timeseries | [Top $TopResources NFSv3 SVMs by Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=53) |
| ONTAP: SVM | NFSv4 | stat | [NFSv4 Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=155) |
| ONTAP: SVM | NFSv4.1 | stat | [NFSv4.1 Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=165) |
| ONTAP: SVM | NFSv4.2 | stat | [NFSv4.2 Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=530) |
///



### svm_nfs_total_throughput

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4_1.yaml |



### svm_nfs_verify_avg_latency

Average latency in microseconds of VERIFY procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `verify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> verify.total | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `verify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> verify.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `verify.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> verify.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `verify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> verify_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `verify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> verify_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `verify_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> verify_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_verify_total

Total number of VERIFY procedures

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `verify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `verify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `verify.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `verify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `verify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `verify_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_want_delegation_avg_latency

Average latency in microseconds of WANT_DELEGATION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `want_delegation.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> want_delegation.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `want_delegation.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> want_delegation.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `want_delegation_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> want_delegation_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `want_delegation_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> want_delegation_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_want_delegation_total

Total number of WANT_DELEGATION operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `want_delegation.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `want_delegation.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `want_delegation_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `want_delegation_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |



### svm_nfs_write_avg_latency

Average latency in microseconds of Write procedure requests. The counter keeps track of the average response time of Write requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `write.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write.total | conf/restperf/9.12.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v3.iops_raw.write | conf/keyperf/9.15.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `write.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write.total | conf/restperf/9.12.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v4.iops_raw.write | conf/keyperf/9.15.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `write.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write.total | conf/restperf/9.12.0/nfsv4_1.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> svm_nfs_statistics.v41.iops_raw.write | conf/keyperf/9.15.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `write.average_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write.total | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `write_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> write_total | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `write_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> write_total | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `write_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> write_total | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `write_avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average,no-zero-values<br><span class="key">Base:</span> write_total | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |

The `svm_nfs_write_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv3 | stat | [NFSv3 Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=47) |
| ONTAP: SVM | NFSv3 | timeseries | [Top $TopResources NFSv3 SVMs by Read and Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=51) |
| ONTAP: SVM | NFSv4 | stat | [NFSv4 Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=159) |
| ONTAP: SVM | NFSv4 | timeseries | [Top $TopResources NFSv4 SVMs by Read and Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=145) |
| ONTAP: SVM | NFSv4.1 | stat | [NFSv4.1 Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=167) |
| ONTAP: SVM | NFSv4.1 | timeseries | [Top $TopResources NFSv4.1 SVMs by Read and Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=172) |
| ONTAP: SVM | NFSv4.2 | stat | [NFSv4.2 Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=536) |
| ONTAP: SVM | NFSv4.2 | timeseries | [Top $TopResources NFSv4.2 SVMs by Read and Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=546) |
///



### svm_nfs_write_ops

Total observed NFSv3 write operations per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `nfsv3_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |

The `svm_nfs_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv3 | stat | [NFSv3 Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=153) |
| ONTAP: SVM | NFSv3 | timeseries | [Top $TopResources NFSv3 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=42) |
///



### svm_nfs_write_throughput

Rate of NFSv3 write data transfers in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v3.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `total.write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v4.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `total.throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| KeyPerf | `api/protocols/nfs/services` | `statistics.v41.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `total.throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `nfsv3_write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `nfs4_write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `nfs41_write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `nfs42_write_throughput`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate,no-zero-values<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |

The `svm_nfs_write_throughput` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv3 | stat | [NFSv3 Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=151) |
| ONTAP: SVM | NFSv3 | timeseries | [Top $TopResources NFSv3 SVMs by Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=53) |
| ONTAP: SVM | NFSv4 | stat | [NFSv4 Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=161) |
| ONTAP: SVM | NFSv4 | timeseries | [Top $TopResources NFSv4 SVMs by Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=146) |
| ONTAP: SVM | NFSv4.1 | stat | [NFSv4.1 Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=169) |
| ONTAP: SVM | NFSv4.1 | timeseries | [Top $TopResources NFSv4.1 SVMs by Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=173) |
| ONTAP: SVM | NFSv4.2 | stat | [NFSv4.2 Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=540) |
| ONTAP: SVM | NFSv4.2 | timeseries | [Top $TopResources NFSv4.2 SVMs by Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=548) |
///



### svm_nfs_write_total

Total number of Write procedure requests. It is the total number of write success and write error requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_nfs_v3` | `write.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv3.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v4` | `write.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v41` | `write.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_1.yaml |
| RestPerf | `api/cluster/counter/tables/svm_nfs_v42` | `write.total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/nfsv4_2.yaml |
| ZapiPerf | `perf-object-get-instances nfsv3` | `write_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv3.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4` | `write_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_1` | `write_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml |
| ZapiPerf | `perf-object-get-instances nfsv4_2` | `write_total`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.11.0/nfsv4_2.yaml |

The `svm_nfs_write_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | NFSv4 | stat | [NFSv4 Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=163) |
| ONTAP: SVM | NFSv4 | timeseries | [Top $TopResources NFSv4 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=147) |
| ONTAP: SVM | NFSv4.1 | stat | [NFSv4.1 Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=171) |
| ONTAP: SVM | NFSv4.1 | timeseries | [Top $TopResources NFSv4.1 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=174) |
| ONTAP: SVM | NFSv4.2 | stat | [NFSv4.2 Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=544) |
| ONTAP: SVM | NFSv4.2 | timeseries | [Top $TopResources NFSv4.2 SVMs by IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=550) |
///



### svm_vol_avg_latency

Performance metric aggregated over all types of I/O operations. svm_vol_avg_latency in microseconds is [volume_avg_latency](#volume_avg_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.total | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | SVM Metrics | timeseries | [Top $TopResources Average Latency by SVMs](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=268) |
| ONTAP: Cluster | SVM Performance | timeseries | [Top $TopResources Latency](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=216) |
| ONTAP: SVM | Highlights | stat | [SVM Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=64) |
///



### svm_vol_nfs_access_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_access_latency is [volume_nfs_access_latency](#volume_nfs_access_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.access.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.access.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_access_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_access_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_access_ops

Number of operations of the given type performed on this volume. svm_vol_nfs_access_ops is [volume_nfs_access_ops](#volume_nfs_access_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.access.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_access_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_getattr_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_getattr_latency is [volume_nfs_getattr_latency](#volume_nfs_getattr_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.getattr.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.getattr.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_getattr_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_getattr_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_getattr_ops

Number of operations of the given type performed on this volume. svm_vol_nfs_getattr_ops is [volume_nfs_getattr_ops](#volume_nfs_getattr_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.getattr.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_getattr_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_lookup_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_lookup_latency is [volume_nfs_lookup_latency](#volume_nfs_lookup_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.lookup.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.lookup.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_lookup_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_lookup_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_lookup_ops

Number of operations of the given type performed on this volume. svm_vol_nfs_lookup_ops is [volume_nfs_lookup_ops](#volume_nfs_lookup_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.lookup.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_lookup_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_other_latency

Average time for the WAFL filesystem to process other NFS operations to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency in microseconds (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_other_latency is [volume_nfs_other_latency](#volume_nfs_other_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_other_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_other_ops

Number of other NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_other_ops is [volume_nfs_other_ops](#volume_nfs_other_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_punch_hole_latency

Average time for the WAFL filesystem to process NFS protocol hole-punch requests to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_punch_hole_latency in microseconds is [volume_nfs_punch_hole_latency](#volume_nfs_punch_hole_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_punch_hole_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_punch_hole_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_punch_hole_ops

Number of NFS hole-punch requests per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_punch_hole_ops is [volume_nfs_punch_hole_ops](#volume_nfs_punch_hole_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_punch_hole_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_read_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_read_latency is [volume_nfs_read_latency](#volume_nfs_read_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.read.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.read.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_read_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_read_ops

Number of operations of the given type performed on this volume. svm_vol_nfs_read_ops is [volume_nfs_read_ops](#volume_nfs_read_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.read.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_setattr_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_setattr_latency is [volume_nfs_setattr_latency](#volume_nfs_setattr_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.setattr.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.setattr.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_setattr_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_setattr_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_setattr_ops

Number of operations of the given type performed on this volume. svm_vol_nfs_setattr_ops is [volume_nfs_setattr_ops](#volume_nfs_setattr_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.setattr.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_setattr_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_total_ops

Number of total NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.). svm_vol_nfs_total_ops is [volume_nfs_total_ops](#volume_nfs_total_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_write_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type. svm_vol_nfs_write_latency is [volume_nfs_write_latency](#volume_nfs_write_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.write.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.write.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_write_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_nfs_write_ops

Number of operations of the given type performed on this volume. svm_vol_nfs_write_ops is [volume_nfs_write_ops](#volume_nfs_write_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.write.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_other_data

Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on. svm_vol_other_data is [volume_other_data](#volume_other_data) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.other`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |



### svm_vol_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on. svm_vol_other_latency in microseconds is [volume_other_latency](#volume_other_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.other | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_other_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Highlights | timeseries | [SVM Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=75) |
///



### svm_vol_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on. svm_vol_other_ops is [volume_other_ops](#volume_other_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_other_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFS Troubleshooting | Highlights | table | [SVM Performance Table](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=15) |
| ONTAP: SVM | Highlights | timeseries | [SVM IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=77) |
///



### svm_vol_read_data

Performance metric for read I/O operations in bytes per seconds. svm_vol_read_data is [volume_read_data](#volume_read_data) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | SVM Metrics | timeseries | [Top $TopResources Average Throughput by SVMs](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=269) |
| ONTAP: Cluster | SVM Performance | timeseries | [Top $TopResources Throughput](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=218) |
| ONTAP: NFS Troubleshooting | Highlights | table | [SVM Performance Table](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=15) |
| ONTAP: SVM | Highlights | stat | [SVM Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=70) |
| ONTAP: SVM | Highlights | timeseries | [SVM Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=76) |
///



### svm_vol_read_latency

Performance metric for read I/O operations. svm_vol_read_latency in microseconds is [volume_read_latency](#volume_read_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.read | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Highlights | stat | [SVM Average Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=69) |
| ONTAP: SVM | Highlights | timeseries | [SVM Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=75) |
///



### svm_vol_read_ops

Performance metric for read I/O operations. svm_vol_read_ops is [volume_read_ops](#volume_read_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFS Troubleshooting | Highlights | table | [SVM Performance Table](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=15) |
| ONTAP: SVM | Highlights | stat | [SVM Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=71) |
| ONTAP: SVM | Highlights | timeseries | [SVM IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=77) |
///



### svm_vol_total_data

Performance metric aggregated over all types of I/O operations in bytes per seconds. svm_vol_total_data is [volume_total_data](#volume_total_data) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `NA` | `Harvest generated`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### svm_vol_total_ops

Performance metric aggregated over all types of I/O operations. svm_vol_total_ops is [volume_total_ops](#volume_total_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | SVM Metrics | timeseries | [Top $TopResources IOPs by SVMs](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=270) |
| ONTAP: Cluster | SVM Performance | timeseries | [Top $TopResources IOPs](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=219) |
| ONTAP: NFS Troubleshooting | Highlights | table | [SVM Performance Table](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=15) |
| ONTAP: SVM | Highlights | stat | [SVM IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=66) |
///



### svm_vol_write_data

Performance metric for write I/O operations in bytes per seconds. svm_vol_write_data is [volume_write_data](#volume_write_data) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | SVM Metrics | timeseries | [Top $TopResources Average Throughput by SVMs](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=269) |
| ONTAP: Cluster | SVM Performance | timeseries | [Top $TopResources Throughput](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=218) |
| ONTAP: NFS Troubleshooting | Highlights | table | [SVM Performance Table](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=15) |
| ONTAP: SVM | Highlights | stat | [SVM Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=65) |
| ONTAP: SVM | Highlights | stat | [SVM Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=73) |
| ONTAP: SVM | Highlights | timeseries | [SVM Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=76) |
///



### svm_vol_write_latency

Performance metric for write I/O operations. svm_vol_write_latency in microseconds is [volume_write_latency](#volume_write_latency) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.write | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Highlights | stat | [SVM Average Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=72) |
| ONTAP: SVM | Highlights | timeseries | [SVM Average Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=75) |
///



### svm_vol_write_ops

Performance metric for write I/O operations. svm_vol_write_ops is [volume_write_ops](#volume_write_ops) aggregated by `svm`.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `svm_vol_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: NFS Troubleshooting | Highlights | table | [SVM Performance Table](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=15) |
| ONTAP: SVM | Highlights | stat | [SVM Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=74) |
| ONTAP: SVM | Highlights | timeseries | [SVM IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=77) |
///



### svm_vscan_connections_active

Total number of current active connections

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_vscan` | `connections_active`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.13.0/vscan_svm.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan` | `connections_active`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/vscan_svm.yaml |

The `svm_vscan_connections_active` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | timeseries | [Virus Scan Connections Active](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=244) |
| ONTAP: Vscan | Highlights | timeseries | [Active Connections](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=597) |
///



### svm_vscan_dispatch_latency

Average dispatch latency in microseconds

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_vscan` | `dispatch.latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> dispatch.requests | conf/restperf/9.13.0/vscan_svm.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan` | `dispatch_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> dispatch_latency_base | conf/zapiperf/cdot/9.8.0/vscan_svm.yaml |

The `svm_vscan_dispatch_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | timeseries | [Virus Scan Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=242) |
| ONTAP: Vscan | Highlights | timeseries | [Top $TopResources SVM by Dispatch Latency](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=592) |
///



### svm_vscan_scan_latency

Average scan latency in microseconds

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_vscan` | `scan.latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> scan.requests | conf/restperf/9.13.0/vscan_svm.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan` | `scan_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> scan_latency_base | conf/zapiperf/cdot/9.8.0/vscan_svm.yaml |

The `svm_vscan_scan_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | timeseries | [Virus Scan Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=242) |
| ONTAP: Vscan | Highlights | timeseries | [Top $TopResources SVMs by Scan Latency](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=593) |
///



### svm_vscan_scan_noti_received_rate

Total number of scan notifications received by the dispatcher per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_vscan` | `scan.notification_received_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.13.0/vscan_svm.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan` | `scan_noti_received_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/vscan_svm.yaml |

The `svm_vscan_scan_noti_received_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | timeseries | [Virus Scan Requests](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=243) |
| ONTAP: Vscan | Highlights | timeseries | [Top $TopResources SVMs by Scan Notifications Received Throughput](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=594) |
///



### svm_vscan_scan_request_dispatched_rate

Total number of scan requests sent to the Vscanner per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/svm_vscan` | `scan.request_dispatched_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.13.0/vscan_svm.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan` | `scan_request_dispatched_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/vscan_svm.yaml |

The `svm_vscan_scan_request_dispatched_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | CIFS | timeseries | [Virus Scan Requests](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=243) |
| ONTAP: Vscan | Highlights | timeseries | [Top $TopResources SVMs by Scan Requests Sent to Vscanner Throughput](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=595) |
///



### token_copy_bytes

Total number of bytes copied.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/token_manager` | `token_copy.bytes`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/token_manager.yaml |
| ZapiPerf | `perf-object-get-instances token_manager` | `token_copy_bytes`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/token_manager.yaml |



### token_copy_failure

Number of failed token copy requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/token_manager` | `token_copy.failures`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/token_manager.yaml |
| ZapiPerf | `perf-object-get-instances token_manager` | `token_copy_failure`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/token_manager.yaml |



### token_copy_success

Number of successful token copy requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/token_manager` | `token_copy.successes`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/token_manager.yaml |
| ZapiPerf | `perf-object-get-instances token_manager` | `token_copy_success`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/token_manager.yaml |



### token_create_bytes

Total number of bytes for which tokens are created.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/token_manager` | `token_create.bytes`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/token_manager.yaml |
| ZapiPerf | `perf-object-get-instances token_manager` | `token_create_bytes`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/token_manager.yaml |



### token_create_failure

Number of failed token create requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/token_manager` | `token_create.failures`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/token_manager.yaml |
| ZapiPerf | `perf-object-get-instances token_manager` | `token_create_failure`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/token_manager.yaml |



### token_create_success

Number of successful token create requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/token_manager` | `token_create.successes`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/token_manager.yaml |
| ZapiPerf | `perf-object-get-instances token_manager` | `token_create_success`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/token_manager.yaml |



### token_zero_bytes

Total number of bytes zeroed.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/token_manager` | `token_zero.bytes`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/token_manager.yaml |
| ZapiPerf | `perf-object-get-instances token_manager` | `token_zero_bytes`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/token_manager.yaml |



### token_zero_failure

Number of failed token zero requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/token_manager` | `token_zero.failures`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/token_manager.yaml |
| ZapiPerf | `perf-object-get-instances token_manager` | `token_zero_failure`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/token_manager.yaml |



### token_zero_success

Number of successful token zero requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/token_manager` | `token_zero.successes`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/token_manager.yaml |
| ZapiPerf | `perf-object-get-instances token_manager` | `token_zero_success`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/token_manager.yaml |



### volume_analytics_bytes_used_by_accessed_time

Number of bytes used on-disk, broken down by date of last access.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/volumes/{volume.uuid}/files` | `analytics.by_accessed_time.bytes_used.values` | conf/rest/9.12.0/volume_analytics.yaml |

The `volume_analytics_bytes_used_by_accessed_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Access ($Activity) History](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=88) |
///



### volume_analytics_bytes_used_by_modified_time

Number of bytes used on-disk, broken down by date of last modification.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/volumes/{volume.uuid}/files` | `analytics.by_modified_time.bytes_used.values` | conf/rest/9.12.0/volume_analytics.yaml |

The `volume_analytics_bytes_used_by_modified_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Modify ($Activity) History](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=91) |
///



### volume_analytics_bytes_used_percent_by_accessed_time

Percent used on-disk, broken down by date of last access.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/volumes/{volume.uuid}/files` | `analytics.by_accessed_time.bytes_used.percentages` | conf/rest/9.12.0/volume_analytics.yaml |

The `volume_analytics_bytes_used_percent_by_accessed_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Access ($Activity) History By Percent](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=92) |
///



### volume_analytics_bytes_used_percent_by_modified_time

Percent used on-disk, broken down by date of last modification.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/volumes/{volume.uuid}/files` | `analytics.by_modified_time.bytes_used.percentages` | conf/rest/9.12.0/volume_analytics.yaml |

The `volume_analytics_bytes_used_percent_by_modified_time` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Modify ($Activity) History By Percent](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=93) |
///



### volume_analytics_dir_bytes_used

The actual number of bytes used on disk by this file.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/volumes/{volume.uuid}/files` | `analytics.bytes_used` | conf/rest/9.12.0/volume_analytics.yaml |

The `volume_analytics_dir_bytes_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: File System Analytics (FSA) | Highlights | timeseries | [Top $TopResources Volumes by Directory Growth](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=86) |
| ONTAP: File System Analytics (FSA) | Highlights | table | [Top $TopResources Volumes by Directory Growth](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=25) |
///



### volume_analytics_dir_file_count

Number of files in a directory.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/volumes/{volume.uuid}/files` | `analytics.file_count` | conf/rest/9.12.0/volume_analytics.yaml |

The `volume_analytics_dir_file_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Files](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=23) |
| ONTAP: File System Analytics (FSA) | Highlights | table | [Top $TopResources Volumes by Directory Growth](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=25) |
///



### volume_analytics_dir_subdir_count

Number of sub directories in a directory.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/storage/volumes/{volume.uuid}/files` | `analytics.subdir_count` | conf/rest/9.12.0/volume_analytics.yaml |

The `volume_analytics_dir_subdir_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Directories](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=22) |
| ONTAP: File System Analytics (FSA) | Highlights | table | [Top $TopResources Volumes by Directory Growth](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=25) |
///



### volume_arw_status

Represent the cluster level ARW status

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| Rest | `NA` | `Harvest generated` | conf/rest/9.14.0/volume.yaml |

The `volume_arw_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
///



### volume_autosize_grow_threshold_percent

Used space threshold which triggers autogrow. When the size-used is greater than this percent of size-total, the volume will be grown. The computed value is rounded down. The default value of this element varies from 85% to 98%, depending on the volume size. It is an error for the grow threshold to be less than or equal to the shrink threshold.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `autosize_grow_threshold_percent` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-autosize-attributes.grow-threshold-percent` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_autosize_grow_threshold_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Volume Autosize Table | table | [Volumes Autogrow](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=186) |
///



### volume_autosize_maximum_size

The maximum size (in bytes) to which the volume would be grown automatically. The default value is 20% greater than the volume size. It is an error for the maximum volume size to be less than the current volume size. It is also an error for the maximum size to be less than or equal to the minimum size.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `max_autosize` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-autosize-attributes.maximum-size` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_autosize_maximum_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Volume Autosize Table | table | [Volumes Autogrow](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=186) |
///



### volume_autosize_minimum_size

Minimum size in bytes up to which the volume shrinks automatically. This size cannot be greater than or equal to the maximum size of volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `min_autosize` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-autosize-attributes.minimum-size` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_autosize_minimum_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Volume Autosize Table | table | [Volumes Autogrow](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=186) |
///



### volume_autosize_shrink_threshold_percent

Used space threshold size, in percentage, for the automatic shrinkage of the volume. When the amount of used space in the volume drops below this threshold, the volume automatically shrinks unless it has reached the minimum size. The volume shrinks when the 'space.used' is less than the 'shrink_threshold' percent of 'space.size'. The 'shrink_threshold' size cannot be greater than or equal to the 'grow_threshold' size.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `autosize_shrink_threshold_percent` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-autosize-attributes.shrink-threshold-percent` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_autosize_shrink_threshold_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Volume Autosize Table | table | [Volumes Autogrow](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=186) |
///



### volume_avg_latency

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.total`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.total | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `avg_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> total_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_avg_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by Average Latency](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=813) |
| ONTAP: cDOT | Volume Metrics | timeseries | [Top $TopResources Volumes by Average Latency](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=258) |
| ONTAP: Cluster | Throughput | timeseries | [Max Latency](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=106) |
| ONTAP: Datacenter | Performance | timeseries | [Top $TopResources Latency by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=106) |
| ONTAP: FlexGroup | Highlights | timeseries | [Top $TopResources Constituents by Average Latency](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=125) |
| ONTAP: MetroCluster | Highlights | stat | [Volume Average Latency](/d/cdot-metrocluster/ontap3a-metrocluster?orgId=1&viewPanel=24) |
| ONTAP: Node | Volume Performance | timeseries | [Top $TopResources Volumes by Average Latency](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=133) |
| ONTAP: Volume | Highlights | stat | [Volume Average Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=4) |
| ONTAP: Volume | Highlights | timeseries | [Top $TopResources Volumes by Average Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=10) |
| ONTAP: Volume Deep Dive | Highlights | stat | [Avg Latency](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=16) |
| ONTAP: Volume Deep Dive | Highlights | stat | [Max Latency](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=15) |
///



### volume_capacity_tier_footprint

This field represents the footprint of blocks written to the volume in bytes for bin 1.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/footprint` | `volume_blocks_footprint_bin1` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-footprint-get-iter` | `volume-blocks-footprint-bin1` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_capacity_tier_footprint` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Capacity | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=99) |
| ONTAP: FlexGroup | Top Volume FabricPool | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=121) |
| ONTAP: Volume | Volume Hot-Cold Data | table | [Volumes by Cold data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=45) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Cold Data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=47) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Cold Data %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=49) |
| ONTAP: Volume | FabricPool | table | [Volumes Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=185) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=121) |
///



### volume_capacity_tier_footprint_percent

This field represents the footprint of blocks written to the volume in bin 1 as a percentage of aggregate size.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/footprint` | `volume_blocks_footprint_bin1_percent` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-footprint-get-iter` | `volume-blocks-footprint-bin1-percent` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_capacity_tier_footprint_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Capacity | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=101) |
| ONTAP: FlexGroup | Top Volume FabricPool | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint %](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=122) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=122) |
///



### volume_clone_split_estimate

Display an estimate of additional storage required in the underlying aggregate to perform a volume clone split operation.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `clone.split_estimate` | `api/storage/volumes` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `volume-clone-get-iter` | `split-estimate` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_clone_split_estimate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Volume Table | table | [Volumes in Cluster](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=18) |
| ONTAP: Volume Deep Dive | Volume Capacity: $Volume | table | [Volumes in Cluster](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=22) |
///



### volume_delayed_free_footprint

This field represents the delayed free blocks footprint in bytes. This system is used to improve delete performance by batching delete requests.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/footprint` | `delayed_free_footprint` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-footprint-get-iter` | `delayed-free-footprint` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_delayed_free_footprint` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | FabricPool | table | [Volumes Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=185) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Delayed Free Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=179) |
///



### volume_filesystem_size

Filesystem size (in bytes) of the volume.  This is the total usable size of the volume, not including WAFL reserve.  This value is the same as Size except for certain SnapMirror destination volumes.  It is possible for destination volumes to have a different filesystem-size because the filesystem-size is sent across from the source volume.  This field is valid only when the volume is online.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `filesystem_size` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.filesystem-size` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_guarantee_footprint

This field represents the volume guarantee footprint in bytes. Alternatively, it is the space reserved for future writes in the volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/footprint` | `volume_guarantee_footprint` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-footprint-get-iter` | `volume-guarantee-footprint` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_guarantee_footprint` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | FabricPool | table | [Volumes Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=185) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=182) |
///



### volume_hot_data

Hot data size that is physically used in the volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_hot_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Volume Hot-Cold Data | table | [Volumes by Cold data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=45) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Hot Data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=48) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Hot Data %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=50) |
///



### volume_inode_files_total

Total user-visible file (inode) count, i.e., current maximum number of user-visible files (inodes) that this volume can currently hold.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `files` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-inode-attributes.files-total` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_inode_files_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Inode | timeseries | [Top $TopResources Volumes by Inode Files Total](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=102) |
| ONTAP: Volume Deep Dive | Inodes | timeseries | [Inode Files Total](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=40) |
///



### volume_inode_files_used

Number of user-visible files (inodes) used. This field is valid only when the volume is online.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `files_used` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-inode-attributes.files-used` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_inode_files_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Inode | timeseries | [Top $TopResources Volumes by Inode Files Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=100) |
| ONTAP: Volume Deep Dive | Inodes | timeseries | [Inode Files Used](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=38) |
///



### volume_inode_used_percent

volume_inode_files_used / volume_inode_total

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `inode_files_used, inode_files_total` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `inode_files_used, inode_files_total` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_inode_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Inode | timeseries | [Top $TopResources Volumes by Inode Files Used Percentage](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=101) |
| ONTAP: Volume Deep Dive | Inodes | timeseries | [Inode Files Used Percentage](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=42) |
///



### volume_labels

This metric provides information about Volume

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `Harvest generated` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `Harvest generated` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | Capacity Metrics | table | [Top $TopResources Volumes by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=242) |
| ONTAP: cDOT | Capacity Metrics | timeseries | [Top $TopResources Volumes by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=266) |
| ONTAP: Data Protection | Highlights | piechart | [Snapshot copies (local)](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=155) |
| ONTAP: Data Protection | Highlights | piechart | [SnapMirrors (local or remote)](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=157) |
| ONTAP: Data Protection | Highlights | piechart | [Back up to cloud](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=156) |
| ONTAP: Data Protection | Highlights | table | [Volumes](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=162) |
| ONTAP: Data Protection | Snapshot Copies | stat | [Volumes not protected](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=79) |
| ONTAP: Data Protection | Snapshot Copies | stat | [Volumes protected](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=75) |
| ONTAP: Data Protection | Snapshot Copies | stat | [Volumes breached](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=77) |
| ONTAP: Data Protection | Snapshot Copies | stat | [Volumes not breached](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=81) |
| ONTAP: Data Protection | Snapshot Copies | table | [Volumes Protected With Snapshot Copies (local)](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=83) |
| ONTAP: Data Protection | Snapshot Copies | table | [Volumes Breaching Snapshot Copy Reserve Space](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=91) |
| ONTAP: Data Protection | Snapshot Copies | stat | [<10 Copies ](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=96) |
| ONTAP: Data Protection | Snapshot Copies | stat | [10-100 Copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=97) |
| ONTAP: Data Protection | Snapshot Copies | stat | [101-500 Copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=98) |
| ONTAP: Data Protection | Snapshot Copies | stat | [>500 Copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=99) |
| ONTAP: Data Protection | Snapshot Copies | table | [Volume count by the number of Snapshot copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=94) |
| ONTAP: Datacenter | Highlights | table | [Object Count](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=660) |
| ONTAP: Datacenter | Snapshots | piechart | [Protected Status](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=629) |
| ONTAP: Datacenter | Snapshots | piechart | [Breached Status](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=631) |
| ONTAP: Datacenter | Snapshots | piechart | [Snapshot Copies](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=632) |
| ONTAP: FlexGroup | Volume Table | table | [FlexGroup Constituents in Cluster](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=18) |
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Used](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=95) |
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Available](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=96) |
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Size](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=97) |
| ONTAP: File System Analytics (FSA) | Highlights | bargauge | [Used Percentage](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=103) |
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Directories](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=22) |
| ONTAP: File System Analytics (FSA) | Highlights | timeseries | [Top $TopResources Volumes by Directory Growth](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=86) |
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Files](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=23) |
| ONTAP: File System Analytics (FSA) | Highlights | table | [Top $TopResources Volumes by Directory Growth](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=25) |
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Access ($Activity) History](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=88) |
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Access ($Activity) History By Percent](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=92) |
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Modify ($Activity) History](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=91) |
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Modify ($Activity) History By Percent](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=93) |
| ONTAP: Health | Volume | table | [Volumes with Ransomware Issues (9.10+ Only)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=264) |
| ONTAP: Health | Volume | table | [Volumes Move Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=271) |
| ONTAP: NFS Troubleshooting | Highlights | table | [SVM Performance Table](/d/cdot-nfs-troubleshooting/ontap3a-nfs troubleshooting?orgId=1&viewPanel=15) |
| ONTAP: Quota | Highlights | table | [Reports](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=5) |
| ONTAP: Quota | Space Usage | timeseries | [Top $TopResources Quotas by Space Used](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=7) |
| ONTAP: Quota | Space Usage | timeseries | [Top $TopResources Quotas by Space Used %](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=8) |
| ONTAP: Quota | Space Usage | timeseries | [Top $TopResources Quotas by Files Used](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=9) |
| ONTAP: Quota | Space Usage | timeseries | [Top $TopResources Quotas by Files Used %](/d/cdot-quota/ontap3a-quota?orgId=1&viewPanel=10) |
| ONTAP: S3 Object Storage | Highlights | table | [Bucket Overview](/d/cdot-s3-object-storage/ontap3a-s3 object storage?orgId=1&viewPanel=5) |
| ONTAP: Security | Highlights | stat | [Volume Encryption %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=207) |
| ONTAP: Security | Highlights | stat | [Volume Autonomous Ransomware Protection %](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=208) |
| ONTAP: Security | Highlights | piechart | [Volume Encryption](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=204) |
| ONTAP: Security | Highlights | piechart | [Volume Autonomous Ransomware Protection](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=205) |
| ONTAP: Security | Volume Encryption & Autonomous Ransomware Protection | table | [Volume Encryption & Autonomous Ransomware Protection](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=155) |
| ONTAP: Security | Cluster Compliance | table | [Cluster Compliance](/d/cdot-security/ontap3a-security?orgId=1&viewPanel=219) |
| ONTAP: SnapMirror Sources | Highlights | stat | [Unprotected Volumes](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=148) |
| ONTAP: SnapMirror Sources | Highlights | stat | [Protected Volumes](/d/cdot-snapmirror/ontap3a-snapmirror sources?orgId=1&viewPanel=145) |
| ONTAP: Volume | Highlights | stat | [Volume Average Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=4) |
| ONTAP: Volume | Highlights | stat | [Top $TopResources Volumes Total Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=23) |
| ONTAP: Volume | Highlights | stat | [Top $TopResources Volumes by Total IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=5) |
| ONTAP: Volume | Highlights | timeseries | [Top $TopResources Volumes by Average Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=10) |
| ONTAP: Volume | Highlights | timeseries | [Top $TopResources Volumes by Average Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=11) |
| ONTAP: Volume | Highlights | timeseries | [Top $TopResources Volumes by Total IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=12) |
| ONTAP: Volume | Volume Table | table | [Volumes in Cluster](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=18) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Read Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=39) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=41) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Read IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=43) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Write Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=40) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=42) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Write IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=44) |
| ONTAP: Volume | Snaplock | table | [Volume Snaplock](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=188) |
| ONTAP: Volume | Volume Hot-Cold Data | table | [Volumes by Cold data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=45) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Cold Data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=47) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Hot Data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=48) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Cold Data %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=49) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Hot Data %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=50) |
| ONTAP: Volume | Volume Autosize Table | table | [Volumes Autogrow](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=186) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Read Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=30) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Write Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=33) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Other Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=116) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Read IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=32) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Write IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=35) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Other IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=115) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=31) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=34) |
| ONTAP: Volume | QoS | stat | [Top $TopResources QoS Volumes Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=103) |
| ONTAP: Volume | QoS | stat | [Top $TopResources Qos Volumes Total Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=104) |
| ONTAP: Volume | QoS | stat | [Top $TopResources QoS Volumes by Total IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=109) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources QoS Volumes by Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=111) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources QoS Volumes by Average Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=113) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources QoS Volumes by Total IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=117) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Read Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=72) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Write Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=73) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Read IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=76) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Write IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=77) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Other IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=118) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=74) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=75) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Sequential Reads](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=79) |
| ONTAP: Volume | QoS | timeseries | [Top $TopResources Volumes by QoS Volume Sequential Writes](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=80) |
| ONTAP: Volume | I/O Density | timeseries | [Top $TopResources Volumes by IO Density (IOPs/TiB)](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=139) |
| ONTAP: Volume | I/O Density | timeseries | [Bottom $TopResources Volumes by IO Density (IOPs/TiB)](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=140) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Physical Space Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=83) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Logical Space Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=84) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Volume Size Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=85) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Volume Total Size](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=86) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Size Available](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=87) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Reserve Size](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=128) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Reserve Available](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=129) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Size Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=130) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes by Inactive Data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=150) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Physical Space Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=123) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Logical Space Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=124) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Volume Size Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=125) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Snapshot Reserve](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=126) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Snapshot Reserve Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=127) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes by Inactive Data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=151) |
| ONTAP: Volume | Clients | timeseries | [Top $TopResources Volumes Clients by Read IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=157) |
| ONTAP: Volume | Clients | timeseries | [Top $TopResources Volumes Clients by Write IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=159) |
| ONTAP: Volume | Clients | timeseries | [Top $TopResources Volumes Clients by Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=161) |
| ONTAP: Volume | Clients | timeseries | [Top $TopResources Volumes Clients by Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=163) |
| ONTAP: Volume | Files | timeseries | [Top $TopResources Volumes Files by Read IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=169) |
| ONTAP: Volume | Files | timeseries | [Top $TopResources Volumes Files by Write IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=171) |
| ONTAP: Volume | Files | timeseries | [Top $TopResources Volumes Files by Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=173) |
| ONTAP: Volume | Files | timeseries | [Top $TopResources Volumes Files by Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=175) |
| ONTAP: Volume | Object Storage | timeseries | [Top $TopResources Volumes by Object Storage GET Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=93) |
| ONTAP: Volume | Object Storage | timeseries | [Top $TopResources Volumes by Object Storage GET Request Count](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=94) |
| ONTAP: Volume | Object Storage | timeseries | [Top $TopResources Volumes by Object Storage PUT Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=95) |
| ONTAP: Volume | Object Storage | timeseries | [Top $TopResources Volumes by Object Storage PUT Request Count](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=96) |
| ONTAP: Volume | Object Storage | table | [Top $TopResources Volumes by Object Storage Requests](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=90) |
| ONTAP: Volume | FabricPool | table | [Volumes Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=185) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Performance Tier Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=119) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Performance Tier Footprint %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=120) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=121) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=122) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Delayed Free Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=179) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Metadata Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=181) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Total Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=180) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Total Metadata Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=183) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Capacity Tier Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=182) |
| ONTAP: Volume | Inode | timeseries | [Top $TopResources Volumes by Inode Files Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=100) |
| ONTAP: Volume | Inode | timeseries | [Top $TopResources Volumes by Inode Files Total](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=102) |
| ONTAP: Volume | Inode | timeseries | [Top $TopResources Volumes by Inode Files Used Percentage](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=101) |
| ONTAP: Volume | Sis Stat | timeseries | [Top $TopResources Volumes by Number of Compress Fail %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=108) |
| ONTAP: Volume | Sis Stat | timeseries | [Top $TopResources Volumes by Number of Compress Attempts](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=106) |
| ONTAP: Volume | Sis Stat | timeseries | [Top $TopResources Volumes by Number of Compress Fail](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=107) |
| ONTAP: Volume | Growth Rate | timeseries | [Top $TopResources Volumes Per Growth Rate of Physical Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=137) |
| ONTAP: Volume | Growth Rate | timeseries | [Top $TopResources Volumes Per Growth Rate of Logical Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=138) |
| ONTAP: Volume | Growth Rate | table | [Top $TopResources Volumes by Physical Usage: Delta](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=177) |
| ONTAP: Volume | Growth Rate | table | [Top $TopResources Volumes by Logical Usage: Delta](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=178) |
| ONTAP: Volume | Forecast Volume Capacity | table | [Top $TopResources Volumes Per Size Used Percentage Trend](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=149) |
| ONTAP: Volume by SVM | Highlights | table | [Volume Performance for $SVM (Click volume for detailed drill-down)](/d/cdot-volume-by-svm/ontap3a-volume by svm?orgId=1&viewPanel=2) |
| ONTAP: Volume Deep Dive | Highlights | table | [Volume Performance](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=6) |
| ONTAP: Volume Deep Dive | Volume Capacity: $Volume | table | [Volumes in Cluster](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=22) |
///



### volume_metadata_footprint

This field represents flexible volume or flexgroup metadata in bytes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/footprint` | `flexvol_metadata_footprint` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-footprint-get-iter` | `flexvol-metadata-footprint` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_metadata_footprint` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | FabricPool | table | [Volumes Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=185) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Metadata Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=181) |
///



### volume_new_status

This metric indicates a value of 1 if the volume state is online (indicating the volume is operational) and a value of 0 for any other state.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `NA` | `Harvest generated` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_new_status` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexGroup | Volume Table | table | [FlexGroup Constituents in Cluster](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=18) |
| ONTAP: Health | Volume | table | [Volumes with Ransomware Issues (9.10+ Only)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=264) |
| ONTAP: Health | Volume | table | [Volumes Move Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=271) |
| ONTAP: Volume | Volume Table | table | [Volumes in Cluster](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=18) |
| ONTAP: Volume Deep Dive | Volume Capacity: $Volume | table | [Volumes in Cluster](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=22) |
///



### volume_nfs_access_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.access.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.access.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_access_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_access_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_access_ops

Number of operations of the given type performed on this volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.access.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_access_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_getattr_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.getattr.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.getattr.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_getattr_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_getattr_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_getattr_ops

Number of operations of the given type performed on this volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.getattr.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_getattr_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_lookup_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.lookup.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.lookup.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_lookup_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_lookup_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_lookup_ops

Number of operations of the given type performed on this volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.lookup.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_lookup_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_other_latency

Average time for the WAFL filesystem to process other NFS operations to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency in microseconds (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_other_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_other_ops

Number of other NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_punch_hole_latency

Average time for the WAFL filesystem to process NFS protocol hole-punch requests to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_punch_hole_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_punch_hole_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_punch_hole_ops

Number of NFS hole-punch requests per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_punch_hole_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_read_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.read.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.read.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_read_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_read_ops

Number of operations of the given type performed on this volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.read.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_setattr_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.setattr.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.setattr.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_setattr_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_setattr_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_setattr_ops

Number of operations of the given type performed on this volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.setattr.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_setattr_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_total_ops

Number of total NFS operations per second to the volume (Note: This is applicable only for ONTAP 9.9 and below. Harvest uses KeyPerf collector for ONTAP 9.10 onwards.)

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances volume` | `nfs_total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_write_latency

The raw data component latency in microseconds measured within ONTAP for all operations of the given type.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.write.total_time`<br><span class="key">Unit:</span> statistics.nfs_ops_raw.write.count<br><span class="key">Type:</span> average<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> nfs_write_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_nfs_write_ops

Number of operations of the given type performed on this volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.nfs_ops_raw.write.count`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `nfs_write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |



### volume_num_compress_attempts



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/efficiency/stat` | `num_compress_attempts` | conf/rest/9.14.0/volume.yaml |

The `volume_num_compress_attempts` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Sis Stat | timeseries | [Top $TopResources Volumes by Number of Compress Fail %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=108) |
| ONTAP: Volume | Sis Stat | timeseries | [Top $TopResources Volumes by Number of Compress Attempts](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=106) |
///



### volume_num_compress_fail



| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/efficiency/stat` | `num_compress_fail` | conf/rest/9.14.0/volume.yaml |

The `volume_num_compress_fail` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Sis Stat | timeseries | [Top $TopResources Volumes by Number of Compress Fail %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=108) |
| ONTAP: Volume | Sis Stat | timeseries | [Top $TopResources Volumes by Number of Compress Fail](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=107) |
///



### volume_other_data

Performance metric for other I/O operations in bytes per seconds. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.other`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |



### volume_other_latency

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.other`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.other | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `other_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> other_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_other_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexGroup | Volume WAFL Layer | timeseries | [Top $TopResources Volumes by Other Latency](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=116) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Other Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=116) |
///



### volume_other_ops

Performance metric for other I/O operations. Other I/O operations can be metadata operations, such as directory lookups and so on.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.other`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `other_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_other_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexGroup | Volume WAFL Layer | timeseries | [Top $TopResources Volumes by Other IOPs](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=115) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Other IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=115) |
| ONTAP: Volume by SVM | Highlights | table | [Volume Performance for $SVM (Click volume for detailed drill-down)](/d/cdot-volume-by-svm/ontap3a-volume by svm?orgId=1&viewPanel=2) |
| ONTAP: Volume Deep Dive | Highlights | table | [Volume Performance](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=6) |
| ONTAP: Volume Deep Dive | Highlights | timeseries | [Other IOPs](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=67) |
///



### volume_overwrite_reserve_available

amount of storage space that is currently available for overwrites, calculated by subtracting the total amount of overwrite reserve space from the amount that has already been used.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `overwrite_reserve_total, overwrite_reserve_used` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `overwrite_reserve_total, overwrite_reserve_used` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_overwrite_reserve_total

The size (in bytes) that is reserved for overwriting snapshotted data in an otherwise full volume. This space is usable only by space-reserved LUNs and files, and then only when the volume is full.This field is valid only when the volume is online.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `overwrite_reserve` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.overwrite-reserve` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_overwrite_reserve_used

The reserved size (in bytes) that is not available for new overwrites. The number includes both the reserved size which has actually been used for overwrites as well as the size which was never allocated in the first place. This field is valid only when the volume is online.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `overwrite_reserve_used` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.overwrite-reserve-used` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_performance_tier_footprint

This field represents the footprint of blocks written to the volume in bytes for the performance tier (bin 0).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/footprint` | `volume_blocks_footprint_bin0` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-footprint-get-iter` | `volume-blocks-footprint-bin0` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_performance_tier_footprint` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Capacity | timeseries | [Top $TopResources Volumes by Performance Tier Footprint](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=95) |
| ONTAP: FlexGroup | Top Volume FabricPool | timeseries | [Top $TopResources Volumes by Performance Tier Footprint](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=119) |
| ONTAP: Volume | FabricPool | table | [Volumes Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=185) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Performance Tier Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=119) |
///



### volume_performance_tier_footprint_percent

This field represents the footprint of blocks written to the volume in bin 0 as a percentage of aggregate size.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/footprint` | `volume_blocks_footprint_bin0_percent` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-footprint-get-iter` | `volume-blocks-footprint-bin0-percent` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_performance_tier_footprint_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Capacity | timeseries | [Top $TopResources Volumes by Performance Tier Footprint %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=97) |
| ONTAP: FlexGroup | Top Volume FabricPool | timeseries | [Top $TopResources Volumes by Performance Tier Footprint %](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=120) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Performance Tier Footprint %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=120) |
///



### volume_read_data

Performance metric for read I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.read`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `read_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by Average Throughput](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=815) |
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by Read Throughput](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=89) |
| ONTAP: cDOT | Volume Metrics | timeseries | [Top $TopResources Volumes by Average Throughput](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=260) |
| ONTAP: FlexGroup | Highlights | timeseries | [Top $TopResources Constituents by Average Throughput](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=126) |
| ONTAP: FlexGroup | Volume Table | table | [Top $TopResources Volumes by Read Throughput](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=41) |
| ONTAP: FlexGroup | Volume WAFL Layer | timeseries | [Top $TopResources Volumes by Read Throughput](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=31) |
| ONTAP: Node | Volume Performance | timeseries | [Top $TopResources Volumes by Average Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=135) |
| ONTAP: SVM | Volume Performance | timeseries | [Top $TopResources Volumes by Read Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=443) |
| ONTAP: Volume | Highlights | stat | [Top $TopResources Volumes Total Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=23) |
| ONTAP: Volume | Highlights | timeseries | [Top $TopResources Volumes by Average Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=11) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=41) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=31) |
| ONTAP: Volume by SVM | Highlights | table | [Volume Performance for $SVM (Click volume for detailed drill-down)](/d/cdot-volume-by-svm/ontap3a-volume by svm?orgId=1&viewPanel=2) |
| ONTAP: Volume Deep Dive | Highlights | table | [Volume Performance](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=6) |
| ONTAP: Volume Deep Dive | Highlights | stat | [Max Read Op Size](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=13) |
| ONTAP: Volume Deep Dive | Highlights | timeseries | [Read Throughput](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=11) |
///



### volume_read_latency

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.read`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.read | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `read_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> read_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_read_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by Read Latency](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=88) |
| ONTAP: FlexGroup | Volume Table | table | [Top $TopResources Volumes by Read Latency](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=39) |
| ONTAP: FlexGroup | Volume WAFL Layer | timeseries | [Top $TopResources Volumes by Read Latency](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=30) |
| ONTAP: SVM | Volume Performance | timeseries | [Top $TopResources Volumes by Read Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=442) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Read Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=39) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Read Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=30) |
| ONTAP: Volume Deep Dive | Highlights | timeseries | [Read Latency](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=19) |
///



### volume_read_ops

Performance metric for read I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.read`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `read_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by Read IOPs](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=90) |
| ONTAP: FlexGroup | Volume Table | table | [Top $TopResources Volumes by Read IOPS](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=43) |
| ONTAP: FlexGroup | Volume WAFL Layer | timeseries | [Top $TopResources Volumes by Read IOPs](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=32) |
| ONTAP: SVM | Volume Performance | timeseries | [Top $TopResources Volumes by Read IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=444) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Read IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=43) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Read IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=32) |
| ONTAP: Volume by SVM | Highlights | table | [Volume Performance for $SVM (Click volume for detailed drill-down)](/d/cdot-volume-by-svm/ontap3a-volume by svm?orgId=1&viewPanel=2) |
| ONTAP: Volume Deep Dive | Highlights | table | [Volume Performance](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=6) |
| ONTAP: Volume Deep Dive | Highlights | stat | [Max Read Op Size](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=13) |
| ONTAP: Volume Deep Dive | Highlights | timeseries | [Read IOPs](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=10) |
///



### volume_sis_compress_saved

The total disk space (in bytes) that is saved by compressing blocks on the referenced file system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `compression_space_saved` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-sis-attributes.compression-space-saved` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_sis_compress_saved` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexGroup | Volume Table | table | [FlexGroup Constituents in Cluster](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=18) |
| ONTAP: Health | Volume | table | [Volumes with Ransomware Issues (9.10+ Only)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=264) |
| ONTAP: Health | Volume | table | [Volumes Move Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=271) |
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes by Compression Savings](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=239) |
| ONTAP: Volume | Volume Table | table | [Volumes in Cluster](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=18) |
| ONTAP: Volume Deep Dive | Volume Capacity: $Volume | table | [Volumes in Cluster](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=22) |
///



### volume_sis_compress_saved_percent

Percentage of the total disk space that is saved by compressing blocks on the referenced file system

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `compression_space_saved_percent` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-sis-attributes.percentage-compression-space-saved` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_sis_compress_saved_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Top Volume and LUN Capacity | timeseries | [Top $TopResources Volumes by Compression Percent Saved](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=81) |
| ONTAP: SVM | Volume Capacity % | timeseries | [Top $TopResources Volumes by Compression Saved %](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=237) |
///



### volume_sis_dedup_saved

The total disk space (in bytes) that is saved by deduplication and file cloning.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `dedupe_space_saved` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-sis-attributes.deduplication-space-saved` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_sis_dedup_saved` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexGroup | Volume Table | table | [FlexGroup Constituents in Cluster](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=18) |
| ONTAP: Health | Volume | table | [Volumes with Ransomware Issues (9.10+ Only)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=264) |
| ONTAP: Health | Volume | table | [Volumes Move Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=271) |
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes by Deduplication Savings](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=238) |
| ONTAP: Volume | Volume Table | table | [Volumes in Cluster](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=18) |
| ONTAP: Volume Deep Dive | Volume Capacity: $Volume | table | [Volumes in Cluster](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=22) |
///



### volume_sis_dedup_saved_percent

Percentage of the total disk space that is saved by deduplication and file cloning.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `dedupe_space_saved_percent` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-sis-attributes.percentage-deduplication-space-saved` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_sis_dedup_saved_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: LUN | Top Volume and LUN Capacity | timeseries | [Top $TopResources Volumes by Deduplication Percent Saved](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=80) |
| ONTAP: SVM | Volume Capacity % | timeseries | [Top $TopResources Volumes by Deduplication Saved %](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=236) |
///



### volume_sis_total_saved

Total space saved (in bytes) in the volume due to deduplication, compression, and file cloning.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `sis_space_saved` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-sis-attributes.total-space-saved` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_sis_total_saved` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes by Total Efficiency Savings](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=240) |
///



### volume_sis_total_saved_percent

Percentage of total disk space that is saved by compressing blocks, deduplication and file cloning.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `sis_space_saved_percent` | conf/rest/9.12.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-sis-attributes.percentage-total-space-saved` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_size

Physical size of the volume, in bytes. The minimum size for a FlexVol volume is 20MB and the minimum size for a FlexGroup volume is 200MB per constituent. The recommended size for a FlexGroup volume is a minimum of 100GB per constituent. For all volumes, the default size is equal to the minimum size.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `size` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.size` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Access ($Activity) History By Percent](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=92) |
| ONTAP: File System Analytics (FSA) | Volume Activity | barchart | [Volume Modify ($Activity) History By Percent](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=93) |
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes Per Volume Total Size](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=313) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Volume Total Size](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=86) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Space Used](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=28) |
///



### volume_size_available

The size (in bytes) that is still available in the volume. This field is valid only when the volume is online.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `available` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.size-available` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_size_available` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Available](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=96) |
///



### volume_size_total

Total usable size (in bytes) of the volume, not including WAFL reserve or volume snapshot reserve.  If the volume is restricted or offline, a value of 0 is returned.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `total` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.size-total` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_size_total` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: cDOT | Capacity Metrics | table | [Top $TopResources SVMs by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=233) |
| ONTAP: cDOT | Capacity Metrics | table | [Top $TopResources Volumes by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=242) |
| ONTAP: cDOT | Capacity Metrics | timeseries | [Top $TopResources SVMs by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=272) |
| ONTAP: cDOT | Capacity Metrics | timeseries | [Top $TopResources Volumes by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=266) |
| ONTAP: FlexGroup | Volume Table | table | [FlexGroup Constituents in Cluster](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=18) |
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Size](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=97) |
| ONTAP: File System Analytics (FSA) | Highlights | bargauge | [Used Percentage](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=103) |
| ONTAP: Health | Volume | table | [Volumes with Ransomware Issues (9.10+ Only)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=264) |
| ONTAP: Health | Volume | table | [Volumes Move Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=271) |
| ONTAP: Volume | Volume Table | table | [Volumes in Cluster](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=18) |
| ONTAP: Volume | Volume Autosize Table | table | [Volumes Autogrow](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=186) |
| ONTAP: Volume Deep Dive | Volume Capacity: $Volume | table | [Volumes in Cluster](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=22) |
///



### volume_size_used

Number of bytes used in the volume.  If the volume is restricted or offline, a value of 0 is returned.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `used` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.size-used` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_size_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Capacity | timeseries | [Top $TopResources Volumes by Space Used by Aggregate](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=83) |
| ONTAP: cDOT | Capacity Metrics | table | [Top $TopResources SVMs by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=233) |
| ONTAP: cDOT | Capacity Metrics | table | [Top $TopResources Volumes by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=242) |
| ONTAP: cDOT | Capacity Metrics | timeseries | [Top $TopResources SVMs by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=272) |
| ONTAP: cDOT | Capacity Metrics | timeseries | [Top $TopResources Volumes by Capacity Used %](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=266) |
| ONTAP: File System Analytics (FSA) | Highlights | stat | [Used](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=95) |
| ONTAP: File System Analytics (FSA) | Highlights | bargauge | [Used Percentage](/d/cdot-fsa/ontap3a-file system analytics (fsa)?orgId=1&viewPanel=103) |
| ONTAP: SVM | Capacity | timeseries | [Top $TopResources SVMs by Volume Space Usage](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=631) |
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes Per Volume Size Used](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=312) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Volume Size Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=85) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Space Used](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=28) |
///



### volume_size_used_percent

percentage of utilized storage space in a volume relative to its total capacity

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `percent_used` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.percentage-size-used` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_size_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Capacity | timeseries | [Top $TopResources Volumes by Space Used %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=84) |
| ONTAP: FlexGroup | Volume Table | table | [FlexGroup Constituents in Cluster](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=18) |
| ONTAP: Health | Volume | table | [Volumes with Ransomware Issues (9.10+ Only)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=264) |
| ONTAP: Health | Volume | table | [Volumes Move Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=271) |
| ONTAP: LUN | Top Volume and LUN Capacity | timeseries | [Top $TopResources Volumes by Used %](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=77) |
| ONTAP: SVM | Volume Capacity % | timeseries | [Top $TopResources Volumes Per Volume Size Used](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=320) |
| ONTAP: Volume | Volume Table | table | [Volumes in Cluster](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=18) |
| ONTAP: Volume | Volume Autosize Table | table | [Volumes Autogrow](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=186) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Volume Size Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=125) |
| ONTAP: Volume | Forecast Volume Capacity | table | [Top $TopResources Volumes Per Size Used Percentage Trend](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=149) |
| ONTAP: Volume Deep Dive | Volume Capacity: $Volume | table | [Volumes in Cluster](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=22) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Space Used Percent](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=30) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Snapshot Space Used Percent](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=34) |
///



### volume_snaplock_labels

This metric provides information about VolumeSnaplock

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/snaplock` | `Harvest generated` | conf/rest/9.12.0/volume_snaplock.yaml |

The `volume_snaplock_labels` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Snaplock | table | [Volume Snaplock](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=188) |
///



### volume_snapshot_count

Number of snapshots in the volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `snapshot_count` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-snapshot-attributes.snapshot-count` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_snapshot_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Snapshot Copies | stat | [<10 Copies ](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=96) |
| ONTAP: Data Protection | Snapshot Copies | stat | [10-100 Copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=97) |
| ONTAP: Data Protection | Snapshot Copies | stat | [101-500 Copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=98) |
| ONTAP: Data Protection | Snapshot Copies | stat | [>500 Copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=99) |
| ONTAP: Data Protection | Snapshot Copies | table | [Volume count by the number of Snapshot copies](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=94) |
| ONTAP: Datacenter | Snapshots | piechart | [Snapshot Copies](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=632) |
///



### volume_snapshot_reserve_available

The size (in bytes) that is available for Snapshot copies inside the Snapshot reserve. This value is zero if Snapshot spill is present. For 'none' guaranteed volumes, this may get reduced due to less available space in the aggregate. This parameter is not supported on Infinite Volumes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `snapshot_reserve_available` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.snapshot-reserve-available` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_snapshot_reserve_available` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Reserve Available](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=316) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Reserve Available](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=129) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Snapshot Space Used](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=32) |
///



### volume_snapshot_reserve_percent

The percentage of volume disk space that has been set aside as reserve for snapshot usage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `percent_snapshot_space` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.percentage-snapshot-reserve` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_snapshot_reserve_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Volume Capacity % | timeseries | [Top $TopResources Volumes Per Snapshot Reserve](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=321) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Snapshot Reserve](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=126) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Snapshot Space Used Percent](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=34) |
///



### volume_snapshot_reserve_size

The size (in bytes) in the volume that has been set aside as reserve for snapshot usage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `snapshot_reserve_size` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.snapshot-reserve-size` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_snapshot_reserve_size` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Data Protection | Snapshot Copies | stat | [Volumes breached](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=77) |
| ONTAP: Data Protection | Snapshot Copies | stat | [Volumes not breached](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=81) |
| ONTAP: Data Protection | Snapshot Copies | table | [Volumes Breaching Snapshot Copy Reserve Space](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=91) |
| ONTAP: Datacenter | Snapshots | piechart | [Breached Status](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=631) |
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Reserve Size](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=315) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Reserve Size](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=128) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Snapshot Space Used](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=32) |
///



### volume_snapshot_reserve_used

amount of storage space currently used by a volume's snapshot reserve, which is calculated by subtracting the snapshot reserve available space from the snapshot reserve size.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `snapshot_reserve_size, snapshot_reserve_available` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `snapshot_reserve_size, snapshot_reserve_available` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_snapshot_reserve_used_percent

Percentage of the volume reserved for snapshots that has been used. Note that in some scenarios, it is possible to pass 100% of the space allocated.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `snapshot_space_used` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.percentage-snapshot-reserve-used` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_snapshot_reserve_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Capacity | timeseries | [Top $TopResources Volumes by Snapshot Space Used %](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=85) |
| ONTAP: LUN | Top Volume and LUN Capacity | timeseries | [Top $TopResources Volumes by Snapshot Used %](/d/cdot-lun/ontap3a-lun?orgId=1&viewPanel=78) |
| ONTAP: SVM | Volume Capacity % | timeseries | [Top $TopResources Volumes Per Snapshot Reserve Used](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=322) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Snapshot Reserve Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=127) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Snapshot Space Used Percent](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=34) |
///



### volume_snapshots_size_available

Total free space (in bytes) available in the volume and the snapshot reserve. If this value is 0 or negative, a new snapshot cannot be created.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `size_available_for_snapshots` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.size-available-for-snapshots` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_snapshots_size_available` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Size Available](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=314) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Size Available](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=87) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Snapshot Space Used](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=32) |
///



### volume_snapshots_size_used

The size (in bytes) that is used by snapshots in the volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `size_used_by_snapshots` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.size-used-by-snapshots` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_snapshots_size_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Capacity | timeseries | [Top $TopResources Volumes by Snapshot Space Used](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=87) |
| ONTAP: Data Protection | Snapshot Copies | stat | [Volumes breached](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=77) |
| ONTAP: Data Protection | Snapshot Copies | stat | [Volumes not breached](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=81) |
| ONTAP: Data Protection | Snapshot Copies | table | [Volumes Breaching Snapshot Copy Reserve Space](/d/cdot-data-protection/ontap3a-data protection?orgId=1&viewPanel=91) |
| ONTAP: Datacenter | Snapshots | piechart | [Breached Status](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=631) |
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Size Used](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=317) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Snapshot Size Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=130) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Snapshot Space Used](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=32) |
///



### volume_space_expected_available

The size (in bytes) that should be available for the volume irrespective of available size in the aggregate.This is same as size-available for 'volume' guaranteed volumes.For 'none' guaranteed volumes this value is calculated as if the aggregate has enough backing disk space to fully support the volume's size.Similar to the size-available property, this does not include Snapshot reserve.This count gets reduced if snapshots consume space above Snapshot reserve threshold.This parameter is not supported on Infinite Volumes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `expected_available` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.expected-available` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_space_logical_available

The size (in bytes) that is logically available in the volume.This is the amount of free space available considering space saved by the storage efficiency features as being used.This does not include Snapshot reserve.This parameter is not supported on FlexGroups or Infinite Volumes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `logical_available` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.logical-available` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_space_logical_used

The size (in bytes) that is logically used in the volume.This value includes all the space saved by the storage efficiency features along with the physically used space.This does not include Snapshot reserve but does consider Snapshot spill.This parameter is not supported on FlexGroups or Infinite Volumes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `logical_used` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.logical-used` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_space_logical_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexGroup | Volume Table | table | [FlexGroup Constituents in Cluster](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=18) |
| ONTAP: Health | Volume | table | [Volumes with Ransomware Issues (9.10+ Only)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=264) |
| ONTAP: Health | Volume | table | [Volumes Move Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=271) |
| ONTAP: SVM | Capacity | timeseries | [Top $TopResources SVMs by Logical Space Usage Across Volumes](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=630) |
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes Per Logical Space Used](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=311) |
| ONTAP: Volume | Volume Table | table | [Volumes in Cluster](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=18) |
| ONTAP: Volume | I/O Density | timeseries | [Top $TopResources Volumes by IO Density (IOPs/TiB)](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=139) |
| ONTAP: Volume | I/O Density | timeseries | [Bottom $TopResources Volumes by IO Density (IOPs/TiB)](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=140) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Logical Space Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=84) |
| ONTAP: Volume | Growth Rate | timeseries | [Top $TopResources Volumes Per Growth Rate of Logical Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=138) |
| ONTAP: Volume | Growth Rate | table | [Top $TopResources Volumes by Logical Usage: Delta](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=178) |
| ONTAP: Volume Deep Dive | Volume Capacity: $Volume | table | [Volumes in Cluster](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=22) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Space Used](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=28) |
///



### volume_space_logical_used_by_afs

The size (in bytes) that is logically used by the active filesystem of the volume.This value differs from 'logical-used' by the amount of Snapshot spill that exceeds Snapshot reserve.This parameter is not supported on FlexGroups or Infinite Volumes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `logical_used_by_afs` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.logical-used-by-afs` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_space_logical_used_by_snapshots

The size (in bytes) that is logically used across all Snapshot copies in the volume. This value differs from 'size-used-by-snapshots' by the space saved by the storage efficiency features across the Snapshot copies.This parameter is not supported on FlexGroups or Infinite Volumes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `logical_used_by_snapshots` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.logical-used-by-snapshots` | conf/zapi/cdot/9.8.0/volume.yaml |



### volume_space_logical_used_percent

Percentage of the logical used size of the volume.This parameter is not supported on FlexGroups or Infinite Volumes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `logical_used_percent` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.logical-used-percent` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_space_logical_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Volume Capacity % | timeseries | [Top $TopResources Volumes Per Logical Space Used](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=319) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Logical Space Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=124) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Space Used Percent](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=30) |
///



### volume_space_performance_tier_inactive_user_data

The size that is physically used in the performance tier of the volume and has a cold temperature. This parameter is only supported if the volume is in an aggregate that is either attached to object store or could be attached to an object store.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `performance_tier_inactive_user_data` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.performance-tier-inactive-user-data` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_space_performance_tier_inactive_user_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes by Inactive Data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=150) |
///



### volume_space_performance_tier_inactive_user_data_percent

The size (in percent) that is physically used in the performance tier of the volume and has a cold temperature. This parameter is only supported if the volume is in an aggregate that is either attached to object store or could be attached to an object store.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `performance_tier_inactive_user_data_percent` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.performance-tier-inactive-user-data-percent` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_space_performance_tier_inactive_user_data_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes by Inactive Data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=151) |
///



### volume_space_physical_used

The size (in bytes) that is physically used in the volume.This differs from 'total-used' space by the space that is reserved for future writes.The value includes blocks in use by Snapshot copies.This field is valid only if the volume is online.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `virtual_used` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.physical-used` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_space_physical_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: FlexGroup | Volume Table | table | [FlexGroup Constituents in Cluster](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=18) |
| ONTAP: Health | Volume | table | [Volumes with Ransomware Issues (9.10+ Only)](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=264) |
| ONTAP: Health | Volume | table | [Volumes Move Issues](/d/cdot-health/ontap3a-health?orgId=1&viewPanel=271) |
| ONTAP: SVM | Volume Capacity | timeseries | [Top $TopResources Volumes Per Physical Space Used](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=310) |
| ONTAP: Volume | Volume Table | table | [Volumes in Cluster](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=18) |
| ONTAP: Volume | Capacity | timeseries | [Top $TopResources Volumes Per Physical Space Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=83) |
| ONTAP: Volume | Growth Rate | timeseries | [Top $TopResources Volumes Per Growth Rate of Physical Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=137) |
| ONTAP: Volume | Growth Rate | table | [Top $TopResources Volumes by Physical Usage: Delta](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=177) |
| ONTAP: Volume Deep Dive | Volume Capacity: $Volume | table | [Volumes in Cluster](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=22) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Space Used](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=28) |
///



### volume_space_physical_used_percent

The size (in percent) that is physically used in the volume.The percentage is based on volume size including the space reserved for Snapshot copies.This field is valid only if the volume is online.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume` | `virtual_used_percent` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-get-iter` | `volume-attributes.volume-space-attributes.physical-used-percent` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_space_physical_used_percent` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: SVM | Volume Capacity % | timeseries | [Top $TopResources Volumes Per Physical Space Used](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=318) |
| ONTAP: Volume | Capacity % | timeseries | [Top $TopResources Volumes Per Physical Space Used](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=123) |
| ONTAP: Volume Deep Dive | Per Volume Statistics | timeseries | [Per Volume Space Used Percent](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=30) |
///



### volume_tags

Displays tags at the volume level.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `NA` | `Harvest generated` | conf/rest/9.12.0/volume.yaml |



### volume_top_clients_read_data

This metric measures the amount of data read by the top clients to a specific volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/storage/volumes/*/top-metrics/clients` | `throughput.read`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/volume.yaml |
| KeyPerf | `api/storage/volumes/*/top-metrics/clients` | `throughput.read`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |

The `volume_top_clients_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Clients | timeseries | [Top $TopResources Volumes Clients by Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=161) |
///



### volume_top_clients_read_ops

This metric tracks the number of read operations performed by the top clients on a specific volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/storage/volumes/*/top-metrics/clients` | `iops.read`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/volume.yaml |
| KeyPerf | `api/storage/volumes/*/top-metrics/clients` | `iops.read`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |

The `volume_top_clients_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Clients | timeseries | [Top $TopResources Volumes Clients by Read IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=157) |
///



### volume_top_clients_write_data

This metric measures the amount of data written by the top clients to a specific volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/storage/volumes/*/top-metrics/clients` | `throughput.write`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/volume.yaml |
| KeyPerf | `api/storage/volumes/*/top-metrics/files` | `throughput.write`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |

The `volume_top_clients_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Clients | timeseries | [Top $TopResources Volumes Clients by Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=163) |
///



### volume_top_clients_write_ops

This metric tracks the number of write operations performed by the top clients on a specific volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/storage/volumes/*/top-metrics/clients` | `iops.write`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/volume.yaml |
| KeyPerf | `api/storage/volumes/*/top-metrics/clients` | `iops.write`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |

The `volume_top_clients_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Clients | timeseries | [Top $TopResources Volumes Clients by Write IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=159) |
///



### volume_top_files_read_data

This metric measures the amount of data read from the files of a specific volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/storage/volumes/*/top-metrics/files` | `throughput.read`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/volume.yaml |
| KeyPerf | `api/storage/volumes/*/top-metrics/files` | `throughput.read`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |

The `volume_top_files_read_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Files | timeseries | [Top $TopResources Volumes Files by Read Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=173) |
///



### volume_top_files_read_ops

This metric tracks the number of read operations performed on the files of a specific volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/storage/volumes/*/top-metrics/files` | `iops.read`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/volume.yaml |
| KeyPerf | `api/storage/volumes/*/top-metrics/files` | `iops.read`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |

The `volume_top_files_read_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Files | timeseries | [Top $TopResources Volumes Files by Read IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=169) |
///



### volume_top_files_write_data

This metric measures the amount of data written to the top files of a specific volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/storage/volumes/*/top-metrics/files` | `throughput.write`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/volume.yaml |
| KeyPerf | `api/storage/volumes/*/top-metrics/files` | `throughput.write`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |

The `volume_top_files_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Files | timeseries | [Top $TopResources Volumes Files by Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=175) |
///



### volume_top_files_write_ops

This metric tracks the number of write operations performed on the files of a specific volume.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/storage/volumes/*/top-metrics/files` | `iops.write`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/restperf/9.12.0/volume.yaml |
| KeyPerf | `api/storage/volumes/*/top-metrics/files` | `iops.write`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |

The `volume_top_files_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Files | timeseries | [Top $TopResources Volumes Files by Write IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=171) |
///



### volume_total_data

This metric represents the total amount of data that has been read from and written to a specific volume in bytes per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.total`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `volume` | `read_data, write_data`<br><span class="key">Unit:</span> <br><span class="key">Type:</span> <br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_total_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Cluster | Throughput | timeseries | [Data](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=107) |
| ONTAP: Datacenter | Performance | timeseries | [Top $TopResources Throughput by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=107) |
| ONTAP: SnapMirror Destinations | Highlights | timeseries | [Top $TopResources Destination Volumes by Average Throughput](/d/cdot-snapmirror-destinations/ontap3a-snapmirror destinations?orgId=1&viewPanel=147) |
///



### volume_total_footprint

This field represents the total footprint in bytes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/footprint` | `total_footprint` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-footprint-get-iter` | `total-footprint` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_total_footprint` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | Volume Hot-Cold Data | table | [Volumes by Cold data](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=45) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Cold Data %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=49) |
| ONTAP: Volume | Volume Hot-Cold Data | timeseries | [Top $TopResources Volumes by Hot Data %](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=50) |
| ONTAP: Volume | FabricPool | table | [Volumes Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=185) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Total Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=180) |
///



### volume_total_metadata_footprint

This field represents the total metadata footprint in bytes.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/private/cli/volume/footprint` | `total_metadata_footprint` | conf/rest/9.14.0/volume.yaml |
| ZAPI | `volume-footprint-get-iter` | `volume_total_metadata_footprint` | conf/zapi/cdot/9.8.0/volume.yaml |

The `volume_total_metadata_footprint` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Volume | FabricPool | table | [Volumes Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=185) |
| ONTAP: Volume | FabricPool | timeseries | [Top $TopResources Volumes by Total Metadata Footprint](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=183) |
///



### volume_total_ops

Performance metric aggregated over all types of I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `total_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_total_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by IOPs](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=817) |
| ONTAP: cDOT | Cluster Metrics | timeseries | [Top $TopResources Total IOPs by Cluster](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=240) |
| ONTAP: cDOT | Volume Metrics | timeseries | [Top $TopResources Volumes by IOPs](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=262) |
| ONTAP: Cluster | Throughput | timeseries | [IOPs](/d/cdot-cluster/ontap3a-cluster?orgId=1&viewPanel=108) |
| ONTAP: Datacenter | Performance | timeseries | [Top $TopResources  IOPs by Cluster](/d/cdot-datacenter/ontap3a-datacenter?orgId=1&viewPanel=108) |
| ONTAP: FlexGroup | Highlights | timeseries | [Top $TopResources Constituents by Total IOPs](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=12) |
| ONTAP: Node | Volume Performance | timeseries | [Top $TopResources Volumes by IOPs](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=137) |
| ONTAP: Volume | Highlights | stat | [Top $TopResources Volumes by Total IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=5) |
| ONTAP: Volume | Highlights | timeseries | [Top $TopResources Volumes by Total IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=12) |
| ONTAP: Volume | I/O Density | timeseries | [Top $TopResources Volumes by IO Density (IOPs/TiB)](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=139) |
| ONTAP: Volume | I/O Density | timeseries | [Bottom $TopResources Volumes by IO Density (IOPs/TiB)](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=140) |
| ONTAP: Volume by SVM | Highlights | table | [Volume Performance for $SVM (Click volume for detailed drill-down)](/d/cdot-volume-by-svm/ontap3a-volume by svm?orgId=1&viewPanel=2) |
| ONTAP: Volume Deep Dive | Highlights | table | [Volume Performance](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=6) |
///



### volume_write_data

Performance metric for write I/O operations in bytes per seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.throughput_raw.write`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `write_data`<br><span class="key">Unit:</span> b_per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_write_data` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by Average Throughput](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=815) |
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by Write Throughput](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=92) |
| ONTAP: cDOT | Volume Metrics | timeseries | [Top $TopResources Volumes by Average Throughput](/d/cdot-cdot/ontap3a-cdot?orgId=1&viewPanel=260) |
| ONTAP: FlexGroup | Highlights | timeseries | [Top $TopResources Constituents by Average Throughput](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=126) |
| ONTAP: FlexGroup | Volume Table | table | [Top $TopResources Volumes by Write Throughput](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=42) |
| ONTAP: FlexGroup | Volume WAFL Layer | timeseries | [Top $TopResources Volumes by Write Throughput](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=34) |
| ONTAP: Node | Volume Performance | timeseries | [Top $TopResources Volumes by Average Throughput](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=135) |
| ONTAP: SVM | Volume Performance | timeseries | [Top $TopResources Volumes by Write Throughput](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=390) |
| ONTAP: Volume | Highlights | stat | [Top $TopResources Volumes Total Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=23) |
| ONTAP: Volume | Highlights | timeseries | [Top $TopResources Volumes by Average Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=11) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=42) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Write Throughput](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=34) |
| ONTAP: Volume by SVM | Highlights | table | [Volume Performance for $SVM (Click volume for detailed drill-down)](/d/cdot-volume-by-svm/ontap3a-volume by svm?orgId=1&viewPanel=2) |
| ONTAP: Volume Deep Dive | Highlights | table | [Volume Performance](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=6) |
| ONTAP: Volume Deep Dive | Highlights | stat | [Max Write Op Size](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=14) |
| ONTAP: Volume Deep Dive | Highlights | timeseries | [Write Throughput](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=18) |
///



### volume_write_latency

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.latency_raw.write`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> volume_statistics.iops_raw.write | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `write_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> write_ops | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_write_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by Write Latency](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=91) |
| ONTAP: FlexGroup | Volume Table | table | [Top $TopResources Volumes by Write Latency](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=40) |
| ONTAP: FlexGroup | Volume WAFL Layer | timeseries | [Top $TopResources Volumes by Write Latency](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=33) |
| ONTAP: SVM | Volume Performance | timeseries | [Top $TopResources Volumes by Write Latency](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=339) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Write Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=40) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Write Latency](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=33) |
| ONTAP: Volume Deep Dive | Highlights | timeseries | [Write Latency](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=20) |
///



### volume_write_ops

Performance metric for write I/O operations.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| KeyPerf | `api/storage/volumes` | `statistics.iops_raw.write`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/keyperf/9.15.0/volume.yaml |
| ZapiPerf | `perf-object-get-instances volume` | `write_ops`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/volume.yaml |

The `volume_write_ops` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Aggregate | Volume Performance | timeseries | [Top $TopResources Volumes by Write IOPs](/d/cdot-aggregate/ontap3a-aggregate?orgId=1&viewPanel=93) |
| ONTAP: FlexGroup | Volume Table | table | [Top $TopResources Volumes by Write IOPS](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=44) |
| ONTAP: FlexGroup | Volume WAFL Layer | timeseries | [Top $TopResources Volumes by Write IOPs](/d/cdot-flexgroup/ontap3a-flexgroup?orgId=1&viewPanel=35) |
| ONTAP: SVM | Volume Performance | timeseries | [Top $TopResources Volumes by Write IOPs](/d/cdot-svm/ontap3a-svm?orgId=1&viewPanel=391) |
| ONTAP: Volume | Volume Table | table | [Top $TopResources Volumes by Write IOPS](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=44) |
| ONTAP: Volume | Performance | timeseries | [Top $TopResources Volumes by Write IOPs](/d/cdot-volume/ontap3a-volume?orgId=1&viewPanel=35) |
| ONTAP: Volume by SVM | Highlights | table | [Volume Performance for $SVM (Click volume for detailed drill-down)](/d/cdot-volume-by-svm/ontap3a-volume by svm?orgId=1&viewPanel=2) |
| ONTAP: Volume Deep Dive | Highlights | table | [Volume Performance](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=6) |
| ONTAP: Volume Deep Dive | Highlights | stat | [Max Write Op Size](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=14) |
| ONTAP: Volume Deep Dive | Highlights | timeseries | [Write IOPs](/d/cdot-volume-deep-dive/ontap3a-volume deep dive?orgId=1&viewPanel=17) |
///



### vscan_labels

This metric provides information about Vscan

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| REST | `api/protocols/vscan` | `Harvest generated` | conf/rest/9.12.0/vscan.yaml |



### vscan_scan_latency

Average scan latency in microseconds

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/vscan` | `scan.latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> scan.requests | conf/restperf/9.13.0/vscan.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan_server` | `scan_latency`<br><span class="key">Unit:</span> microsec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> scan_latency_base | conf/zapiperf/cdot/9.8.0/vscan.yaml |

The `vscan_scan_latency` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Vscan | Connection Status Counters | timeseries | [Top $TopResources Scanners by Scanner Latency](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=591) |
///



### vscan_scan_request_dispatched_rate

Total number of scan requests sent to the scanner per second

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/vscan` | `scan.request_dispatched_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.13.0/vscan.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan_server` | `scan_request_dispatched_rate`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/vscan.yaml |

The `vscan_scan_request_dispatched_rate` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Vscan | Connection Status Counters | timeseries | [Top $TopResources Scanners by Scanner Requests Throughput](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=75) |
///



### vscan_scanner_stats_pct_cpu_used

Percentage CPU utilization on scanner calculated over the last 15 seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/vscan` | `scanner.stats_percent_cpu_used`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.13.0/vscan.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan_server` | `scanner_stats_pct_cpu_used`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/vscan.yaml |

The `vscan_scanner_stats_pct_cpu_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Vscan | Scanner utilization | timeseries | [Scanner CPU Utilization](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=442) |
///



### vscan_scanner_stats_pct_mem_used

Percentage RAM utilization on scanner calculated over the last 15 seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/vscan` | `scanner.stats_percent_mem_used`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.13.0/vscan.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan_server` | `scanner_stats_pct_mem_used`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/vscan.yaml |

The `vscan_scanner_stats_pct_mem_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Vscan | Scanner utilization | timeseries | [Scanner Mem Utilization](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=586) |
///



### vscan_scanner_stats_pct_network_used

Percentage network utilization on scanner calculated for the last 15 seconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/vscan` | `scanner.stats_percent_network_used`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.13.0/vscan.yaml |
| ZapiPerf | `perf-object-get-instances offbox_vscan_server` | `scanner_stats_pct_network_used`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/vscan.yaml |

The `vscan_scanner_stats_pct_network_used` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Vscan | Scanner utilization | timeseries | [Scanner Network Utilization](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=587) |
///



### vscan_server_disconnected

Represent the disconnected vscan servers to the vscan pool

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| Rest | `NA` | `Harvest generated` | conf/rest/9.12.0/vscan.yaml |

The `vscan_server_disconnected` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Vscan | Vscan Server | table | [Disconnected Vscan Servers in Cluster](/d/cdot-vscan/ontap3a-vscan?orgId=1&viewPanel=599) |
///



### wafl_avg_msg_latency

Average turnaround time for WAFL messages in milliseconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `average_msg_latency`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> msg_total | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `avg_wafl_msg_latency`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> wafl_msg_total | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_avg_non_wafl_msg_latency

Average turnaround time for non-WAFL messages in milliseconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `average_non_wafl_msg_latency`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> non_wafl_msg_total | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `avg_non_wafl_msg_latency`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> non_wafl_msg_total | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_avg_repl_msg_latency

Average turnaround time for replication WAFL messages in milliseconds.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `average_replication_msg_latency`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> replication_msg_total | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `avg_wafl_repl_msg_latency`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> average<br><span class="key">Base:</span> wafl_repl_msg_total | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_cp_count

Array of counts of different types of Consistency Points (CP).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `cp_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `cp_count`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |

The `wafl_cp_count` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Disk | Disk Utilization | timeseries | [CP (Consistency Points) Counts](/d/cdot-disk/ontap3a-disk?orgId=1&viewPanel=63) |
///



### wafl_cp_phase_times

Array of percentage time spent in different phases of Consistency Point (CP).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `cp_phase_times`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> total_cp_msecs | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `cp_phase_times`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> total_cp_msecs | conf/zapiperf/cdot/9.8.0/wafl.yaml |

The `wafl_cp_phase_times` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Backend | timeseries | [System Utilization](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=41) |
///



### wafl_memory_free

The current WAFL memory available in the system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `memory_free`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_memory_free`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_memory_used

The current WAFL memory used in the system.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `memory_used`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_memory_used`<br><span class="key">Unit:</span> mb<br><span class="key">Type:</span> raw<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_msg_total

Total number of WAFL messages per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `msg_total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_msg_total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_non_wafl_msg_total

Total number of non-WAFL messages per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `non_wafl_msg_total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `non_wafl_msg_total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_read_io_type

Percentage of reads served from buffer cache, external cache, or disk.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `read_io_type`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_io_type_base | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `read_io_type`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> read_io_type_base | conf/zapiperf/cdot/9.8.0/wafl.yaml |

The `wafl_read_io_type` metric is visualized in the following Grafana dashboards:
    
/// html | div.grafana-table
| Dashboard | Row | Type | Panel |
|--------|----------|--------|--------|
| ONTAP: Node | Backend | timeseries | [Reads From](/d/cdot-node/ontap3a-node?orgId=1&viewPanel=43) |
///



### wafl_reads_from_cache

WAFL reads from cache.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `reads_from_cache`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_reads_from_cache`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_reads_from_cloud

WAFL reads from cloud storage.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `reads_from_cloud`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_reads_from_cloud`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_reads_from_cloud_s2c_bin

WAFL reads from cloud storage via s2c bin.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `reads_from_cloud_s2c_bin`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_reads_from_cloud_s2c_bin`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_reads_from_disk

WAFL reads from disk.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `reads_from_disk`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_reads_from_disk`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_reads_from_ext_cache

WAFL reads from external cache.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `reads_from_external_cache`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_reads_from_ext_cache`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_reads_from_fc_miss

WAFL reads from remote volume for fc_miss.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `reads_from_fc_miss`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_reads_from_fc_miss`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_reads_from_pmem

Wafl reads from persistent mmeory.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_reads_from_pmem`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_reads_from_ssd

WAFL reads from SSD.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `reads_from_ssd`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_reads_from_ssd`<br><span class="key">Unit:</span> none<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_repl_msg_total

Total number of replication WAFL messages per second.

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `replication_msg_total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `wafl_repl_msg_total`<br><span class="key">Unit:</span> per_sec<br><span class="key">Type:</span> rate<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_total_cp_msecs

Milliseconds spent in Consistency Point (CP).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `total_cp_msecs`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `total_cp_msecs`<br><span class="key">Unit:</span> millisec<br><span class="key">Type:</span> delta<br><span class="key">Base:</span>  | conf/zapiperf/cdot/9.8.0/wafl.yaml |



### wafl_total_cp_util

Percentage of time spent in a Consistency Point (CP).

| API    | Endpoint | Metric | Template |
|--------|----------|--------|---------|
| RestPerf | `api/cluster/counter/tables/wafl` | `total_cp_util`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> cpu_elapsed_time | conf/restperf/9.12.0/wafl.yaml |
| ZapiPerf | `perf-object-get-instances wafl` | `total_cp_util`<br><span class="key">Unit:</span> percent<br><span class="key">Type:</span> percent<br><span class="key">Base:</span> cpu_elapsed_time | conf/zapiperf/cdot/9.8.0/wafl.yaml |



